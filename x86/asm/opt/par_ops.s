	.file	"par_ops.c"
	.text
	.globl	initialize
	.type	initialize, @function
initialize:
.LFB329:
	.cfi_startproc
	endbr64
	testq	%rdi, %rdi
	jne	.L1
	movl	$1, %eax
	movsd	.LC0(%rip), %xmm0
.L3:
	movl	%eax, 8(%rsi,%rax,4)
	movsd	%xmm0, 72(%rsi,%rax,8)
	addq	$1, %rax
	cmpq	$17, %rax
	jne	.L3
.L1:
	ret
	.cfi_endproc
.LFE329:
	.size	initialize, .-initialize
	.globl	integer_bit_0
	.type	integer_bit_0, @function
integer_bit_0:
.LFB73:
	.cfi_startproc
	endbr64
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movq	%rdi, %rcx
	movl	12(%rsi), %edx
	leal	1(%rdx), %edi
	addl	$2, %edx
	leaq	-1(%rcx), %rax
	testq	%rcx, %rcx
	jne	.L7
	jmp	.L6
.L8:
	movl	%edi, %edx
.L7:
	orl	%edx, %edi
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L8
.L6:
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE73:
	.size	integer_bit_0, .-integer_bit_0
	.globl	integer_bit_1
	.type	integer_bit_1, @function
integer_bit_1:
.LFB74:
	.cfi_startproc
	endbr64
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movq	%rdi, %r8
	movl	12(%rsi), %ecx
	leal	1(%rcx), %edi
	addl	$2, %ecx
	movl	16(%rsi), %edx
	leal	1(%rdx), %ebx
	addl	$3, %edx
	leaq	-1(%r8), %rax
	testq	%r8, %r8
	jne	.L12
	jmp	.L11
.L13:
	movl	%ebx, %edx
	movl	%edi, %ecx
.L12:
	orl	%ecx, %edi
	orl	%edx, %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L13
.L11:
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE74:
	.size	integer_bit_1, .-integer_bit_1
	.globl	integer_bit_2
	.type	integer_bit_2, @function
integer_bit_2:
.LFB75:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	pushq	%rbx
	.cfi_def_cfa_offset 24
	.cfi_offset 3, -24
	subq	$8, %rsp
	.cfi_def_cfa_offset 32
	movq	%rdi, %r9
	movl	12(%rsi), %r8d
	leal	1(%r8), %edi
	addl	$2, %r8d
	movl	16(%rsi), %ecx
	leal	1(%rcx), %ebp
	addl	$3, %ecx
	movl	20(%rsi), %edx
	leal	1(%rdx), %ebx
	addl	$4, %edx
	leaq	-1(%r9), %rax
	testq	%r9, %r9
	jne	.L17
	jmp	.L16
.L18:
	movl	%ebx, %edx
	movl	%ebp, %ecx
	movl	%edi, %r8d
.L17:
	orl	%r8d, %edi
	orl	%ecx, %ebp
	orl	%edx, %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L18
.L16:
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 24
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE75:
	.size	integer_bit_2, .-integer_bit_2
	.globl	integer_bit_3
	.type	integer_bit_3, @function
integer_bit_3:
.LFB76:
	.cfi_startproc
	endbr64
	pushq	%r12
	.cfi_def_cfa_offset 16
	.cfi_offset 12, -16
	pushq	%rbp
	.cfi_def_cfa_offset 24
	.cfi_offset 6, -24
	pushq	%rbx
	.cfi_def_cfa_offset 32
	.cfi_offset 3, -32
	movq	%rdi, %r10
	movl	12(%rsi), %r9d
	leal	1(%r9), %edi
	addl	$2, %r9d
	movl	16(%rsi), %r8d
	leal	1(%r8), %r12d
	addl	$3, %r8d
	movl	20(%rsi), %ecx
	leal	1(%rcx), %ebp
	addl	$4, %ecx
	movl	24(%rsi), %edx
	leal	1(%rdx), %ebx
	addl	$5, %edx
	leaq	-1(%r10), %rax
	testq	%r10, %r10
	jne	.L22
	jmp	.L21
.L23:
	movl	%ebx, %edx
	movl	%ebp, %ecx
	movl	%r12d, %r8d
	movl	%edi, %r9d
.L22:
	orl	%r9d, %edi
	orl	%r8d, %r12d
	orl	%ecx, %ebp
	orl	%edx, %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L23
.L21:
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%rbp
	.cfi_def_cfa_offset 16
	popq	%r12
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE76:
	.size	integer_bit_3, .-integer_bit_3
	.globl	integer_bit_4
	.type	integer_bit_4, @function
integer_bit_4:
.LFB77:
	.cfi_startproc
	endbr64
	pushq	%r13
	.cfi_def_cfa_offset 16
	.cfi_offset 13, -16
	pushq	%r12
	.cfi_def_cfa_offset 24
	.cfi_offset 12, -24
	pushq	%rbp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset 3, -40
	subq	$8, %rsp
	.cfi_def_cfa_offset 48
	movq	%rdi, %r10
	movl	12(%rsi), %r11d
	leal	1(%r11), %edi
	addl	$2, %r11d
	movl	16(%rsi), %r9d
	leal	1(%r9), %r13d
	addl	$3, %r9d
	movl	20(%rsi), %r8d
	leal	1(%r8), %r12d
	addl	$4, %r8d
	movl	24(%rsi), %ecx
	leal	1(%rcx), %ebp
	addl	$5, %ecx
	movl	28(%rsi), %edx
	leal	1(%rdx), %ebx
	addl	$6, %edx
	leaq	-1(%r10), %rax
	testq	%r10, %r10
	jne	.L27
	jmp	.L26
.L28:
	movl	%ebx, %edx
	movl	%ebp, %ecx
	movl	%r12d, %r8d
	movl	%r13d, %r9d
	movl	%edi, %r11d
.L27:
	orl	%r11d, %edi
	orl	%r9d, %r13d
	orl	%r8d, %r12d
	orl	%ecx, %ebp
	orl	%edx, %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L28
.L26:
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%rbp
	.cfi_def_cfa_offset 24
	popq	%r12
	.cfi_def_cfa_offset 16
	popq	%r13
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE77:
	.size	integer_bit_4, .-integer_bit_4
	.globl	integer_bit_5
	.type	integer_bit_5, @function
integer_bit_5:
.LFB78:
	.cfi_startproc
	endbr64
	pushq	%r14
	.cfi_def_cfa_offset 16
	.cfi_offset 14, -16
	pushq	%r13
	.cfi_def_cfa_offset 24
	.cfi_offset 13, -24
	pushq	%r12
	.cfi_def_cfa_offset 32
	.cfi_offset 12, -32
	pushq	%rbp
	.cfi_def_cfa_offset 40
	.cfi_offset 6, -40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	.cfi_offset 3, -48
	movq	%rdi, %r10
	movq	%rsi, %rax
	movl	12(%rsi), %r11d
	leal	1(%r11), %edi
	addl	$2, %r11d
	movl	16(%rsi), %r9d
	leal	1(%r9), %r14d
	addl	$3, %r9d
	movl	20(%rsi), %r8d
	leal	1(%r8), %r13d
	addl	$4, %r8d
	movl	24(%rsi), %esi
	leal	1(%rsi), %r12d
	addl	$5, %esi
	movl	28(%rax), %ecx
	leal	1(%rcx), %ebp
	addl	$6, %ecx
	movl	32(%rax), %edx
	leal	1(%rdx), %ebx
	addl	$7, %edx
	leaq	-1(%r10), %rax
	testq	%r10, %r10
	jne	.L32
	jmp	.L31
.L33:
	movl	%ebx, %edx
	movl	%ebp, %ecx
	movl	%r12d, %esi
	movl	%r13d, %r8d
	movl	%r14d, %r9d
	movl	%edi, %r11d
.L32:
	orl	%r11d, %edi
	orl	%r9d, %r14d
	orl	%r8d, %r13d
	orl	%esi, %r12d
	orl	%ecx, %ebp
	orl	%edx, %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L33
.L31:
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%rbp
	.cfi_def_cfa_offset 32
	popq	%r12
	.cfi_def_cfa_offset 24
	popq	%r13
	.cfi_def_cfa_offset 16
	popq	%r14
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE78:
	.size	integer_bit_5, .-integer_bit_5
	.globl	integer_bit_6
	.type	integer_bit_6, @function
integer_bit_6:
.LFB79:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$24, %rsp
	.cfi_def_cfa_offset 80
	movq	%rdi, 8(%rsp)
	movq	%rsi, %rax
	movl	12(%rsi), %r11d
	leal	1(%r11), %edi
	addl	$2, %r11d
	movl	16(%rsi), %r10d
	leal	1(%r10), %r15d
	addl	$3, %r10d
	movl	20(%rsi), %r9d
	leal	1(%r9), %r14d
	addl	$4, %r9d
	movl	24(%rsi), %r8d
	leal	1(%r8), %r13d
	addl	$5, %r8d
	movl	28(%rsi), %esi
	leal	1(%rsi), %r12d
	addl	$6, %esi
	movl	32(%rax), %ecx
	leal	1(%rcx), %ebp
	addl	$7, %ecx
	movl	36(%rax), %edx
	leal	1(%rdx), %ebx
	addl	$8, %edx
	movq	8(%rsp), %rax
	subq	$1, %rax
	cmpq	$0, 8(%rsp)
	jne	.L37
	jmp	.L36
.L38:
	movl	%ebx, %edx
	movl	%ebp, %ecx
	movl	%r12d, %esi
	movl	%r13d, %r8d
	movl	%r14d, %r9d
	movl	%r15d, %r10d
	movl	%edi, %r11d
.L37:
	orl	%r11d, %edi
	orl	%r10d, %r15d
	orl	%r9d, %r14d
	orl	%r8d, %r13d
	orl	%esi, %r12d
	orl	%ecx, %ebp
	orl	%edx, %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L38
.L36:
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE79:
	.size	integer_bit_6, .-integer_bit_6
	.globl	integer_bit_7
	.type	integer_bit_7, @function
integer_bit_7:
.LFB80:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$24, %rsp
	.cfi_def_cfa_offset 80
	movq	%rdi, (%rsp)
	movq	%rsi, %rax
	movl	12(%rsi), %r11d
	leal	1(%r11), %edi
	addl	$2, %r11d
	movl	16(%rsi), %r9d
	leal	1(%r9), %r15d
	addl	$3, %r9d
	movl	20(%rsi), %r8d
	leal	1(%r8), %r14d
	addl	$4, %r8d
	movl	24(%rsi), %esi
	leal	1(%rsi), %r13d
	addl	$5, %esi
	movl	28(%rax), %edx
	leal	1(%rdx), %r12d
	leal	6(%rdx), %ebx
	movl	%ebx, 8(%rsp)
	movl	32(%rax), %edx
	leal	1(%rdx), %ebp
	leal	7(%rdx), %r10d
	movl	36(%rax), %ecx
	leal	1(%rcx), %ebx
	movl	%ebx, 12(%rsp)
	addl	$8, %ecx
	movl	40(%rax), %edx
	leal	1(%rdx), %ebx
	addl	$9, %edx
	movq	(%rsp), %rax
	subq	$1, %rax
	cmpq	$0, (%rsp)
	je	.L42
	movl	%r9d, (%rsp)
	movl	12(%rsp), %r9d
	jmp	.L43
.L44:
	movl	%ebx, %edx
	movl	%r9d, %ecx
	movl	%ebp, %r10d
	movl	%r12d, 8(%rsp)
	movl	%r13d, %esi
	movl	%r14d, %r8d
	movl	%r15d, (%rsp)
	movl	%edi, %r11d
.L43:
	orl	%r11d, %edi
	orl	(%rsp), %r15d
	orl	%r8d, %r14d
	orl	%esi, %r13d
	orl	8(%rsp), %r12d
	orl	%r10d, %ebp
	orl	%ecx, %r9d
	orl	%edx, %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L44
	movl	%r9d, 12(%rsp)
.L42:
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	12(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE80:
	.size	integer_bit_7, .-integer_bit_7
	.globl	integer_bit_8
	.type	integer_bit_8, @function
integer_bit_8:
.LFB81:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$40, %rsp
	.cfi_def_cfa_offset 96
	movq	%rdi, 8(%rsp)
	movq	%rsi, %rax
	movl	12(%rsi), %r11d
	leal	1(%r11), %edi
	addl	$2, %r11d
	movl	16(%rsi), %r9d
	leal	1(%r9), %ebx
	movl	%ebx, 24(%rsp)
	addl	$3, %r9d
	movl	20(%rsi), %r8d
	leal	1(%r8), %r15d
	addl	$4, %r8d
	movl	24(%rsi), %edx
	leal	1(%rdx), %r14d
	leal	5(%rdx), %ecx
	movl	%ecx, 4(%rsp)
	movl	28(%rsi), %edx
	leal	1(%rdx), %r13d
	leal	6(%rdx), %ebx
	movl	%ebx, 16(%rsp)
	movl	32(%rsi), %edx
	leal	1(%rdx), %r12d
	leal	7(%rdx), %r10d
	movl	36(%rsi), %esi
	leal	1(%rsi), %ecx
	movl	%ecx, 28(%rsp)
	addl	$8, %esi
	movl	40(%rax), %ecx
	leal	1(%rcx), %ebp
	addl	$9, %ecx
	movl	44(%rax), %edx
	leal	1(%rdx), %ebx
	addl	$10, %edx
	movq	8(%rsp), %rax
	subq	$1, %rax
	cmpq	$0, 8(%rsp)
	je	.L48
	movl	%r10d, 8(%rsp)
	movl	%esi, 20(%rsp)
	movl	24(%rsp), %esi
	movl	28(%rsp), %r10d
	jmp	.L49
.L50:
	movl	%ebx, %edx
	movl	%ebp, %ecx
	movl	%r10d, 20(%rsp)
	movl	%r12d, 8(%rsp)
	movl	%r13d, 16(%rsp)
	movl	%r14d, 4(%rsp)
	movl	%r15d, %r8d
	movl	%esi, %r9d
	movl	%edi, %r11d
.L49:
	orl	%r11d, %edi
	orl	%r9d, %esi
	orl	%r8d, %r15d
	orl	4(%rsp), %r14d
	orl	16(%rsp), %r13d
	orl	8(%rsp), %r12d
	orl	20(%rsp), %r10d
	orl	%ecx, %ebp
	orl	%edx, %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L50
	movl	%esi, 24(%rsp)
	movl	%r10d, 28(%rsp)
.L48:
	call	use_int@PLT
	movl	24(%rsp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	28(%rsp), %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE81:
	.size	integer_bit_8, .-integer_bit_8
	.globl	integer_bit_9
	.type	integer_bit_9, @function
integer_bit_9:
.LFB82:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$56, %rsp
	.cfi_def_cfa_offset 112
	movq	%rdi, 16(%rsp)
	movq	%rsi, %rax
	movl	12(%rsi), %r11d
	leal	1(%r11), %edi
	addl	$2, %r11d
	movl	16(%rsi), %r9d
	leal	1(%r9), %ebx
	movl	%ebx, 36(%rsp)
	addl	$3, %r9d
	movl	20(%rsi), %r8d
	leal	1(%r8), %ecx
	movl	%ecx, 40(%rsp)
	addl	$4, %r8d
	movl	24(%rsi), %esi
	leal	1(%rsi), %ebx
	movl	%ebx, 44(%rsp)
	addl	$5, %esi
	movl	28(%rax), %edx
	leal	1(%rdx), %r15d
	leal	6(%rdx), %ecx
	movl	%ecx, 8(%rsp)
	movl	32(%rax), %edx
	leal	1(%rdx), %r14d
	leal	7(%rdx), %ebx
	movl	%ebx, 12(%rsp)
	movl	36(%rax), %edx
	leal	1(%rdx), %r13d
	leal	8(%rdx), %ecx
	movl	%ecx, 24(%rsp)
	movl	40(%rax), %edx
	leal	1(%rdx), %r12d
	leal	9(%rdx), %r10d
	movl	44(%rax), %ecx
	leal	1(%rcx), %ebp
	addl	$10, %ecx
	movl	48(%rax), %edx
	leal	1(%rdx), %ebx
	addl	$11, %edx
	movq	16(%rsp), %rax
	subq	$1, %rax
	cmpq	$0, 16(%rsp)
	je	.L54
	movl	%r10d, 16(%rsp)
	movl	%ecx, 28(%rsp)
	movl	%edx, 32(%rsp)
	movl	36(%rsp), %edx
	movl	40(%rsp), %ecx
	movl	44(%rsp), %r10d
	jmp	.L55
.L56:
	movl	%ebx, 32(%rsp)
	movl	%ebp, 28(%rsp)
	movl	%r12d, 16(%rsp)
	movl	%r13d, 24(%rsp)
	movl	%r14d, 12(%rsp)
	movl	%r15d, 8(%rsp)
	movl	%r10d, %esi
	movl	%ecx, %r8d
	movl	%edx, %r9d
	movl	%edi, %r11d
.L55:
	orl	%r11d, %edi
	orl	%r9d, %edx
	orl	%r8d, %ecx
	orl	%esi, %r10d
	orl	8(%rsp), %r15d
	orl	12(%rsp), %r14d
	orl	24(%rsp), %r13d
	orl	16(%rsp), %r12d
	orl	28(%rsp), %ebp
	orl	32(%rsp), %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L56
	movl	%edx, 36(%rsp)
	movl	%ecx, 40(%rsp)
	movl	%r10d, 44(%rsp)
.L54:
	call	use_int@PLT
	movl	36(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	44(%rsp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE82:
	.size	integer_bit_9, .-integer_bit_9
	.globl	integer_bit_10
	.type	integer_bit_10, @function
integer_bit_10:
.LFB83:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$72, %rsp
	.cfi_def_cfa_offset 128
	movq	%rdi, 24(%rsp)
	movq	%rsi, %rax
	movl	12(%rsi), %r11d
	leal	1(%r11), %edi
	addl	$2, %r11d
	movl	16(%rsi), %r10d
	leal	1(%r10), %ebx
	movl	%ebx, 48(%rsp)
	addl	$3, %r10d
	movl	20(%rsi), %r9d
	leal	1(%r9), %ecx
	movl	%ecx, 52(%rsp)
	addl	$4, %r9d
	movl	24(%rsi), %r8d
	leal	1(%r8), %ebx
	movl	%ebx, 56(%rsp)
	addl	$5, %r8d
	movl	28(%rsi), %edx
	leal	1(%rdx), %ecx
	movl	%ecx, 60(%rsp)
	leal	6(%rdx), %ebx
	movl	%ebx, 12(%rsp)
	movl	32(%rsi), %edx
	leal	1(%rdx), %r15d
	leal	7(%rdx), %esi
	movl	%esi, 16(%rsp)
	movl	36(%rax), %edx
	leal	1(%rdx), %r14d
	leal	8(%rdx), %ecx
	movl	%ecx, 20(%rsp)
	movl	40(%rax), %edx
	leal	1(%rdx), %r13d
	leal	9(%rdx), %ebx
	movl	%ebx, 32(%rsp)
	movl	44(%rax), %esi
	leal	1(%rsi), %r12d
	addl	$10, %esi
	movl	48(%rax), %ecx
	leal	1(%rcx), %ebp
	addl	$11, %ecx
	movl	52(%rax), %edx
	leal	1(%rdx), %ebx
	addl	$12, %edx
	movq	24(%rsp), %rax
	subq	$1, %rax
	cmpq	$0, 24(%rsp)
	je	.L60
	movl	%r8d, 24(%rsp)
	movl	%esi, 36(%rsp)
	movl	%ecx, 40(%rsp)
	movl	%edx, 44(%rsp)
	movl	48(%rsp), %r8d
	movl	52(%rsp), %edx
	movl	56(%rsp), %ecx
	movl	60(%rsp), %esi
	jmp	.L61
.L62:
	movl	%ebx, 44(%rsp)
	movl	%ebp, 40(%rsp)
	movl	%r12d, 36(%rsp)
	movl	%r13d, 32(%rsp)
	movl	%r14d, 20(%rsp)
	movl	%r15d, 16(%rsp)
	movl	%esi, 12(%rsp)
	movl	%ecx, 24(%rsp)
	movl	%edx, %r9d
	movl	%r8d, %r10d
	movl	%edi, %r11d
.L61:
	orl	%r11d, %edi
	orl	%r10d, %r8d
	orl	%r9d, %edx
	orl	24(%rsp), %ecx
	orl	12(%rsp), %esi
	orl	16(%rsp), %r15d
	orl	20(%rsp), %r14d
	orl	32(%rsp), %r13d
	orl	36(%rsp), %r12d
	orl	40(%rsp), %ebp
	orl	44(%rsp), %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L62
	movl	%r8d, 48(%rsp)
	movl	%edx, 52(%rsp)
	movl	%ecx, 56(%rsp)
	movl	%esi, 60(%rsp)
.L60:
	call	use_int@PLT
	movl	48(%rsp), %edi
	call	use_int@PLT
	movl	52(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	movl	60(%rsp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE83:
	.size	integer_bit_10, .-integer_bit_10
	.globl	integer_bit_11
	.type	integer_bit_11, @function
integer_bit_11:
.LFB84:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$72, %rsp
	.cfi_def_cfa_offset 128
	movq	%rdi, 16(%rsp)
	movq	%rsi, %rax
	movl	12(%rsi), %r11d
	leal	1(%r11), %edi
	addl	$2, %r11d
	movl	16(%rsi), %r10d
	leal	1(%r10), %ebx
	movl	%ebx, 44(%rsp)
	addl	$3, %r10d
	movl	20(%rsi), %r9d
	leal	1(%r9), %ecx
	movl	%ecx, 48(%rsp)
	addl	$4, %r9d
	movl	24(%rsi), %r8d
	leal	1(%r8), %ebx
	movl	%ebx, 52(%rsp)
	addl	$5, %r8d
	movl	28(%rsi), %esi
	leal	1(%rsi), %ecx
	movl	%ecx, 56(%rsp)
	addl	$6, %esi
	movl	32(%rax), %edx
	leal	1(%rdx), %ebx
	movl	%ebx, 60(%rsp)
	leal	7(%rdx), %ecx
	movl	%ecx, (%rsp)
	movl	36(%rax), %edx
	leal	1(%rdx), %r15d
	leal	8(%rdx), %ebx
	movl	%ebx, 4(%rsp)
	movl	40(%rax), %edx
	leal	1(%rdx), %r14d
	leal	9(%rdx), %ecx
	movl	%ecx, 8(%rsp)
	movl	44(%rax), %edx
	leal	1(%rdx), %r13d
	leal	10(%rdx), %ebx
	movl	%ebx, 12(%rsp)
	movl	48(%rax), %edx
	leal	1(%rdx), %r12d
	leal	11(%rdx), %ecx
	movl	%ecx, 24(%rsp)
	movl	52(%rax), %ecx
	leal	1(%rcx), %ebp
	addl	$12, %ecx
	movl	56(%rax), %edx
	leal	1(%rdx), %ebx
	addl	$13, %edx
	movq	16(%rsp), %rax
	subq	$1, %rax
	cmpq	$0, 16(%rsp)
	je	.L66
	movl	%r9d, 16(%rsp)
	movl	%r8d, 28(%rsp)
	movl	%esi, 32(%rsp)
	movl	%ecx, 36(%rsp)
	movl	%edx, 40(%rsp)
	movl	44(%rsp), %esi
	movl	48(%rsp), %r8d
	movl	52(%rsp), %r9d
	movl	56(%rsp), %edx
	movl	60(%rsp), %ecx
	jmp	.L67
.L68:
	movl	%ebx, 40(%rsp)
	movl	%ebp, 36(%rsp)
	movl	%r12d, 24(%rsp)
	movl	%r13d, 12(%rsp)
	movl	%r14d, 8(%rsp)
	movl	%r15d, 4(%rsp)
	movl	%ecx, (%rsp)
	movl	%edx, 32(%rsp)
	movl	%r9d, 28(%rsp)
	movl	%r8d, 16(%rsp)
	movl	%esi, %r10d
	movl	%edi, %r11d
.L67:
	orl	%r11d, %edi
	orl	%r10d, %esi
	orl	16(%rsp), %r8d
	orl	28(%rsp), %r9d
	orl	32(%rsp), %edx
	orl	(%rsp), %ecx
	orl	4(%rsp), %r15d
	orl	8(%rsp), %r14d
	orl	12(%rsp), %r13d
	orl	24(%rsp), %r12d
	orl	36(%rsp), %ebp
	orl	40(%rsp), %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L68
	movl	%esi, 44(%rsp)
	movl	%r8d, 48(%rsp)
	movl	%r9d, 52(%rsp)
	movl	%edx, 56(%rsp)
	movl	%ecx, 60(%rsp)
.L66:
	call	use_int@PLT
	movl	44(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	call	use_int@PLT
	movl	52(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	movl	60(%rsp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE84:
	.size	integer_bit_11, .-integer_bit_11
	.globl	integer_bit_12
	.type	integer_bit_12, @function
integer_bit_12:
.LFB85:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$88, %rsp
	.cfi_def_cfa_offset 144
	movq	%rdi, 24(%rsp)
	movq	%rsi, %rax
	movl	12(%rsi), %r11d
	leal	1(%r11), %edi
	addl	$2, %r11d
	movl	16(%rsi), %r10d
	leal	1(%r10), %ebx
	movl	%ebx, 56(%rsp)
	addl	$3, %r10d
	movl	20(%rsi), %r9d
	leal	1(%r9), %ecx
	movl	%ecx, 60(%rsp)
	addl	$4, %r9d
	movl	24(%rsi), %r8d
	leal	1(%r8), %ebx
	movl	%ebx, 64(%rsp)
	addl	$5, %r8d
	movl	28(%rsi), %edx
	leal	1(%rdx), %ecx
	movl	%ecx, 68(%rsp)
	leal	6(%rdx), %ebx
	movl	%ebx, 4(%rsp)
	movl	32(%rsi), %edx
	leal	1(%rdx), %esi
	movl	%esi, 72(%rsp)
	leal	7(%rdx), %ecx
	movl	%ecx, 8(%rsp)
	movl	36(%rax), %edx
	leal	1(%rdx), %ebx
	movl	%ebx, 76(%rsp)
	leal	8(%rdx), %esi
	movl	%esi, 12(%rsp)
	movl	40(%rax), %edx
	leal	1(%rdx), %r15d
	leal	9(%rdx), %ecx
	movl	%ecx, 16(%rsp)
	movl	44(%rax), %edx
	leal	1(%rdx), %r14d
	leal	10(%rdx), %ebx
	movl	%ebx, 20(%rsp)
	movl	48(%rax), %edx
	leal	1(%rdx), %r13d
	leal	11(%rdx), %esi
	movl	%esi, 32(%rsp)
	movl	52(%rax), %esi
	leal	1(%rsi), %r12d
	addl	$12, %esi
	movl	56(%rax), %ecx
	leal	1(%rcx), %ebp
	addl	$13, %ecx
	movl	60(%rax), %edx
	leal	1(%rdx), %ebx
	addl	$14, %edx
	movq	24(%rsp), %rax
	subq	$1, %rax
	cmpq	$0, 24(%rsp)
	je	.L72
	movl	%r10d, 24(%rsp)
	movl	%r9d, 36(%rsp)
	movl	%r8d, 40(%rsp)
	movl	%esi, 44(%rsp)
	movl	%ecx, 48(%rsp)
	movl	%edx, 52(%rsp)
	movl	56(%rsp), %r8d
	movl	60(%rsp), %r9d
	movl	64(%rsp), %r10d
	movl	68(%rsp), %edx
	movl	72(%rsp), %ecx
	movl	76(%rsp), %esi
	jmp	.L73
.L74:
	movl	%ebx, 52(%rsp)
	movl	%ebp, 48(%rsp)
	movl	%r12d, 44(%rsp)
	movl	%r13d, 32(%rsp)
	movl	%r14d, 20(%rsp)
	movl	%r15d, 16(%rsp)
	movl	%esi, 12(%rsp)
	movl	%ecx, 8(%rsp)
	movl	%edx, 4(%rsp)
	movl	%r10d, 40(%rsp)
	movl	%r9d, 36(%rsp)
	movl	%r8d, 24(%rsp)
	movl	%edi, %r11d
.L73:
	orl	%r11d, %edi
	orl	24(%rsp), %r8d
	orl	36(%rsp), %r9d
	orl	40(%rsp), %r10d
	orl	4(%rsp), %edx
	orl	8(%rsp), %ecx
	orl	12(%rsp), %esi
	orl	16(%rsp), %r15d
	orl	20(%rsp), %r14d
	orl	32(%rsp), %r13d
	orl	44(%rsp), %r12d
	orl	48(%rsp), %ebp
	orl	52(%rsp), %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L74
	movl	%r8d, 56(%rsp)
	movl	%r9d, 60(%rsp)
	movl	%r10d, 64(%rsp)
	movl	%edx, 68(%rsp)
	movl	%ecx, 72(%rsp)
	movl	%esi, 76(%rsp)
.L72:
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	movl	60(%rsp), %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	call	use_int@PLT
	movl	68(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	call	use_int@PLT
	movl	76(%rsp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE85:
	.size	integer_bit_12, .-integer_bit_12
	.globl	integer_bit_13
	.type	integer_bit_13, @function
integer_bit_13:
.LFB86:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$104, %rsp
	.cfi_def_cfa_offset 160
	movq	%rdi, 32(%rsp)
	movq	%rsi, %rax
	movl	12(%rsi), %r11d
	leal	1(%r11), %edi
	addl	$2, %r11d
	movl	16(%rsi), %r10d
	leal	1(%r10), %ebx
	movl	%ebx, 68(%rsp)
	addl	$3, %r10d
	movl	20(%rsi), %r9d
	leal	1(%r9), %ecx
	movl	%ecx, 72(%rsp)
	addl	$4, %r9d
	movl	24(%rsi), %r8d
	leal	1(%r8), %ebx
	movl	%ebx, 76(%rsp)
	addl	$5, %r8d
	movl	28(%rsi), %edx
	leal	1(%rdx), %ecx
	movl	%ecx, 80(%rsp)
	leal	6(%rdx), %ebx
	movl	%ebx, 8(%rsp)
	movl	32(%rsi), %edx
	leal	1(%rdx), %esi
	movl	%esi, 84(%rsp)
	leal	7(%rdx), %ecx
	movl	%ecx, 12(%rsp)
	movl	36(%rax), %edx
	leal	1(%rdx), %ebx
	movl	%ebx, 88(%rsp)
	leal	8(%rdx), %esi
	movl	%esi, 16(%rsp)
	movl	40(%rax), %edx
	leal	1(%rdx), %ecx
	movl	%ecx, 92(%rsp)
	leal	9(%rdx), %ebx
	movl	%ebx, 20(%rsp)
	movl	44(%rax), %edx
	leal	1(%rdx), %r15d
	leal	10(%rdx), %esi
	movl	%esi, 24(%rsp)
	movl	48(%rax), %edx
	leal	1(%rdx), %r14d
	leal	11(%rdx), %ecx
	movl	%ecx, 28(%rsp)
	movl	52(%rax), %edx
	leal	1(%rdx), %r13d
	leal	12(%rdx), %ebx
	movl	%ebx, 40(%rsp)
	movl	56(%rax), %esi
	leal	1(%rsi), %r12d
	addl	$13, %esi
	movl	60(%rax), %ecx
	leal	1(%rcx), %ebp
	addl	$14, %ecx
	movl	64(%rax), %edx
	leal	1(%rdx), %ebx
	addl	$15, %edx
	movq	32(%rsp), %rax
	subq	$1, %rax
	cmpq	$0, 32(%rsp)
	je	.L78
	movl	%r11d, 32(%rsp)
	movl	%r10d, 44(%rsp)
	movl	%r9d, 48(%rsp)
	movl	%r8d, 52(%rsp)
	movl	%esi, 56(%rsp)
	movl	%ecx, 60(%rsp)
	movl	%edx, 64(%rsp)
	movl	68(%rsp), %r8d
	movl	72(%rsp), %r9d
	movl	76(%rsp), %r10d
	movl	80(%rsp), %r11d
	movl	84(%rsp), %edx
	movl	88(%rsp), %ecx
	movl	92(%rsp), %esi
	jmp	.L79
.L80:
	movl	%ebx, 64(%rsp)
	movl	%ebp, 60(%rsp)
	movl	%r12d, 56(%rsp)
	movl	%r13d, 40(%rsp)
	movl	%r14d, 28(%rsp)
	movl	%r15d, 24(%rsp)
	movl	%esi, 20(%rsp)
	movl	%ecx, 16(%rsp)
	movl	%edx, 12(%rsp)
	movl	%r11d, 8(%rsp)
	movl	%r10d, 52(%rsp)
	movl	%r9d, 48(%rsp)
	movl	%r8d, 44(%rsp)
	movl	%edi, 32(%rsp)
.L79:
	orl	32(%rsp), %edi
	orl	44(%rsp), %r8d
	orl	48(%rsp), %r9d
	orl	52(%rsp), %r10d
	orl	8(%rsp), %r11d
	orl	12(%rsp), %edx
	orl	16(%rsp), %ecx
	orl	20(%rsp), %esi
	orl	24(%rsp), %r15d
	orl	28(%rsp), %r14d
	orl	40(%rsp), %r13d
	orl	56(%rsp), %r12d
	orl	60(%rsp), %ebp
	orl	64(%rsp), %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L80
	movl	%r8d, 68(%rsp)
	movl	%r9d, 72(%rsp)
	movl	%r10d, 76(%rsp)
	movl	%r11d, 80(%rsp)
	movl	%edx, 84(%rsp)
	movl	%ecx, 88(%rsp)
	movl	%esi, 92(%rsp)
.L78:
	call	use_int@PLT
	movl	68(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	call	use_int@PLT
	movl	76(%rsp), %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	call	use_int@PLT
	movl	84(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	call	use_int@PLT
	movl	92(%rsp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE86:
	.size	integer_bit_13, .-integer_bit_13
	.globl	integer_bit_14
	.type	integer_bit_14, @function
integer_bit_14:
.LFB87:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$104, %rsp
	.cfi_def_cfa_offset 160
	movq	%rdi, 40(%rsp)
	movq	%rsi, %rax
	movl	12(%rsi), %r11d
	leal	1(%r11), %edi
	addl	$2, %r11d
	movl	16(%rsi), %r10d
	leal	1(%r10), %ebx
	movl	%ebx, 76(%rsp)
	addl	$3, %r10d
	movl	20(%rsi), %r9d
	leal	1(%r9), %ecx
	movl	%ecx, 80(%rsp)
	addl	$4, %r9d
	movl	24(%rsi), %r8d
	leal	1(%r8), %ebx
	movl	%ebx, 84(%rsp)
	addl	$5, %r8d
	movl	28(%rsi), %edx
	leal	1(%rdx), %ecx
	movl	%ecx, 88(%rsp)
	leal	6(%rdx), %ebx
	movl	%ebx, 12(%rsp)
	movl	32(%rsi), %edx
	leal	1(%rdx), %esi
	movl	%esi, (%rsp)
	leal	7(%rdx), %ecx
	movl	%ecx, 16(%rsp)
	movl	36(%rax), %edx
	leal	1(%rdx), %ebx
	movl	%ebx, 4(%rsp)
	leal	8(%rdx), %esi
	movl	%esi, 20(%rsp)
	movl	40(%rax), %edx
	leal	1(%rdx), %ecx
	movl	%ecx, 8(%rsp)
	leal	9(%rdx), %ebx
	movl	%ebx, 24(%rsp)
	movl	44(%rax), %edx
	leal	1(%rdx), %esi
	movl	%esi, 92(%rsp)
	leal	10(%rdx), %ecx
	movl	%ecx, 28(%rsp)
	movl	48(%rax), %edx
	leal	1(%rdx), %r15d
	leal	11(%rdx), %ebx
	movl	%ebx, 32(%rsp)
	movl	52(%rax), %edx
	leal	1(%rdx), %r14d
	leal	12(%rdx), %esi
	movl	%esi, 36(%rsp)
	movl	56(%rax), %edx
	leal	1(%rdx), %r13d
	leal	13(%rdx), %ecx
	movl	%ecx, 48(%rsp)
	movl	60(%rax), %esi
	leal	1(%rsi), %r12d
	addl	$14, %esi
	movl	64(%rax), %ecx
	leal	1(%rcx), %ebp
	addl	$15, %ecx
	movl	68(%rax), %edx
	leal	1(%rdx), %ebx
	addl	$16, %edx
	movq	40(%rsp), %rax
	subq	$1, %rax
	cmpq	$0, 40(%rsp)
	je	.L84
	movl	%r11d, 40(%rsp)
	movl	%r10d, 52(%rsp)
	movl	%r9d, 56(%rsp)
	movl	%r8d, 60(%rsp)
	movl	%esi, 64(%rsp)
	movl	%ecx, 68(%rsp)
	movl	%edx, 72(%rsp)
	movl	76(%rsp), %r8d
	movl	80(%rsp), %r9d
	movl	84(%rsp), %r10d
	movl	88(%rsp), %r11d
	movl	92(%rsp), %esi
	jmp	.L85
.L86:
	movl	%ebx, 72(%rsp)
	movl	%ebp, 68(%rsp)
	movl	%r12d, 64(%rsp)
	movl	%r13d, 48(%rsp)
	movl	%r14d, 36(%rsp)
	movl	%r15d, 32(%rsp)
	movl	%esi, 28(%rsp)
	movl	8(%rsp), %edx
	movl	%edx, 24(%rsp)
	movl	4(%rsp), %ecx
	movl	%ecx, 20(%rsp)
	movl	(%rsp), %edx
	movl	%edx, 16(%rsp)
	movl	%r11d, 12(%rsp)
	movl	%r10d, 60(%rsp)
	movl	%r9d, 56(%rsp)
	movl	%r8d, 52(%rsp)
	movl	%edi, 40(%rsp)
.L85:
	orl	40(%rsp), %edi
	orl	52(%rsp), %r8d
	orl	56(%rsp), %r9d
	orl	60(%rsp), %r10d
	orl	12(%rsp), %r11d
	movl	16(%rsp), %edx
	orl	%edx, (%rsp)
	movl	20(%rsp), %ecx
	orl	%ecx, 4(%rsp)
	movl	24(%rsp), %ecx
	orl	%ecx, 8(%rsp)
	orl	28(%rsp), %esi
	orl	32(%rsp), %r15d
	orl	36(%rsp), %r14d
	orl	48(%rsp), %r13d
	orl	64(%rsp), %r12d
	orl	68(%rsp), %ebp
	orl	72(%rsp), %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L86
	movl	%r8d, 76(%rsp)
	movl	%r9d, 80(%rsp)
	movl	%r10d, 84(%rsp)
	movl	%r11d, 88(%rsp)
	movl	%esi, 92(%rsp)
.L84:
	call	use_int@PLT
	movl	76(%rsp), %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	call	use_int@PLT
	movl	84(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	call	use_int@PLT
	movl	(%rsp), %edi
	call	use_int@PLT
	movl	4(%rsp), %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	92(%rsp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE87:
	.size	integer_bit_14, .-integer_bit_14
	.globl	integer_bit_15
	.type	integer_bit_15, @function
integer_bit_15:
.LFB88:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$120, %rsp
	.cfi_def_cfa_offset 176
	movq	%rdi, 56(%rsp)
	movq	%rsi, %rax
	movl	12(%rsi), %r11d
	leal	1(%r11), %edi
	addl	$2, %r11d
	movl	16(%rsi), %r10d
	leal	1(%r10), %ebx
	movl	%ebx, 92(%rsp)
	addl	$3, %r10d
	movl	20(%rsi), %r9d
	leal	1(%r9), %ecx
	movl	%ecx, 96(%rsp)
	addl	$4, %r9d
	movl	24(%rsi), %r8d
	leal	1(%r8), %ebx
	movl	%ebx, 100(%rsp)
	addl	$5, %r8d
	movl	28(%rsi), %edx
	leal	1(%rdx), %ecx
	movl	%ecx, 104(%rsp)
	leal	6(%rdx), %ebx
	movl	%ebx, 24(%rsp)
	movl	32(%rsi), %edx
	leal	1(%rdx), %esi
	movl	%esi, 8(%rsp)
	leal	7(%rdx), %ecx
	movl	%ecx, 28(%rsp)
	movl	36(%rax), %edx
	leal	1(%rdx), %ebx
	movl	%ebx, 12(%rsp)
	leal	8(%rdx), %esi
	movl	%esi, 32(%rsp)
	movl	40(%rax), %edx
	leal	1(%rdx), %ecx
	movl	%ecx, 16(%rsp)
	leal	9(%rdx), %ebx
	movl	%ebx, 36(%rsp)
	movl	44(%rax), %edx
	leal	1(%rdx), %esi
	movl	%esi, 20(%rsp)
	leal	10(%rdx), %ecx
	movl	%ecx, 40(%rsp)
	movl	48(%rax), %edx
	leal	1(%rdx), %ebx
	movl	%ebx, 108(%rsp)
	leal	11(%rdx), %esi
	movl	%esi, 44(%rsp)
	movl	52(%rax), %edx
	leal	1(%rdx), %r15d
	leal	12(%rdx), %ecx
	movl	%ecx, 48(%rsp)
	movl	56(%rax), %edx
	leal	1(%rdx), %r14d
	leal	13(%rdx), %ebx
	movl	%ebx, 52(%rsp)
	movl	60(%rax), %edx
	leal	1(%rdx), %r13d
	leal	14(%rdx), %esi
	movl	%esi, 64(%rsp)
	movl	64(%rax), %esi
	leal	1(%rsi), %r12d
	addl	$15, %esi
	movl	68(%rax), %ecx
	leal	1(%rcx), %ebp
	addl	$16, %ecx
	movl	72(%rax), %edx
	leal	1(%rdx), %ebx
	addl	$17, %edx
	movq	56(%rsp), %rax
	subq	$1, %rax
	cmpq	$0, 56(%rsp)
	je	.L90
	movl	%r11d, 56(%rsp)
	movl	%r10d, 68(%rsp)
	movl	%r9d, 72(%rsp)
	movl	%r8d, 76(%rsp)
	movl	%esi, 80(%rsp)
	movl	%ecx, 84(%rsp)
	movl	%edx, 88(%rsp)
	movl	92(%rsp), %r8d
	movl	96(%rsp), %r9d
	movl	100(%rsp), %r10d
	movl	104(%rsp), %r11d
	movl	108(%rsp), %esi
	jmp	.L91
.L92:
	movl	%ebx, 88(%rsp)
	movl	%ebp, 84(%rsp)
	movl	%r12d, 80(%rsp)
	movl	%r13d, 64(%rsp)
	movl	%r14d, 52(%rsp)
	movl	%r15d, 48(%rsp)
	movl	%esi, 44(%rsp)
	movl	20(%rsp), %ecx
	movl	%ecx, 40(%rsp)
	movl	16(%rsp), %ecx
	movl	%ecx, 36(%rsp)
	movl	12(%rsp), %edx
	movl	%edx, 32(%rsp)
	movl	8(%rsp), %edx
	movl	%edx, 28(%rsp)
	movl	%r11d, 24(%rsp)
	movl	%r10d, 76(%rsp)
	movl	%r9d, 72(%rsp)
	movl	%r8d, 68(%rsp)
	movl	%edi, 56(%rsp)
.L91:
	orl	56(%rsp), %edi
	orl	68(%rsp), %r8d
	orl	72(%rsp), %r9d
	orl	76(%rsp), %r10d
	orl	24(%rsp), %r11d
	movl	28(%rsp), %edx
	orl	%edx, 8(%rsp)
	movl	32(%rsp), %edx
	orl	%edx, 12(%rsp)
	movl	36(%rsp), %ecx
	orl	%ecx, 16(%rsp)
	movl	40(%rsp), %ecx
	orl	%ecx, 20(%rsp)
	orl	44(%rsp), %esi
	orl	48(%rsp), %r15d
	orl	52(%rsp), %r14d
	orl	64(%rsp), %r13d
	orl	80(%rsp), %r12d
	orl	84(%rsp), %ebp
	orl	88(%rsp), %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L92
	movl	%r8d, 92(%rsp)
	movl	%r9d, 96(%rsp)
	movl	%r10d, 100(%rsp)
	movl	%r11d, 104(%rsp)
	movl	%esi, 108(%rsp)
.L90:
	call	use_int@PLT
	movl	92(%rsp), %edi
	call	use_int@PLT
	movl	96(%rsp), %edi
	call	use_int@PLT
	movl	100(%rsp), %edi
	call	use_int@PLT
	movl	104(%rsp), %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	12(%rsp), %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	call	use_int@PLT
	movl	20(%rsp), %edi
	call	use_int@PLT
	movl	108(%rsp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$120, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE88:
	.size	integer_bit_15, .-integer_bit_15
	.globl	integer_add_0
	.type	integer_add_0, @function
integer_add_0:
.LFB89:
	.cfi_startproc
	endbr64
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movl	12(%rsi), %edx
	leal	57(%rdx), %eax
	addl	$31, %edx
	leaq	-1(%rdi), %rcx
	testq	%rdi, %rdi
	je	.L96
.L97:
	movl	%edx, %esi
	movl	%eax, %edx
	negl	%eax
	subl	%esi, %eax
	subq	$1, %rcx
	cmpq	$-1, %rcx
	jne	.L97
.L96:
	leal	(%rax,%rdx), %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE89:
	.size	integer_add_0, .-integer_add_0
	.globl	integer_add_1
	.type	integer_add_1, @function
integer_add_1:
.LFB90:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	pushq	%rbx
	.cfi_def_cfa_offset 24
	.cfi_offset 3, -24
	subq	$8, %rsp
	.cfi_def_cfa_offset 32
	movl	12(%rsi), %edx
	leal	57(%rdx), %eax
	addl	$31, %edx
	movl	16(%rsi), %ebp
	leal	57(%rbp), %ebx
	addl	$31, %ebp
	leaq	-1(%rdi), %rcx
	testq	%rdi, %rdi
	je	.L101
.L102:
	movl	%edx, %edi
	movl	%eax, %edx
	movl	%ebp, %esi
	movl	%ebx, %ebp
	negl	%eax
	subl	%edi, %eax
	negl	%ebx
	subl	%esi, %ebx
	subq	$1, %rcx
	cmpq	$-1, %rcx
	jne	.L102
.L101:
	leal	(%rax,%rdx), %edi
	call	use_int@PLT
	leal	(%rbx,%rbp), %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 24
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE90:
	.size	integer_add_1, .-integer_add_1
	.globl	integer_add_2
	.type	integer_add_2, @function
integer_add_2:
.LFB91:
	.cfi_startproc
	endbr64
	pushq	%r13
	.cfi_def_cfa_offset 16
	.cfi_offset 13, -16
	pushq	%r12
	.cfi_def_cfa_offset 24
	.cfi_offset 12, -24
	pushq	%rbp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset 3, -40
	subq	$8, %rsp
	.cfi_def_cfa_offset 48
	movl	12(%rsi), %r9d
	leal	57(%r9), %eax
	addl	$31, %r9d
	movl	16(%rsi), %r13d
	leal	57(%r13), %ebp
	addl	$31, %r13d
	movl	20(%rsi), %r12d
	leal	57(%r12), %ebx
	addl	$31, %r12d
	leaq	-1(%rdi), %rdx
	testq	%rdi, %rdi
	je	.L106
.L107:
	movl	%r9d, %r8d
	movl	%eax, %r9d
	movl	%r13d, %esi
	movl	%ebp, %r13d
	movl	%r12d, %ecx
	movl	%ebx, %r12d
	negl	%eax
	subl	%r8d, %eax
	negl	%ebp
	subl	%esi, %ebp
	negl	%ebx
	subl	%ecx, %ebx
	subq	$1, %rdx
	cmpq	$-1, %rdx
	jne	.L107
.L106:
	leal	(%rax,%r9), %edi
	call	use_int@PLT
	leal	0(%rbp,%r13), %edi
	call	use_int@PLT
	leal	(%rbx,%r12), %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%rbp
	.cfi_def_cfa_offset 24
	popq	%r12
	.cfi_def_cfa_offset 16
	popq	%r13
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE91:
	.size	integer_add_2, .-integer_add_2
	.globl	integer_add_3
	.type	integer_add_3, @function
integer_add_3:
.LFB92:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$8, %rsp
	.cfi_def_cfa_offset 64
	movl	12(%rsi), %edx
	leal	57(%rdx), %eax
	addl	$31, %edx
	movl	16(%rsi), %r15d
	leal	57(%r15), %r12d
	addl	$31, %r15d
	movl	20(%rsi), %r14d
	leal	57(%r14), %ebp
	addl	$31, %r14d
	movl	24(%rsi), %r13d
	leal	57(%r13), %ebx
	addl	$31, %r13d
	leaq	-1(%rdi), %rcx
	testq	%rdi, %rdi
	je	.L111
.L112:
	movl	%edx, %r10d
	movl	%eax, %edx
	movl	%r15d, %r9d
	movl	%r12d, %r15d
	movl	%r14d, %r8d
	movl	%ebp, %r14d
	movl	%r13d, %esi
	movl	%ebx, %r13d
	negl	%eax
	subl	%r10d, %eax
	negl	%r12d
	subl	%r9d, %r12d
	negl	%ebp
	subl	%r8d, %ebp
	negl	%ebx
	subl	%esi, %ebx
	subq	$1, %rcx
	cmpq	$-1, %rcx
	jne	.L112
.L111:
	leal	(%rax,%rdx), %edi
	call	use_int@PLT
	leal	(%r12,%r15), %edi
	call	use_int@PLT
	leal	0(%rbp,%r14), %edi
	call	use_int@PLT
	leal	(%rbx,%r13), %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE92:
	.size	integer_add_3, .-integer_add_3
	.globl	integer_add_4
	.type	integer_add_4, @function
integer_add_4:
.LFB93:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$24, %rsp
	.cfi_def_cfa_offset 80
	movl	12(%rsi), %edx
	leal	57(%rdx), %eax
	addl	$31, %edx
	movl	16(%rsi), %r15d
	leal	57(%r15), %ebp
	addl	$31, %r15d
	movl	20(%rsi), %r14d
	leal	57(%r14), %ebx
	addl	$31, %r14d
	movl	24(%rsi), %r13d
	leal	57(%r13), %ecx
	movl	%ecx, 8(%rsp)
	addl	$31, %r13d
	movl	28(%rsi), %r12d
	leal	57(%r12), %ecx
	movl	%ecx, 12(%rsp)
	addl	$31, %r12d
	leaq	-1(%rdi), %rcx
	testq	%rdi, %rdi
	je	.L116
.L117:
	movl	%edx, %r11d
	movl	%eax, %edx
	movl	%r15d, %r10d
	movl	%ebp, %r15d
	movl	%r14d, %r9d
	movl	%ebx, %r14d
	movl	%r13d, %r8d
	movl	8(%rsp), %r13d
	movl	%r12d, %esi
	movl	12(%rsp), %r12d
	negl	%eax
	subl	%r11d, %eax
	negl	%ebp
	subl	%r10d, %ebp
	negl	%ebx
	subl	%r9d, %ebx
	movl	%r13d, %edi
	negl	%edi
	subl	%r8d, %edi
	movl	%edi, 8(%rsp)
	movl	%r12d, %edi
	negl	%edi
	subl	%esi, %edi
	movl	%edi, 12(%rsp)
	subq	$1, %rcx
	cmpq	$-1, %rcx
	jne	.L117
.L116:
	leal	(%rax,%rdx), %edi
	call	use_int@PLT
	leal	0(%rbp,%r15), %edi
	call	use_int@PLT
	leal	(%rbx,%r14), %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	addl	%r13d, %edi
	call	use_int@PLT
	movl	12(%rsp), %edi
	addl	%r12d, %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE93:
	.size	integer_add_4, .-integer_add_4
	.globl	integer_add_5
	.type	integer_add_5, @function
integer_add_5:
.LFB94:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$24, %rsp
	.cfi_def_cfa_offset 80
	movl	12(%rsi), %edx
	leal	57(%rdx), %eax
	addl	$31, %edx
	movl	16(%rsi), %r15d
	leal	57(%r15), %ebx
	addl	$31, %r15d
	movl	20(%rsi), %r14d
	leal	57(%r14), %ecx
	movl	%ecx, (%rsp)
	addl	$31, %r14d
	movl	24(%rsi), %r13d
	leal	57(%r13), %ecx
	movl	%ecx, 4(%rsp)
	addl	$31, %r13d
	movl	28(%rsi), %r12d
	leal	57(%r12), %ecx
	movl	%ecx, 8(%rsp)
	addl	$31, %r12d
	movl	32(%rsi), %ebp
	leal	57(%rbp), %ecx
	movl	%ecx, 12(%rsp)
	addl	$31, %ebp
	leaq	-1(%rdi), %rcx
	testq	%rdi, %rdi
	je	.L121
.L122:
	movl	%edx, %edi
	movl	%eax, %edx
	movl	%r15d, %r11d
	movl	%ebx, %r15d
	movl	%r14d, %r10d
	movl	(%rsp), %r14d
	movl	%r13d, %r9d
	movl	4(%rsp), %r13d
	movl	%r12d, %r8d
	movl	8(%rsp), %r12d
	movl	%ebp, %esi
	movl	12(%rsp), %ebp
	negl	%eax
	subl	%edi, %eax
	negl	%ebx
	subl	%r11d, %ebx
	movl	%r14d, %edi
	negl	%edi
	subl	%r10d, %edi
	movl	%edi, (%rsp)
	movl	%r13d, %edi
	negl	%edi
	subl	%r9d, %edi
	movl	%edi, 4(%rsp)
	movl	%r12d, %edi
	negl	%edi
	subl	%r8d, %edi
	movl	%edi, 8(%rsp)
	movl	%ebp, %edi
	negl	%edi
	subl	%esi, %edi
	movl	%edi, 12(%rsp)
	subq	$1, %rcx
	cmpq	$-1, %rcx
	jne	.L122
.L121:
	leal	(%rax,%rdx), %edi
	call	use_int@PLT
	leal	(%rbx,%r15), %edi
	call	use_int@PLT
	movl	(%rsp), %edi
	addl	%r14d, %edi
	call	use_int@PLT
	movl	4(%rsp), %edi
	addl	%r13d, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	addl	%r12d, %edi
	call	use_int@PLT
	movl	12(%rsp), %edi
	addl	%ebp, %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE94:
	.size	integer_add_5, .-integer_add_5
	.globl	integer_add_6
	.type	integer_add_6, @function
integer_add_6:
.LFB95:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$40, %rsp
	.cfi_def_cfa_offset 96
	movq	%rdi, %rcx
	movq	%rsi, %rdx
	movl	12(%rsi), %esi
	leal	57(%rsi), %eax
	leal	31(%rsi), %edi
	movl	16(%rdx), %esi
	leal	57(%rsi), %r15d
	leal	31(%rsi), %r11d
	movl	%r11d, 12(%rsp)
	movl	20(%rdx), %esi
	leal	57(%rsi), %r14d
	leal	31(%rsi), %r8d
	movl	%r8d, 16(%rsp)
	movl	24(%rdx), %esi
	leal	57(%rsi), %r13d
	leal	31(%rsi), %r9d
	movl	%r9d, 20(%rsp)
	movl	28(%rdx), %esi
	leal	57(%rsi), %r12d
	leal	31(%rsi), %r10d
	movl	%r10d, 24(%rsp)
	movl	32(%rdx), %esi
	leal	57(%rsi), %ebp
	addl	$31, %esi
	movl	%esi, 4(%rsp)
	movl	36(%rdx), %edx
	leal	57(%rdx), %ebx
	addl	$31, %edx
	movl	%edx, 8(%rsp)
	testq	%rcx, %rcx
	je	.L126
	leaq	-1(%rcx), %rdx
	movl	%r8d, %ecx
	movl	%r9d, %esi
	movl	%r10d, %r8d
	movl	4(%rsp), %r9d
	movl	8(%rsp), %r10d
.L127:
	movl	%edi, 4(%rsp)
	movl	%eax, %edi
	movl	%r11d, 8(%rsp)
	movl	%r15d, %r11d
	movl	%ecx, 12(%rsp)
	movl	%r14d, %ecx
	movl	%esi, 16(%rsp)
	movl	%r13d, %esi
	movl	%r8d, 20(%rsp)
	movl	%r12d, %r8d
	movl	%r9d, 24(%rsp)
	movl	%ebp, %r9d
	movl	%r10d, 28(%rsp)
	movl	%ebx, %r10d
	negl	%eax
	subl	4(%rsp), %eax
	negl	%r15d
	subl	8(%rsp), %r15d
	negl	%r14d
	subl	12(%rsp), %r14d
	negl	%r13d
	subl	16(%rsp), %r13d
	negl	%r12d
	subl	20(%rsp), %r12d
	negl	%ebp
	subl	24(%rsp), %ebp
	negl	%ebx
	subl	28(%rsp), %ebx
	subq	$1, %rdx
	cmpq	$-1, %rdx
	jne	.L127
	movl	%r11d, 12(%rsp)
	movl	%ecx, 16(%rsp)
	movl	%esi, 20(%rsp)
	movl	%r8d, 24(%rsp)
	movl	%r9d, 4(%rsp)
	movl	%r10d, 8(%rsp)
.L126:
	addl	%eax, %edi
	call	use_int@PLT
	movl	12(%rsp), %edi
	addl	%r15d, %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	addl	%r14d, %edi
	call	use_int@PLT
	movl	20(%rsp), %edi
	addl	%r13d, %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	addl	%r12d, %edi
	call	use_int@PLT
	movl	4(%rsp), %edi
	addl	%ebp, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	addl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE95:
	.size	integer_add_6, .-integer_add_6
	.globl	integer_add_7
	.type	integer_add_7, @function
integer_add_7:
.LFB96:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$56, %rsp
	.cfi_def_cfa_offset 112
	movq	%rdi, %rdx
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	57(%rcx), %esi
	leal	31(%rcx), %edi
	movl	16(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 8(%rsp)
	leal	31(%rcx), %r8d
	movl	%r8d, 24(%rsp)
	movl	20(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 12(%rsp)
	leal	31(%rcx), %r9d
	movl	%r9d, 28(%rsp)
	movl	24(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 20(%rsp)
	leal	31(%rcx), %r10d
	movl	%r10d, 32(%rsp)
	movl	28(%rax), %ecx
	leal	57(%rcx), %r13d
	leal	31(%rcx), %r11d
	movl	%r11d, 36(%rsp)
	movl	32(%rax), %ecx
	leal	57(%rcx), %r12d
	addl	$31, %ecx
	movl	%ecx, 16(%rsp)
	movl	36(%rax), %r15d
	leal	57(%r15), %ebp
	addl	$31, %r15d
	movl	40(%rax), %r14d
	leal	57(%r14), %ebx
	addl	$31, %r14d
	testq	%rdx, %rdx
	je	.L131
	leaq	-1(%rdx), %rax
	movl	%r8d, %ecx
	movl	%r9d, %r8d
	movl	%r10d, %r9d
	movl	%r11d, %r10d
	movl	16(%rsp), %r11d
	movl	20(%rsp), %edx
.L132:
	movl	%edi, 16(%rsp)
	movl	%esi, %edi
	movl	%ecx, 20(%rsp)
	movl	8(%rsp), %ecx
	movl	%r8d, 24(%rsp)
	movl	12(%rsp), %r8d
	movl	%r9d, 28(%rsp)
	movl	%edx, %r9d
	movl	%r10d, 32(%rsp)
	movl	%r13d, %r10d
	movl	%r11d, 36(%rsp)
	movl	%r12d, %r11d
	movl	%r15d, 40(%rsp)
	movl	%ebp, %r15d
	movl	%r14d, 44(%rsp)
	movl	%ebx, %r14d
	negl	%esi
	subl	16(%rsp), %esi
	movl	%ecx, %edx
	negl	%edx
	subl	20(%rsp), %edx
	movl	%edx, 8(%rsp)
	movl	%r8d, %edx
	negl	%edx
	subl	24(%rsp), %edx
	movl	%edx, 12(%rsp)
	movl	%r9d, %edx
	negl	%edx
	subl	28(%rsp), %edx
	negl	%r13d
	subl	32(%rsp), %r13d
	negl	%r12d
	subl	36(%rsp), %r12d
	negl	%ebp
	subl	40(%rsp), %ebp
	negl	%ebx
	subl	44(%rsp), %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L132
	movl	%ecx, 24(%rsp)
	movl	%r8d, 28(%rsp)
	movl	%r9d, 32(%rsp)
	movl	%r10d, 36(%rsp)
	movl	%r11d, 16(%rsp)
	movl	%edx, 20(%rsp)
.L131:
	addl	%esi, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	addl	24(%rsp), %edi
	call	use_int@PLT
	movl	12(%rsp), %edi
	addl	28(%rsp), %edi
	call	use_int@PLT
	movl	20(%rsp), %edi
	addl	32(%rsp), %edi
	call	use_int@PLT
	movl	36(%rsp), %edi
	addl	%r13d, %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	addl	%r12d, %edi
	call	use_int@PLT
	leal	0(%rbp,%r15), %edi
	call	use_int@PLT
	leal	(%rbx,%r14), %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE96:
	.size	integer_add_7, .-integer_add_7
	.globl	integer_add_8
	.type	integer_add_8, @function
integer_add_8:
.LFB97:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$72, %rsp
	.cfi_def_cfa_offset 128
	movq	%rdi, %rdx
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	57(%rcx), %esi
	leal	31(%rcx), %edi
	movl	16(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 16(%rsp)
	addl	$31, %ecx
	movl	%ecx, 12(%rsp)
	movl	20(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 20(%rsp)
	leal	31(%rcx), %r8d
	movl	%r8d, 36(%rsp)
	movl	24(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 24(%rsp)
	leal	31(%rcx), %r9d
	movl	%r9d, 40(%rsp)
	movl	28(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 32(%rsp)
	leal	31(%rcx), %r10d
	movl	%r10d, 44(%rsp)
	movl	32(%rax), %ecx
	leal	57(%rcx), %r13d
	leal	31(%rcx), %r11d
	movl	%r11d, 48(%rsp)
	movl	36(%rax), %ecx
	leal	57(%rcx), %r12d
	addl	$31, %ecx
	movl	%ecx, 28(%rsp)
	movl	40(%rax), %ecx
	leal	57(%rcx), %ebp
	leal	31(%rcx), %r15d
	movl	44(%rax), %r14d
	leal	57(%r14), %ebx
	addl	$31, %r14d
	testq	%rdx, %rdx
	je	.L136
	leaq	-1(%rdx), %rax
	movl	%r8d, %ecx
	movl	%r9d, %r8d
	movl	%r10d, %r9d
	movl	%r11d, %r10d
	movl	28(%rsp), %r11d
	movl	32(%rsp), %edx
.L137:
	movl	%edi, 28(%rsp)
	movl	%esi, %edi
	movl	12(%rsp), %esi
	movl	%esi, 32(%rsp)
	movl	16(%rsp), %esi
	movl	%esi, 12(%rsp)
	movl	%ecx, 36(%rsp)
	movl	20(%rsp), %ecx
	movl	%r8d, 40(%rsp)
	movl	24(%rsp), %r8d
	movl	%r9d, 44(%rsp)
	movl	%edx, %r9d
	movl	%r10d, 48(%rsp)
	movl	%r13d, %r10d
	movl	%r11d, 52(%rsp)
	movl	%r12d, %r11d
	movl	%r15d, 56(%rsp)
	movl	%ebp, %r15d
	movl	%r14d, 60(%rsp)
	movl	%ebx, %r14d
	movl	%edi, %ebx
	negl	%ebx
	movl	%ebx, %esi
	subl	28(%rsp), %esi
	movl	12(%rsp), %edx
	negl	%edx
	subl	32(%rsp), %edx
	movl	%edx, 16(%rsp)
	movl	%ecx, %edx
	negl	%edx
	subl	36(%rsp), %edx
	movl	%edx, 20(%rsp)
	movl	%r8d, %edx
	negl	%edx
	subl	40(%rsp), %edx
	movl	%edx, 24(%rsp)
	movl	%r9d, %edx
	negl	%edx
	subl	44(%rsp), %edx
	negl	%r13d
	subl	48(%rsp), %r13d
	negl	%r12d
	subl	52(%rsp), %r12d
	negl	%ebp
	subl	56(%rsp), %ebp
	movl	%r14d, %ebx
	negl	%ebx
	subl	60(%rsp), %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L137
	movl	%ecx, 36(%rsp)
	movl	%r8d, 40(%rsp)
	movl	%r9d, 44(%rsp)
	movl	%r10d, 48(%rsp)
	movl	%r11d, 28(%rsp)
	movl	%edx, 32(%rsp)
.L136:
	addl	%esi, %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	addl	12(%rsp), %edi
	call	use_int@PLT
	movl	20(%rsp), %edi
	addl	36(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	addl	40(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	addl	44(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	addl	%r13d, %edi
	call	use_int@PLT
	movl	28(%rsp), %edi
	addl	%r12d, %edi
	call	use_int@PLT
	leal	0(%rbp,%r15), %edi
	call	use_int@PLT
	leal	(%rbx,%r14), %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE97:
	.size	integer_add_8, .-integer_add_8
	.globl	integer_add_9
	.type	integer_add_9, @function
integer_add_9:
.LFB98:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$72, %rsp
	.cfi_def_cfa_offset 128
	movq	%rdi, %rdx
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	57(%rcx), %esi
	leal	31(%rcx), %edi
	movl	16(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 8(%rsp)
	leal	31(%rcx), %ebx
	movl	%ebx, (%rsp)
	movl	20(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 12(%rsp)
	leal	31(%rcx), %ebx
	movl	%ebx, 4(%rsp)
	movl	24(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 16(%rsp)
	leal	31(%rcx), %r8d
	movl	%r8d, 32(%rsp)
	movl	28(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 20(%rsp)
	leal	31(%rcx), %r9d
	movl	%r9d, 36(%rsp)
	movl	32(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 24(%rsp)
	leal	31(%rcx), %r10d
	movl	%r10d, 40(%rsp)
	movl	36(%rax), %ecx
	leal	57(%rcx), %r11d
	movl	%r11d, 28(%rsp)
	leal	31(%rcx), %r14d
	movl	%r14d, %r11d
	movl	%r14d, 44(%rsp)
	movl	40(%rax), %ecx
	leal	57(%rcx), %r12d
	leal	31(%rcx), %r15d
	movl	44(%rax), %ecx
	leal	57(%rcx), %ebp
	leal	31(%rcx), %r14d
	movl	48(%rax), %r13d
	leal	57(%r13), %ebx
	addl	$31, %r13d
	testq	%rdx, %rdx
	je	.L141
	leaq	-1(%rdx), %rax
	movl	24(%rsp), %edx
	movl	28(%rsp), %ecx
.L142:
	movl	%edi, 24(%rsp)
	movl	%esi, %edi
	movl	(%rsp), %esi
	movl	%esi, 28(%rsp)
	movl	8(%rsp), %esi
	movl	%esi, (%rsp)
	movl	4(%rsp), %esi
	movl	%esi, 32(%rsp)
	movl	12(%rsp), %esi
	movl	%esi, 4(%rsp)
	movl	%r8d, 36(%rsp)
	movl	16(%rsp), %r8d
	movl	%r9d, 40(%rsp)
	movl	20(%rsp), %r9d
	movl	%r10d, 44(%rsp)
	movl	%edx, %r10d
	movl	%r11d, 48(%rsp)
	movl	%ecx, %r11d
	movl	%r15d, 52(%rsp)
	movl	%r12d, %r15d
	movl	%r14d, 56(%rsp)
	movl	%ebp, %r14d
	movl	%r13d, 60(%rsp)
	movl	%ebx, %r13d
	movl	%edi, %ebx
	negl	%ebx
	movl	%ebx, %esi
	subl	24(%rsp), %esi
	movl	(%rsp), %edx
	negl	%edx
	movl	%edx, %ecx
	subl	28(%rsp), %ecx
	movl	%ecx, 8(%rsp)
	movl	4(%rsp), %edx
	negl	%edx
	subl	32(%rsp), %edx
	movl	%edx, 12(%rsp)
	movl	%r8d, %edx
	negl	%edx
	subl	36(%rsp), %edx
	movl	%edx, 16(%rsp)
	movl	%r9d, %edx
	negl	%edx
	subl	40(%rsp), %edx
	movl	%edx, 20(%rsp)
	movl	%r10d, %edx
	negl	%edx
	subl	44(%rsp), %edx
	movl	%r11d, %ecx
	negl	%ecx
	subl	48(%rsp), %ecx
	negl	%r12d
	subl	52(%rsp), %r12d
	negl	%ebp
	subl	56(%rsp), %ebp
	movl	%r13d, %ebx
	negl	%ebx
	subl	60(%rsp), %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L142
	movl	%r8d, 32(%rsp)
	movl	%r9d, 36(%rsp)
	movl	%r10d, 40(%rsp)
	movl	%r11d, 44(%rsp)
	movl	%edx, 24(%rsp)
	movl	%ecx, 28(%rsp)
.L141:
	addl	%esi, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	addl	(%rsp), %edi
	call	use_int@PLT
	movl	12(%rsp), %edi
	addl	4(%rsp), %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	addl	32(%rsp), %edi
	call	use_int@PLT
	movl	20(%rsp), %edi
	addl	36(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	addl	40(%rsp), %edi
	call	use_int@PLT
	movl	28(%rsp), %edi
	addl	44(%rsp), %edi
	call	use_int@PLT
	leal	(%r12,%r15), %edi
	call	use_int@PLT
	leal	0(%rbp,%r14), %edi
	call	use_int@PLT
	leal	(%rbx,%r13), %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE98:
	.size	integer_add_9, .-integer_add_9
	.globl	integer_add_10
	.type	integer_add_10, @function
integer_add_10:
.LFB99:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$88, %rsp
	.cfi_def_cfa_offset 144
	movq	%rdi, %rdx
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	57(%rcx), %esi
	leal	31(%rcx), %edi
	movl	16(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 20(%rsp)
	addl	$31, %ecx
	movl	%ecx, 4(%rsp)
	movl	20(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 24(%rsp)
	addl	$31, %ecx
	movl	%ecx, 8(%rsp)
	movl	24(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 28(%rsp)
	addl	$31, %ecx
	movl	%ecx, 12(%rsp)
	movl	28(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 32(%rsp)
	addl	$31, %ecx
	movl	%ecx, 16(%rsp)
	movl	32(%rax), %ecx
	leal	57(%rcx), %r8d
	movl	%r8d, 56(%rsp)
	leal	31(%rcx), %r9d
	movl	%r9d, 44(%rsp)
	movl	36(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 36(%rsp)
	leal	31(%rcx), %r10d
	movl	%r10d, 48(%rsp)
	movl	40(%rax), %ecx
	leal	57(%rcx), %r11d
	movl	%r11d, 40(%rsp)
	leal	31(%rcx), %r14d
	movl	%r14d, %r11d
	movl	%r14d, 52(%rsp)
	movl	44(%rax), %ecx
	leal	57(%rcx), %r12d
	leal	31(%rcx), %r15d
	movl	48(%rax), %ecx
	leal	57(%rcx), %ebp
	leal	31(%rcx), %r14d
	movl	52(%rax), %eax
	leal	57(%rax), %ebx
	leal	31(%rax), %r13d
	testq	%rdx, %rdx
	je	.L146
	leaq	-1(%rdx), %rax
	movl	%r8d, %edx
	movl	36(%rsp), %ecx
	movl	40(%rsp), %r8d
.L147:
	movl	%edi, 36(%rsp)
	movl	%esi, %edi
	movl	4(%rsp), %esi
	movl	%esi, 40(%rsp)
	movl	20(%rsp), %esi
	movl	%esi, 4(%rsp)
	movl	8(%rsp), %esi
	movl	%esi, 44(%rsp)
	movl	24(%rsp), %esi
	movl	%esi, 8(%rsp)
	movl	12(%rsp), %esi
	movl	%esi, 48(%rsp)
	movl	28(%rsp), %esi
	movl	%esi, 12(%rsp)
	movl	16(%rsp), %esi
	movl	%esi, 52(%rsp)
	movl	32(%rsp), %esi
	movl	%esi, 16(%rsp)
	movl	%r9d, 56(%rsp)
	movl	%edx, %r9d
	movl	%r10d, 60(%rsp)
	movl	%ecx, %r10d
	movl	%r11d, 64(%rsp)
	movl	%r8d, %r11d
	movl	%r15d, 68(%rsp)
	movl	%r12d, %r15d
	movl	%r14d, 72(%rsp)
	movl	%ebp, %r14d
	movl	%r13d, 76(%rsp)
	movl	%ebx, %r13d
	movl	%edi, %ebx
	negl	%ebx
	movl	%ebx, %esi
	subl	36(%rsp), %esi
	movl	4(%rsp), %edx
	negl	%edx
	movl	%edx, %ebx
	subl	40(%rsp), %ebx
	movl	%ebx, 20(%rsp)
	movl	8(%rsp), %edx
	negl	%edx
	movl	%edx, %ecx
	subl	44(%rsp), %ecx
	movl	%ecx, 24(%rsp)
	movl	12(%rsp), %edx
	negl	%edx
	movl	%edx, %ebx
	subl	48(%rsp), %ebx
	movl	%ebx, 28(%rsp)
	movl	16(%rsp), %edx
	negl	%edx
	subl	52(%rsp), %edx
	movl	%edx, 32(%rsp)
	movl	%r9d, %edx
	negl	%edx
	subl	56(%rsp), %edx
	movl	%r10d, %ecx
	negl	%ecx
	subl	60(%rsp), %ecx
	negl	%r8d
	subl	64(%rsp), %r8d
	negl	%r12d
	subl	68(%rsp), %r12d
	negl	%ebp
	subl	72(%rsp), %ebp
	movl	%r13d, %ebx
	negl	%ebx
	subl	76(%rsp), %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L147
	movl	%r9d, 44(%rsp)
	movl	%r10d, 48(%rsp)
	movl	%r11d, 52(%rsp)
	movl	%edx, 56(%rsp)
	movl	%ecx, 36(%rsp)
	movl	%r8d, 40(%rsp)
.L146:
	addl	%esi, %edi
	call	use_int@PLT
	movl	20(%rsp), %edi
	addl	4(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	addl	8(%rsp), %edi
	call	use_int@PLT
	movl	28(%rsp), %edi
	addl	12(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	addl	16(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	addl	44(%rsp), %edi
	call	use_int@PLT
	movl	36(%rsp), %edi
	addl	48(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	addl	52(%rsp), %edi
	call	use_int@PLT
	leal	(%r12,%r15), %edi
	call	use_int@PLT
	leal	0(%rbp,%r14), %edi
	call	use_int@PLT
	leal	(%rbx,%r13), %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE99:
	.size	integer_add_10, .-integer_add_10
	.globl	integer_add_11
	.type	integer_add_11, @function
integer_add_11:
.LFB100:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$104, %rsp
	.cfi_def_cfa_offset 160
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	57(%rcx), %edx
	leal	31(%rcx), %esi
	movl	16(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 28(%rsp)
	leal	31(%rcx), %ebx
	movl	%ebx, 8(%rsp)
	movl	20(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 32(%rsp)
	leal	31(%rcx), %ebx
	movl	%ebx, 12(%rsp)
	movl	24(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 36(%rsp)
	leal	31(%rcx), %ebx
	movl	%ebx, 16(%rsp)
	movl	28(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 40(%rsp)
	leal	31(%rcx), %ebx
	movl	%ebx, 20(%rsp)
	movl	32(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 44(%rsp)
	leal	31(%rcx), %ebx
	movl	%ebx, 24(%rsp)
	movl	36(%rax), %ecx
	leal	57(%rcx), %r8d
	movl	%r8d, 64(%rsp)
	leal	31(%rcx), %r10d
	movl	%r10d, 56(%rsp)
	movl	40(%rax), %ecx
	leal	57(%rcx), %r9d
	movl	%r9d, 68(%rsp)
	leal	31(%rcx), %r11d
	movl	%r11d, 60(%rsp)
	movl	44(%rax), %ecx
	leal	57(%rcx), %ebx
	movl	%ebx, 48(%rsp)
	leal	31(%rcx), %r15d
	movl	48(%rax), %ecx
	leal	57(%rcx), %r14d
	movl	%r14d, 52(%rsp)
	leal	31(%rcx), %r14d
	movl	52(%rax), %ecx
	leal	57(%rcx), %ebp
	leal	31(%rcx), %r13d
	movl	56(%rax), %eax
	leal	57(%rax), %ebx
	leal	31(%rax), %r12d
	testq	%rdi, %rdi
	je	.L151
	leaq	-1(%rdi), %rax
	movl	%r8d, %ecx
	movl	%r9d, %edi
	movl	48(%rsp), %r8d
	movl	52(%rsp), %r9d
.L152:
	movl	%esi, 48(%rsp)
	movl	%edx, %esi
	movl	8(%rsp), %edx
	movl	%edx, 52(%rsp)
	movl	28(%rsp), %edx
	movl	%edx, 8(%rsp)
	movl	12(%rsp), %edx
	movl	%edx, 56(%rsp)
	movl	32(%rsp), %edx
	movl	%edx, 12(%rsp)
	movl	16(%rsp), %edx
	movl	%edx, 60(%rsp)
	movl	36(%rsp), %edx
	movl	%edx, 16(%rsp)
	movl	20(%rsp), %edx
	movl	%edx, 64(%rsp)
	movl	40(%rsp), %edx
	movl	%edx, 20(%rsp)
	movl	24(%rsp), %edx
	movl	%edx, 68(%rsp)
	movl	44(%rsp), %edx
	movl	%edx, 24(%rsp)
	movl	%r10d, 72(%rsp)
	movl	%ecx, %r10d
	movl	%r11d, 76(%rsp)
	movl	%edi, %r11d
	movl	%r15d, 80(%rsp)
	movl	%r8d, %r15d
	movl	%r14d, 84(%rsp)
	movl	%r9d, %r14d
	movl	%r13d, 88(%rsp)
	movl	%ebp, %r13d
	movl	%r12d, 92(%rsp)
	movl	%ebx, %r12d
	movl	%esi, %ebx
	negl	%ebx
	movl	%ebx, %edx
	subl	48(%rsp), %edx
	movl	8(%rsp), %ecx
	negl	%ecx
	movl	%ecx, %ebx
	subl	52(%rsp), %ebx
	movl	%ebx, 28(%rsp)
	movl	12(%rsp), %ecx
	negl	%ecx
	movl	%ecx, %edi
	subl	56(%rsp), %edi
	movl	%edi, 32(%rsp)
	movl	16(%rsp), %ecx
	negl	%ecx
	movl	%ecx, %ebx
	subl	60(%rsp), %ebx
	movl	%ebx, 36(%rsp)
	movl	20(%rsp), %ecx
	negl	%ecx
	movl	%ecx, %edi
	subl	64(%rsp), %edi
	movl	%edi, 40(%rsp)
	movl	24(%rsp), %ecx
	negl	%ecx
	subl	68(%rsp), %ecx
	movl	%ecx, 44(%rsp)
	movl	%r10d, %ecx
	negl	%ecx
	subl	72(%rsp), %ecx
	movl	%r11d, %edi
	negl	%edi
	subl	76(%rsp), %edi
	negl	%r8d
	subl	80(%rsp), %r8d
	negl	%r9d
	subl	84(%rsp), %r9d
	negl	%ebp
	subl	88(%rsp), %ebp
	movl	%r12d, %ebx
	negl	%ebx
	subl	92(%rsp), %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L152
	movl	%r10d, 56(%rsp)
	movl	%r11d, 60(%rsp)
	movl	%ecx, 64(%rsp)
	movl	%edi, 68(%rsp)
	movl	%r8d, 48(%rsp)
	movl	%r9d, 52(%rsp)
.L151:
	leal	(%rdx,%rsi), %edi
	call	use_int@PLT
	movl	28(%rsp), %edi
	addl	8(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	addl	12(%rsp), %edi
	call	use_int@PLT
	movl	36(%rsp), %edi
	addl	16(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	addl	20(%rsp), %edi
	call	use_int@PLT
	movl	44(%rsp), %edi
	addl	24(%rsp), %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	addl	56(%rsp), %edi
	call	use_int@PLT
	movl	68(%rsp), %edi
	addl	60(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	addl	%r15d, %edi
	call	use_int@PLT
	movl	52(%rsp), %edi
	addl	%r14d, %edi
	call	use_int@PLT
	leal	0(%rbp,%r13), %edi
	call	use_int@PLT
	leal	(%rbx,%r12), %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE100:
	.size	integer_add_11, .-integer_add_11
	.globl	integer_add_12
	.type	integer_add_12, @function
integer_add_12:
.LFB101:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$120, %rsp
	.cfi_def_cfa_offset 176
	movq	%rsi, %rax
	movl	12(%rsi), %edx
	leal	57(%rdx), %ecx
	addl	$31, %edx
	movl	16(%rsi), %esi
	leal	57(%rsi), %ebx
	movl	%ebx, 40(%rsp)
	addl	$31, %esi
	movl	%esi, 12(%rsp)
	movl	20(%rax), %esi
	leal	57(%rsi), %ebx
	movl	%ebx, 44(%rsp)
	addl	$31, %esi
	movl	%esi, 16(%rsp)
	movl	24(%rax), %esi
	leal	57(%rsi), %ebx
	movl	%ebx, 48(%rsp)
	addl	$31, %esi
	movl	%esi, 20(%rsp)
	movl	28(%rax), %esi
	leal	57(%rsi), %ebx
	movl	%ebx, 52(%rsp)
	addl	$31, %esi
	movl	%esi, 24(%rsp)
	movl	32(%rax), %esi
	leal	57(%rsi), %ebx
	movl	%ebx, 56(%rsp)
	addl	$31, %esi
	movl	%esi, 28(%rsp)
	movl	36(%rax), %esi
	leal	57(%rsi), %r9d
	movl	%r9d, 68(%rsp)
	addl	$31, %esi
	movl	%esi, 32(%rsp)
	movl	40(%rax), %esi
	leal	57(%rsi), %r10d
	movl	%r10d, 72(%rsp)
	addl	$31, %esi
	movl	%esi, 36(%rsp)
	movl	44(%rax), %esi
	leal	57(%rsi), %r8d
	movl	%r8d, 76(%rsp)
	leal	31(%rsi), %r15d
	movl	48(%rax), %esi
	leal	57(%rsi), %r11d
	movl	%r11d, 80(%rsp)
	leal	31(%rsi), %r13d
	movl	52(%rax), %esi
	leal	57(%rsi), %ebx
	movl	%ebx, 60(%rsp)
	leal	31(%rsi), %r14d
	movl	56(%rax), %esi
	leal	57(%rsi), %ebp
	movl	%ebp, 64(%rsp)
	leal	31(%rsi), %r12d
	movl	60(%rax), %eax
	leal	57(%rax), %ebx
	leal	31(%rax), %ebp
	testq	%rdi, %rdi
	je	.L156
	leaq	-1(%rdi), %rax
	movl	%r9d, %esi
	movl	%r10d, %edi
	movl	%r11d, %r9d
	movl	60(%rsp), %r10d
	movl	64(%rsp), %r11d
.L157:
	movl	%edx, 60(%rsp)
	movl	%ecx, %edx
	movl	12(%rsp), %ecx
	movl	%ecx, 64(%rsp)
	movl	40(%rsp), %ecx
	movl	%ecx, 12(%rsp)
	movl	16(%rsp), %ecx
	movl	%ecx, 68(%rsp)
	movl	44(%rsp), %ecx
	movl	%ecx, 16(%rsp)
	movl	20(%rsp), %ecx
	movl	%ecx, 72(%rsp)
	movl	48(%rsp), %ecx
	movl	%ecx, 20(%rsp)
	movl	24(%rsp), %ecx
	movl	%ecx, 76(%rsp)
	movl	52(%rsp), %ecx
	movl	%ecx, 24(%rsp)
	movl	28(%rsp), %ecx
	movl	%ecx, 80(%rsp)
	movl	56(%rsp), %ecx
	movl	%ecx, 28(%rsp)
	movl	32(%rsp), %ecx
	movl	%ecx, 84(%rsp)
	movl	%esi, 32(%rsp)
	movl	36(%rsp), %esi
	movl	%esi, 88(%rsp)
	movl	%edi, 36(%rsp)
	movl	%r15d, 92(%rsp)
	movl	%r8d, %r15d
	movl	%r13d, 96(%rsp)
	movl	%r9d, %r13d
	movl	%r14d, 100(%rsp)
	movl	%r10d, %r14d
	movl	%r12d, 104(%rsp)
	movl	%r11d, %r12d
	movl	%ebp, 108(%rsp)
	movl	%ebx, %ebp
	movl	%edx, %r8d
	negl	%r8d
	movl	%r8d, %ecx
	subl	60(%rsp), %ecx
	movl	12(%rsp), %ebx
	negl	%ebx
	movl	%ebx, %r9d
	subl	64(%rsp), %r9d
	movl	%r9d, 40(%rsp)
	movl	16(%rsp), %ebx
	negl	%ebx
	movl	%ebx, %r10d
	subl	68(%rsp), %r10d
	movl	%r10d, 44(%rsp)
	movl	20(%rsp), %ebx
	negl	%ebx
	movl	%ebx, %r11d
	subl	72(%rsp), %r11d
	movl	%r11d, 48(%rsp)
	movl	24(%rsp), %ebx
	negl	%ebx
	movl	%ebx, %r8d
	subl	76(%rsp), %r8d
	movl	%r8d, 52(%rsp)
	movl	28(%rsp), %ebx
	negl	%ebx
	subl	80(%rsp), %ebx
	movl	%ebx, 56(%rsp)
	movl	32(%rsp), %esi
	negl	%esi
	subl	84(%rsp), %esi
	negl	%edi
	subl	88(%rsp), %edi
	movl	%r15d, %r8d
	negl	%r8d
	subl	92(%rsp), %r8d
	movl	%r13d, %r9d
	negl	%r9d
	subl	96(%rsp), %r9d
	movl	%r14d, %r10d
	negl	%r10d
	subl	100(%rsp), %r10d
	movl	%r12d, %r11d
	negl	%r11d
	subl	104(%rsp), %r11d
	movl	%ebp, %ebx
	negl	%ebx
	subl	108(%rsp), %ebx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L157
	movl	%esi, 68(%rsp)
	movl	%edi, 72(%rsp)
	movl	%r8d, 76(%rsp)
	movl	%r9d, 80(%rsp)
	movl	%r10d, 60(%rsp)
	movl	%r11d, 64(%rsp)
.L156:
	leal	(%rcx,%rdx), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	addl	12(%rsp), %edi
	call	use_int@PLT
	movl	44(%rsp), %edi
	addl	16(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	addl	20(%rsp), %edi
	call	use_int@PLT
	movl	52(%rsp), %edi
	addl	24(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	addl	28(%rsp), %edi
	call	use_int@PLT
	movl	68(%rsp), %edi
	addl	32(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	addl	36(%rsp), %edi
	call	use_int@PLT
	movl	76(%rsp), %edi
	addl	%r15d, %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	addl	%r13d, %edi
	call	use_int@PLT
	movl	60(%rsp), %edi
	addl	%r14d, %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	addl	%r12d, %edi
	call	use_int@PLT
	leal	(%rbx,%rbp), %edi
	call	use_int@PLT
	addq	$120, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE101:
	.size	integer_add_12, .-integer_add_12
	.globl	integer_add_13
	.type	integer_add_13, @function
integer_add_13:
.LFB102:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$136, %rsp
	.cfi_def_cfa_offset 192
	movq	%rdi, %rdx
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	57(%rcx), %r15d
	leal	31(%rcx), %ebx
	movl	%ebx, 12(%rsp)
	movl	16(%rsi), %ecx
	leal	57(%rcx), %r14d
	leal	31(%rcx), %ebx
	movl	%ebx, 16(%rsp)
	movl	20(%rsi), %ecx
	leal	57(%rcx), %r13d
	leal	31(%rcx), %esi
	movl	%esi, 20(%rsp)
	movl	24(%rax), %ecx
	leal	57(%rcx), %r12d
	leal	31(%rcx), %ebx
	movl	%ebx, 24(%rsp)
	movl	28(%rax), %ecx
	leal	57(%rcx), %ebp
	leal	31(%rcx), %esi
	movl	%esi, 28(%rsp)
	movl	32(%rax), %ecx
	leal	57(%rcx), %ebx
	leal	31(%rcx), %esi
	movl	%esi, 32(%rsp)
	movl	36(%rax), %ecx
	leal	57(%rcx), %esi
	movl	%esi, 72(%rsp)
	leal	31(%rcx), %esi
	movl	%esi, 36(%rsp)
	movl	40(%rax), %ecx
	leal	57(%rcx), %edi
	movl	%edi, 80(%rsp)
	leal	31(%rcx), %esi
	movl	%esi, 40(%rsp)
	movl	44(%rax), %ecx
	leal	57(%rcx), %r8d
	movl	%r8d, 84(%rsp)
	leal	31(%rcx), %esi
	movl	%esi, 44(%rsp)
	movl	48(%rax), %ecx
	leal	57(%rcx), %r9d
	movl	%r9d, 88(%rsp)
	leal	31(%rcx), %esi
	movl	%esi, 48(%rsp)
	movl	52(%rax), %ecx
	leal	57(%rcx), %r10d
	movl	%r10d, 92(%rsp)
	leal	31(%rcx), %esi
	movl	%esi, 52(%rsp)
	movl	56(%rax), %ecx
	leal	57(%rcx), %r11d
	movl	%r11d, 96(%rsp)
	leal	31(%rcx), %esi
	movl	%esi, 56(%rsp)
	movl	60(%rax), %ecx
	leal	57(%rcx), %esi
	movl	%esi, 76(%rsp)
	addl	$31, %ecx
	movl	%ecx, 60(%rsp)
	movl	64(%rax), %eax
	leal	57(%rax), %ecx
	movl	%ecx, 68(%rsp)
	addl	$31, %eax
	movl	%eax, 64(%rsp)
	testq	%rdx, %rdx
	je	.L161
	leaq	-1(%rdx), %rax
	movl	72(%rsp), %edx
	movl	%edi, %ecx
	movl	%r8d, %esi
	movl	%r9d, %edi
	movl	%r10d, %r8d
	movl	%r11d, %r9d
	movl	76(%rsp), %r10d
.L162:
	movl	12(%rsp), %r11d
	movl	%r11d, 72(%rsp)
	movl	%r15d, 12(%rsp)
	movl	16(%rsp), %r11d
	movl	%r11d, 76(%rsp)
	movl	%r14d, 16(%rsp)
	movl	20(%rsp), %r11d
	movl	%r11d, 80(%rsp)
	movl	%r13d, 20(%rsp)
	movl	24(%rsp), %r11d
	movl	%r11d, 84(%rsp)
	movl	%r12d, 24(%rsp)
	movl	28(%rsp), %r11d
	movl	%r11d, 88(%rsp)
	movl	%ebp, 28(%rsp)
	movl	32(%rsp), %r11d
	movl	%r11d, 92(%rsp)
	movl	%ebx, 32(%rsp)
	movl	36(%rsp), %r11d
	movl	%r11d, 96(%rsp)
	movl	%edx, 36(%rsp)
	movl	40(%rsp), %r11d
	movl	%r11d, 100(%rsp)
	movl	%ecx, 40(%rsp)
	movl	44(%rsp), %r11d
	movl	%r11d, 104(%rsp)
	movl	%esi, 44(%rsp)
	movl	48(%rsp), %r11d
	movl	%r11d, 108(%rsp)
	movl	%edi, 48(%rsp)
	movl	52(%rsp), %r11d
	movl	%r11d, 112(%rsp)
	movl	%r8d, 52(%rsp)
	movl	56(%rsp), %r11d
	movl	%r11d, 116(%rsp)
	movl	%r9d, 56(%rsp)
	movl	60(%rsp), %r11d
	movl	%r11d, 120(%rsp)
	movl	%r10d, 60(%rsp)
	movl	64(%rsp), %r11d
	movl	%r11d, 124(%rsp)
	movl	68(%rsp), %r11d
	movl	%r11d, 64(%rsp)
	negl	%r15d
	subl	72(%rsp), %r15d
	negl	%r14d
	subl	76(%rsp), %r14d
	negl	%r13d
	subl	80(%rsp), %r13d
	negl	%r12d
	subl	84(%rsp), %r12d
	negl	%ebp
	subl	88(%rsp), %ebp
	negl	%ebx
	subl	92(%rsp), %ebx
	negl	%edx
	subl	96(%rsp), %edx
	negl	%ecx
	subl	100(%rsp), %ecx
	negl	%esi
	subl	104(%rsp), %esi
	negl	%edi
	subl	108(%rsp), %edi
	negl	%r8d
	subl	112(%rsp), %r8d
	negl	%r9d
	subl	116(%rsp), %r9d
	negl	%r10d
	subl	120(%rsp), %r10d
	negl	%r11d
	subl	124(%rsp), %r11d
	movl	%r11d, 68(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L162
	movl	%edx, 72(%rsp)
	movl	%ecx, 80(%rsp)
	movl	%esi, 84(%rsp)
	movl	%edi, 88(%rsp)
	movl	%r8d, 92(%rsp)
	movl	%r9d, 96(%rsp)
	movl	%r10d, 76(%rsp)
.L161:
	movl	12(%rsp), %edi
	addl	%r15d, %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	addl	%r14d, %edi
	call	use_int@PLT
	movl	20(%rsp), %edi
	addl	%r13d, %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	addl	%r12d, %edi
	call	use_int@PLT
	movl	28(%rsp), %edi
	addl	%ebp, %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	addl	%ebx, %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	addl	36(%rsp), %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	addl	40(%rsp), %edi
	call	use_int@PLT
	movl	84(%rsp), %edi
	addl	44(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	addl	48(%rsp), %edi
	call	use_int@PLT
	movl	92(%rsp), %edi
	addl	52(%rsp), %edi
	call	use_int@PLT
	movl	96(%rsp), %edi
	addl	56(%rsp), %edi
	call	use_int@PLT
	movl	76(%rsp), %edi
	addl	60(%rsp), %edi
	call	use_int@PLT
	movl	68(%rsp), %edi
	addl	64(%rsp), %edi
	call	use_int@PLT
	addq	$136, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE102:
	.size	integer_add_13, .-integer_add_13
	.globl	integer_add_14
	.type	integer_add_14, @function
integer_add_14:
.LFB103:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$152, %rsp
	.cfi_def_cfa_offset 208
	movq	%rdi, %rdx
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	57(%rcx), %r15d
	leal	31(%rcx), %esi
	movl	%esi, 36(%rsp)
	movl	16(%rax), %ecx
	leal	57(%rcx), %r14d
	leal	31(%rcx), %esi
	movl	%esi, 40(%rsp)
	movl	20(%rax), %ecx
	leal	57(%rcx), %r13d
	leal	31(%rcx), %esi
	movl	%esi, 44(%rsp)
	movl	24(%rax), %ecx
	leal	57(%rcx), %r12d
	leal	31(%rcx), %esi
	movl	%esi, 48(%rsp)
	movl	28(%rax), %ecx
	leal	57(%rcx), %ebp
	leal	31(%rcx), %esi
	movl	%esi, 52(%rsp)
	movl	32(%rax), %ecx
	leal	57(%rcx), %ebx
	leal	31(%rcx), %esi
	movl	%esi, 12(%rsp)
	movl	36(%rax), %ecx
	leal	57(%rcx), %esi
	movl	%esi, 84(%rsp)
	leal	31(%rcx), %esi
	movl	%esi, 16(%rsp)
	movl	40(%rax), %ecx
	leal	57(%rcx), %esi
	movl	%esi, 88(%rsp)
	leal	31(%rcx), %esi
	movl	%esi, 20(%rsp)
	movl	44(%rax), %ecx
	leal	57(%rcx), %edi
	movl	%edi, 32(%rsp)
	leal	31(%rcx), %esi
	movl	%esi, 56(%rsp)
	movl	48(%rax), %ecx
	leal	57(%rcx), %r8d
	movl	%r8d, 24(%rsp)
	leal	31(%rcx), %esi
	movl	%esi, 60(%rsp)
	movl	52(%rax), %ecx
	leal	57(%rcx), %r9d
	movl	%r9d, 96(%rsp)
	leal	31(%rcx), %esi
	movl	%esi, 64(%rsp)
	movl	56(%rax), %ecx
	leal	57(%rcx), %r10d
	movl	%r10d, 100(%rsp)
	leal	31(%rcx), %esi
	movl	%esi, 68(%rsp)
	movl	60(%rax), %ecx
	leal	57(%rcx), %r11d
	movl	%r11d, 104(%rsp)
	leal	31(%rcx), %esi
	movl	%esi, 72(%rsp)
	movl	64(%rax), %ecx
	leal	57(%rcx), %esi
	movl	%esi, 92(%rsp)
	addl	$31, %ecx
	movl	%ecx, 76(%rsp)
	movl	68(%rax), %eax
	leal	57(%rax), %ecx
	movl	%ecx, 108(%rsp)
	addl	$31, %eax
	movl	%eax, 80(%rsp)
	testq	%rdx, %rdx
	je	.L166
	leaq	-1(%rdx), %rax
	movq	%rax, 24(%rsp)
	movl	%r8d, %esi
	movl	%r9d, %edi
	movl	%r10d, %r8d
	movl	%r11d, %r9d
	movl	92(%rsp), %r10d
	movl	%ecx, %r11d
.L167:
	movl	36(%rsp), %eax
	movl	%eax, 92(%rsp)
	movl	%r15d, 36(%rsp)
	movl	40(%rsp), %eax
	movl	%r14d, 40(%rsp)
	movl	44(%rsp), %edx
	movl	%edx, 96(%rsp)
	movl	%r13d, 44(%rsp)
	movl	48(%rsp), %edx
	movl	%r12d, 48(%rsp)
	movl	52(%rsp), %ecx
	movl	%ecx, 100(%rsp)
	movl	%ebp, 52(%rsp)
	movl	12(%rsp), %ecx
	movl	%ecx, 104(%rsp)
	movl	%ebx, 12(%rsp)
	movl	16(%rsp), %ebx
	movl	%ebx, 108(%rsp)
	movl	84(%rsp), %ebx
	movl	%ebx, 16(%rsp)
	movl	20(%rsp), %ecx
	movl	%ecx, 112(%rsp)
	movl	88(%rsp), %ecx
	movl	%ecx, 20(%rsp)
	movl	56(%rsp), %ecx
	movl	%ecx, 116(%rsp)
	movl	32(%rsp), %ecx
	movl	%ecx, 56(%rsp)
	movl	60(%rsp), %ebx
	movl	%ebx, 120(%rsp)
	movl	%esi, 60(%rsp)
	movl	64(%rsp), %ebx
	movl	%ebx, 124(%rsp)
	movl	%edi, 64(%rsp)
	movl	68(%rsp), %ebx
	movl	%ebx, 128(%rsp)
	movl	%r8d, 68(%rsp)
	movl	72(%rsp), %ebx
	movl	%ebx, 132(%rsp)
	movl	%r9d, 72(%rsp)
	movl	76(%rsp), %ebx
	movl	%ebx, 136(%rsp)
	movl	%r10d, 76(%rsp)
	movl	80(%rsp), %ebx
	movl	%ebx, 140(%rsp)
	movl	%r11d, 80(%rsp)
	negl	%r15d
	subl	92(%rsp), %r15d
	negl	%r14d
	subl	%eax, %r14d
	negl	%r13d
	subl	96(%rsp), %r13d
	negl	%r12d
	subl	%edx, %r12d
	negl	%ebp
	subl	100(%rsp), %ebp
	movl	12(%rsp), %ebx
	negl	%ebx
	subl	104(%rsp), %ebx
	movl	16(%rsp), %edx
	negl	%edx
	subl	108(%rsp), %edx
	movl	%edx, 84(%rsp)
	movl	20(%rsp), %edx
	negl	%edx
	subl	112(%rsp), %edx
	movl	%edx, 88(%rsp)
	negl	%ecx
	subl	116(%rsp), %ecx
	movl	%ecx, 32(%rsp)
	negl	%esi
	subl	120(%rsp), %esi
	negl	%edi
	subl	124(%rsp), %edi
	negl	%r8d
	subl	128(%rsp), %r8d
	negl	%r9d
	subl	132(%rsp), %r9d
	negl	%r10d
	subl	136(%rsp), %r10d
	negl	%r11d
	subl	140(%rsp), %r11d
	subq	$1, 24(%rsp)
	movq	24(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L167
	movl	%esi, 24(%rsp)
	movl	%edi, 96(%rsp)
	movl	%r8d, 100(%rsp)
	movl	%r9d, 104(%rsp)
	movl	%r10d, 92(%rsp)
	movl	%r11d, 108(%rsp)
.L166:
	movl	36(%rsp), %edi
	addl	%r15d, %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	addl	%r14d, %edi
	call	use_int@PLT
	movl	44(%rsp), %edi
	addl	%r13d, %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	addl	%r12d, %edi
	call	use_int@PLT
	movl	52(%rsp), %edi
	addl	%ebp, %edi
	call	use_int@PLT
	movl	12(%rsp), %edi
	addl	%ebx, %edi
	call	use_int@PLT
	movl	84(%rsp), %edi
	addl	16(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	addl	20(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	addl	56(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	addl	60(%rsp), %edi
	call	use_int@PLT
	movl	96(%rsp), %edi
	addl	64(%rsp), %edi
	call	use_int@PLT
	movl	100(%rsp), %edi
	addl	68(%rsp), %edi
	call	use_int@PLT
	movl	104(%rsp), %edi
	addl	72(%rsp), %edi
	call	use_int@PLT
	movl	92(%rsp), %edi
	addl	76(%rsp), %edi
	call	use_int@PLT
	movl	108(%rsp), %edi
	addl	80(%rsp), %edi
	call	use_int@PLT
	addq	$152, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE103:
	.size	integer_add_14, .-integer_add_14
	.globl	integer_add_15
	.type	integer_add_15, @function
integer_add_15:
.LFB104:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$152, %rsp
	.cfi_def_cfa_offset 208
	movq	%rdi, %rdx
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	57(%rcx), %r15d
	leal	31(%rcx), %esi
	movl	%esi, 32(%rsp)
	movl	16(%rax), %ecx
	leal	57(%rcx), %r14d
	leal	31(%rcx), %esi
	movl	%esi, 4(%rsp)
	movl	20(%rax), %ecx
	leal	57(%rcx), %r13d
	leal	31(%rcx), %esi
	movl	%esi, 36(%rsp)
	movl	24(%rax), %ecx
	leal	57(%rcx), %r12d
	leal	31(%rcx), %esi
	movl	%esi, 40(%rsp)
	movl	28(%rax), %ecx
	leal	57(%rcx), %ebp
	addl	$31, %ecx
	movl	%ecx, 44(%rsp)
	movl	32(%rax), %ecx
	leal	57(%rcx), %ebx
	leal	31(%rcx), %esi
	movl	%esi, 8(%rsp)
	movl	36(%rax), %ecx
	leal	57(%rcx), %esi
	movl	%esi, 76(%rsp)
	addl	$31, %ecx
	movl	%ecx, 12(%rsp)
	movl	40(%rax), %ecx
	leal	57(%rcx), %esi
	movl	%esi, 80(%rsp)
	addl	$31, %ecx
	movl	%ecx, 16(%rsp)
	movl	44(%rax), %ecx
	leal	57(%rcx), %esi
	movl	%esi, 84(%rsp)
	addl	$31, %ecx
	movl	%ecx, 20(%rsp)
	movl	48(%rax), %ecx
	leal	57(%rcx), %edi
	movl	%edi, 24(%rsp)
	leal	31(%rcx), %esi
	movl	%esi, 48(%rsp)
	movl	52(%rax), %ecx
	leal	57(%rcx), %r8d
	movl	%r8d, 96(%rsp)
	addl	$31, %ecx
	movl	%ecx, 52(%rsp)
	movl	56(%rax), %ecx
	leal	57(%rcx), %r9d
	movl	%r9d, 100(%rsp)
	leal	31(%rcx), %esi
	movl	%esi, 56(%rsp)
	movl	60(%rax), %ecx
	leal	57(%rcx), %r10d
	movl	%r10d, 104(%rsp)
	addl	$31, %ecx
	movl	%ecx, 60(%rsp)
	movl	64(%rax), %ecx
	leal	57(%rcx), %r11d
	movl	%r11d, 108(%rsp)
	leal	31(%rcx), %esi
	movl	%esi, 64(%rsp)
	movl	68(%rax), %ecx
	leal	57(%rcx), %esi
	movl	%esi, 88(%rsp)
	addl	$31, %ecx
	movl	%ecx, 68(%rsp)
	movl	72(%rax), %eax
	leal	57(%rax), %ecx
	movl	%ecx, 92(%rsp)
	addl	$31, %eax
	movl	%eax, 72(%rsp)
	testq	%rdx, %rdx
	je	.L171
	leaq	-1(%rdx), %rcx
	movq	%rcx, 24(%rsp)
	movl	84(%rsp), %eax
	movl	%edi, %edx
	movl	%r8d, %esi
	movl	%r9d, %edi
	movl	%r10d, %r8d
	movl	%r11d, %r9d
	movl	88(%rsp), %r10d
	movl	92(%rsp), %r11d
.L172:
	movl	32(%rsp), %ecx
	movl	%ecx, 84(%rsp)
	movl	%r15d, 32(%rsp)
	movl	4(%rsp), %ecx
	movl	%r14d, 4(%rsp)
	movl	36(%rsp), %r14d
	movl	%r14d, 88(%rsp)
	movl	%r13d, 36(%rsp)
	movl	40(%rsp), %r14d
	movl	%r14d, 92(%rsp)
	movl	%r12d, 40(%rsp)
	movl	44(%rsp), %r14d
	movl	%r14d, 96(%rsp)
	movl	%ebp, 44(%rsp)
	movl	8(%rsp), %r14d
	movl	%r14d, 100(%rsp)
	movl	%ebx, 8(%rsp)
	movl	12(%rsp), %ebx
	movl	%ebx, 104(%rsp)
	movl	76(%rsp), %ebx
	movl	%ebx, 12(%rsp)
	movl	16(%rsp), %ebx
	movl	%ebx, 108(%rsp)
	movl	80(%rsp), %r14d
	movl	%r14d, 16(%rsp)
	movl	20(%rsp), %ebx
	movl	%ebx, 112(%rsp)
	movl	%eax, 20(%rsp)
	movl	48(%rsp), %eax
	movl	%eax, 116(%rsp)
	movl	%edx, 48(%rsp)
	movl	52(%rsp), %eax
	movl	%eax, 120(%rsp)
	movl	%esi, 52(%rsp)
	movl	56(%rsp), %eax
	movl	%eax, 124(%rsp)
	movl	%edi, 56(%rsp)
	movl	60(%rsp), %ebx
	movl	%ebx, 128(%rsp)
	movl	%r8d, 60(%rsp)
	movl	64(%rsp), %eax
	movl	%eax, 132(%rsp)
	movl	%r9d, 64(%rsp)
	movl	68(%rsp), %ebx
	movl	%ebx, 136(%rsp)
	movl	%r10d, 68(%rsp)
	movl	72(%rsp), %r14d
	movl	%r14d, 140(%rsp)
	movl	%r11d, 72(%rsp)
	negl	%r15d
	subl	84(%rsp), %r15d
	movl	4(%rsp), %r14d
	negl	%r14d
	subl	%ecx, %r14d
	negl	%r13d
	subl	88(%rsp), %r13d
	negl	%r12d
	subl	92(%rsp), %r12d
	negl	%ebp
	subl	96(%rsp), %ebp
	movl	8(%rsp), %ebx
	negl	%ebx
	subl	100(%rsp), %ebx
	movl	12(%rsp), %eax
	negl	%eax
	subl	104(%rsp), %eax
	movl	%eax, 76(%rsp)
	movl	16(%rsp), %eax
	negl	%eax
	subl	108(%rsp), %eax
	movl	%eax, 80(%rsp)
	movl	20(%rsp), %eax
	negl	%eax
	subl	112(%rsp), %eax
	negl	%edx
	subl	116(%rsp), %edx
	negl	%esi
	subl	120(%rsp), %esi
	negl	%edi
	subl	124(%rsp), %edi
	negl	%r8d
	subl	128(%rsp), %r8d
	negl	%r9d
	subl	132(%rsp), %r9d
	negl	%r10d
	subl	136(%rsp), %r10d
	negl	%r11d
	subl	140(%rsp), %r11d
	subq	$1, 24(%rsp)
	movq	24(%rsp), %rcx
	cmpq	$-1, %rcx
	jne	.L172
	movl	%eax, 84(%rsp)
	movl	%edx, 24(%rsp)
	movl	%esi, 96(%rsp)
	movl	%edi, 100(%rsp)
	movl	%r8d, 104(%rsp)
	movl	%r9d, 108(%rsp)
	movl	%r10d, 88(%rsp)
	movl	%r11d, 92(%rsp)
.L171:
	movl	32(%rsp), %edi
	addl	%r15d, %edi
	call	use_int@PLT
	movl	4(%rsp), %edi
	addl	%r14d, %edi
	call	use_int@PLT
	movl	36(%rsp), %edi
	addl	%r13d, %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	addl	%r12d, %edi
	call	use_int@PLT
	movl	44(%rsp), %edi
	addl	%ebp, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	addl	%ebx, %edi
	call	use_int@PLT
	movl	76(%rsp), %edi
	addl	12(%rsp), %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	addl	16(%rsp), %edi
	call	use_int@PLT
	movl	84(%rsp), %edi
	addl	20(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	addl	48(%rsp), %edi
	call	use_int@PLT
	movl	96(%rsp), %edi
	addl	52(%rsp), %edi
	call	use_int@PLT
	movl	100(%rsp), %edi
	addl	56(%rsp), %edi
	call	use_int@PLT
	movl	104(%rsp), %edi
	addl	60(%rsp), %edi
	call	use_int@PLT
	movl	108(%rsp), %edi
	addl	64(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	addl	68(%rsp), %edi
	call	use_int@PLT
	movl	92(%rsp), %edi
	addl	72(%rsp), %edi
	call	use_int@PLT
	addq	$152, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE104:
	.size	integer_add_15, .-integer_add_15
	.globl	integer_mul_0
	.type	integer_mul_0, @function
integer_mul_0:
.LFB105:
	.cfi_startproc
	endbr64
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movq	%rdi, %r8
	movl	12(%rsi), %eax
	leal	37432(%rax), %edx
	addl	$5, %eax
	movl	%edx, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	movl	%edi, %ecx
	subl	%edx, %ecx
	leaq	-1(%r8), %rdx
	testq	%r8, %r8
	je	.L176
.L177:
	subl	%ecx, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	imull	%eax, %edi
	subq	$1, %rdx
	cmpq	$-1, %rdx
	jne	.L177
.L176:
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE105:
	.size	integer_mul_0, .-integer_mul_0
	.globl	integer_mul_1
	.type	integer_mul_1, @function
integer_mul_1:
.LFB106:
	.cfi_startproc
	endbr64
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movq	%rdi, %r9
	movl	12(%rsi), %edx
	leal	37432(%rdx), %ecx
	addl	$5, %edx
	movl	%ecx, %edi
	imull	%edx, %edi
	imull	%edx, %edi
	imull	%edx, %edi
	imull	%edx, %edi
	imull	%edx, %edi
	imull	%edx, %edi
	imull	%edx, %edi
	imull	%edx, %edi
	imull	%edx, %edi
	imull	%edx, %edi
	movl	%edi, %r8d
	subl	%ecx, %r8d
	movl	16(%rsi), %eax
	leal	37431(%rax), %ecx
	addl	$4, %eax
	movl	%ecx, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	movl	%ebx, %esi
	subl	%ecx, %esi
	leaq	-1(%r9), %rcx
	testq	%r9, %r9
	je	.L181
.L182:
	subl	%r8d, %edi
	subl	%esi, %ebx
	imull	%edx, %edi
	imull	%eax, %ebx
	imull	%edx, %edi
	imull	%eax, %ebx
	imull	%edx, %edi
	imull	%eax, %ebx
	imull	%edx, %edi
	imull	%eax, %ebx
	imull	%edx, %edi
	imull	%eax, %ebx
	imull	%edx, %edi
	imull	%eax, %ebx
	imull	%edx, %edi
	imull	%eax, %ebx
	imull	%edx, %edi
	imull	%eax, %ebx
	imull	%edx, %edi
	imull	%eax, %ebx
	imull	%edx, %edi
	imull	%eax, %ebx
	subq	$1, %rcx
	cmpq	$-1, %rcx
	jne	.L182
.L181:
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE106:
	.size	integer_mul_1, .-integer_mul_1
	.globl	integer_mul_2
	.type	integer_mul_2, @function
integer_mul_2:
.LFB107:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	pushq	%rbx
	.cfi_def_cfa_offset 24
	.cfi_offset 3, -24
	subq	$8, %rsp
	.cfi_def_cfa_offset 32
	movq	%rdi, %r11
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	37432(%rcx), %edx
	addl	$5, %ecx
	movl	%edx, %edi
	imull	%ecx, %edi
	imull	%ecx, %edi
	imull	%ecx, %edi
	imull	%ecx, %edi
	imull	%ecx, %edi
	imull	%ecx, %edi
	imull	%ecx, %edi
	imull	%ecx, %edi
	imull	%ecx, %edi
	imull	%ecx, %edi
	movl	%edi, %r10d
	subl	%edx, %r10d
	movl	16(%rsi), %edx
	leal	37431(%rdx), %esi
	addl	$4, %edx
	movl	%esi, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	movl	%ebp, %r9d
	subl	%esi, %r9d
	movl	20(%rax), %eax
	leal	37430(%rax), %esi
	addl	$3, %eax
	movl	%esi, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	movl	%ebx, %r8d
	subl	%esi, %r8d
	leaq	-1(%r11), %rsi
	testq	%r11, %r11
	je	.L186
.L187:
	subl	%r10d, %edi
	subl	%r9d, %ebp
	subl	%r8d, %ebx
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	subq	$1, %rsi
	cmpq	$-1, %rsi
	jne	.L187
.L186:
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 24
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE107:
	.size	integer_mul_2, .-integer_mul_2
	.globl	integer_mul_3
	.type	integer_mul_3, @function
integer_mul_3:
.LFB108:
	.cfi_startproc
	endbr64
	pushq	%r14
	.cfi_def_cfa_offset 16
	.cfi_offset 14, -16
	pushq	%r13
	.cfi_def_cfa_offset 24
	.cfi_offset 13, -24
	pushq	%r12
	.cfi_def_cfa_offset 32
	.cfi_offset 12, -32
	pushq	%rbp
	.cfi_def_cfa_offset 40
	.cfi_offset 6, -40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	.cfi_offset 3, -48
	movq	%rdi, %r14
	movq	%rsi, %rax
	movl	12(%rsi), %esi
	leal	37432(%rsi), %edx
	addl	$5, %esi
	movl	%edx, %edi
	imull	%esi, %edi
	imull	%esi, %edi
	imull	%esi, %edi
	imull	%esi, %edi
	imull	%esi, %edi
	imull	%esi, %edi
	imull	%esi, %edi
	imull	%esi, %edi
	imull	%esi, %edi
	imull	%esi, %edi
	movl	%edi, %r11d
	subl	%edx, %r11d
	movl	16(%rax), %ecx
	leal	37431(%rcx), %edx
	addl	$4, %ecx
	movl	%edx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	movl	%r12d, %r10d
	subl	%edx, %r10d
	movl	20(%rax), %edx
	leal	37430(%rdx), %r8d
	addl	$3, %edx
	movl	%r8d, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	movl	%ebp, %r9d
	subl	%r8d, %r9d
	movl	24(%rax), %eax
	leal	37429(%rax), %r8d
	addl	$2, %eax
	movl	%r8d, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	movl	%ebx, %r13d
	subl	%r8d, %r13d
	leaq	-1(%r14), %r8
	testq	%r14, %r14
	je	.L191
.L192:
	subl	%r11d, %edi
	subl	%r10d, %r12d
	subl	%r9d, %ebp
	subl	%r13d, %ebx
	imull	%esi, %edi
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%esi, %edi
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%esi, %edi
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%esi, %edi
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%esi, %edi
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%esi, %edi
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%esi, %edi
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%esi, %edi
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%esi, %edi
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%esi, %edi
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	subq	$1, %r8
	cmpq	$-1, %r8
	jne	.L192
.L191:
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%rbp
	.cfi_def_cfa_offset 32
	popq	%r12
	.cfi_def_cfa_offset 24
	popq	%r13
	.cfi_def_cfa_offset 16
	popq	%r14
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE108:
	.size	integer_mul_3, .-integer_mul_3
	.globl	integer_mul_4
	.type	integer_mul_4, @function
integer_mul_4:
.LFB109:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$24, %rsp
	.cfi_def_cfa_offset 80
	movq	%rdi, 8(%rsp)
	movq	%rsi, %rax
	movl	12(%rsi), %r8d
	leal	37432(%r8), %edx
	addl	$5, %r8d
	movl	%edx, %ecx
	imull	%r8d, %ecx
	movl	%ecx, %edi
	imull	%r8d, %edi
	imull	%r8d, %edi
	imull	%r8d, %edi
	imull	%r8d, %edi
	imull	%r8d, %edi
	imull	%r8d, %edi
	imull	%r8d, %edi
	imull	%r8d, %edi
	imull	%r8d, %edi
	movl	%edi, %r11d
	subl	%edx, %r11d
	movl	16(%rsi), %esi
	leal	37431(%rsi), %edx
	addl	$4, %esi
	movl	%edx, %r13d
	imull	%esi, %r13d
	imull	%esi, %r13d
	imull	%esi, %r13d
	imull	%esi, %r13d
	imull	%esi, %r13d
	imull	%esi, %r13d
	imull	%esi, %r13d
	imull	%esi, %r13d
	imull	%esi, %r13d
	imull	%esi, %r13d
	movl	%r13d, %r15d
	subl	%edx, %r15d
	movl	20(%rax), %ecx
	leal	37430(%rcx), %edx
	addl	$3, %ecx
	movl	%edx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	imull	%ecx, %r12d
	movl	%r12d, %ebx
	subl	%edx, %ebx
	movl	%ebx, 4(%rsp)
	movl	24(%rax), %edx
	leal	37429(%rdx), %r9d
	addl	$2, %edx
	movl	%r9d, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	movl	%ebp, %r14d
	subl	%r9d, %r14d
	movl	28(%rax), %eax
	leal	37428(%rax), %r9d
	addl	$1, %eax
	movl	%r9d, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	movl	%ebx, %r10d
	subl	%r9d, %r10d
	movq	8(%rsp), %r9
	subq	$1, %r9
	cmpq	$0, 8(%rsp)
	je	.L196
.L197:
	subl	%r11d, %edi
	subl	%r15d, %r13d
	subl	4(%rsp), %r12d
	subl	%r14d, %ebp
	subl	%r10d, %ebx
	imull	%r8d, %edi
	imull	%esi, %r13d
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r8d, %edi
	imull	%esi, %r13d
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r8d, %edi
	imull	%esi, %r13d
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r8d, %edi
	imull	%esi, %r13d
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r8d, %edi
	imull	%esi, %r13d
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r8d, %edi
	imull	%esi, %r13d
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r8d, %edi
	imull	%esi, %r13d
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r8d, %edi
	imull	%esi, %r13d
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r8d, %edi
	imull	%esi, %r13d
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r8d, %edi
	imull	%esi, %r13d
	imull	%ecx, %r12d
	imull	%edx, %ebp
	imull	%eax, %ebx
	subq	$1, %r9
	cmpq	$-1, %r9
	jne	.L197
.L196:
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE109:
	.size	integer_mul_4, .-integer_mul_4
	.globl	integer_mul_5
	.type	integer_mul_5, @function
integer_mul_5:
.LFB110:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$40, %rsp
	.cfi_def_cfa_offset 96
	movq	%rdi, %r10
	movq	%rsi, %r9
	movl	12(%rsi), %r8d
	leal	37432(%r8), %eax
	addl	$5, %r8d
	movl	%eax, %edi
	imull	%r8d, %edi
	imull	%r8d, %edi
	imull	%r8d, %edi
	imull	%r8d, %edi
	imull	%r8d, %edi
	imull	%r8d, %edi
	imull	%r8d, %edi
	imull	%r8d, %edi
	imull	%r8d, %edi
	imull	%r8d, %edi
	movl	%edi, %edx
	subl	%eax, %edx
	movl	%edx, 28(%rsp)
	movl	16(%rsi), %esi
	leal	37431(%rsi), %eax
	addl	$4, %esi
	movl	%eax, %r12d
	imull	%esi, %r12d
	imull	%esi, %r12d
	imull	%esi, %r12d
	imull	%esi, %r12d
	imull	%esi, %r12d
	imull	%esi, %r12d
	imull	%esi, %r12d
	imull	%esi, %r12d
	imull	%esi, %r12d
	imull	%esi, %r12d
	movl	%r12d, %ecx
	subl	%eax, %ecx
	movl	%ecx, 8(%rsp)
	movl	20(%r9), %ecx
	leal	37430(%rcx), %edx
	addl	$3, %ecx
	movl	%edx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	movl	%eax, %r13d
	subl	%edx, %eax
	movl	%eax, 12(%rsp)
	movl	24(%r9), %edx
	leal	37429(%rdx), %eax
	addl	$2, %edx
	movl	%eax, %r11d
	imull	%edx, %r11d
	imull	%edx, %r11d
	imull	%edx, %r11d
	imull	%edx, %r11d
	imull	%edx, %r11d
	imull	%edx, %r11d
	imull	%edx, %r11d
	imull	%edx, %r11d
	imull	%edx, %r11d
	movl	%r11d, %r14d
	imull	%edx, %r14d
	movl	%r14d, %ebx
	subl	%eax, %ebx
	movl	%ebx, 16(%rsp)
	movl	28(%r9), %eax
	leal	37428(%rax), %r11d
	addl	$1, %eax
	movl	%r11d, %ebp
	imull	%eax, %ebp
	imull	%eax, %ebp
	imull	%eax, %ebp
	imull	%eax, %ebp
	imull	%eax, %ebp
	imull	%eax, %ebp
	imull	%eax, %ebp
	imull	%eax, %ebp
	imull	%eax, %ebp
	imull	%eax, %ebp
	movl	%ebp, %ebx
	subl	%r11d, %ebx
	movl	%ebx, 20(%rsp)
	movl	32(%r9), %r9d
	leal	37427(%r9), %r11d
	movl	%r9d, %ebx
	imull	%r11d, %ebx
	imull	%r9d, %ebx
	imull	%r9d, %ebx
	imull	%r9d, %ebx
	imull	%r9d, %ebx
	imull	%r9d, %ebx
	imull	%r9d, %ebx
	imull	%r9d, %ebx
	imull	%r9d, %ebx
	imull	%r9d, %ebx
	movl	%ebx, %r15d
	subl	%r11d, %r15d
	movl	%r15d, 24(%rsp)
	testq	%r10, %r10
	je	.L201
	leaq	-1(%r10), %r15
	movl	28(%rsp), %r11d
.L202:
	subl	%r11d, %edi
	movl	%r12d, %r10d
	subl	8(%rsp), %r10d
	subl	12(%rsp), %r13d
	subl	16(%rsp), %r14d
	subl	20(%rsp), %ebp
	subl	24(%rsp), %ebx
	imull	%r8d, %edi
	imull	%esi, %r10d
	movl	%r10d, %r12d
	imull	%ecx, %r13d
	imull	%edx, %r14d
	imull	%eax, %ebp
	imull	%r9d, %ebx
	imull	%r8d, %edi
	imull	%esi, %r12d
	imull	%ecx, %r13d
	imull	%edx, %r14d
	imull	%eax, %ebp
	imull	%r9d, %ebx
	imull	%r8d, %edi
	imull	%esi, %r12d
	imull	%ecx, %r13d
	imull	%edx, %r14d
	imull	%eax, %ebp
	imull	%r9d, %ebx
	imull	%r8d, %edi
	imull	%esi, %r12d
	imull	%ecx, %r13d
	imull	%edx, %r14d
	imull	%eax, %ebp
	imull	%r9d, %ebx
	imull	%r8d, %edi
	imull	%esi, %r12d
	imull	%ecx, %r13d
	imull	%edx, %r14d
	imull	%eax, %ebp
	imull	%r9d, %ebx
	imull	%r8d, %edi
	imull	%esi, %r12d
	imull	%ecx, %r13d
	imull	%edx, %r14d
	imull	%eax, %ebp
	imull	%r9d, %ebx
	imull	%r8d, %edi
	imull	%esi, %r12d
	imull	%ecx, %r13d
	imull	%edx, %r14d
	imull	%eax, %ebp
	imull	%r9d, %ebx
	imull	%r8d, %edi
	imull	%esi, %r12d
	imull	%ecx, %r13d
	imull	%edx, %r14d
	imull	%eax, %ebp
	imull	%r9d, %ebx
	imull	%r8d, %edi
	imull	%esi, %r12d
	imull	%ecx, %r13d
	imull	%edx, %r14d
	imull	%eax, %ebp
	imull	%r9d, %ebx
	imull	%r8d, %edi
	imull	%esi, %r12d
	imull	%ecx, %r13d
	imull	%edx, %r14d
	imull	%eax, %ebp
	imull	%r9d, %ebx
	subq	$1, %r15
	cmpq	$-1, %r15
	jne	.L202
.L201:
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE110:
	.size	integer_mul_5, .-integer_mul_5
	.globl	integer_mul_6
	.type	integer_mul_6, @function
integer_mul_6:
.LFB111:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$56, %rsp
	.cfi_def_cfa_offset 112
	movq	%rdi, %r11
	movq	%rsi, %rax
	movl	12(%rsi), %r8d
	leal	37432(%r8), %ecx
	addl	$5, %r8d
	movl	%ecx, %edx
	imull	%r8d, %edx
	imull	%r8d, %edx
	imull	%r8d, %edx
	imull	%r8d, %edx
	imull	%r8d, %edx
	imull	%r8d, %edx
	imull	%r8d, %edx
	imull	%r8d, %edx
	imull	%r8d, %edx
	imull	%r8d, %edx
	movl	%edx, %r14d
	movl	%edx, %esi
	subl	%ecx, %esi
	movl	%esi, 12(%rsp)
	movl	16(%rax), %edi
	leal	37431(%rdi), %ecx
	addl	$4, %edi
	movl	%ecx, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	movl	%edx, %r13d
	movl	%edx, %esi
	subl	%ecx, %esi
	movl	%esi, 16(%rsp)
	movl	20(%rax), %esi
	leal	37430(%rsi), %ecx
	addl	$3, %esi
	movl	%ecx, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	movl	%edx, %r12d
	subl	%ecx, %edx
	movl	%edx, 20(%rsp)
	movl	24(%rax), %ecx
	leal	37429(%rcx), %edx
	addl	$2, %ecx
	movl	%edx, %r9d
	imull	%ecx, %r9d
	imull	%ecx, %r9d
	imull	%ecx, %r9d
	imull	%ecx, %r9d
	imull	%ecx, %r9d
	imull	%ecx, %r9d
	imull	%ecx, %r9d
	imull	%ecx, %r9d
	imull	%ecx, %r9d
	imull	%ecx, %r9d
	movl	%r9d, 40(%rsp)
	movl	%r9d, %ebx
	subl	%edx, %ebx
	movl	%ebx, 24(%rsp)
	movl	28(%rax), %edx
	leal	37428(%rdx), %r9d
	addl	$1, %edx
	movl	%r9d, %r10d
	imull	%edx, %r10d
	imull	%edx, %r10d
	imull	%edx, %r10d
	imull	%edx, %r10d
	imull	%edx, %r10d
	imull	%edx, %r10d
	imull	%edx, %r10d
	imull	%edx, %r10d
	imull	%edx, %r10d
	imull	%edx, %r10d
	movl	%r10d, 44(%rsp)
	movl	%r10d, %r15d
	subl	%r9d, %r15d
	movl	%r15d, 28(%rsp)
	movl	32(%rax), %r9d
	leal	37427(%r9), %r10d
	movl	%r9d, %ebp
	imull	%r10d, %ebp
	imull	%r9d, %ebp
	imull	%r9d, %ebp
	imull	%r9d, %ebp
	imull	%r9d, %ebp
	imull	%r9d, %ebp
	imull	%r9d, %ebp
	imull	%r9d, %ebp
	imull	%r9d, %ebp
	imull	%r9d, %ebp
	movl	%ebp, %ebx
	subl	%r10d, %ebx
	movl	%ebx, 32(%rsp)
	movl	36(%rax), %eax
	leal	37426(%rax), %r10d
	subl	$1, %eax
	movl	%r10d, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	movl	%ebx, %r15d
	subl	%r10d, %r15d
	movl	%r15d, 36(%rsp)
	testq	%r11, %r11
	je	.L206
	leaq	-1(%r11), %r15
	movl	%r9d, %r11d
	movl	%r8d, %r10d
	movl	40(%rsp), %r8d
	movl	44(%rsp), %r9d
.L207:
	subl	12(%rsp), %r14d
	subl	16(%rsp), %r13d
	subl	20(%rsp), %r12d
	subl	24(%rsp), %r8d
	subl	28(%rsp), %r9d
	subl	32(%rsp), %ebp
	subl	36(%rsp), %ebx
	imull	%r10d, %r14d
	imull	%edi, %r13d
	imull	%esi, %r12d
	imull	%ecx, %r8d
	imull	%edx, %r9d
	imull	%r11d, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%edi, %r13d
	imull	%esi, %r12d
	imull	%ecx, %r8d
	imull	%edx, %r9d
	imull	%r11d, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%edi, %r13d
	imull	%esi, %r12d
	imull	%ecx, %r8d
	imull	%edx, %r9d
	imull	%r11d, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%edi, %r13d
	imull	%esi, %r12d
	imull	%ecx, %r8d
	imull	%edx, %r9d
	imull	%r11d, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%edi, %r13d
	imull	%esi, %r12d
	imull	%ecx, %r8d
	imull	%edx, %r9d
	imull	%r11d, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%edi, %r13d
	imull	%esi, %r12d
	imull	%ecx, %r8d
	imull	%edx, %r9d
	imull	%r11d, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%edi, %r13d
	imull	%esi, %r12d
	imull	%ecx, %r8d
	imull	%edx, %r9d
	imull	%r11d, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%edi, %r13d
	imull	%esi, %r12d
	imull	%ecx, %r8d
	imull	%edx, %r9d
	imull	%r11d, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%edi, %r13d
	imull	%esi, %r12d
	imull	%ecx, %r8d
	imull	%edx, %r9d
	imull	%r11d, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%edi, %r13d
	imull	%esi, %r12d
	imull	%ecx, %r8d
	imull	%edx, %r9d
	imull	%r11d, %ebp
	imull	%eax, %ebx
	subq	$1, %r15
	cmpq	$-1, %r15
	jne	.L207
	movl	%r8d, 40(%rsp)
	movl	%r9d, 44(%rsp)
.L206:
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	44(%rsp), %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE111:
	.size	integer_mul_6, .-integer_mul_6
	.globl	integer_mul_7
	.type	integer_mul_7, @function
integer_mul_7:
.LFB112:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$72, %rsp
	.cfi_def_cfa_offset 128
	movq	%rdi, %r10
	movq	%rsi, %rax
	movl	12(%rsi), %edi
	leal	37432(%rdi), %ecx
	addl	$5, %edi
	movl	%ecx, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	movl	%edx, %r14d
	subl	%ecx, %edx
	movl	%edx, 20(%rsp)
	movl	16(%rsi), %esi
	leal	37431(%rsi), %ecx
	addl	$4, %esi
	movl	%ecx, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	movl	%edx, %r13d
	subl	%ecx, %edx
	movl	%edx, 24(%rsp)
	movl	20(%rax), %edx
	leal	37430(%rdx), %ecx
	leal	3(%rdx), %r15d
	movl	%ecx, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	movl	%edx, %r12d
	subl	%ecx, %edx
	movl	%edx, 28(%rsp)
	movl	24(%rax), %ecx
	leal	37429(%rcx), %edx
	leal	2(%rcx), %ebx
	movl	%ebx, 16(%rsp)
	movl	%ebx, %ecx
	imull	%edx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, 52(%rsp)
	movl	%ecx, %ebx
	subl	%edx, %ebx
	movl	%ebx, 32(%rsp)
	movl	28(%rax), %ecx
	leal	37428(%rcx), %edx
	addl	$1, %ecx
	movl	%edx, %r8d
	imull	%ecx, %r8d
	imull	%ecx, %r8d
	imull	%ecx, %r8d
	imull	%ecx, %r8d
	imull	%ecx, %r8d
	imull	%ecx, %r8d
	imull	%ecx, %r8d
	imull	%ecx, %r8d
	imull	%ecx, %r8d
	imull	%ecx, %r8d
	movl	%r8d, 56(%rsp)
	movl	%r8d, %r11d
	subl	%edx, %r11d
	movl	%r11d, 36(%rsp)
	movl	32(%rax), %r8d
	leal	37427(%r8), %edx
	movl	%r8d, %r9d
	imull	%edx, %r9d
	imull	%r8d, %r9d
	imull	%r8d, %r9d
	imull	%r8d, %r9d
	imull	%r8d, %r9d
	imull	%r8d, %r9d
	imull	%r8d, %r9d
	imull	%r8d, %r9d
	imull	%r8d, %r9d
	imull	%r8d, %r9d
	movl	%r9d, 60(%rsp)
	movl	%r9d, %ebp
	subl	%edx, %ebp
	movl	%ebp, 40(%rsp)
	movl	36(%rax), %edx
	leal	37426(%rdx), %r9d
	subl	$1, %edx
	movl	%r9d, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	movl	%ebp, %ebx
	subl	%r9d, %ebx
	movl	%ebx, 44(%rsp)
	movl	40(%rax), %eax
	leal	37425(%rax), %r9d
	subl	$2, %eax
	movl	%r9d, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	movl	%ebx, %r11d
	subl	%r9d, %r11d
	movl	%r11d, 48(%rsp)
	leaq	-1(%r10), %r11
	movq	%r11, 8(%rsp)
	testq	%r10, %r10
	je	.L211
	movl	%r8d, %r11d
	movl	%edi, %r10d
	movl	%esi, %r9d
	movl	52(%rsp), %r8d
	movl	56(%rsp), %esi
	movl	60(%rsp), %edi
.L212:
	subl	20(%rsp), %r14d
	subl	24(%rsp), %r13d
	subl	28(%rsp), %r12d
	subl	32(%rsp), %r8d
	subl	36(%rsp), %esi
	subl	40(%rsp), %edi
	subl	44(%rsp), %ebp
	subl	48(%rsp), %ebx
	imull	%r10d, %r14d
	imull	%r9d, %r13d
	imull	%r15d, %r12d
	imull	16(%rsp), %r8d
	imull	%ecx, %esi
	imull	%r11d, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r9d, %r13d
	imull	%r15d, %r12d
	imull	16(%rsp), %r8d
	imull	%ecx, %esi
	imull	%r11d, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r9d, %r13d
	imull	%r15d, %r12d
	imull	16(%rsp), %r8d
	imull	%ecx, %esi
	imull	%r11d, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r9d, %r13d
	imull	%r15d, %r12d
	imull	16(%rsp), %r8d
	imull	%ecx, %esi
	imull	%r11d, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r9d, %r13d
	imull	%r15d, %r12d
	imull	16(%rsp), %r8d
	imull	%ecx, %esi
	imull	%r11d, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r9d, %r13d
	imull	%r15d, %r12d
	imull	16(%rsp), %r8d
	imull	%ecx, %esi
	imull	%r11d, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r9d, %r13d
	imull	%r15d, %r12d
	imull	16(%rsp), %r8d
	imull	%ecx, %esi
	imull	%r11d, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r9d, %r13d
	imull	%r15d, %r12d
	imull	16(%rsp), %r8d
	imull	%ecx, %esi
	imull	%r11d, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r9d, %r13d
	imull	%r15d, %r12d
	imull	16(%rsp), %r8d
	imull	%ecx, %esi
	imull	%r11d, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r9d, %r13d
	imull	%r15d, %r12d
	imull	16(%rsp), %r8d
	imull	%ecx, %esi
	imull	%r11d, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	subq	$1, 8(%rsp)
	cmpq	$-1, 8(%rsp)
	jne	.L212
	movl	%r8d, 52(%rsp)
	movl	%esi, 56(%rsp)
	movl	%edi, 60(%rsp)
.L211:
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	52(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	movl	60(%rsp), %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE112:
	.size	integer_mul_7, .-integer_mul_7
	.globl	integer_mul_8
	.type	integer_mul_8, @function
integer_mul_8:
.LFB113:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$88, %rsp
	.cfi_def_cfa_offset 144
	movq	%rdi, %r9
	movq	%rsi, %rax
	movl	12(%rsi), %esi
	leal	37432(%rsi), %ecx
	addl	$5, %esi
	movl	%ecx, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	movl	%edx, %r14d
	subl	%ecx, %edx
	movl	%edx, 28(%rsp)
	movl	16(%rax), %edx
	leal	37431(%rdx), %ecx
	leal	4(%rdx), %r15d
	movl	%ecx, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	imull	%r15d, %edx
	movl	%edx, %r13d
	subl	%ecx, %edx
	movl	%edx, 32(%rsp)
	movl	20(%rax), %edx
	leal	37430(%rdx), %ecx
	leal	3(%rdx), %ebx
	movl	%ebx, 8(%rsp)
	movl	%ebx, %edx
	imull	%ecx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	movl	%edx, %r12d
	subl	%ecx, %edx
	movl	%edx, 36(%rsp)
	movl	24(%rax), %ecx
	leal	37429(%rcx), %edx
	leal	2(%rcx), %edi
	movl	%edi, 12(%rsp)
	movl	%edi, %ecx
	imull	%edx, %ecx
	imull	%edi, %ecx
	imull	%edi, %ecx
	imull	%edi, %ecx
	imull	%edi, %ecx
	imull	%edi, %ecx
	imull	%edi, %ecx
	imull	%edi, %ecx
	imull	%edi, %ecx
	imull	%edi, %ecx
	movl	%ecx, 64(%rsp)
	movl	%ecx, %edi
	subl	%edx, %edi
	movl	%edi, 40(%rsp)
	movl	28(%rax), %ecx
	leal	37428(%rcx), %edx
	leal	1(%rcx), %ebx
	movl	%ebx, 24(%rsp)
	movl	%ebx, %ecx
	imull	%edx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, %ebx
	movl	%ecx, 68(%rsp)
	subl	%edx, %ebx
	movl	%ebx, 44(%rsp)
	movl	32(%rax), %edi
	leal	37427(%rdi), %edx
	movl	%edi, %ecx
	imull	%edx, %ecx
	imull	%edi, %ecx
	imull	%edi, %ecx
	imull	%edi, %ecx
	imull	%edi, %ecx
	imull	%edi, %ecx
	imull	%edi, %ecx
	imull	%edi, %ecx
	imull	%edi, %ecx
	imull	%edi, %ecx
	movl	%ecx, %r10d
	movl	%ecx, 72(%rsp)
	subl	%edx, %r10d
	movl	%r10d, 48(%rsp)
	movl	36(%rax), %ecx
	leal	37426(%rcx), %r8d
	subl	$1, %ecx
	movl	%r8d, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	movl	%edx, 76(%rsp)
	movl	%edx, %r11d
	subl	%r8d, %r11d
	movl	%r11d, 52(%rsp)
	movl	40(%rax), %edx
	leal	37425(%rdx), %r8d
	subl	$2, %edx
	movl	%r8d, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	movl	%ebp, %r11d
	subl	%r8d, %r11d
	movl	%r11d, 56(%rsp)
	movl	44(%rax), %eax
	leal	37424(%rax), %r8d
	subl	$3, %eax
	movl	%r8d, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	movl	%ebx, %r11d
	subl	%r8d, %r11d
	movl	%r11d, 60(%rsp)
	leaq	-1(%r9), %r8
	movq	%r8, 16(%rsp)
	testq	%r9, %r9
	je	.L216
	movl	%edi, %r11d
	movl	%esi, %r10d
	movl	64(%rsp), %r8d
	movl	68(%rsp), %r9d
	movl	72(%rsp), %esi
	movl	76(%rsp), %edi
.L217:
	subl	28(%rsp), %r14d
	subl	32(%rsp), %r13d
	subl	36(%rsp), %r12d
	subl	40(%rsp), %r8d
	subl	44(%rsp), %r9d
	subl	48(%rsp), %esi
	subl	52(%rsp), %edi
	subl	56(%rsp), %ebp
	subl	60(%rsp), %ebx
	imull	%r10d, %r14d
	imull	%r15d, %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %r8d
	imull	24(%rsp), %r9d
	imull	%r11d, %esi
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r15d, %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %r8d
	imull	24(%rsp), %r9d
	imull	%r11d, %esi
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r15d, %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %r8d
	imull	24(%rsp), %r9d
	imull	%r11d, %esi
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r15d, %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %r8d
	imull	24(%rsp), %r9d
	imull	%r11d, %esi
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r15d, %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %r8d
	imull	24(%rsp), %r9d
	imull	%r11d, %esi
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r15d, %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %r8d
	imull	24(%rsp), %r9d
	imull	%r11d, %esi
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r15d, %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %r8d
	imull	24(%rsp), %r9d
	imull	%r11d, %esi
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r15d, %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %r8d
	imull	24(%rsp), %r9d
	imull	%r11d, %esi
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r15d, %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %r8d
	imull	24(%rsp), %r9d
	imull	%r11d, %esi
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	imull	%r10d, %r14d
	imull	%r15d, %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %r8d
	imull	24(%rsp), %r9d
	imull	%r11d, %esi
	imull	%ecx, %edi
	imull	%edx, %ebp
	imull	%eax, %ebx
	subq	$1, 16(%rsp)
	cmpq	$-1, 16(%rsp)
	jne	.L217
	movl	%r8d, 64(%rsp)
	movl	%r9d, 68(%rsp)
	movl	%esi, 72(%rsp)
	movl	%edi, 76(%rsp)
.L216:
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	call	use_int@PLT
	movl	68(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	call	use_int@PLT
	movl	76(%rsp), %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE113:
	.size	integer_mul_8, .-integer_mul_8
	.globl	integer_mul_9
	.type	integer_mul_9, @function
integer_mul_9:
.LFB114:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$104, %rsp
	.cfi_def_cfa_offset 160
	movq	%rdi, %r8
	movq	%rsi, %rax
	movl	12(%rsi), %edx
	leal	37432(%rdx), %ecx
	leal	5(%rdx), %esi
	movl	%esi, 8(%rsp)
	movl	%esi, %edx
	imull	%ecx, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	movl	%edx, %r14d
	movl	%edx, %edi
	subl	%ecx, %edi
	movl	%edi, 36(%rsp)
	movl	16(%rax), %edx
	leal	37431(%rdx), %ecx
	leal	4(%rdx), %edi
	movl	%edi, 12(%rsp)
	movl	%edi, %edx
	imull	%ecx, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	imull	%edi, %edx
	movl	%edx, %r13d
	movl	%edx, %edi
	subl	%ecx, %edi
	movl	%edi, 40(%rsp)
	movl	20(%rax), %edx
	leal	37430(%rdx), %ecx
	leal	3(%rdx), %ebx
	movl	%ebx, 16(%rsp)
	movl	%ebx, %edx
	imull	%ecx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	movl	%edx, %r12d
	movl	%edx, %edi
	subl	%ecx, %edi
	movl	%edi, 44(%rsp)
	movl	24(%rax), %ecx
	leal	37429(%rcx), %edx
	leal	2(%rcx), %esi
	movl	%esi, 20(%rsp)
	movl	%esi, %ecx
	imull	%edx, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	movl	%ecx, 76(%rsp)
	movl	%ecx, %esi
	subl	%edx, %esi
	movl	%esi, 48(%rsp)
	movl	28(%rax), %ecx
	leal	37428(%rcx), %edx
	leal	1(%rcx), %ebx
	movl	%ebx, 32(%rsp)
	movl	%ebx, %ecx
	imull	%edx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, %edi
	imull	%ebx, %edi
	movl	%edi, 80(%rsp)
	movl	%edi, %ebx
	subl	%edx, %ebx
	movl	%ebx, 52(%rsp)
	movl	32(%rax), %r15d
	leal	37427(%r15), %edx
	movl	%r15d, %ecx
	imull	%edx, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	movl	%ecx, %r9d
	movl	%ecx, 84(%rsp)
	subl	%edx, %r9d
	movl	%r9d, 56(%rsp)
	movl	36(%rax), %esi
	leal	37426(%rsi), %ecx
	subl	$1, %esi
	movl	%ecx, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	movl	%edx, %r10d
	movl	%edx, 88(%rsp)
	subl	%ecx, %r10d
	movl	%r10d, 60(%rsp)
	movl	40(%rax), %ecx
	leal	37425(%rcx), %edi
	subl	$2, %ecx
	movl	%edi, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	movl	%edx, %r11d
	movl	%edx, 92(%rsp)
	subl	%edi, %r11d
	movl	%r11d, 64(%rsp)
	movl	44(%rax), %edx
	leal	37424(%rdx), %edi
	subl	$3, %edx
	movl	%edi, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	movl	%ebp, %ebx
	subl	%edi, %ebx
	movl	%ebx, 68(%rsp)
	movl	48(%rax), %eax
	leal	37423(%rax), %edi
	subl	$4, %eax
	movl	%edi, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	movl	%ebx, %r9d
	subl	%edi, %r9d
	movl	%r9d, 72(%rsp)
	leaq	-1(%r8), %rdi
	movq	%rdi, 24(%rsp)
	testq	%r8, %r8
	je	.L221
	movl	%esi, %r11d
	movl	%ecx, %r10d
	movl	%edx, %r9d
	movl	76(%rsp), %edi
	movl	80(%rsp), %r8d
	movl	84(%rsp), %edx
	movl	88(%rsp), %ecx
	movl	92(%rsp), %esi
.L222:
	subl	36(%rsp), %r14d
	subl	40(%rsp), %r13d
	subl	44(%rsp), %r12d
	subl	48(%rsp), %edi
	subl	52(%rsp), %r8d
	subl	56(%rsp), %edx
	subl	60(%rsp), %ecx
	subl	64(%rsp), %esi
	subl	68(%rsp), %ebp
	subl	72(%rsp), %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %edi
	imull	32(%rsp), %r8d
	imull	%r15d, %edx
	imull	%r11d, %ecx
	imull	%r10d, %esi
	imull	%r9d, %ebp
	imull	%eax, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %edi
	imull	32(%rsp), %r8d
	imull	%r15d, %edx
	imull	%r11d, %ecx
	imull	%r10d, %esi
	imull	%r9d, %ebp
	imull	%eax, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %edi
	imull	32(%rsp), %r8d
	imull	%r15d, %edx
	imull	%r11d, %ecx
	imull	%r10d, %esi
	imull	%r9d, %ebp
	imull	%eax, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %edi
	imull	32(%rsp), %r8d
	imull	%r15d, %edx
	imull	%r11d, %ecx
	imull	%r10d, %esi
	imull	%r9d, %ebp
	imull	%eax, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %edi
	imull	32(%rsp), %r8d
	imull	%r15d, %edx
	imull	%r11d, %ecx
	imull	%r10d, %esi
	imull	%r9d, %ebp
	imull	%eax, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %edi
	imull	32(%rsp), %r8d
	imull	%r15d, %edx
	imull	%r11d, %ecx
	imull	%r10d, %esi
	imull	%r9d, %ebp
	imull	%eax, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %edi
	imull	32(%rsp), %r8d
	imull	%r15d, %edx
	imull	%r11d, %ecx
	imull	%r10d, %esi
	imull	%r9d, %ebp
	imull	%eax, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %edi
	imull	32(%rsp), %r8d
	imull	%r15d, %edx
	imull	%r11d, %ecx
	imull	%r10d, %esi
	imull	%r9d, %ebp
	imull	%eax, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %edi
	imull	32(%rsp), %r8d
	imull	%r15d, %edx
	imull	%r11d, %ecx
	imull	%r10d, %esi
	imull	%r9d, %ebp
	imull	%eax, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %edi
	imull	32(%rsp), %r8d
	imull	%r15d, %edx
	imull	%r11d, %ecx
	imull	%r10d, %esi
	imull	%r9d, %ebp
	imull	%eax, %ebx
	subq	$1, 24(%rsp)
	cmpq	$-1, 24(%rsp)
	jne	.L222
	movl	%edi, 76(%rsp)
	movl	%r8d, 80(%rsp)
	movl	%edx, 84(%rsp)
	movl	%ecx, 88(%rsp)
	movl	%esi, 92(%rsp)
.L221:
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	76(%rsp), %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	call	use_int@PLT
	movl	84(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	call	use_int@PLT
	movl	92(%rsp), %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE114:
	.size	integer_mul_9, .-integer_mul_9
	.globl	integer_mul_10
	.type	integer_mul_10, @function
integer_mul_10:
.LFB115:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$120, %rsp
	.cfi_def_cfa_offset 176
	movq	%rsi, %rax
	movl	12(%rsi), %edx
	leal	37432(%rdx), %ecx
	leal	5(%rdx), %esi
	movl	%esi, 8(%rsp)
	movl	%esi, %edx
	imull	%ecx, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	movl	%edx, %r14d
	movl	%edx, %esi
	subl	%ecx, %esi
	movl	%esi, 44(%rsp)
	movl	16(%rax), %edx
	leal	37431(%rdx), %ecx
	leal	4(%rdx), %ebx
	movl	%ebx, 12(%rsp)
	movl	%ebx, %edx
	imull	%ecx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	movl	%edx, %r13d
	movl	%edx, %esi
	subl	%ecx, %esi
	movl	%esi, 48(%rsp)
	movl	20(%rax), %edx
	leal	37430(%rdx), %ecx
	leal	3(%rdx), %esi
	movl	%esi, 16(%rsp)
	movl	%esi, %edx
	imull	%ecx, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	movl	%edx, %r12d
	movl	%edx, %esi
	subl	%ecx, %esi
	movl	%esi, 52(%rsp)
	movl	24(%rax), %ecx
	leal	37429(%rcx), %edx
	leal	2(%rcx), %ebx
	movl	%ebx, 20(%rsp)
	movl	%ebx, %ecx
	imull	%edx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, %r8d
	movl	%ecx, 108(%rsp)
	subl	%edx, %ecx
	movl	%ecx, 56(%rsp)
	movl	28(%rax), %ecx
	leal	37428(%rcx), %edx
	leal	1(%rcx), %ebx
	movl	%ebx, 24(%rsp)
	movl	%ebx, %ecx
	imull	%edx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, 88(%rsp)
	movl	%ecx, %esi
	subl	%edx, %esi
	movl	%esi, 60(%rsp)
	movl	32(%rax), %r15d
	leal	37427(%r15), %edx
	movl	%r15d, %ecx
	imull	%edx, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	movl	%ecx, %ebx
	movl	%ecx, 92(%rsp)
	subl	%edx, %ebx
	movl	%ebx, 64(%rsp)
	movl	36(%rax), %edx
	leal	37426(%rdx), %ecx
	leal	-1(%rdx), %ebx
	movl	%ebx, 28(%rsp)
	movl	%ebx, %edx
	imull	%ecx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	movl	%edx, 96(%rsp)
	movl	%edx, %r9d
	subl	%ecx, %r9d
	movl	%r9d, 68(%rsp)
	movl	40(%rax), %edx
	leal	37425(%rdx), %ecx
	leal	-2(%rdx), %ebx
	movl	%ebx, 40(%rsp)
	movl	%ebx, %edx
	imull	%ecx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	movl	%edx, %r10d
	movl	%edx, 100(%rsp)
	subl	%ecx, %r10d
	movl	%r10d, 72(%rsp)
	movl	44(%rax), %ecx
	leal	37424(%rcx), %esi
	subl	$3, %ecx
	movl	%esi, %r11d
	imull	%ecx, %r11d
	movl	%r11d, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	imull	%ecx, %edx
	movl	%edx, %r11d
	imull	%ecx, %r11d
	movl	%r11d, 104(%rsp)
	movl	%r11d, %ebp
	subl	%esi, %ebp
	movl	%ebp, 76(%rsp)
	movl	48(%rax), %edx
	leal	37423(%rdx), %esi
	subl	$4, %edx
	movl	%esi, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	imull	%edx, %ebp
	movl	%ebp, %r9d
	subl	%esi, %r9d
	movl	%r9d, 80(%rsp)
	movl	52(%rax), %eax
	leal	37422(%rax), %esi
	subl	$5, %eax
	movl	%esi, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	movl	%ebx, %r9d
	subl	%esi, %r9d
	movl	%r9d, 84(%rsp)
	leaq	-1(%rdi), %rsi
	movq	%rsi, 32(%rsp)
	testq	%rdi, %rdi
	je	.L226
	movl	%ecx, %r11d
	movl	%edx, %r10d
	movl	%eax, %r9d
	movl	%r8d, %esi
	movl	88(%rsp), %edi
	movl	92(%rsp), %r8d
	movl	96(%rsp), %eax
	movl	100(%rsp), %edx
	movl	104(%rsp), %ecx
.L227:
	subl	44(%rsp), %r14d
	subl	48(%rsp), %r13d
	subl	52(%rsp), %r12d
	subl	56(%rsp), %esi
	subl	60(%rsp), %edi
	subl	64(%rsp), %r8d
	subl	68(%rsp), %eax
	subl	72(%rsp), %edx
	subl	76(%rsp), %ecx
	subl	80(%rsp), %ebp
	subl	84(%rsp), %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %esi
	imull	24(%rsp), %edi
	imull	%r15d, %r8d
	imull	28(%rsp), %eax
	imull	40(%rsp), %edx
	imull	%r11d, %ecx
	imull	%r10d, %ebp
	imull	%r9d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %esi
	imull	24(%rsp), %edi
	imull	%r15d, %r8d
	imull	28(%rsp), %eax
	imull	40(%rsp), %edx
	imull	%r11d, %ecx
	imull	%r10d, %ebp
	imull	%r9d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %esi
	imull	24(%rsp), %edi
	imull	%r15d, %r8d
	imull	28(%rsp), %eax
	imull	40(%rsp), %edx
	imull	%r11d, %ecx
	imull	%r10d, %ebp
	imull	%r9d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %esi
	imull	24(%rsp), %edi
	imull	%r15d, %r8d
	imull	28(%rsp), %eax
	imull	40(%rsp), %edx
	imull	%r11d, %ecx
	imull	%r10d, %ebp
	imull	%r9d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %esi
	imull	24(%rsp), %edi
	imull	%r15d, %r8d
	imull	28(%rsp), %eax
	imull	40(%rsp), %edx
	imull	%r11d, %ecx
	imull	%r10d, %ebp
	imull	%r9d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %esi
	imull	24(%rsp), %edi
	imull	%r15d, %r8d
	imull	28(%rsp), %eax
	imull	40(%rsp), %edx
	imull	%r11d, %ecx
	imull	%r10d, %ebp
	imull	%r9d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %esi
	imull	24(%rsp), %edi
	imull	%r15d, %r8d
	imull	28(%rsp), %eax
	imull	40(%rsp), %edx
	imull	%r11d, %ecx
	imull	%r10d, %ebp
	imull	%r9d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %esi
	imull	24(%rsp), %edi
	imull	%r15d, %r8d
	imull	28(%rsp), %eax
	imull	40(%rsp), %edx
	imull	%r11d, %ecx
	imull	%r10d, %ebp
	imull	%r9d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %esi
	imull	24(%rsp), %edi
	imull	%r15d, %r8d
	imull	28(%rsp), %eax
	imull	40(%rsp), %edx
	imull	%r11d, %ecx
	imull	%r10d, %ebp
	imull	%r9d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %esi
	imull	24(%rsp), %edi
	imull	%r15d, %r8d
	imull	28(%rsp), %eax
	imull	40(%rsp), %edx
	imull	%r11d, %ecx
	imull	%r10d, %ebp
	imull	%r9d, %ebx
	subq	$1, 32(%rsp)
	cmpq	$-1, 32(%rsp)
	jne	.L227
	movl	%esi, 108(%rsp)
	movl	%edi, 88(%rsp)
	movl	%r8d, 92(%rsp)
	movl	%eax, 96(%rsp)
	movl	%edx, 100(%rsp)
	movl	%ecx, 104(%rsp)
.L226:
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	108(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	call	use_int@PLT
	movl	92(%rsp), %edi
	call	use_int@PLT
	movl	96(%rsp), %edi
	call	use_int@PLT
	movl	100(%rsp), %edi
	call	use_int@PLT
	movl	104(%rsp), %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$120, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE115:
	.size	integer_mul_10, .-integer_mul_10
	.globl	integer_mul_11
	.type	integer_mul_11, @function
integer_mul_11:
.LFB116:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$136, %rsp
	.cfi_def_cfa_offset 192
	movq	%rsi, %rax
	movl	12(%rsi), %edx
	leal	37432(%rdx), %ecx
	leal	5(%rdx), %esi
	movl	%esi, 8(%rsp)
	movl	%esi, %edx
	imull	%ecx, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	movl	%edx, %r14d
	subl	%ecx, %edx
	movl	%edx, 52(%rsp)
	movl	16(%rax), %edx
	leal	37431(%rdx), %ecx
	leal	4(%rdx), %ebx
	movl	%ebx, 12(%rsp)
	movl	%ebx, %edx
	imull	%ecx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	movl	%edx, %r13d
	subl	%ecx, %edx
	movl	%edx, 56(%rsp)
	movl	20(%rax), %edx
	leal	37430(%rdx), %ecx
	leal	3(%rdx), %esi
	movl	%esi, 16(%rsp)
	movl	%esi, %edx
	imull	%ecx, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	imull	%esi, %edx
	movl	%edx, %r12d
	subl	%ecx, %edx
	movl	%edx, 60(%rsp)
	movl	24(%rax), %ecx
	leal	37429(%rcx), %edx
	leal	2(%rcx), %ebx
	movl	%ebx, 20(%rsp)
	movl	%ebx, %ecx
	imull	%edx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, %r8d
	movl	%ecx, 116(%rsp)
	movl	%ecx, %esi
	subl	%edx, %esi
	movl	%esi, 64(%rsp)
	movl	28(%rax), %ecx
	leal	37428(%rcx), %edx
	leal	1(%rcx), %esi
	movl	%esi, 24(%rsp)
	movl	%esi, %ecx
	imull	%edx, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	movl	%ecx, %esi
	movl	%ecx, 120(%rsp)
	subl	%edx, %ecx
	movl	%ecx, 68(%rsp)
	movl	32(%rax), %r15d
	leal	37427(%r15), %edx
	movl	%r15d, %ecx
	imull	%edx, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	movl	%ecx, %r9d
	movl	%ecx, 124(%rsp)
	subl	%edx, %ecx
	movl	%ecx, 72(%rsp)
	movl	36(%rax), %ecx
	leal	37426(%rcx), %edx
	leal	-1(%rcx), %ebx
	movl	%ebx, 28(%rsp)
	movl	%ebx, %ecx
	imull	%edx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, 100(%rsp)
	movl	%ecx, %ebx
	subl	%edx, %ebx
	movl	%ebx, 76(%rsp)
	movl	40(%rax), %ecx
	leal	37425(%rcx), %edx
	leal	-2(%rcx), %ebx
	movl	%ebx, 32(%rsp)
	movl	%ebx, %ecx
	imull	%edx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, %r10d
	movl	%ecx, 104(%rsp)
	subl	%edx, %r10d
	movl	%r10d, 80(%rsp)
	movl	44(%rax), %ecx
	leal	37424(%rcx), %edx
	leal	-3(%rcx), %ebx
	movl	%ebx, 36(%rsp)
	movl	%ebx, %ecx
	imull	%edx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, %r11d
	movl	%ecx, 108(%rsp)
	subl	%edx, %r11d
	movl	%r11d, 84(%rsp)
	movl	48(%rax), %edx
	leal	37423(%rdx), %ecx
	leal	-4(%rdx), %ebx
	movl	%ebx, 48(%rsp)
	movl	%ebx, %edx
	imull	%ecx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	imull	%ebx, %edx
	movl	%edx, 112(%rsp)
	movl	%edx, %ebp
	subl	%ecx, %ebp
	movl	%ebp, 88(%rsp)
	movl	52(%rax), %ecx
	leal	37422(%rcx), %edx
	subl	$5, %ecx
	movl	%edx, %ebp
	imull	%ecx, %ebp
	imull	%ecx, %ebp
	imull	%ecx, %ebp
	imull	%ecx, %ebp
	imull	%ecx, %ebp
	imull	%ecx, %ebp
	imull	%ecx, %ebp
	imull	%ecx, %ebp
	imull	%ecx, %ebp
	imull	%ecx, %ebp
	movl	%ebp, %ebx
	subl	%edx, %ebx
	movl	%ebx, 92(%rsp)
	movl	56(%rax), %edx
	leal	37421(%rdx), %eax
	subl	$6, %edx
	movl	%eax, %ebx
	imull	%edx, %ebx
	imull	%edx, %ebx
	imull	%edx, %ebx
	imull	%edx, %ebx
	imull	%edx, %ebx
	imull	%edx, %ebx
	imull	%edx, %ebx
	imull	%edx, %ebx
	imull	%edx, %ebx
	imull	%edx, %ebx
	movl	%ebx, %r10d
	subl	%eax, %r10d
	movl	%r10d, 96(%rsp)
	leaq	-1(%rdi), %rax
	movq	%rax, 40(%rsp)
	testq	%rdi, %rdi
	je	.L231
	movl	%ecx, %r11d
	movl	%edx, %r10d
	movl	%r8d, %ecx
	movl	%r9d, %edi
	movl	100(%rsp), %r8d
	movl	104(%rsp), %r9d
	movl	108(%rsp), %eax
	movl	112(%rsp), %edx
.L232:
	subl	52(%rsp), %r14d
	subl	56(%rsp), %r13d
	subl	60(%rsp), %r12d
	subl	64(%rsp), %ecx
	subl	68(%rsp), %esi
	subl	72(%rsp), %edi
	subl	76(%rsp), %r8d
	subl	80(%rsp), %r9d
	subl	84(%rsp), %eax
	subl	88(%rsp), %edx
	subl	92(%rsp), %ebp
	subl	96(%rsp), %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ecx
	imull	24(%rsp), %esi
	imull	%r15d, %edi
	imull	28(%rsp), %r8d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %eax
	imull	48(%rsp), %edx
	imull	%r11d, %ebp
	imull	%r10d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ecx
	imull	24(%rsp), %esi
	imull	%r15d, %edi
	imull	28(%rsp), %r8d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %eax
	imull	48(%rsp), %edx
	imull	%r11d, %ebp
	imull	%r10d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ecx
	imull	24(%rsp), %esi
	imull	%r15d, %edi
	imull	28(%rsp), %r8d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %eax
	imull	48(%rsp), %edx
	imull	%r11d, %ebp
	imull	%r10d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ecx
	imull	24(%rsp), %esi
	imull	%r15d, %edi
	imull	28(%rsp), %r8d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %eax
	imull	48(%rsp), %edx
	imull	%r11d, %ebp
	imull	%r10d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ecx
	imull	24(%rsp), %esi
	imull	%r15d, %edi
	imull	28(%rsp), %r8d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %eax
	imull	48(%rsp), %edx
	imull	%r11d, %ebp
	imull	%r10d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ecx
	imull	24(%rsp), %esi
	imull	%r15d, %edi
	imull	28(%rsp), %r8d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %eax
	imull	48(%rsp), %edx
	imull	%r11d, %ebp
	imull	%r10d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ecx
	imull	24(%rsp), %esi
	imull	%r15d, %edi
	imull	28(%rsp), %r8d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %eax
	imull	48(%rsp), %edx
	imull	%r11d, %ebp
	imull	%r10d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ecx
	imull	24(%rsp), %esi
	imull	%r15d, %edi
	imull	28(%rsp), %r8d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %eax
	imull	48(%rsp), %edx
	imull	%r11d, %ebp
	imull	%r10d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ecx
	imull	24(%rsp), %esi
	imull	%r15d, %edi
	imull	28(%rsp), %r8d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %eax
	imull	48(%rsp), %edx
	imull	%r11d, %ebp
	imull	%r10d, %ebx
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ecx
	imull	24(%rsp), %esi
	imull	%r15d, %edi
	imull	28(%rsp), %r8d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %eax
	imull	48(%rsp), %edx
	imull	%r11d, %ebp
	imull	%r10d, %ebx
	subq	$1, 40(%rsp)
	cmpq	$-1, 40(%rsp)
	jne	.L232
	movl	%ecx, 116(%rsp)
	movl	%esi, 120(%rsp)
	movl	%edi, 124(%rsp)
	movl	%r8d, 100(%rsp)
	movl	%r9d, 104(%rsp)
	movl	%eax, 108(%rsp)
	movl	%edx, 112(%rsp)
.L231:
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	116(%rsp), %edi
	call	use_int@PLT
	movl	120(%rsp), %edi
	call	use_int@PLT
	movl	124(%rsp), %edi
	call	use_int@PLT
	movl	100(%rsp), %edi
	call	use_int@PLT
	movl	104(%rsp), %edi
	call	use_int@PLT
	movl	108(%rsp), %edi
	call	use_int@PLT
	movl	112(%rsp), %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$136, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE116:
	.size	integer_mul_11, .-integer_mul_11
	.globl	integer_mul_12
	.type	integer_mul_12, @function
integer_mul_12:
.LFB117:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$152, %rsp
	.cfi_def_cfa_offset 208
	movq	%rsi, %rdx
	movl	12(%rsi), %eax
	leal	37432(%rax), %ecx
	leal	5(%rax), %esi
	movl	%esi, 4(%rsp)
	movl	%esi, %eax
	imull	%ecx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	movl	%eax, %r13d
	subl	%ecx, %eax
	movl	%eax, 56(%rsp)
	movl	16(%rdx), %eax
	leal	37431(%rax), %ecx
	leal	4(%rax), %ebx
	movl	%ebx, 8(%rsp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	movl	%eax, %r12d
	subl	%ecx, %eax
	movl	%eax, 60(%rsp)
	movl	20(%rdx), %eax
	leal	37430(%rax), %ecx
	leal	3(%rax), %esi
	movl	%esi, 12(%rsp)
	movl	%esi, %eax
	imull	%ecx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	movl	%eax, %ebp
	subl	%ecx, %eax
	movl	%eax, 64(%rsp)
	movl	24(%rdx), %ecx
	leal	37429(%rcx), %eax
	leal	2(%rcx), %ebx
	movl	%ebx, 16(%rsp)
	movl	%ebx, %ecx
	imull	%eax, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, %r8d
	movl	%ecx, 124(%rsp)
	movl	%ecx, %esi
	subl	%eax, %esi
	movl	%esi, 68(%rsp)
	movl	28(%rdx), %ecx
	leal	37428(%rcx), %eax
	leal	1(%rcx), %esi
	movl	%esi, 20(%rsp)
	movl	%esi, %ecx
	imull	%eax, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	movl	%ecx, %esi
	movl	%ecx, 128(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 72(%rsp)
	movl	32(%rdx), %r15d
	leal	37427(%r15), %eax
	movl	%r15d, %ecx
	imull	%eax, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	imull	%r15d, %ecx
	movl	%ecx, %r9d
	movl	%ecx, 132(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 76(%rsp)
	movl	36(%rdx), %ecx
	leal	37426(%rcx), %eax
	leal	-1(%rcx), %ebx
	movl	%ebx, 24(%rsp)
	movl	%ebx, %ecx
	imull	%eax, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, %r10d
	movl	%ecx, 136(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 80(%rsp)
	movl	40(%rdx), %ecx
	leal	37425(%rcx), %eax
	leal	-2(%rcx), %ebx
	movl	%ebx, 28(%rsp)
	movl	%ebx, %ecx
	imull	%eax, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, %r11d
	movl	%ecx, 140(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 84(%rsp)
	movl	44(%rdx), %ecx
	leal	37424(%rcx), %eax
	leal	-3(%rcx), %ebx
	movl	%ebx, 32(%rsp)
	movl	%ebx, %ecx
	imull	%eax, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, 108(%rsp)
	movl	%ecx, %ebx
	subl	%eax, %ebx
	movl	%ebx, 88(%rsp)
	movl	48(%rdx), %eax
	leal	37423(%rax), %ecx
	leal	-4(%rax), %ebx
	movl	%ebx, 36(%rsp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	movl	%eax, 112(%rsp)
	movl	%eax, %r14d
	subl	%ecx, %r14d
	movl	%r14d, 92(%rsp)
	movl	52(%rdx), %eax
	leal	37422(%rax), %ecx
	leal	-5(%rax), %ebx
	movl	%ebx, 40(%rsp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	movl	%eax, 116(%rsp)
	movl	%eax, %ebx
	subl	%ecx, %ebx
	movl	%ebx, 96(%rsp)
	movl	56(%rdx), %eax
	leal	37421(%rax), %ecx
	leal	-6(%rax), %ebx
	movl	%ebx, 44(%rsp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	movl	%eax, %r14d
	imull	%ebx, %r14d
	movl	%r14d, 120(%rsp)
	subl	%ecx, %r14d
	movl	%r14d, 100(%rsp)
	movl	60(%rdx), %r14d
	leal	37420(%r14), %eax
	subl	$7, %r14d
	movl	%eax, %edx
	imull	%r14d, %edx
	movl	%edx, %ebx
	imull	%r14d, %ebx
	imull	%r14d, %ebx
	imull	%r14d, %ebx
	imull	%r14d, %ebx
	imull	%r14d, %ebx
	imull	%r14d, %ebx
	imull	%r14d, %ebx
	imull	%r14d, %ebx
	imull	%r14d, %ebx
	movl	%ebx, %edx
	subl	%eax, %edx
	movl	%edx, 104(%rsp)
	leaq	-1(%rdi), %rdx
	movq	%rdx, 48(%rsp)
	testq	%rdi, %rdi
	je	.L236
	movl	%r8d, %ecx
	movl	%r9d, %edi
	movl	%r10d, %r8d
	movl	%r11d, %r9d
	movl	108(%rsp), %r10d
	movl	112(%rsp), %r11d
	movl	116(%rsp), %eax
	movl	120(%rsp), %edx
.L237:
	subl	56(%rsp), %r13d
	subl	60(%rsp), %r12d
	subl	64(%rsp), %ebp
	subl	68(%rsp), %ecx
	subl	72(%rsp), %esi
	subl	76(%rsp), %edi
	subl	80(%rsp), %r8d
	subl	84(%rsp), %r9d
	subl	88(%rsp), %r10d
	subl	92(%rsp), %r11d
	subl	96(%rsp), %eax
	subl	100(%rsp), %edx
	subl	104(%rsp), %ebx
	imull	4(%rsp), %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %ebp
	imull	16(%rsp), %ecx
	imull	20(%rsp), %esi
	imull	%r15d, %edi
	imull	24(%rsp), %r8d
	imull	28(%rsp), %r9d
	imull	32(%rsp), %r10d
	imull	36(%rsp), %r11d
	imull	40(%rsp), %eax
	imull	44(%rsp), %edx
	imull	%r14d, %ebx
	imull	4(%rsp), %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %ebp
	imull	16(%rsp), %ecx
	imull	20(%rsp), %esi
	imull	%r15d, %edi
	imull	24(%rsp), %r8d
	imull	28(%rsp), %r9d
	imull	32(%rsp), %r10d
	imull	36(%rsp), %r11d
	imull	40(%rsp), %eax
	imull	44(%rsp), %edx
	imull	%r14d, %ebx
	imull	4(%rsp), %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %ebp
	imull	16(%rsp), %ecx
	imull	20(%rsp), %esi
	imull	%r15d, %edi
	imull	24(%rsp), %r8d
	imull	28(%rsp), %r9d
	imull	32(%rsp), %r10d
	imull	36(%rsp), %r11d
	imull	40(%rsp), %eax
	imull	44(%rsp), %edx
	imull	%r14d, %ebx
	imull	4(%rsp), %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %ebp
	imull	16(%rsp), %ecx
	imull	20(%rsp), %esi
	imull	%r15d, %edi
	imull	24(%rsp), %r8d
	imull	28(%rsp), %r9d
	imull	32(%rsp), %r10d
	imull	36(%rsp), %r11d
	imull	40(%rsp), %eax
	imull	44(%rsp), %edx
	imull	%r14d, %ebx
	imull	4(%rsp), %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %ebp
	imull	16(%rsp), %ecx
	imull	20(%rsp), %esi
	imull	%r15d, %edi
	imull	24(%rsp), %r8d
	imull	28(%rsp), %r9d
	imull	32(%rsp), %r10d
	imull	36(%rsp), %r11d
	imull	40(%rsp), %eax
	imull	44(%rsp), %edx
	imull	%r14d, %ebx
	imull	4(%rsp), %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %ebp
	imull	16(%rsp), %ecx
	imull	20(%rsp), %esi
	imull	%r15d, %edi
	imull	24(%rsp), %r8d
	imull	28(%rsp), %r9d
	imull	32(%rsp), %r10d
	imull	36(%rsp), %r11d
	imull	40(%rsp), %eax
	imull	44(%rsp), %edx
	imull	%r14d, %ebx
	imull	4(%rsp), %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %ebp
	imull	16(%rsp), %ecx
	imull	20(%rsp), %esi
	imull	%r15d, %edi
	imull	24(%rsp), %r8d
	imull	28(%rsp), %r9d
	imull	32(%rsp), %r10d
	imull	36(%rsp), %r11d
	imull	40(%rsp), %eax
	imull	44(%rsp), %edx
	imull	%r14d, %ebx
	imull	4(%rsp), %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %ebp
	imull	16(%rsp), %ecx
	imull	20(%rsp), %esi
	imull	%r15d, %edi
	imull	24(%rsp), %r8d
	imull	28(%rsp), %r9d
	imull	32(%rsp), %r10d
	imull	36(%rsp), %r11d
	imull	40(%rsp), %eax
	imull	44(%rsp), %edx
	imull	%r14d, %ebx
	imull	4(%rsp), %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %ebp
	imull	16(%rsp), %ecx
	imull	20(%rsp), %esi
	imull	%r15d, %edi
	imull	24(%rsp), %r8d
	imull	28(%rsp), %r9d
	imull	32(%rsp), %r10d
	imull	36(%rsp), %r11d
	imull	40(%rsp), %eax
	imull	44(%rsp), %edx
	imull	%r14d, %ebx
	imull	4(%rsp), %r13d
	imull	8(%rsp), %r12d
	imull	12(%rsp), %ebp
	imull	16(%rsp), %ecx
	imull	20(%rsp), %esi
	imull	%r15d, %edi
	imull	24(%rsp), %r8d
	imull	28(%rsp), %r9d
	imull	32(%rsp), %r10d
	imull	36(%rsp), %r11d
	imull	40(%rsp), %eax
	imull	44(%rsp), %edx
	imull	%r14d, %ebx
	subq	$1, 48(%rsp)
	cmpq	$-1, 48(%rsp)
	jne	.L237
	movl	%ecx, 124(%rsp)
	movl	%esi, 128(%rsp)
	movl	%edi, 132(%rsp)
	movl	%r8d, 136(%rsp)
	movl	%r9d, 140(%rsp)
	movl	%r10d, 108(%rsp)
	movl	%r11d, 112(%rsp)
	movl	%eax, 116(%rsp)
	movl	%edx, 120(%rsp)
.L236:
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	124(%rsp), %edi
	call	use_int@PLT
	movl	128(%rsp), %edi
	call	use_int@PLT
	movl	132(%rsp), %edi
	call	use_int@PLT
	movl	136(%rsp), %edi
	call	use_int@PLT
	movl	140(%rsp), %edi
	call	use_int@PLT
	movl	108(%rsp), %edi
	call	use_int@PLT
	movl	112(%rsp), %edi
	call	use_int@PLT
	movl	116(%rsp), %edi
	call	use_int@PLT
	movl	120(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$152, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE117:
	.size	integer_mul_12, .-integer_mul_12
	.globl	integer_mul_13
	.type	integer_mul_13, @function
integer_mul_13:
.LFB118:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$168, %rsp
	.cfi_def_cfa_offset 224
	movq	%rsi, %rdx
	movl	12(%rsi), %eax
	leal	37432(%rax), %ecx
	leal	5(%rax), %ebx
	movl	%ebx, 12(%rsp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	movl	%eax, %r14d
	subl	%ecx, %eax
	movl	%eax, 68(%rsp)
	movl	16(%rsi), %eax
	leal	37431(%rax), %ecx
	leal	4(%rax), %esi
	movl	%esi, 16(%rsp)
	movl	%esi, %eax
	imull	%ecx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	movl	%eax, %r13d
	subl	%ecx, %eax
	movl	%eax, 72(%rsp)
	movl	20(%rdx), %eax
	leal	37430(%rax), %ecx
	leal	3(%rax), %ebx
	movl	%ebx, 20(%rsp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	movl	%eax, %r12d
	subl	%ecx, %eax
	movl	%eax, 76(%rsp)
	movl	24(%rdx), %ecx
	leal	37429(%rcx), %eax
	leal	2(%rcx), %esi
	movl	%esi, 24(%rsp)
	movl	%esi, %ecx
	imull	%eax, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	movl	%ecx, %ebp
	movl	%ecx, %esi
	subl	%eax, %esi
	movl	%esi, 80(%rsp)
	movl	28(%rdx), %ecx
	leal	37428(%rcx), %eax
	leal	1(%rcx), %ebx
	movl	%ebx, 28(%rsp)
	movl	%ebx, %ecx
	imull	%eax, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, %r8d
	movl	%ecx, 140(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 84(%rsp)
	movl	32(%rdx), %esi
	movl	%esi, 8(%rsp)
	leal	37427(%rsi), %eax
	movl	%esi, %ecx
	imull	%eax, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	movl	%ecx, %r9d
	movl	%ecx, 144(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 88(%rsp)
	movl	36(%rdx), %ecx
	leal	37426(%rcx), %eax
	leal	-1(%rcx), %esi
	movl	%esi, 32(%rsp)
	movl	%esi, %ecx
	imull	%eax, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	movl	%ecx, %esi
	movl	%ecx, 148(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 92(%rsp)
	movl	40(%rdx), %ecx
	leal	37425(%rcx), %eax
	leal	-2(%rcx), %ebx
	movl	%ebx, 36(%rsp)
	movl	%ebx, %ecx
	imull	%eax, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, %r10d
	movl	%ecx, 152(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 96(%rsp)
	movl	44(%rdx), %ecx
	leal	37424(%rcx), %eax
	leal	-3(%rcx), %ebx
	movl	%ebx, 40(%rsp)
	movl	%ebx, %ecx
	imull	%eax, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, %r11d
	movl	%ecx, 156(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 100(%rsp)
	movl	48(%rdx), %ecx
	leal	37423(%rcx), %eax
	leal	-4(%rcx), %ebx
	movl	%ebx, 44(%rsp)
	movl	%ebx, %ecx
	imull	%eax, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, 124(%rsp)
	movl	%ecx, %ebx
	subl	%eax, %ebx
	movl	%ebx, 104(%rsp)
	movl	52(%rdx), %eax
	leal	37422(%rax), %ecx
	leal	-5(%rax), %ebx
	movl	%ebx, 48(%rsp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	movl	%eax, 128(%rsp)
	movl	%eax, %r15d
	subl	%ecx, %r15d
	movl	%r15d, 108(%rsp)
	movl	56(%rdx), %eax
	leal	37421(%rax), %ecx
	leal	-6(%rax), %ebx
	movl	%ebx, 52(%rsp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	movl	%eax, 132(%rsp)
	movl	%eax, %ebx
	subl	%ecx, %ebx
	movl	%ebx, 112(%rsp)
	movl	60(%rdx), %eax
	leal	37420(%rax), %ecx
	leal	-7(%rax), %ebx
	movl	%ebx, 64(%rsp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	movl	%eax, %r15d
	imull	%ebx, %r15d
	movl	%r15d, 136(%rsp)
	subl	%ecx, %r15d
	movl	%r15d, 116(%rsp)
	movl	64(%rdx), %r15d
	leal	37419(%r15), %eax
	subl	$8, %r15d
	movl	%eax, %edx
	imull	%r15d, %edx
	movl	%edx, %ebx
	imull	%r15d, %ebx
	imull	%r15d, %ebx
	imull	%r15d, %ebx
	imull	%r15d, %ebx
	imull	%r15d, %ebx
	imull	%r15d, %ebx
	imull	%r15d, %ebx
	imull	%r15d, %ebx
	imull	%r15d, %ebx
	movl	%ebx, %edx
	subl	%eax, %edx
	movl	%edx, 120(%rsp)
	leaq	-1(%rdi), %rdx
	movq	%rdx, 56(%rsp)
	testq	%rdi, %rdi
	je	.L241
	movl	%r8d, %edx
	movl	%r9d, %ecx
	movl	%r10d, %edi
	movl	%r11d, %r8d
	movl	124(%rsp), %r9d
	movl	128(%rsp), %r10d
	movl	132(%rsp), %eax
	movl	136(%rsp), %r11d
.L242:
	subl	68(%rsp), %r14d
	subl	72(%rsp), %r13d
	subl	76(%rsp), %r12d
	subl	80(%rsp), %ebp
	subl	84(%rsp), %edx
	subl	88(%rsp), %ecx
	subl	92(%rsp), %esi
	subl	96(%rsp), %edi
	subl	100(%rsp), %r8d
	subl	104(%rsp), %r9d
	subl	108(%rsp), %r10d
	subl	112(%rsp), %eax
	subl	116(%rsp), %r11d
	subl	120(%rsp), %ebx
	imull	12(%rsp), %r14d
	imull	16(%rsp), %r13d
	imull	20(%rsp), %r12d
	imull	24(%rsp), %ebp
	imull	28(%rsp), %edx
	imull	8(%rsp), %ecx
	imull	32(%rsp), %esi
	imull	36(%rsp), %edi
	imull	40(%rsp), %r8d
	imull	44(%rsp), %r9d
	imull	48(%rsp), %r10d
	imull	52(%rsp), %eax
	imull	64(%rsp), %r11d
	imull	%r15d, %ebx
	imull	12(%rsp), %r14d
	imull	16(%rsp), %r13d
	imull	20(%rsp), %r12d
	imull	24(%rsp), %ebp
	imull	28(%rsp), %edx
	imull	8(%rsp), %ecx
	imull	32(%rsp), %esi
	imull	36(%rsp), %edi
	imull	40(%rsp), %r8d
	imull	44(%rsp), %r9d
	imull	48(%rsp), %r10d
	imull	52(%rsp), %eax
	imull	64(%rsp), %r11d
	imull	%r15d, %ebx
	imull	12(%rsp), %r14d
	imull	16(%rsp), %r13d
	imull	20(%rsp), %r12d
	imull	24(%rsp), %ebp
	imull	28(%rsp), %edx
	imull	8(%rsp), %ecx
	imull	32(%rsp), %esi
	imull	36(%rsp), %edi
	imull	40(%rsp), %r8d
	imull	44(%rsp), %r9d
	imull	48(%rsp), %r10d
	imull	52(%rsp), %eax
	imull	64(%rsp), %r11d
	imull	%r15d, %ebx
	imull	12(%rsp), %r14d
	imull	16(%rsp), %r13d
	imull	20(%rsp), %r12d
	imull	24(%rsp), %ebp
	imull	28(%rsp), %edx
	imull	8(%rsp), %ecx
	imull	32(%rsp), %esi
	imull	36(%rsp), %edi
	imull	40(%rsp), %r8d
	imull	44(%rsp), %r9d
	imull	48(%rsp), %r10d
	imull	52(%rsp), %eax
	imull	64(%rsp), %r11d
	imull	%r15d, %ebx
	imull	12(%rsp), %r14d
	imull	16(%rsp), %r13d
	imull	20(%rsp), %r12d
	imull	24(%rsp), %ebp
	imull	28(%rsp), %edx
	imull	8(%rsp), %ecx
	imull	32(%rsp), %esi
	imull	36(%rsp), %edi
	imull	40(%rsp), %r8d
	imull	44(%rsp), %r9d
	imull	48(%rsp), %r10d
	imull	52(%rsp), %eax
	imull	64(%rsp), %r11d
	imull	%r15d, %ebx
	imull	12(%rsp), %r14d
	imull	16(%rsp), %r13d
	imull	20(%rsp), %r12d
	imull	24(%rsp), %ebp
	imull	28(%rsp), %edx
	imull	8(%rsp), %ecx
	imull	32(%rsp), %esi
	imull	36(%rsp), %edi
	imull	40(%rsp), %r8d
	imull	44(%rsp), %r9d
	imull	48(%rsp), %r10d
	imull	52(%rsp), %eax
	imull	64(%rsp), %r11d
	imull	%r15d, %ebx
	imull	12(%rsp), %r14d
	imull	16(%rsp), %r13d
	imull	20(%rsp), %r12d
	imull	24(%rsp), %ebp
	imull	28(%rsp), %edx
	imull	8(%rsp), %ecx
	imull	32(%rsp), %esi
	imull	36(%rsp), %edi
	imull	40(%rsp), %r8d
	imull	44(%rsp), %r9d
	imull	48(%rsp), %r10d
	imull	52(%rsp), %eax
	imull	64(%rsp), %r11d
	imull	%r15d, %ebx
	imull	12(%rsp), %r14d
	imull	16(%rsp), %r13d
	imull	20(%rsp), %r12d
	imull	24(%rsp), %ebp
	imull	28(%rsp), %edx
	imull	8(%rsp), %ecx
	imull	32(%rsp), %esi
	imull	36(%rsp), %edi
	imull	40(%rsp), %r8d
	imull	44(%rsp), %r9d
	imull	48(%rsp), %r10d
	imull	52(%rsp), %eax
	imull	64(%rsp), %r11d
	imull	%r15d, %ebx
	imull	12(%rsp), %r14d
	imull	16(%rsp), %r13d
	imull	20(%rsp), %r12d
	imull	24(%rsp), %ebp
	imull	28(%rsp), %edx
	imull	8(%rsp), %ecx
	imull	32(%rsp), %esi
	imull	36(%rsp), %edi
	imull	40(%rsp), %r8d
	imull	44(%rsp), %r9d
	imull	48(%rsp), %r10d
	imull	52(%rsp), %eax
	imull	64(%rsp), %r11d
	imull	%r15d, %ebx
	imull	12(%rsp), %r14d
	imull	16(%rsp), %r13d
	imull	20(%rsp), %r12d
	imull	24(%rsp), %ebp
	imull	28(%rsp), %edx
	imull	8(%rsp), %ecx
	imull	32(%rsp), %esi
	imull	36(%rsp), %edi
	imull	40(%rsp), %r8d
	imull	44(%rsp), %r9d
	imull	48(%rsp), %r10d
	imull	52(%rsp), %eax
	imull	64(%rsp), %r11d
	imull	%r15d, %ebx
	subq	$1, 56(%rsp)
	cmpq	$-1, 56(%rsp)
	jne	.L242
	movl	%edx, 140(%rsp)
	movl	%ecx, 144(%rsp)
	movl	%esi, 148(%rsp)
	movl	%edi, 152(%rsp)
	movl	%r8d, 156(%rsp)
	movl	%r9d, 124(%rsp)
	movl	%r10d, 128(%rsp)
	movl	%eax, 132(%rsp)
	movl	%r11d, 136(%rsp)
.L241:
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	140(%rsp), %edi
	call	use_int@PLT
	movl	144(%rsp), %edi
	call	use_int@PLT
	movl	148(%rsp), %edi
	call	use_int@PLT
	movl	152(%rsp), %edi
	call	use_int@PLT
	movl	156(%rsp), %edi
	call	use_int@PLT
	movl	124(%rsp), %edi
	call	use_int@PLT
	movl	128(%rsp), %edi
	call	use_int@PLT
	movl	132(%rsp), %edi
	call	use_int@PLT
	movl	136(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$168, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE118:
	.size	integer_mul_13, .-integer_mul_13
	.globl	integer_mul_14
	.type	integer_mul_14, @function
integer_mul_14:
.LFB119:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$184, %rsp
	.cfi_def_cfa_offset 240
	movq	%rsi, %rdx
	movl	12(%rsi), %eax
	leal	37432(%rax), %ecx
	leal	5(%rax), %ebx
	movl	%ebx, 16(%rsp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	movl	%eax, %r15d
	subl	%ecx, %eax
	movl	%eax, 80(%rsp)
	movl	16(%rsi), %eax
	leal	37431(%rax), %ecx
	leal	4(%rax), %esi
	movl	%esi, 20(%rsp)
	movl	%esi, %eax
	imull	%ecx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	movl	%eax, %r14d
	subl	%ecx, %eax
	movl	%eax, 84(%rsp)
	movl	20(%rdx), %eax
	leal	37430(%rax), %ecx
	leal	3(%rax), %ebx
	movl	%ebx, 24(%rsp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	movl	%eax, %r13d
	subl	%ecx, %eax
	movl	%eax, 88(%rsp)
	movl	24(%rdx), %ecx
	leal	37429(%rcx), %eax
	leal	2(%rcx), %esi
	movl	%esi, 28(%rsp)
	movl	%esi, %ecx
	imull	%eax, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	movl	%ecx, %r12d
	subl	%eax, %ecx
	movl	%ecx, 92(%rsp)
	movl	28(%rdx), %ecx
	leal	37428(%rcx), %eax
	leal	1(%rcx), %ebx
	movl	%ebx, 32(%rsp)
	movl	%ebx, %ecx
	imull	%eax, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, %ebp
	subl	%eax, %ecx
	movl	%ecx, 96(%rsp)
	movl	32(%rdx), %esi
	movl	%esi, 12(%rsp)
	leal	37427(%rsi), %eax
	movl	%esi, %ecx
	imull	%eax, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	movl	%ecx, %r8d
	movl	%ecx, 156(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 100(%rsp)
	movl	36(%rdx), %ecx
	leal	37426(%rcx), %eax
	leal	-1(%rcx), %esi
	movl	%esi, 36(%rsp)
	movl	%esi, %ecx
	imull	%eax, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	movl	%ecx, %r9d
	movl	%ecx, 160(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 104(%rsp)
	movl	40(%rdx), %ecx
	leal	37425(%rcx), %eax
	leal	-2(%rcx), %ebx
	movl	%ebx, 40(%rsp)
	movl	%ebx, %ecx
	imull	%eax, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, %r10d
	movl	%ecx, 164(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 108(%rsp)
	movl	44(%rdx), %ecx
	leal	37424(%rcx), %eax
	leal	-3(%rcx), %esi
	movl	%esi, 44(%rsp)
	movl	%esi, %ecx
	imull	%eax, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	movl	%ecx, %esi
	movl	%ecx, 168(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 112(%rsp)
	movl	48(%rdx), %ecx
	leal	37423(%rcx), %eax
	leal	-4(%rcx), %ebx
	movl	%ebx, 48(%rsp)
	movl	%ebx, %ecx
	imull	%eax, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, %r11d
	movl	%ecx, 172(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 116(%rsp)
	movl	52(%rdx), %eax
	leal	37422(%rax), %ecx
	leal	-5(%rax), %ebx
	movl	%ebx, 52(%rsp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	movl	%eax, 140(%rsp)
	movl	%eax, %ebx
	subl	%ecx, %ebx
	movl	%ebx, 120(%rsp)
	movl	56(%rdx), %eax
	leal	37421(%rax), %ecx
	leal	-6(%rax), %ebx
	movl	%ebx, 56(%rsp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	movl	%eax, 144(%rsp)
	movl	%eax, %ebx
	subl	%ecx, %ebx
	movl	%ebx, 124(%rsp)
	movl	60(%rdx), %eax
	leal	37420(%rax), %ecx
	leal	-7(%rax), %ebx
	movl	%ebx, 60(%rsp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	movl	%eax, 148(%rsp)
	movl	%eax, %ebx
	subl	%ecx, %ebx
	movl	%ebx, 128(%rsp)
	movl	64(%rdx), %eax
	leal	37419(%rax), %ecx
	leal	-8(%rax), %ebx
	movl	%ebx, 64(%rsp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	movl	%eax, 152(%rsp)
	movl	%eax, %ebx
	subl	%ecx, %ebx
	movl	%ebx, 132(%rsp)
	movl	68(%rdx), %eax
	leal	37418(%rax), %edx
	subl	$9, %eax
	movl	%eax, 68(%rsp)
	movl	%eax, %ebx
	imull	%edx, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	imull	%eax, %ebx
	movl	%ebx, %ecx
	subl	%edx, %ecx
	movl	%ecx, 136(%rsp)
	leaq	-1(%rdi), %rdx
	movq	%rdx, 72(%rsp)
	testq	%rdi, %rdi
	je	.L246
	movl	%r8d, %eax
	movl	%r9d, %edx
	movl	%r10d, %ecx
	movl	%r11d, %edi
	movl	140(%rsp), %r8d
	movl	144(%rsp), %r9d
	movl	148(%rsp), %r10d
	movl	152(%rsp), %r11d
.L247:
	subl	80(%rsp), %r15d
	subl	84(%rsp), %r14d
	subl	88(%rsp), %r13d
	subl	92(%rsp), %r12d
	subl	96(%rsp), %ebp
	subl	100(%rsp), %eax
	subl	104(%rsp), %edx
	subl	108(%rsp), %ecx
	subl	112(%rsp), %esi
	subl	116(%rsp), %edi
	subl	120(%rsp), %r8d
	subl	124(%rsp), %r9d
	subl	128(%rsp), %r10d
	subl	132(%rsp), %r11d
	subl	136(%rsp), %ebx
	imull	16(%rsp), %r15d
	imull	20(%rsp), %r14d
	imull	24(%rsp), %r13d
	imull	28(%rsp), %r12d
	imull	32(%rsp), %ebp
	imull	12(%rsp), %eax
	imull	36(%rsp), %edx
	imull	40(%rsp), %ecx
	imull	44(%rsp), %esi
	imull	48(%rsp), %edi
	imull	52(%rsp), %r8d
	imull	56(%rsp), %r9d
	imull	60(%rsp), %r10d
	imull	64(%rsp), %r11d
	imull	68(%rsp), %ebx
	imull	16(%rsp), %r15d
	imull	20(%rsp), %r14d
	imull	24(%rsp), %r13d
	imull	28(%rsp), %r12d
	imull	32(%rsp), %ebp
	imull	12(%rsp), %eax
	imull	36(%rsp), %edx
	imull	40(%rsp), %ecx
	imull	44(%rsp), %esi
	imull	48(%rsp), %edi
	imull	52(%rsp), %r8d
	imull	56(%rsp), %r9d
	imull	60(%rsp), %r10d
	imull	64(%rsp), %r11d
	imull	68(%rsp), %ebx
	imull	16(%rsp), %r15d
	imull	20(%rsp), %r14d
	imull	24(%rsp), %r13d
	imull	28(%rsp), %r12d
	imull	32(%rsp), %ebp
	imull	12(%rsp), %eax
	imull	36(%rsp), %edx
	imull	40(%rsp), %ecx
	imull	44(%rsp), %esi
	imull	48(%rsp), %edi
	imull	52(%rsp), %r8d
	imull	56(%rsp), %r9d
	imull	60(%rsp), %r10d
	imull	64(%rsp), %r11d
	imull	68(%rsp), %ebx
	imull	16(%rsp), %r15d
	imull	20(%rsp), %r14d
	imull	24(%rsp), %r13d
	imull	28(%rsp), %r12d
	imull	32(%rsp), %ebp
	imull	12(%rsp), %eax
	imull	36(%rsp), %edx
	imull	40(%rsp), %ecx
	imull	44(%rsp), %esi
	imull	48(%rsp), %edi
	imull	52(%rsp), %r8d
	imull	56(%rsp), %r9d
	imull	60(%rsp), %r10d
	imull	64(%rsp), %r11d
	imull	68(%rsp), %ebx
	imull	16(%rsp), %r15d
	imull	20(%rsp), %r14d
	imull	24(%rsp), %r13d
	imull	28(%rsp), %r12d
	imull	32(%rsp), %ebp
	imull	12(%rsp), %eax
	imull	36(%rsp), %edx
	imull	40(%rsp), %ecx
	imull	44(%rsp), %esi
	imull	48(%rsp), %edi
	imull	52(%rsp), %r8d
	imull	56(%rsp), %r9d
	imull	60(%rsp), %r10d
	imull	64(%rsp), %r11d
	imull	68(%rsp), %ebx
	imull	16(%rsp), %r15d
	imull	20(%rsp), %r14d
	imull	24(%rsp), %r13d
	imull	28(%rsp), %r12d
	imull	32(%rsp), %ebp
	imull	12(%rsp), %eax
	imull	36(%rsp), %edx
	imull	40(%rsp), %ecx
	imull	44(%rsp), %esi
	imull	48(%rsp), %edi
	imull	52(%rsp), %r8d
	imull	56(%rsp), %r9d
	imull	60(%rsp), %r10d
	imull	64(%rsp), %r11d
	imull	68(%rsp), %ebx
	imull	16(%rsp), %r15d
	imull	20(%rsp), %r14d
	imull	24(%rsp), %r13d
	imull	28(%rsp), %r12d
	imull	32(%rsp), %ebp
	imull	12(%rsp), %eax
	imull	36(%rsp), %edx
	imull	40(%rsp), %ecx
	imull	44(%rsp), %esi
	imull	48(%rsp), %edi
	imull	52(%rsp), %r8d
	imull	56(%rsp), %r9d
	imull	60(%rsp), %r10d
	imull	64(%rsp), %r11d
	imull	68(%rsp), %ebx
	imull	16(%rsp), %r15d
	imull	20(%rsp), %r14d
	imull	24(%rsp), %r13d
	imull	28(%rsp), %r12d
	imull	32(%rsp), %ebp
	imull	12(%rsp), %eax
	imull	36(%rsp), %edx
	imull	40(%rsp), %ecx
	imull	44(%rsp), %esi
	imull	48(%rsp), %edi
	imull	52(%rsp), %r8d
	imull	56(%rsp), %r9d
	imull	60(%rsp), %r10d
	imull	64(%rsp), %r11d
	imull	68(%rsp), %ebx
	imull	16(%rsp), %r15d
	imull	20(%rsp), %r14d
	imull	24(%rsp), %r13d
	imull	28(%rsp), %r12d
	imull	32(%rsp), %ebp
	imull	12(%rsp), %eax
	imull	36(%rsp), %edx
	imull	40(%rsp), %ecx
	imull	44(%rsp), %esi
	imull	48(%rsp), %edi
	imull	52(%rsp), %r8d
	imull	56(%rsp), %r9d
	imull	60(%rsp), %r10d
	imull	64(%rsp), %r11d
	imull	68(%rsp), %ebx
	imull	16(%rsp), %r15d
	imull	20(%rsp), %r14d
	imull	24(%rsp), %r13d
	imull	28(%rsp), %r12d
	imull	32(%rsp), %ebp
	imull	12(%rsp), %eax
	imull	36(%rsp), %edx
	imull	40(%rsp), %ecx
	imull	44(%rsp), %esi
	imull	48(%rsp), %edi
	imull	52(%rsp), %r8d
	imull	56(%rsp), %r9d
	imull	60(%rsp), %r10d
	imull	64(%rsp), %r11d
	imull	68(%rsp), %ebx
	subq	$1, 72(%rsp)
	cmpq	$-1, 72(%rsp)
	jne	.L247
	movl	%eax, 156(%rsp)
	movl	%edx, 160(%rsp)
	movl	%ecx, 164(%rsp)
	movl	%esi, 168(%rsp)
	movl	%edi, 172(%rsp)
	movl	%r8d, 140(%rsp)
	movl	%r9d, 144(%rsp)
	movl	%r10d, 148(%rsp)
	movl	%r11d, 152(%rsp)
.L246:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	156(%rsp), %edi
	call	use_int@PLT
	movl	160(%rsp), %edi
	call	use_int@PLT
	movl	164(%rsp), %edi
	call	use_int@PLT
	movl	168(%rsp), %edi
	call	use_int@PLT
	movl	172(%rsp), %edi
	call	use_int@PLT
	movl	140(%rsp), %edi
	call	use_int@PLT
	movl	144(%rsp), %edi
	call	use_int@PLT
	movl	148(%rsp), %edi
	call	use_int@PLT
	movl	152(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$184, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE119:
	.size	integer_mul_14, .-integer_mul_14
	.globl	integer_mul_15
	.type	integer_mul_15, @function
integer_mul_15:
.LFB120:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$184, %rsp
	.cfi_def_cfa_offset 240
	movq	%rsi, %rdx
	movl	12(%rsi), %eax
	leal	37432(%rax), %ecx
	leal	5(%rax), %ebx
	movl	%ebx, 4(%rsp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	movl	%eax, %r15d
	subl	%ecx, %eax
	movl	%eax, 84(%rsp)
	movl	16(%rsi), %eax
	leal	37431(%rax), %ecx
	leal	4(%rax), %ebx
	movl	%ebx, 8(%rsp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	movl	%eax, %r14d
	subl	%ecx, %eax
	movl	%eax, 88(%rsp)
	movl	20(%rsi), %eax
	leal	37430(%rax), %ecx
	leal	3(%rax), %esi
	movl	%esi, 12(%rsp)
	movl	%esi, %eax
	imull	%ecx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	movl	%eax, %r13d
	subl	%ecx, %eax
	movl	%eax, 92(%rsp)
	movl	24(%rdx), %ecx
	leal	37429(%rcx), %eax
	leal	2(%rcx), %ebx
	movl	%ebx, 16(%rsp)
	movl	%ebx, %ecx
	imull	%eax, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	imull	%ebx, %ecx
	movl	%ecx, %r12d
	movl	%ecx, %ebx
	subl	%eax, %ebx
	movl	%ebx, 96(%rsp)
	movl	28(%rdx), %ecx
	leal	37428(%rcx), %eax
	leal	1(%rcx), %esi
	movl	%esi, 20(%rsp)
	movl	%esi, %ecx
	imull	%eax, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	movl	%ecx, %ebp
	movl	%ecx, %ebx
	subl	%eax, %ebx
	movl	%ebx, 100(%rsp)
	movl	32(%rdx), %esi
	movl	%esi, (%rsp)
	leal	37427(%rsi), %eax
	movl	%esi, %ecx
	imull	%eax, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	movl	%ecx, %ebx
	imull	%esi, %ebx
	movl	%ebx, %ecx
	subl	%eax, %ecx
	movl	%ecx, 104(%rsp)
	movl	36(%rdx), %ecx
	leal	37426(%rcx), %eax
	leal	-1(%rcx), %esi
	movl	%esi, 24(%rsp)
	movl	%esi, %ecx
	imull	%eax, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	movl	%ecx, %r11d
	movl	%ecx, 160(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 108(%rsp)
	movl	40(%rdx), %ecx
	leal	37425(%rcx), %eax
	leal	-2(%rcx), %esi
	movl	%esi, 28(%rsp)
	movl	%esi, %ecx
	imull	%eax, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	movl	%ecx, %r10d
	movl	%ecx, 164(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 112(%rsp)
	movl	44(%rdx), %ecx
	leal	37424(%rcx), %eax
	leal	-3(%rcx), %esi
	movl	%esi, 32(%rsp)
	movl	%esi, %ecx
	imull	%eax, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	movl	%ecx, %r9d
	movl	%ecx, 168(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 116(%rsp)
	movl	48(%rdx), %ecx
	leal	37423(%rcx), %eax
	leal	-4(%rcx), %esi
	movl	%esi, 36(%rsp)
	movl	%esi, %ecx
	imull	%eax, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	movl	%ecx, %r8d
	movl	%ecx, 172(%rsp)
	subl	%eax, %ecx
	movl	%ecx, 120(%rsp)
	movl	52(%rdx), %ecx
	leal	37422(%rcx), %eax
	leal	-5(%rcx), %esi
	movl	%esi, 40(%rsp)
	movl	%esi, %ecx
	imull	%eax, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	imull	%esi, %ecx
	movl	%ecx, 80(%rsp)
	movl	%ecx, %esi
	subl	%eax, %esi
	movl	%esi, 124(%rsp)
	movl	56(%rdx), %eax
	leal	37421(%rax), %ecx
	leal	-6(%rax), %esi
	movl	%esi, 44(%rsp)
	movl	%esi, %eax
	imull	%ecx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	movl	%eax, 148(%rsp)
	subl	%ecx, %eax
	movl	%eax, 128(%rsp)
	movl	60(%rdx), %eax
	leal	37420(%rax), %ecx
	leal	-7(%rax), %esi
	movl	%esi, 48(%rsp)
	movl	%esi, %eax
	imull	%ecx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	movl	%eax, %esi
	movl	%eax, 152(%rsp)
	subl	%ecx, %esi
	movl	%esi, 132(%rsp)
	movl	64(%rdx), %eax
	leal	37419(%rax), %ecx
	leal	-8(%rax), %esi
	movl	%esi, 52(%rsp)
	movl	%esi, %eax
	imull	%ecx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	movl	%eax, 156(%rsp)
	subl	%ecx, %eax
	movl	%eax, 136(%rsp)
	movl	68(%rdx), %eax
	leal	37418(%rax), %ecx
	leal	-9(%rax), %esi
	movl	%esi, 56(%rsp)
	movl	%esi, %eax
	imull	%ecx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	movl	%eax, %esi
	movl	%eax, 72(%rsp)
	subl	%ecx, %esi
	movl	%esi, 140(%rsp)
	movl	72(%rdx), %eax
	leal	37417(%rax), %edx
	leal	-10(%rax), %ecx
	movl	%ecx, 60(%rsp)
	movl	%ecx, %eax
	imull	%edx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	movl	%eax, %ecx
	movl	%eax, 76(%rsp)
	subl	%edx, %ecx
	movl	%ecx, 144(%rsp)
	leaq	-1(%rdi), %rdx
	movq	%rdx, 64(%rsp)
	testq	%rdi, %rdi
	je	.L251
	movl	80(%rsp), %edi
	movl	148(%rsp), %esi
	movl	152(%rsp), %ecx
	movl	156(%rsp), %edx
.L252:
	subl	84(%rsp), %r15d
	subl	88(%rsp), %r14d
	subl	92(%rsp), %r13d
	subl	96(%rsp), %r12d
	subl	100(%rsp), %ebp
	subl	104(%rsp), %ebx
	subl	108(%rsp), %r11d
	subl	112(%rsp), %r10d
	subl	116(%rsp), %r9d
	subl	120(%rsp), %r8d
	subl	124(%rsp), %edi
	subl	128(%rsp), %esi
	subl	132(%rsp), %ecx
	subl	136(%rsp), %edx
	movl	%edx, 80(%rsp)
	movl	72(%rsp), %eax
	subl	140(%rsp), %eax
	movl	76(%rsp), %edx
	subl	144(%rsp), %edx
	imull	4(%rsp), %r15d
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ebp
	imull	(%rsp), %ebx
	imull	24(%rsp), %r11d
	imull	28(%rsp), %r10d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %r8d
	imull	40(%rsp), %edi
	imull	44(%rsp), %esi
	movl	%esi, 72(%rsp)
	imull	48(%rsp), %ecx
	movl	80(%rsp), %esi
	imull	52(%rsp), %esi
	movl	%esi, 76(%rsp)
	imull	56(%rsp), %eax
	imull	60(%rsp), %edx
	movl	%edx, 80(%rsp)
	imull	4(%rsp), %r15d
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ebp
	imull	(%rsp), %ebx
	imull	24(%rsp), %r11d
	imull	28(%rsp), %r10d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %r8d
	imull	40(%rsp), %edi
	movl	72(%rsp), %esi
	imull	44(%rsp), %esi
	imull	48(%rsp), %ecx
	movl	%ecx, 72(%rsp)
	movl	76(%rsp), %ecx
	imull	52(%rsp), %ecx
	movl	%ecx, %edx
	imull	56(%rsp), %eax
	movl	%eax, 76(%rsp)
	movl	80(%rsp), %eax
	imull	60(%rsp), %eax
	movl	%eax, 80(%rsp)
	imull	4(%rsp), %r15d
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ebp
	imull	(%rsp), %ebx
	imull	24(%rsp), %r11d
	imull	28(%rsp), %r10d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %r8d
	imull	40(%rsp), %edi
	imull	44(%rsp), %esi
	movl	72(%rsp), %ecx
	imull	48(%rsp), %ecx
	imull	52(%rsp), %edx
	movl	76(%rsp), %eax
	imull	56(%rsp), %eax
	movl	%eax, 72(%rsp)
	movl	80(%rsp), %eax
	imull	60(%rsp), %eax
	movl	%eax, 76(%rsp)
	imull	4(%rsp), %r15d
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ebp
	imull	(%rsp), %ebx
	imull	24(%rsp), %r11d
	imull	28(%rsp), %r10d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %r8d
	imull	40(%rsp), %edi
	imull	44(%rsp), %esi
	imull	48(%rsp), %ecx
	imull	52(%rsp), %edx
	movl	72(%rsp), %eax
	imull	56(%rsp), %eax
	movl	%eax, 72(%rsp)
	movl	76(%rsp), %eax
	imull	60(%rsp), %eax
	movl	%eax, 76(%rsp)
	imull	4(%rsp), %r15d
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ebp
	imull	(%rsp), %ebx
	imull	24(%rsp), %r11d
	imull	28(%rsp), %r10d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %r8d
	imull	40(%rsp), %edi
	imull	44(%rsp), %esi
	imull	48(%rsp), %ecx
	imull	52(%rsp), %edx
	movl	72(%rsp), %eax
	imull	56(%rsp), %eax
	movl	%eax, 72(%rsp)
	movl	76(%rsp), %eax
	imull	60(%rsp), %eax
	movl	%eax, 76(%rsp)
	imull	4(%rsp), %r15d
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ebp
	imull	(%rsp), %ebx
	imull	24(%rsp), %r11d
	imull	28(%rsp), %r10d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %r8d
	imull	40(%rsp), %edi
	imull	44(%rsp), %esi
	imull	48(%rsp), %ecx
	imull	52(%rsp), %edx
	movl	72(%rsp), %eax
	imull	56(%rsp), %eax
	movl	%eax, 72(%rsp)
	movl	76(%rsp), %eax
	imull	60(%rsp), %eax
	movl	%eax, 76(%rsp)
	imull	4(%rsp), %r15d
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ebp
	imull	(%rsp), %ebx
	imull	24(%rsp), %r11d
	imull	28(%rsp), %r10d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %r8d
	imull	40(%rsp), %edi
	imull	44(%rsp), %esi
	imull	48(%rsp), %ecx
	imull	52(%rsp), %edx
	movl	72(%rsp), %eax
	imull	56(%rsp), %eax
	movl	%eax, 72(%rsp)
	movl	76(%rsp), %eax
	imull	60(%rsp), %eax
	movl	%eax, 76(%rsp)
	imull	4(%rsp), %r15d
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ebp
	imull	(%rsp), %ebx
	imull	24(%rsp), %r11d
	imull	28(%rsp), %r10d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %r8d
	imull	40(%rsp), %edi
	imull	44(%rsp), %esi
	imull	48(%rsp), %ecx
	imull	52(%rsp), %edx
	movl	72(%rsp), %eax
	imull	56(%rsp), %eax
	movl	%eax, 72(%rsp)
	movl	76(%rsp), %eax
	imull	60(%rsp), %eax
	movl	%eax, 76(%rsp)
	imull	4(%rsp), %r15d
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ebp
	imull	(%rsp), %ebx
	imull	24(%rsp), %r11d
	imull	28(%rsp), %r10d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %r8d
	imull	40(%rsp), %edi
	imull	44(%rsp), %esi
	imull	48(%rsp), %ecx
	imull	52(%rsp), %edx
	movl	72(%rsp), %eax
	imull	56(%rsp), %eax
	movl	%eax, 72(%rsp)
	movl	76(%rsp), %eax
	imull	60(%rsp), %eax
	movl	%eax, 76(%rsp)
	imull	4(%rsp), %r15d
	imull	8(%rsp), %r14d
	imull	12(%rsp), %r13d
	imull	16(%rsp), %r12d
	imull	20(%rsp), %ebp
	imull	(%rsp), %ebx
	imull	24(%rsp), %r11d
	imull	28(%rsp), %r10d
	imull	32(%rsp), %r9d
	imull	36(%rsp), %r8d
	imull	40(%rsp), %edi
	imull	44(%rsp), %esi
	imull	48(%rsp), %ecx
	imull	52(%rsp), %edx
	movl	72(%rsp), %eax
	imull	56(%rsp), %eax
	movl	%eax, 72(%rsp)
	movl	76(%rsp), %eax
	imull	60(%rsp), %eax
	movl	%eax, 76(%rsp)
	subq	$1, 64(%rsp)
	movq	64(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L252
	movl	%r11d, 160(%rsp)
	movl	%r10d, 164(%rsp)
	movl	%r9d, 168(%rsp)
	movl	%r8d, 172(%rsp)
	movl	%edi, 80(%rsp)
	movl	%esi, 148(%rsp)
	movl	%ecx, 152(%rsp)
	movl	%edx, 156(%rsp)
.L251:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	160(%rsp), %edi
	call	use_int@PLT
	movl	164(%rsp), %edi
	call	use_int@PLT
	movl	168(%rsp), %edi
	call	use_int@PLT
	movl	172(%rsp), %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	call	use_int@PLT
	movl	148(%rsp), %edi
	call	use_int@PLT
	movl	152(%rsp), %edi
	call	use_int@PLT
	movl	156(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	call	use_int@PLT
	movl	76(%rsp), %edi
	call	use_int@PLT
	addq	$184, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE120:
	.size	integer_mul_15, .-integer_mul_15
	.globl	integer_div_0
	.type	integer_div_0, @function
integer_div_0:
.LFB121:
	.cfi_startproc
	endbr64
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movq	%rdi, %rax
	movl	12(%rsi), %ecx
	leal	37(%rcx), %edi
	addl	$38, %ecx
	sall	$20, %ecx
	leaq	-1(%rax), %rsi
	testq	%rax, %rax
	je	.L256
.L257:
	movl	%ecx, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	subq	$1, %rsi
	cmpq	$-1, %rsi
	jne	.L257
.L256:
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE121:
	.size	integer_div_0, .-integer_div_0
	.globl	integer_div_1
	.type	integer_div_1, @function
integer_div_1:
.LFB122:
	.cfi_startproc
	endbr64
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movq	%rdi, %rax
	movl	12(%rsi), %r8d
	leal	37(%r8), %edi
	addl	$38, %r8d
	sall	$20, %r8d
	movl	16(%rsi), %ecx
	leal	36(%rcx), %ebx
	addl	$37, %ecx
	sall	$20, %ecx
	leaq	-1(%rax), %rsi
	testq	%rax, %rax
	je	.L261
.L262:
	movl	%r8d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%ebx
	movl	%eax, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ecx, %eax
	cltd
	idivl	%r9d
	movl	%eax, %ebx
	subq	$1, %rsi
	cmpq	$-1, %rsi
	jne	.L262
.L261:
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE122:
	.size	integer_div_1, .-integer_div_1
	.globl	integer_div_2
	.type	integer_div_2, @function
integer_div_2:
.LFB123:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	pushq	%rbx
	.cfi_def_cfa_offset 24
	.cfi_offset 3, -24
	subq	$8, %rsp
	.cfi_def_cfa_offset 32
	movq	%rdi, %rax
	movl	12(%rsi), %r8d
	leal	37(%r8), %r9d
	addl	$38, %r8d
	sall	$20, %r8d
	movl	16(%rsi), %edi
	leal	36(%rdi), %ebp
	addl	$37, %edi
	sall	$20, %edi
	movl	20(%rsi), %ecx
	leal	35(%rcx), %ebx
	addl	$36, %ecx
	sall	$20, %ecx
	leaq	-1(%rax), %rsi
	testq	%rax, %rax
	je	.L266
.L267:
	movl	%r8d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%edi, %eax
	cltd
	idivl	%ebp
	movl	%eax, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%ebx
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%edi, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%edi, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%edi, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%edi, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%edi, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%edi, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%edi, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%edi, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%edi, %eax
	cltd
	idivl	%r11d
	movl	%eax, %ebp
	movl	%ecx, %eax
	cltd
	idivl	%r10d
	movl	%eax, %ebx
	subq	$1, %rsi
	cmpq	$-1, %rsi
	jne	.L267
.L266:
	movl	%r9d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 24
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE123:
	.size	integer_div_2, .-integer_div_2
	.globl	integer_div_3
	.type	integer_div_3, @function
integer_div_3:
.LFB124:
	.cfi_startproc
	endbr64
	pushq	%r13
	.cfi_def_cfa_offset 16
	.cfi_offset 13, -16
	pushq	%r12
	.cfi_def_cfa_offset 24
	.cfi_offset 12, -24
	pushq	%rbp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset 3, -40
	subq	$8, %rsp
	.cfi_def_cfa_offset 48
	movq	%rdi, %rax
	movl	12(%rsi), %r9d
	leal	37(%r9), %r10d
	addl	$38, %r9d
	sall	$20, %r9d
	movl	16(%rsi), %r8d
	leal	36(%r8), %r12d
	addl	$37, %r8d
	sall	$20, %r8d
	movl	20(%rsi), %edi
	leal	35(%rdi), %ebp
	addl	$36, %edi
	sall	$20, %edi
	movl	24(%rsi), %ecx
	leal	34(%rcx), %ebx
	addl	$35, %ecx
	sall	$20, %ecx
	leaq	-1(%rax), %rsi
	testq	%rax, %rax
	je	.L271
.L272:
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r13d
	movl	%edi, %eax
	cltd
	idivl	%ebp
	movl	%eax, %r12d
	movl	%ecx, %eax
	cltd
	idivl	%ebx
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%r13d
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%r12d
	movl	%eax, %ebx
	movl	%ecx, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%ecx, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%ecx, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%ecx, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%ecx, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%ecx, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%ecx, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%ecx, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %r12d
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebp
	movl	%ecx, %eax
	cltd
	idivl	%r11d
	movl	%eax, %ebx
	subq	$1, %rsi
	cmpq	$-1, %rsi
	jne	.L272
.L271:
	movl	%r10d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%rbp
	.cfi_def_cfa_offset 24
	popq	%r12
	.cfi_def_cfa_offset 16
	popq	%r13
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE124:
	.size	integer_div_3, .-integer_div_3
	.globl	integer_div_4
	.type	integer_div_4, @function
integer_div_4:
.LFB125:
	.cfi_startproc
	endbr64
	pushq	%r13
	.cfi_def_cfa_offset 16
	.cfi_offset 13, -16
	pushq	%r12
	.cfi_def_cfa_offset 24
	.cfi_offset 12, -24
	pushq	%rbp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset 3, -40
	subq	$8, %rsp
	.cfi_def_cfa_offset 48
	movq	%rdi, %rdx
	movq	%rsi, %rax
	movl	12(%rsi), %r9d
	leal	37(%r9), %r10d
	addl	$38, %r9d
	sall	$20, %r9d
	movl	16(%rsi), %r8d
	leal	36(%r8), %ebp
	addl	$37, %r8d
	sall	$20, %r8d
	movl	20(%rsi), %edi
	leal	35(%rdi), %ebx
	addl	$36, %edi
	sall	$20, %edi
	movl	24(%rsi), %esi
	leal	34(%rsi), %r13d
	addl	$35, %esi
	sall	$20, %esi
	movl	28(%rax), %ecx
	leal	33(%rcx), %r12d
	addl	$34, %ecx
	sall	$20, %ecx
	leaq	-1(%rdx), %r11
	testq	%rdx, %rdx
	je	.L276
.L277:
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r9d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	subq	$1, %r11
	cmpq	$-1, %r11
	jne	.L277
.L276:
	movl	%r10d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%rbp
	.cfi_def_cfa_offset 24
	popq	%r12
	.cfi_def_cfa_offset 16
	popq	%r13
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE125:
	.size	integer_div_4, .-integer_div_4
	.globl	integer_div_5
	.type	integer_div_5, @function
integer_div_5:
.LFB126:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$8, %rsp
	.cfi_def_cfa_offset 64
	movq	%rdi, %rdx
	movq	%rsi, %rax
	movl	12(%rsi), %r10d
	leal	37(%r10), %r11d
	addl	$38, %r10d
	sall	$20, %r10d
	movl	16(%rsi), %r9d
	leal	36(%r9), %r15d
	addl	$37, %r9d
	sall	$20, %r9d
	movl	20(%rsi), %r8d
	leal	35(%r8), %r14d
	addl	$36, %r8d
	sall	$20, %r8d
	movl	24(%rsi), %edi
	leal	34(%rdi), %r13d
	addl	$35, %edi
	sall	$20, %edi
	movl	28(%rsi), %esi
	leal	33(%rsi), %r12d
	addl	$34, %esi
	sall	$20, %esi
	movl	32(%rax), %ecx
	leal	32(%rcx), %ebp
	addl	$33, %ecx
	sall	$20, %ecx
	leaq	-1(%rdx), %rbx
	testq	%rdx, %rdx
	je	.L281
.L282:
	movl	%r10d, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	%edi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%esi, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%r10d, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	%edi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%esi, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%r10d, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	%edi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%esi, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%r10d, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	%edi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%esi, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%r10d, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	%edi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%esi, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%r10d, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	%edi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%esi, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%r10d, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	%edi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%esi, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%r10d, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	%edi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%esi, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%r10d, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	%edi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%esi, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%r10d, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r9d, %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	%edi, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%esi, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	subq	$1, %rbx
	cmpq	$-1, %rbx
	jne	.L282
.L281:
	movl	%r11d, %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE126:
	.size	integer_div_5, .-integer_div_5
	.globl	integer_div_6
	.type	integer_div_6, @function
integer_div_6:
.LFB127:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$56, %rsp
	.cfi_def_cfa_offset 112
	movq	%rdi, %rdx
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	37(%rcx), %edi
	addl	$38, %ecx
	sall	$20, %ecx
	movl	%ecx, 12(%rsp)
	movl	16(%rsi), %ecx
	leal	36(%rcx), %esi
	movl	%esi, 28(%rsp)
	addl	$37, %ecx
	movl	%ecx, %ebx
	sall	$20, %ebx
	movl	20(%rax), %ecx
	leal	35(%rcx), %r8d
	movl	%r8d, 32(%rsp)
	addl	$36, %ecx
	movl	%ecx, %r15d
	sall	$20, %r15d
	movl	24(%rax), %ecx
	leal	34(%rcx), %r9d
	movl	%r9d, 36(%rsp)
	addl	$35, %ecx
	movl	%ecx, %r14d
	sall	$20, %r14d
	movl	28(%rax), %ecx
	leal	33(%rcx), %r10d
	movl	%r10d, 40(%rsp)
	addl	$34, %ecx
	movl	%ecx, %r13d
	sall	$20, %r13d
	movl	32(%rax), %ecx
	leal	32(%rcx), %r11d
	movl	%r11d, 44(%rsp)
	addl	$33, %ecx
	movl	%ecx, %r12d
	sall	$20, %r12d
	movl	36(%rax), %eax
	leal	31(%rax), %ecx
	movl	%ecx, 24(%rsp)
	addl	$32, %eax
	sall	$20, %eax
	movl	%eax, %ebp
	leaq	-1(%rdx), %rax
	movq	%rax, 16(%rsp)
	testq	%rdx, %rdx
	je	.L286
	movl	%esi, %ecx
	movl	%r8d, %esi
	movl	%r9d, %r8d
	movl	%r10d, %r9d
	movl	%r11d, %r10d
	movl	24(%rsp), %r11d
.L287:
	movl	12(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%ebp, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	12(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%ebp, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	12(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%ebp, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	12(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%ebp, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	12(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%ebp, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	12(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%ebp, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	12(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%ebp, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	12(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%ebp, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	12(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%ebp, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	12(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	%ebp, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	subq	$1, 16(%rsp)
	movq	16(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L287
	movl	%ecx, 28(%rsp)
	movl	%esi, 32(%rsp)
	movl	%r8d, 36(%rsp)
	movl	%r9d, 40(%rsp)
	movl	%r10d, 44(%rsp)
	movl	%r11d, 24(%rsp)
.L286:
	call	use_int@PLT
	movl	28(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	movl	36(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	44(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE127:
	.size	integer_div_6, .-integer_div_6
	.globl	integer_div_7
	.type	integer_div_7, @function
integer_div_7:
.LFB128:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$40, %rsp
	.cfi_def_cfa_offset 96
	movq	%rdi, %rdx
	movq	%rsi, %rax
	movl	12(%rsi), %r8d
	leal	37(%r8), %r9d
	addl	$38, %r8d
	sall	$20, %r8d
	movl	16(%rsi), %edi
	leal	36(%rdi), %r14d
	addl	$37, %edi
	sall	$20, %edi
	movl	20(%rsi), %ecx
	leal	35(%rcx), %r13d
	addl	$36, %ecx
	sall	$20, %ecx
	movl	%ecx, 4(%rsp)
	movl	24(%rsi), %ecx
	leal	34(%rcx), %r12d
	addl	$35, %ecx
	sall	$20, %ecx
	movl	%ecx, 8(%rsp)
	movl	28(%rsi), %ecx
	leal	33(%rcx), %ebp
	addl	$34, %ecx
	sall	$20, %ecx
	movl	%ecx, 12(%rsp)
	movl	32(%rsi), %ecx
	leal	32(%rcx), %esi
	movl	%esi, 24(%rsp)
	addl	$33, %ecx
	sall	$20, %ecx
	movl	%ecx, %r15d
	movl	36(%rax), %esi
	leal	31(%rsi), %ebx
	movl	%ebx, 28(%rsp)
	addl	$32, %esi
	sall	$20, %esi
	movl	40(%rax), %ecx
	leal	30(%rcx), %ebx
	addl	$31, %ecx
	sall	$20, %ecx
	leaq	-1(%rdx), %rax
	movq	%rax, 16(%rsp)
	testq	%rdx, %rdx
	je	.L291
	movl	%r13d, %r11d
	movl	%r15d, %r13d
	movl	%ecx, %r15d
	movl	%r14d, %ecx
	movl	%esi, %r14d
	movl	%r12d, %r10d
	movl	%edi, %r12d
	movl	%ebp, %eax
	movl	%r8d, %ebp
	movl	24(%rsp), %edi
	movl	28(%rsp), %esi
	movl	%eax, %r8d
.L292:
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%ecx
	movl	%eax, 24(%rsp)
	movl	4(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	8(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	12(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r15d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ecx
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	24(%rsp)
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	8(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	12(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	8(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	12(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	8(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	12(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	8(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	12(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	8(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	12(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	8(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	12(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	8(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	12(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	8(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	12(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%eax, 24(%rsp)
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ecx
	movl	4(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	8(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	12(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r15d, %eax
	cltd
	idivl	24(%rsp)
	movl	%eax, %ebx
	subq	$1, 16(%rsp)
	movq	16(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L292
	movl	%ecx, %r14d
	movl	%r11d, %r13d
	movl	%r10d, %r12d
	movl	%r8d, %ebp
	movl	%edi, 24(%rsp)
	movl	%esi, 28(%rsp)
.L291:
	movl	%r9d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	call	use_int@PLT
	movl	28(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE128:
	.size	integer_div_7, .-integer_div_7
	.globl	integer_div_8
	.type	integer_div_8, @function
integer_div_8:
.LFB129:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$56, %rsp
	.cfi_def_cfa_offset 112
	movq	%rdi, %rdx
	movq	%rsi, %rax
	movl	12(%rsi), %r8d
	leal	37(%r8), %r15d
	addl	$38, %r8d
	sall	$20, %r8d
	movl	16(%rsi), %ecx
	leal	36(%rcx), %r14d
	addl	$37, %ecx
	sall	$20, %ecx
	movl	%ecx, 4(%rsp)
	movl	20(%rsi), %ecx
	leal	35(%rcx), %r13d
	addl	$36, %ecx
	sall	$20, %ecx
	movl	%ecx, 8(%rsp)
	movl	24(%rsi), %ecx
	leal	34(%rcx), %r12d
	addl	$35, %ecx
	sall	$20, %ecx
	movl	%ecx, 12(%rsp)
	movl	28(%rsi), %ecx
	leal	33(%rcx), %ebp
	addl	$34, %ecx
	sall	$20, %ecx
	movl	%ecx, 16(%rsp)
	movl	32(%rsi), %ecx
	leal	32(%rcx), %ebx
	movl	%ebx, 36(%rsp)
	addl	$33, %ecx
	sall	$20, %ecx
	movl	%ecx, 20(%rsp)
	movl	36(%rsi), %edi
	leal	31(%rdi), %ebx
	movl	%ebx, 40(%rsp)
	addl	$32, %edi
	sall	$20, %edi
	movl	40(%rsi), %esi
	leal	30(%rsi), %r9d
	movl	%r9d, 32(%rsp)
	addl	$31, %esi
	sall	$20, %esi
	movl	44(%rax), %ecx
	leal	29(%rcx), %ebx
	addl	$30, %ecx
	sall	$20, %ecx
	leaq	-1(%rdx), %rax
	movq	%rax, 24(%rsp)
	testq	%rdx, %rdx
	je	.L296
	movl	%ecx, 32(%rsp)
	movl	%r15d, %ecx
	movl	%esi, %r15d
	movl	%r14d, %r11d
	movl	%edi, %r14d
	movl	%r13d, %eax
	movl	%r8d, %r13d
	movl	%r9d, %esi
	movl	%eax, %r8d
.L297:
	movl	%r13d, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	4(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, 44(%rsp)
	movl	8(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r9d
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r8d
	movl	16(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %edi
	movl	20(%rsp), %eax
	cltd
	idivl	36(%rsp)
	movl	%eax, %ebp
	movl	%r14d, %eax
	cltd
	idivl	40(%rsp)
	movl	%eax, %r11d
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%eax, %r10d
	movl	32(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %esi
	movl	%r13d, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	44(%rsp)
	movl	%eax, %ecx
	movl	8(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	12(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	16(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	20(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%r14d, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r15d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	32(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %r12d
	movl	%r13d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %esi
	movl	4(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	8(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	12(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	16(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %ebx
	movl	20(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%r14d, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r15d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	32(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %edi
	movl	%r13d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	4(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	8(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	12(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	20(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%r14d, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	%r15d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %r8d
	movl	32(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%r13d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	4(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	8(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r10d
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	20(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	%r14d, %eax
	cltd
	idivl	%r11d
	movl	%eax, %r9d
	movl	%r15d, %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	32(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%r13d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	4(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %r11d
	movl	8(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	20(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ecx
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r15d, %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	32(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	%r13d, %eax
	cltd
	idivl	%esi
	movl	%eax, %ebp
	movl	4(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	8(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %esi
	movl	20(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r15d, %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	32(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	4(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	8(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %edi
	movl	16(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	20(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r15d, %eax
	cltd
	idivl	%r8d
	movl	%eax, %r12d
	movl	32(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	4(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	8(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r8d
	movl	12(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	16(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	20(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, 36(%rsp)
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r15d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r10d
	movl	32(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %ecx
	movl	4(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	8(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	12(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %ebp
	movl	20(%rsp), %eax
	cltd
	idivl	36(%rsp)
	movl	%eax, 36(%rsp)
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	movl	%eax, 40(%rsp)
	movl	%r15d, %eax
	cltd
	idivl	%r10d
	movl	%eax, %esi
	movl	32(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	subq	$1, 24(%rsp)
	movq	24(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L297
	movl	%ecx, %r15d
	movl	%r11d, %r14d
	movl	%r8d, %r13d
	movl	%esi, 32(%rsp)
.L296:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	36(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE129:
	.size	integer_div_8, .-integer_div_8
	.globl	integer_div_9
	.type	integer_div_9, @function
integer_div_9:
.LFB130:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$72, %rsp
	.cfi_def_cfa_offset 128
	movq	%rsi, %rax
	movl	12(%rsi), %edx
	leal	37(%rdx), %r8d
	addl	$38, %edx
	sall	$20, %edx
	movl	%edx, 4(%rsp)
	movl	16(%rsi), %edx
	leal	36(%rdx), %ebx
	movl	%ebx, 44(%rsp)
	addl	$37, %edx
	sall	$20, %edx
	movl	%edx, 8(%rsp)
	movl	20(%rsi), %edx
	leal	35(%rdx), %ebx
	movl	%ebx, 48(%rsp)
	addl	$36, %edx
	sall	$20, %edx
	movl	%edx, 12(%rsp)
	movl	24(%rsi), %edx
	leal	34(%rdx), %ebx
	movl	%ebx, 52(%rsp)
	addl	$35, %edx
	sall	$20, %edx
	movl	%edx, 16(%rsp)
	movl	28(%rsi), %edx
	leal	33(%rdx), %esi
	movl	%esi, 56(%rsp)
	addl	$34, %edx
	sall	$20, %edx
	movl	%edx, 20(%rsp)
	movl	32(%rax), %edx
	leal	32(%rdx), %r14d
	addl	$33, %edx
	sall	$20, %edx
	movl	%edx, 24(%rsp)
	movl	36(%rax), %edx
	leal	31(%rdx), %r13d
	addl	$32, %edx
	sall	$20, %edx
	movl	%edx, 28(%rsp)
	movl	40(%rax), %edx
	leal	30(%rdx), %r12d
	addl	$31, %edx
	sall	$20, %edx
	movl	%edx, 40(%rsp)
	movl	44(%rax), %esi
	leal	29(%rsi), %ebp
	addl	$30, %esi
	sall	$20, %esi
	movl	48(%rax), %ecx
	leal	28(%rcx), %ebx
	addl	$29, %ecx
	sall	$20, %ecx
	leaq	-1(%rdi), %rax
	movq	%rax, 32(%rsp)
	testq	%rdi, %rdi
	je	.L301
	movl	%ecx, %r15d
	movl	%r8d, %r9d
	movl	%r14d, %r8d
	movl	%esi, %r14d
.L302:
	movl	4(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, 60(%rsp)
	movl	8(%rsp), %eax
	cltd
	idivl	44(%rsp)
	movl	%eax, 44(%rsp)
	movl	12(%rsp), %eax
	cltd
	idivl	48(%rsp)
	movl	%eax, %r11d
	movl	16(%rsp), %eax
	cltd
	idivl	52(%rsp)
	movl	%eax, %r10d
	movl	20(%rsp), %eax
	cltd
	idivl	56(%rsp)
	movl	%eax, %r9d
	movl	24(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %edi
	movl	28(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %esi
	movl	40(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %ecx
	movl	%r14d, %eax
	cltd
	idivl	%ebp
	movl	%eax, %r8d
	movl	%r15d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	60(%rsp)
	movl	%eax, %ebp
	movl	8(%rsp), %eax
	cltd
	idivl	44(%rsp)
	movl	%eax, %r12d
	movl	12(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	16(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	20(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	24(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	28(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	40(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%eax, %r13d
	movl	%r15d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	8(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	12(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	16(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	20(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r8d
	movl	24(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	28(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	40(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %r9d
	movl	%r14d, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%r15d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	8(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	12(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	16(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %ecx
	movl	20(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	24(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	28(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %r10d
	movl	40(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r14d, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%r15d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	8(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	12(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %esi
	movl	16(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	20(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	24(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %r11d
	movl	28(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	40(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r14d, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%r15d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	8(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %edi
	movl	12(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	16(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	20(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r12d
	movl	24(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	28(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	40(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r14d, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%r15d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %r8d
	movl	8(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	12(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	16(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ebp
	movl	20(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	24(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	28(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	40(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r14d, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%r15d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ecx
	movl	4(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	8(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	12(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %ebx
	movl	16(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	20(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	24(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	28(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	40(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	%r14d, %eax
	cltd
	idivl	%r13d
	movl	%eax, %esi
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	4(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	8(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %r13d
	movl	12(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	16(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	20(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	24(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	28(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	40(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %edi
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	4(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r9d
	movl	8(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, 44(%rsp)
	movl	12(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, 48(%rsp)
	movl	16(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, 52(%rsp)
	movl	20(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, 56(%rsp)
	movl	24(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r8d
	movl	28(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r13d
	movl	40(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %r12d
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%eax, %ebp
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%eax, %ebx
	subq	$1, 32(%rsp)
	movq	32(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L302
	movl	%r8d, %r14d
	movl	%r9d, %r8d
.L301:
	movl	%r8d, %edi
	call	use_int@PLT
	movl	44(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	call	use_int@PLT
	movl	52(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE130:
	.size	integer_div_9, .-integer_div_9
	.globl	integer_div_10
	.type	integer_div_10, @function
integer_div_10:
.LFB131:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$88, %rsp
	.cfi_def_cfa_offset 144
	movq	%rdi, %r8
	movq	%rsi, %rax
	movl	12(%rsi), %edx
	leal	37(%rdx), %edi
	addl	$38, %edx
	sall	$20, %edx
	movl	%edx, 4(%rsp)
	movl	16(%rsi), %edx
	leal	36(%rdx), %esi
	movl	%esi, 52(%rsp)
	addl	$37, %edx
	sall	$20, %edx
	movl	%edx, 8(%rsp)
	movl	20(%rax), %edx
	leal	35(%rdx), %esi
	movl	%esi, 56(%rsp)
	addl	$36, %edx
	sall	$20, %edx
	movl	%edx, 12(%rsp)
	movl	24(%rax), %edx
	leal	34(%rdx), %esi
	movl	%esi, 60(%rsp)
	addl	$35, %edx
	sall	$20, %edx
	movl	%edx, 16(%rsp)
	movl	28(%rax), %edx
	leal	33(%rdx), %esi
	movl	%esi, %r15d
	addl	$34, %edx
	sall	$20, %edx
	movl	%edx, 20(%rsp)
	movl	32(%rax), %edx
	leal	32(%rdx), %ecx
	movl	%ecx, 64(%rsp)
	addl	$33, %edx
	sall	$20, %edx
	movl	%edx, 24(%rsp)
	movl	36(%rax), %edx
	leal	31(%rdx), %r14d
	addl	$32, %edx
	sall	$20, %edx
	movl	%edx, 28(%rsp)
	movl	40(%rax), %edx
	leal	30(%rdx), %r13d
	addl	$31, %edx
	sall	$20, %edx
	movl	%edx, 32(%rsp)
	movl	44(%rax), %edx
	leal	29(%rdx), %r12d
	addl	$30, %edx
	sall	$20, %edx
	movl	%edx, 36(%rsp)
	movl	48(%rax), %edx
	leal	28(%rdx), %ebp
	addl	$29, %edx
	sall	$20, %edx
	movl	%edx, 48(%rsp)
	movl	52(%rax), %ecx
	leal	27(%rcx), %ebx
	addl	$28, %ecx
	sall	$20, %ecx
	leaq	-1(%r8), %rax
	movq	%rax, 40(%rsp)
	testq	%r8, %r8
	je	.L306
	movl	%ecx, %r15d
.L307:
	movl	4(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, 68(%rsp)
	movl	8(%rsp), %eax
	cltd
	idivl	52(%rsp)
	movl	%eax, %r11d
	movl	12(%rsp), %eax
	cltd
	idivl	56(%rsp)
	movl	%eax, %r10d
	movl	16(%rsp), %eax
	cltd
	idivl	60(%rsp)
	movl	%eax, %r9d
	movl	20(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	24(%rsp), %eax
	cltd
	idivl	64(%rsp)
	movl	%eax, %r8d
	movl	28(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %ecx
	movl	32(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	36(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	48(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %edi
	movl	%r15d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	68(%rsp)
	movl	%eax, %ebp
	movl	8(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	12(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	16(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	20(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	24(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	28(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %r14d
	movl	32(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	36(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %ecx
	movl	48(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %r12d
	movl	%r15d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	8(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	12(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	16(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %edi
	movl	20(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %r9d
	movl	24(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	28(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	32(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %esi
	movl	36(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %r13d
	movl	48(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r15d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	8(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	12(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %ecx
	movl	16(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %r10d
	movl	20(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	24(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	28(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %edi
	movl	32(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %r14d
	movl	36(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	48(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r15d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	8(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %esi
	movl	12(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %r11d
	movl	16(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	20(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	24(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %ecx
	movl	28(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %r8d
	movl	32(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	36(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	48(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r15d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	4(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %edi
	movl	8(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %ebp
	movl	12(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	16(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	20(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %esi
	movl	24(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %r9d
	movl	28(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	32(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	36(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	48(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r15d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ecx
	movl	4(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %ebx
	movl	8(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	12(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	16(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %edi
	movl	20(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %r10d
	movl	24(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	28(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	32(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	36(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	48(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %esi
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%eax, %r12d
	movl	4(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	8(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	12(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %ecx
	movl	16(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %r11d
	movl	20(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	24(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	28(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	32(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	36(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %r13d
	movl	%r15d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	4(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	8(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %esi
	movl	12(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ebp
	movl	16(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	20(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	24(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	28(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	32(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %ecx
	movl	36(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, 68(%rsp)
	movl	48(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, 72(%rsp)
	movl	%r15d, %eax
	cltd
	idivl	%r12d
	movl	%eax, 76(%rsp)
	movl	4(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %edi
	movl	8(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, 52(%rsp)
	movl	12(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, 56(%rsp)
	movl	16(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, 60(%rsp)
	movl	20(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %esi
	movl	24(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, 64(%rsp)
	movl	28(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r14d
	movl	32(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %r13d
	movl	36(%rsp), %eax
	cltd
	idivl	68(%rsp)
	movl	%eax, %r12d
	movl	48(%rsp), %eax
	cltd
	idivl	72(%rsp)
	movl	%eax, %ebp
	movl	%r15d, %eax
	cltd
	idivl	76(%rsp)
	movl	%eax, %ebx
	subq	$1, 40(%rsp)
	movq	40(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L307
	movl	%esi, %r15d
.L306:
	call	use_int@PLT
	movl	52(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	movl	60(%rsp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE131:
	.size	integer_div_10, .-integer_div_10
	.globl	integer_div_11
	.type	integer_div_11, @function
integer_div_11:
.LFB132:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$104, %rsp
	.cfi_def_cfa_offset 160
	movq	%rdi, %rcx
	movq	%rsi, %rax
	movl	12(%rsi), %edx
	leal	37(%rdx), %esi
	addl	$38, %edx
	sall	$20, %edx
	movl	%edx, 12(%rsp)
	movl	16(%rax), %edx
	leal	36(%rdx), %ebx
	movl	%ebx, 68(%rsp)
	addl	$37, %edx
	sall	$20, %edx
	movl	%edx, 16(%rsp)
	movl	20(%rax), %edx
	leal	35(%rdx), %ebx
	movl	%ebx, 72(%rsp)
	addl	$36, %edx
	sall	$20, %edx
	movl	%edx, 20(%rsp)
	movl	24(%rax), %edx
	leal	34(%rdx), %ebx
	movl	%ebx, 76(%rsp)
	addl	$35, %edx
	sall	$20, %edx
	movl	%edx, 24(%rsp)
	movl	28(%rax), %edx
	leal	33(%rdx), %ebx
	movl	%ebx, 80(%rsp)
	addl	$34, %edx
	sall	$20, %edx
	movl	%edx, 28(%rsp)
	movl	32(%rax), %edx
	leal	32(%rdx), %ebx
	movl	%ebx, 84(%rsp)
	addl	$33, %edx
	sall	$20, %edx
	movl	%edx, 32(%rsp)
	movl	36(%rax), %edx
	leal	31(%rdx), %edi
	movl	%edi, %r15d
	addl	$32, %edx
	sall	$20, %edx
	movl	%edx, 36(%rsp)
	movl	40(%rax), %edx
	leal	30(%rdx), %r14d
	addl	$31, %edx
	sall	$20, %edx
	movl	%edx, 40(%rsp)
	movl	44(%rax), %edx
	leal	29(%rdx), %r13d
	addl	$30, %edx
	sall	$20, %edx
	movl	%edx, 44(%rsp)
	movl	48(%rax), %edx
	leal	28(%rdx), %r12d
	addl	$29, %edx
	sall	$20, %edx
	movl	%edx, 48(%rsp)
	movl	52(%rax), %edx
	leal	27(%rdx), %ebp
	addl	$28, %edx
	sall	$20, %edx
	movl	%edx, 52(%rsp)
	movl	56(%rax), %eax
	leal	26(%rax), %ebx
	addl	$27, %eax
	sall	$20, %eax
	movl	%eax, 64(%rsp)
	leaq	-1(%rcx), %rax
	movq	%rax, 56(%rsp)
	testq	%rcx, %rcx
	je	.L311
	movl	%edi, %ecx
.L312:
	movl	12(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %r15d
	movl	16(%rsp), %eax
	cltd
	idivl	68(%rsp)
	movl	%eax, %r11d
	movl	20(%rsp), %eax
	cltd
	idivl	72(%rsp)
	movl	%eax, %r10d
	movl	24(%rsp), %eax
	cltd
	idivl	76(%rsp)
	movl	%eax, %r9d
	movl	28(%rsp), %eax
	cltd
	idivl	80(%rsp)
	movl	%eax, %r8d
	movl	32(%rsp), %eax
	cltd
	idivl	84(%rsp)
	movl	%eax, %edi
	movl	36(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %esi
	movl	40(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	44(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	48(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %ecx
	movl	52(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	12(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	20(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	24(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	28(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	32(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	36(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %r15d
	movl	40(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	44(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %esi
	movl	48(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %r13d
	movl	52(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	20(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	24(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	28(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	32(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	36(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	40(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %ecx
	movl	44(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %r14d
	movl	48(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	52(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	20(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	24(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	28(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	32(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	36(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %esi
	movl	40(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %r15d
	movl	44(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	48(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	52(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	20(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	24(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	28(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	32(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %ecx
	movl	36(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %edi
	movl	40(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	44(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	48(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	52(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	20(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	24(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	28(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %esi
	movl	32(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %r8d
	movl	36(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	40(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	44(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	48(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	52(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	20(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	24(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %ecx
	movl	28(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %r9d
	movl	32(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	36(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	40(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	44(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	48(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	52(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	20(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %esi
	movl	24(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %r10d
	movl	28(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	32(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	36(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	40(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	44(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	48(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	52(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %ecx
	movl	20(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %r11d
	movl	24(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	28(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	32(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	36(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	40(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	44(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, 88(%rsp)
	movl	48(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, 92(%rsp)
	movl	52(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %esi
	movl	16(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, 68(%rsp)
	movl	20(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, 72(%rsp)
	movl	24(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, 76(%rsp)
	movl	28(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, 80(%rsp)
	movl	32(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, 84(%rsp)
	movl	36(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %ecx
	movl	40(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r14d
	movl	44(%rsp), %eax
	cltd
	idivl	88(%rsp)
	movl	%eax, %r13d
	movl	48(%rsp), %eax
	cltd
	idivl	92(%rsp)
	movl	%eax, %r12d
	movl	52(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	subq	$1, 56(%rsp)
	movq	56(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L312
	movl	%ecx, %r15d
.L311:
	movl	%esi, %edi
	call	use_int@PLT
	movl	68(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	call	use_int@PLT
	movl	76(%rsp), %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	call	use_int@PLT
	movl	84(%rsp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE132:
	.size	integer_div_11, .-integer_div_11
	.globl	integer_div_12
	.type	integer_div_12, @function
integer_div_12:
.LFB133:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$104, %rsp
	.cfi_def_cfa_offset 160
	movq	%rdi, %rcx
	movq	%rsi, %rax
	movl	12(%rsi), %edx
	leal	37(%rdx), %r15d
	addl	$38, %edx
	sall	$20, %edx
	movl	%edx, 8(%rsp)
	movl	16(%rsi), %edx
	leal	36(%rdx), %r14d
	addl	$37, %edx
	sall	$20, %edx
	movl	%edx, 12(%rsp)
	movl	20(%rsi), %edx
	leal	35(%rdx), %r13d
	addl	$36, %edx
	sall	$20, %edx
	movl	%edx, 16(%rsp)
	movl	24(%rsi), %edx
	leal	34(%rdx), %r12d
	addl	$35, %edx
	sall	$20, %edx
	movl	%edx, 20(%rsp)
	movl	28(%rsi), %edx
	leal	33(%rdx), %ebp
	addl	$34, %edx
	sall	$20, %edx
	movl	%edx, 24(%rsp)
	movl	32(%rsi), %edx
	leal	32(%rdx), %r11d
	movl	%r11d, 72(%rsp)
	addl	$33, %edx
	sall	$20, %edx
	movl	%edx, 28(%rsp)
	movl	36(%rsi), %edx
	leal	31(%rdx), %r10d
	movl	%r10d, 76(%rsp)
	addl	$32, %edx
	sall	$20, %edx
	movl	%edx, 32(%rsp)
	movl	40(%rsi), %edx
	leal	30(%rdx), %r9d
	movl	%r9d, 80(%rsp)
	addl	$31, %edx
	sall	$20, %edx
	movl	%edx, 36(%rsp)
	movl	44(%rsi), %edx
	leal	29(%rdx), %r8d
	movl	%r8d, 84(%rsp)
	addl	$30, %edx
	sall	$20, %edx
	movl	%edx, 40(%rsp)
	movl	48(%rsi), %edx
	leal	28(%rdx), %edi
	movl	%edi, 88(%rsp)
	addl	$29, %edx
	sall	$20, %edx
	movl	%edx, 44(%rsp)
	movl	52(%rsi), %edx
	leal	27(%rdx), %esi
	movl	%esi, 92(%rsp)
	addl	$28, %edx
	sall	$20, %edx
	movl	%edx, 48(%rsp)
	movl	56(%rax), %edx
	leal	26(%rdx), %ebx
	movl	%ebx, 68(%rsp)
	addl	$27, %edx
	sall	$20, %edx
	movl	%edx, 52(%rsp)
	movl	60(%rax), %eax
	leal	25(%rax), %ebx
	addl	$26, %eax
	sall	$20, %eax
	movl	%eax, 64(%rsp)
	leaq	-1(%rcx), %rax
	movq	%rax, 56(%rsp)
	testq	%rcx, %rcx
	je	.L316
	movl	68(%rsp), %ecx
.L317:
	movl	8(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	12(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	16(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	20(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	24(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	8(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	12(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	16(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	20(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	24(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	8(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	12(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	16(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	20(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	24(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	8(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	12(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	16(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	20(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	24(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	8(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	12(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	16(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	20(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	24(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	8(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	12(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	16(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	20(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	24(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	8(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	12(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	16(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	20(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	24(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	8(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	12(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	16(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	20(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	24(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	8(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	12(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	16(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	20(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	24(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	8(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	12(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	16(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	20(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	24(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	subq	$1, 56(%rsp)
	movq	56(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L317
	movl	%r11d, 72(%rsp)
	movl	%r10d, 76(%rsp)
	movl	%r9d, 80(%rsp)
	movl	%r8d, 84(%rsp)
	movl	%edi, 88(%rsp)
	movl	%esi, 92(%rsp)
	movl	%ecx, 68(%rsp)
.L316:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	call	use_int@PLT
	movl	76(%rsp), %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	call	use_int@PLT
	movl	84(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	call	use_int@PLT
	movl	92(%rsp), %edi
	call	use_int@PLT
	movl	68(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE133:
	.size	integer_div_12, .-integer_div_12
	.globl	integer_div_13
	.type	integer_div_13, @function
integer_div_13:
.LFB134:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$104, %rsp
	.cfi_def_cfa_offset 160
	movq	%rdi, 56(%rsp)
	movq	%rsi, %rax
	movl	12(%rsi), %edx
	leal	37(%rdx), %r15d
	addl	$38, %edx
	sall	$20, %edx
	movl	%edx, (%rsp)
	movl	16(%rsi), %edx
	leal	36(%rdx), %r14d
	addl	$37, %edx
	sall	$20, %edx
	movl	%edx, 4(%rsp)
	movl	20(%rsi), %edx
	leal	35(%rdx), %r13d
	addl	$36, %edx
	sall	$20, %edx
	movl	%edx, 8(%rsp)
	movl	24(%rsi), %edx
	leal	34(%rdx), %r12d
	addl	$35, %edx
	sall	$20, %edx
	movl	%edx, 12(%rsp)
	movl	28(%rsi), %edx
	leal	33(%rdx), %ebp
	addl	$34, %edx
	sall	$20, %edx
	movl	%edx, 16(%rsp)
	movl	32(%rsi), %edx
	leal	32(%rdx), %ebx
	addl	$33, %edx
	sall	$20, %edx
	movl	%edx, 20(%rsp)
	movl	36(%rsi), %edx
	leal	31(%rdx), %r11d
	movl	%r11d, 68(%rsp)
	addl	$32, %edx
	sall	$20, %edx
	movl	%edx, 24(%rsp)
	movl	40(%rsi), %edx
	leal	30(%rdx), %r10d
	movl	%r10d, 72(%rsp)
	addl	$31, %edx
	sall	$20, %edx
	movl	%edx, 28(%rsp)
	movl	44(%rsi), %edx
	leal	29(%rdx), %r9d
	movl	%r9d, 76(%rsp)
	addl	$30, %edx
	sall	$20, %edx
	movl	%edx, 32(%rsp)
	movl	48(%rsi), %edx
	leal	28(%rdx), %r8d
	movl	%r8d, 80(%rsp)
	addl	$29, %edx
	sall	$20, %edx
	movl	%edx, 36(%rsp)
	movl	52(%rsi), %edx
	leal	27(%rdx), %edi
	movl	%edi, 84(%rsp)
	addl	$28, %edx
	sall	$20, %edx
	movl	%edx, 40(%rsp)
	movl	56(%rsi), %edx
	leal	26(%rdx), %esi
	movl	%esi, 88(%rsp)
	addl	$27, %edx
	sall	$20, %edx
	movl	%edx, 44(%rsp)
	movl	60(%rax), %edx
	leal	25(%rdx), %ecx
	movl	%ecx, 92(%rsp)
	addl	$26, %edx
	sall	$20, %edx
	movl	%edx, 48(%rsp)
	movl	64(%rax), %eax
	leal	24(%rax), %edx
	movl	%edx, 64(%rsp)
	addl	$25, %eax
	sall	$20, %eax
	movl	%eax, 52(%rsp)
	movq	56(%rsp), %rdx
	leaq	-1(%rdx), %rax
	movq	%rax, 56(%rsp)
	testq	%rdx, %rdx
	je	.L321
.L322:
	movl	(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	4(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	8(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	20(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	24(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	28(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	32(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	36(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	40(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	44(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	48(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	52(%rsp), %eax
	cltd
	idivl	64(%rsp)
	movl	%eax, 64(%rsp)
	movl	(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	4(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	8(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	20(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	24(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	28(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	32(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	36(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	40(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	44(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	48(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	52(%rsp), %eax
	cltd
	idivl	64(%rsp)
	movl	%eax, 64(%rsp)
	movl	(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	4(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	8(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	20(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	24(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	28(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	32(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	36(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	40(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	44(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	48(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	52(%rsp), %eax
	cltd
	idivl	64(%rsp)
	movl	%eax, 64(%rsp)
	movl	(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	4(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	8(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	20(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	24(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	28(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	32(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	36(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	40(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	44(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	48(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	52(%rsp), %eax
	cltd
	idivl	64(%rsp)
	movl	%eax, 64(%rsp)
	movl	(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	4(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	8(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	20(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	24(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	28(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	32(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	36(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	40(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	44(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	48(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	52(%rsp), %eax
	cltd
	idivl	64(%rsp)
	movl	%eax, 64(%rsp)
	movl	(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	4(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	8(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	20(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	24(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	28(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	32(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	36(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	40(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	44(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	48(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	52(%rsp), %eax
	cltd
	idivl	64(%rsp)
	movl	%eax, 64(%rsp)
	movl	(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	4(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	8(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	20(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	24(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	28(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	32(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	36(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	40(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	44(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	48(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	52(%rsp), %eax
	cltd
	idivl	64(%rsp)
	movl	%eax, 64(%rsp)
	movl	(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	4(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	8(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	20(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	24(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	28(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	32(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	36(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	40(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	44(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	48(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	52(%rsp), %eax
	cltd
	idivl	64(%rsp)
	movl	%eax, 64(%rsp)
	movl	(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	4(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	8(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	20(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	24(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	28(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	32(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	36(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	40(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	44(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	48(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	52(%rsp), %eax
	cltd
	idivl	64(%rsp)
	movl	%eax, 64(%rsp)
	movl	(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	4(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	8(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	12(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	16(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	20(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	24(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	28(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	32(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	36(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	40(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	44(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	48(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	52(%rsp), %eax
	cltd
	idivl	64(%rsp)
	movl	%eax, 64(%rsp)
	subq	$1, 56(%rsp)
	movq	56(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L322
	movl	%r11d, 68(%rsp)
	movl	%r10d, 72(%rsp)
	movl	%r9d, 76(%rsp)
	movl	%r8d, 80(%rsp)
	movl	%edi, 84(%rsp)
	movl	%esi, 88(%rsp)
	movl	%ecx, 92(%rsp)
.L321:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	68(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	call	use_int@PLT
	movl	76(%rsp), %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	call	use_int@PLT
	movl	84(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	call	use_int@PLT
	movl	92(%rsp), %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE134:
	.size	integer_div_13, .-integer_div_13
	.globl	integer_div_14
	.type	integer_div_14, @function
integer_div_14:
.LFB135:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$120, %rsp
	.cfi_def_cfa_offset 176
	movq	%rdi, %rcx
	movq	%rsi, %rax
	movl	12(%rsi), %edx
	leal	37(%rdx), %r15d
	addl	$38, %edx
	sall	$20, %edx
	movl	%edx, 4(%rsp)
	movl	16(%rsi), %edx
	leal	36(%rdx), %r14d
	addl	$37, %edx
	sall	$20, %edx
	movl	%edx, 8(%rsp)
	movl	20(%rsi), %edx
	leal	35(%rdx), %r13d
	addl	$36, %edx
	sall	$20, %edx
	movl	%edx, 12(%rsp)
	movl	24(%rsi), %edx
	leal	34(%rdx), %r12d
	addl	$35, %edx
	sall	$20, %edx
	movl	%edx, 16(%rsp)
	movl	28(%rsi), %edx
	leal	33(%rdx), %ebp
	addl	$34, %edx
	sall	$20, %edx
	movl	%edx, 20(%rsp)
	movl	32(%rsi), %edx
	leal	32(%rdx), %ebx
	movl	%ebx, 80(%rsp)
	addl	$33, %edx
	sall	$20, %edx
	movl	%edx, 24(%rsp)
	movl	36(%rsi), %edx
	leal	31(%rdx), %r11d
	movl	%r11d, 88(%rsp)
	addl	$32, %edx
	sall	$20, %edx
	movl	%edx, 28(%rsp)
	movl	40(%rsi), %edx
	leal	30(%rdx), %r10d
	movl	%r10d, 92(%rsp)
	addl	$31, %edx
	sall	$20, %edx
	movl	%edx, 32(%rsp)
	movl	44(%rsi), %edx
	leal	29(%rdx), %r9d
	movl	%r9d, 96(%rsp)
	addl	$30, %edx
	sall	$20, %edx
	movl	%edx, 36(%rsp)
	movl	48(%rsi), %edx
	leal	28(%rdx), %r8d
	movl	%r8d, 100(%rsp)
	addl	$29, %edx
	sall	$20, %edx
	movl	%edx, 40(%rsp)
	movl	52(%rsi), %edx
	leal	27(%rdx), %edi
	movl	%edi, 104(%rsp)
	addl	$28, %edx
	sall	$20, %edx
	movl	%edx, 44(%rsp)
	movl	56(%rsi), %edx
	leal	26(%rdx), %esi
	movl	%esi, 108(%rsp)
	addl	$27, %edx
	sall	$20, %edx
	movl	%edx, 48(%rsp)
	movl	60(%rax), %edx
	leal	25(%rdx), %ebx
	movl	%ebx, 84(%rsp)
	addl	$26, %edx
	sall	$20, %edx
	movl	%edx, 52(%rsp)
	movl	64(%rax), %edx
	leal	24(%rdx), %ebx
	movl	%ebx, 72(%rsp)
	addl	$25, %edx
	sall	$20, %edx
	movl	%edx, 56(%rsp)
	movl	68(%rax), %eax
	leal	23(%rax), %edx
	movl	%edx, 76(%rsp)
	addl	$24, %eax
	sall	$20, %eax
	movl	%eax, 60(%rsp)
	leaq	-1(%rcx), %rax
	movq	%rax, 64(%rsp)
	testq	%rcx, %rcx
	je	.L326
	movl	80(%rsp), %ebx
	movl	84(%rsp), %ecx
.L327:
	movl	4(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	8(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	12(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	16(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	20(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	24(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	56(%rsp), %eax
	cltd
	idivl	72(%rsp)
	movl	%eax, 72(%rsp)
	movl	60(%rsp), %eax
	cltd
	idivl	76(%rsp)
	movl	%eax, 76(%rsp)
	movl	4(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	8(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	12(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	16(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	20(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	24(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	56(%rsp), %eax
	cltd
	idivl	72(%rsp)
	movl	%eax, 72(%rsp)
	movl	60(%rsp), %eax
	cltd
	idivl	76(%rsp)
	movl	%eax, 76(%rsp)
	movl	4(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	8(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	12(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	16(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	20(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	24(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	56(%rsp), %eax
	cltd
	idivl	72(%rsp)
	movl	%eax, 72(%rsp)
	movl	60(%rsp), %eax
	cltd
	idivl	76(%rsp)
	movl	%eax, 76(%rsp)
	movl	4(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	8(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	12(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	16(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	20(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	24(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	56(%rsp), %eax
	cltd
	idivl	72(%rsp)
	movl	%eax, 72(%rsp)
	movl	60(%rsp), %eax
	cltd
	idivl	76(%rsp)
	movl	%eax, 76(%rsp)
	movl	4(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	8(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	12(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	16(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	20(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	24(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	56(%rsp), %eax
	cltd
	idivl	72(%rsp)
	movl	%eax, 72(%rsp)
	movl	60(%rsp), %eax
	cltd
	idivl	76(%rsp)
	movl	%eax, 76(%rsp)
	movl	4(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	8(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	12(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	16(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	20(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	24(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	56(%rsp), %eax
	cltd
	idivl	72(%rsp)
	movl	%eax, 72(%rsp)
	movl	60(%rsp), %eax
	cltd
	idivl	76(%rsp)
	movl	%eax, 76(%rsp)
	movl	4(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	8(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	12(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	16(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	20(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	24(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	56(%rsp), %eax
	cltd
	idivl	72(%rsp)
	movl	%eax, 72(%rsp)
	movl	60(%rsp), %eax
	cltd
	idivl	76(%rsp)
	movl	%eax, 76(%rsp)
	movl	4(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	8(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	12(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	16(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	20(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	24(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	56(%rsp), %eax
	cltd
	idivl	72(%rsp)
	movl	%eax, 72(%rsp)
	movl	60(%rsp), %eax
	cltd
	idivl	76(%rsp)
	movl	%eax, 76(%rsp)
	movl	4(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	8(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	12(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	16(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	20(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	24(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	56(%rsp), %eax
	cltd
	idivl	72(%rsp)
	movl	%eax, 72(%rsp)
	movl	60(%rsp), %eax
	cltd
	idivl	76(%rsp)
	movl	%eax, 76(%rsp)
	movl	4(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	8(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	12(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	16(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	20(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	24(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	28(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	32(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	36(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	40(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	44(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	48(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	52(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	56(%rsp), %eax
	cltd
	idivl	72(%rsp)
	movl	%eax, 72(%rsp)
	movl	60(%rsp), %eax
	cltd
	idivl	76(%rsp)
	movl	%eax, 76(%rsp)
	subq	$1, 64(%rsp)
	movq	64(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L327
	movl	%ebx, 80(%rsp)
	movl	%r11d, 88(%rsp)
	movl	%r10d, 92(%rsp)
	movl	%r9d, 96(%rsp)
	movl	%r8d, 100(%rsp)
	movl	%edi, 104(%rsp)
	movl	%esi, 108(%rsp)
	movl	%ecx, 84(%rsp)
.L326:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	call	use_int@PLT
	movl	92(%rsp), %edi
	call	use_int@PLT
	movl	96(%rsp), %edi
	call	use_int@PLT
	movl	100(%rsp), %edi
	call	use_int@PLT
	movl	104(%rsp), %edi
	call	use_int@PLT
	movl	108(%rsp), %edi
	call	use_int@PLT
	movl	84(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	call	use_int@PLT
	movl	76(%rsp), %edi
	call	use_int@PLT
	addq	$120, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE135:
	.size	integer_div_14, .-integer_div_14
	.globl	integer_div_15
	.type	integer_div_15, @function
integer_div_15:
.LFB136:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$136, %rsp
	.cfi_def_cfa_offset 192
	movq	%rdi, %rcx
	movq	%rsi, %rax
	movl	12(%rsi), %edx
	leal	37(%rdx), %r15d
	addl	$38, %edx
	sall	$20, %edx
	movl	%edx, 12(%rsp)
	movl	16(%rsi), %edx
	leal	36(%rdx), %r14d
	addl	$37, %edx
	sall	$20, %edx
	movl	%edx, 16(%rsp)
	movl	20(%rsi), %edx
	leal	35(%rdx), %r13d
	addl	$36, %edx
	sall	$20, %edx
	movl	%edx, 20(%rsp)
	movl	24(%rsi), %edx
	leal	34(%rdx), %r12d
	addl	$35, %edx
	sall	$20, %edx
	movl	%edx, 24(%rsp)
	movl	28(%rsi), %edx
	leal	33(%rdx), %ebp
	addl	$34, %edx
	sall	$20, %edx
	movl	%edx, 28(%rsp)
	movl	32(%rsi), %edx
	leal	32(%rdx), %ebx
	movl	%ebx, 96(%rsp)
	addl	$33, %edx
	sall	$20, %edx
	movl	%edx, 32(%rsp)
	movl	36(%rsi), %edx
	leal	31(%rdx), %r11d
	movl	%r11d, 104(%rsp)
	addl	$32, %edx
	sall	$20, %edx
	movl	%edx, 36(%rsp)
	movl	40(%rsi), %edx
	leal	30(%rdx), %r10d
	movl	%r10d, 108(%rsp)
	addl	$31, %edx
	sall	$20, %edx
	movl	%edx, 40(%rsp)
	movl	44(%rsi), %edx
	leal	29(%rdx), %r9d
	movl	%r9d, 112(%rsp)
	addl	$30, %edx
	sall	$20, %edx
	movl	%edx, 44(%rsp)
	movl	48(%rsi), %edx
	leal	28(%rdx), %r8d
	movl	%r8d, 116(%rsp)
	addl	$29, %edx
	sall	$20, %edx
	movl	%edx, 48(%rsp)
	movl	52(%rsi), %edx
	leal	27(%rdx), %edi
	movl	%edi, 120(%rsp)
	addl	$28, %edx
	sall	$20, %edx
	movl	%edx, 52(%rsp)
	movl	56(%rsi), %edx
	leal	26(%rdx), %esi
	movl	%esi, 124(%rsp)
	addl	$27, %edx
	sall	$20, %edx
	movl	%edx, 56(%rsp)
	movl	60(%rax), %edx
	leal	25(%rdx), %ebx
	movl	%ebx, 100(%rsp)
	addl	$26, %edx
	sall	$20, %edx
	movl	%edx, 60(%rsp)
	movl	64(%rax), %edx
	leal	24(%rdx), %ebx
	movl	%ebx, 84(%rsp)
	addl	$25, %edx
	sall	$20, %edx
	movl	%edx, 64(%rsp)
	movl	68(%rax), %edx
	leal	23(%rdx), %ebx
	movl	%ebx, 88(%rsp)
	addl	$24, %edx
	sall	$20, %edx
	movl	%edx, 68(%rsp)
	movl	72(%rax), %eax
	leal	22(%rax), %edx
	movl	%edx, 92(%rsp)
	addl	$23, %eax
	sall	$20, %eax
	movl	%eax, 80(%rsp)
	leaq	-1(%rcx), %rax
	movq	%rax, 72(%rsp)
	testq	%rcx, %rcx
	je	.L331
	movl	96(%rsp), %ebx
	movl	100(%rsp), %ecx
.L332:
	movl	12(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	16(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	20(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	24(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	28(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	32(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	36(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	40(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	44(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	48(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	52(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	56(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	60(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	84(%rsp)
	movl	%eax, 84(%rsp)
	movl	68(%rsp), %eax
	cltd
	idivl	88(%rsp)
	movl	%eax, 88(%rsp)
	movl	80(%rsp), %eax
	cltd
	idivl	92(%rsp)
	movl	%eax, 92(%rsp)
	movl	12(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	16(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	20(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	24(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	28(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	32(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	36(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	40(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	44(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	48(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	52(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	56(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	60(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	84(%rsp)
	movl	%eax, 84(%rsp)
	movl	68(%rsp), %eax
	cltd
	idivl	88(%rsp)
	movl	%eax, 88(%rsp)
	movl	80(%rsp), %eax
	cltd
	idivl	92(%rsp)
	movl	%eax, 92(%rsp)
	movl	12(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	16(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	20(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	24(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	28(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	32(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	36(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	40(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	44(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	48(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	52(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	56(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	60(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	84(%rsp)
	movl	%eax, 84(%rsp)
	movl	68(%rsp), %eax
	cltd
	idivl	88(%rsp)
	movl	%eax, 88(%rsp)
	movl	80(%rsp), %eax
	cltd
	idivl	92(%rsp)
	movl	%eax, 92(%rsp)
	movl	12(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	16(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	20(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	24(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	28(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	32(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	36(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	40(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	44(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	48(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	52(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	56(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	60(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	84(%rsp)
	movl	%eax, 84(%rsp)
	movl	68(%rsp), %eax
	cltd
	idivl	88(%rsp)
	movl	%eax, 88(%rsp)
	movl	80(%rsp), %eax
	cltd
	idivl	92(%rsp)
	movl	%eax, 92(%rsp)
	movl	12(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	16(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	20(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	24(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	28(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	32(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	36(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	40(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	44(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	48(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	52(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	56(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	60(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	84(%rsp)
	movl	%eax, 84(%rsp)
	movl	68(%rsp), %eax
	cltd
	idivl	88(%rsp)
	movl	%eax, 88(%rsp)
	movl	80(%rsp), %eax
	cltd
	idivl	92(%rsp)
	movl	%eax, 92(%rsp)
	movl	12(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	16(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	20(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	24(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	28(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	32(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	36(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	40(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	44(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	48(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	52(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	56(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	60(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	84(%rsp)
	movl	%eax, 84(%rsp)
	movl	68(%rsp), %eax
	cltd
	idivl	88(%rsp)
	movl	%eax, 88(%rsp)
	movl	80(%rsp), %eax
	cltd
	idivl	92(%rsp)
	movl	%eax, 92(%rsp)
	movl	12(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	16(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	20(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	24(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	28(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	32(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	36(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	40(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	44(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	48(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	52(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	56(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	60(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	84(%rsp)
	movl	%eax, 84(%rsp)
	movl	68(%rsp), %eax
	cltd
	idivl	88(%rsp)
	movl	%eax, 88(%rsp)
	movl	80(%rsp), %eax
	cltd
	idivl	92(%rsp)
	movl	%eax, 92(%rsp)
	movl	12(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	16(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	20(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	24(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	28(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	32(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	36(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	40(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	44(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	48(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	52(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	56(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	60(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	84(%rsp)
	movl	%eax, 84(%rsp)
	movl	68(%rsp), %eax
	cltd
	idivl	88(%rsp)
	movl	%eax, 88(%rsp)
	movl	80(%rsp), %eax
	cltd
	idivl	92(%rsp)
	movl	%eax, 92(%rsp)
	movl	12(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	16(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	20(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	24(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	28(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	32(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	36(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	40(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	44(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	48(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	52(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	56(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	60(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	84(%rsp)
	movl	%eax, 84(%rsp)
	movl	68(%rsp), %eax
	cltd
	idivl	88(%rsp)
	movl	%eax, 88(%rsp)
	movl	80(%rsp), %eax
	cltd
	idivl	92(%rsp)
	movl	%eax, 92(%rsp)
	movl	12(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	16(%rsp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	20(%rsp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	24(%rsp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	28(%rsp), %eax
	cltd
	idivl	%ebp
	movl	%eax, %ebp
	movl	32(%rsp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	36(%rsp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	40(%rsp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	44(%rsp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	48(%rsp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	52(%rsp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	56(%rsp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	60(%rsp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	84(%rsp)
	movl	%eax, 84(%rsp)
	movl	68(%rsp), %eax
	cltd
	idivl	88(%rsp)
	movl	%eax, 88(%rsp)
	movl	80(%rsp), %eax
	cltd
	idivl	92(%rsp)
	movl	%eax, 92(%rsp)
	subq	$1, 72(%rsp)
	movq	72(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L332
	movl	%ebx, 96(%rsp)
	movl	%r11d, 104(%rsp)
	movl	%r10d, 108(%rsp)
	movl	%r9d, 112(%rsp)
	movl	%r8d, 116(%rsp)
	movl	%edi, 120(%rsp)
	movl	%esi, 124(%rsp)
	movl	%ecx, 100(%rsp)
.L331:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	96(%rsp), %edi
	call	use_int@PLT
	movl	104(%rsp), %edi
	call	use_int@PLT
	movl	108(%rsp), %edi
	call	use_int@PLT
	movl	112(%rsp), %edi
	call	use_int@PLT
	movl	116(%rsp), %edi
	call	use_int@PLT
	movl	120(%rsp), %edi
	call	use_int@PLT
	movl	124(%rsp), %edi
	call	use_int@PLT
	movl	100(%rsp), %edi
	call	use_int@PLT
	movl	84(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	call	use_int@PLT
	movl	92(%rsp), %edi
	call	use_int@PLT
	addq	$136, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE136:
	.size	integer_div_15, .-integer_div_15
	.globl	integer_mod_0
	.type	integer_mod_0, @function
integer_mod_0:
.LFB137:
	.cfi_startproc
	endbr64
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movl	12(%rsi), %ecx
	leal	1(%rcx,%rdi), %eax
	addl	$63, %ecx
	leaq	-1(%rdi), %rsi
	testq	%rdi, %rdi
	je	.L336
.L337:
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	subq	$1, %rsi
	cmpq	$-1, %rsi
	jne	.L337
.L336:
	movl	%eax, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE137:
	.size	integer_mod_0, .-integer_mod_0
	.globl	integer_mod_1
	.type	integer_mod_1, @function
integer_mod_1:
.LFB138:
	.cfi_startproc
	endbr64
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movq	%rdi, %rax
	movl	12(%rsi), %r8d
	leal	1(%r8,%rdi), %edi
	addl	$63, %r8d
	movl	16(%rsi), %ecx
	leal	(%rcx,%rax), %ebx
	addl	$62, %ecx
	leaq	-1(%rax), %r9
	testq	%rax, %rax
	je	.L341
.L342:
	movl	%edi, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	subq	$1, %r9
	cmpq	$-1, %r9
	jne	.L342
.L341:
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE138:
	.size	integer_mod_1, .-integer_mod_1
	.globl	integer_mod_2
	.type	integer_mod_2, @function
integer_mod_2:
.LFB139:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	pushq	%rbx
	.cfi_def_cfa_offset 24
	.cfi_offset 3, -24
	subq	$8, %rsp
	.cfi_def_cfa_offset 32
	movq	%rdi, %rax
	movl	12(%rsi), %r9d
	leal	1(%r9,%rdi), %edi
	addl	$63, %r9d
	movl	16(%rsi), %r8d
	leal	(%r8,%rax), %ebp
	addl	$62, %r8d
	movl	20(%rsi), %ecx
	leal	-1(%rcx,%rax), %ebx
	addl	$61, %ecx
	leaq	-1(%rax), %r11
	testq	%rax, %rax
	je	.L346
.L347:
	movl	%edi, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	orl	%r9d, %edi
	movl	%ebp, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r10d
	orl	%r8d, %r10d
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	orl	%r9d, %edi
	movl	%r10d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r10d
	orl	%r8d, %r10d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	orl	%r9d, %edi
	movl	%r10d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r10d
	orl	%r8d, %r10d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	orl	%r9d, %edi
	movl	%r10d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r10d
	orl	%r8d, %r10d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	orl	%r9d, %edi
	movl	%r10d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r10d
	orl	%r8d, %r10d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	orl	%r9d, %edi
	movl	%r10d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r10d
	orl	%r8d, %r10d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	orl	%r9d, %edi
	movl	%r10d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r10d
	orl	%r8d, %r10d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	orl	%r9d, %edi
	movl	%r10d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r10d
	orl	%r8d, %r10d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	orl	%r9d, %edi
	movl	%r10d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r10d
	orl	%r8d, %r10d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%edi, %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	orl	%r9d, %edi
	movl	%r10d, %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %ebp
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	subq	$1, %r11
	cmpq	$-1, %r11
	jne	.L347
.L346:
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 24
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE139:
	.size	integer_mod_2, .-integer_mod_2
	.globl	integer_mod_3
	.type	integer_mod_3, @function
integer_mod_3:
.LFB140:
	.cfi_startproc
	endbr64
	pushq	%r13
	.cfi_def_cfa_offset 16
	.cfi_offset 13, -16
	pushq	%r12
	.cfi_def_cfa_offset 24
	.cfi_offset 12, -24
	pushq	%rbp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset 3, -40
	subq	$8, %rsp
	.cfi_def_cfa_offset 48
	movq	%rdi, %rax
	movl	12(%rsi), %r9d
	leal	1(%r9,%rdi), %r10d
	addl	$63, %r9d
	movl	16(%rsi), %r8d
	leal	(%r8,%rdi), %r12d
	addl	$62, %r8d
	movl	20(%rsi), %edi
	leal	-1(%rdi,%rax), %ebp
	addl	$61, %edi
	movl	24(%rsi), %ecx
	leal	-2(%rcx,%rax), %ebx
	addl	$60, %ecx
	leaq	-1(%rax), %r13
	testq	%rax, %rax
	je	.L351
.L352:
	movl	%r10d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r10d
	movl	%r12d, %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r11d
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r10d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r10d
	movl	%r12d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r11d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r10d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r10d
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r11d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r10d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r10d
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r11d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r10d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r10d
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r11d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r10d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r10d
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r11d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r10d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r10d
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r11d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r10d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r10d
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r11d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r10d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r10d
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r11d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r10d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r10d
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r12d
	movl	%r11d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %ebp
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	subq	$1, %r13
	cmpq	$-1, %r13
	jne	.L352
.L351:
	movl	%r10d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%rbp
	.cfi_def_cfa_offset 24
	popq	%r12
	.cfi_def_cfa_offset 16
	popq	%r13
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE140:
	.size	integer_mod_3, .-integer_mod_3
	.globl	integer_mod_4
	.type	integer_mod_4, @function
integer_mod_4:
.LFB141:
	.cfi_startproc
	endbr64
	pushq	%r14
	.cfi_def_cfa_offset 16
	.cfi_offset 14, -16
	pushq	%r13
	.cfi_def_cfa_offset 24
	.cfi_offset 13, -24
	pushq	%r12
	.cfi_def_cfa_offset 32
	.cfi_offset 12, -32
	pushq	%rbp
	.cfi_def_cfa_offset 40
	.cfi_offset 6, -40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	.cfi_offset 3, -48
	movq	%rdi, %rax
	movl	12(%rsi), %r10d
	leal	1(%r10,%rdi), %r11d
	addl	$63, %r10d
	movl	16(%rsi), %r9d
	leal	(%r9,%rdi), %ebp
	addl	$62, %r9d
	movl	20(%rsi), %r8d
	leal	-1(%r8,%rdi), %ebx
	addl	$61, %r8d
	movl	24(%rsi), %edi
	leal	-2(%rdi,%rax), %r12d
	addl	$60, %edi
	movl	28(%rsi), %ecx
	leal	-3(%rcx,%rax), %r14d
	addl	$59, %ecx
	leaq	-1(%rax), %r13
	testq	%rax, %rax
	je	.L356
.L357:
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%r14d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%ebp, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%r8d
	movl	%edx, %ebx
	orl	%r8d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%esi, %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %r14d
	subq	$1, %r13
	cmpq	$-1, %r13
	jne	.L357
.L356:
	movl	%r11d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%rbp
	.cfi_def_cfa_offset 32
	popq	%r12
	.cfi_def_cfa_offset 24
	popq	%r13
	.cfi_def_cfa_offset 16
	popq	%r14
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE141:
	.size	integer_mod_4, .-integer_mod_4
	.globl	integer_mod_5
	.type	integer_mod_5, @function
integer_mod_5:
.LFB142:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$8, %rsp
	.cfi_def_cfa_offset 64
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %r10d
	leal	1(%r10,%rdi), %r11d
	addl	$63, %r10d
	movl	16(%rsi), %r9d
	leal	(%r9,%rdi), %r14d
	addl	$62, %r9d
	movl	20(%rsi), %r8d
	leal	-1(%r8,%rdi), %r13d
	addl	$61, %r8d
	movl	24(%rsi), %edi
	leal	-2(%rdi,%rax), %r12d
	addl	$60, %edi
	movl	28(%rsi), %esi
	leal	-3(%rsi,%rax), %ebp
	addl	$59, %esi
	movl	32(%rdx), %ecx
	leal	-4(%rcx,%rax), %ebx
	addl	$58, %ecx
	leaq	-1(%rax), %r15
	testq	%rax, %rax
	je	.L361
.L362:
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	subq	$1, %r15
	cmpq	$-1, %r15
	jne	.L362
.L361:
	movl	%r11d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE142:
	.size	integer_mod_5, .-integer_mod_5
	.globl	integer_mod_6
	.type	integer_mod_6, @function
integer_mod_6:
.LFB143:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$56, %rsp
	.cfi_def_cfa_offset 112
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx,%rdi), %edi
	leal	63(%rcx), %ebx
	movl	%ebx, 12(%rsp)
	movl	16(%rsi), %ecx
	leal	(%rcx,%rax), %esi
	movl	%esi, 28(%rsp)
	leal	62(%rcx), %ebx
	movl	20(%rdx), %ecx
	leal	-1(%rcx,%rax), %r8d
	movl	%r8d, 32(%rsp)
	leal	61(%rcx), %r15d
	movl	24(%rdx), %ecx
	leal	-2(%rcx,%rax), %r9d
	movl	%r9d, 36(%rsp)
	leal	60(%rcx), %r14d
	movl	28(%rdx), %ecx
	leal	-3(%rcx,%rax), %r10d
	movl	%r10d, 40(%rsp)
	leal	59(%rcx), %r13d
	movl	32(%rdx), %ecx
	leal	-4(%rcx,%rax), %r11d
	movl	%r11d, 44(%rsp)
	leal	58(%rcx), %r12d
	movl	36(%rdx), %edx
	leal	-5(%rdx,%rax), %ecx
	movl	%ecx, 24(%rsp)
	leal	57(%rdx), %ebp
	leaq	-1(%rax), %rdx
	movq	%rdx, 16(%rsp)
	testq	%rax, %rax
	je	.L366
	movl	%esi, %ecx
	movl	%r8d, %esi
	movl	%r9d, %r8d
	movl	%r10d, %r9d
	movl	%r11d, %r10d
	movl	24(%rsp), %r11d
.L367:
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ecx, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	orl	%ebx, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r15d
	movl	%edx, %esi
	orl	%r15d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%r11d, %eax
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %r11d
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ecx, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	orl	%ebx, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r15d
	movl	%edx, %esi
	orl	%r15d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%r11d, %eax
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %r11d
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ecx, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	orl	%ebx, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r15d
	movl	%edx, %esi
	orl	%r15d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%r11d, %eax
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %r11d
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ecx, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	orl	%ebx, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r15d
	movl	%edx, %esi
	orl	%r15d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%r11d, %eax
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %r11d
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ecx, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	orl	%ebx, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r15d
	movl	%edx, %esi
	orl	%r15d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%r11d, %eax
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %r11d
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ecx, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	orl	%ebx, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r15d
	movl	%edx, %esi
	orl	%r15d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%r11d, %eax
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %r11d
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ecx, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	orl	%ebx, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r15d
	movl	%edx, %esi
	orl	%r15d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%r11d, %eax
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %r11d
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ecx, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	orl	%ebx, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r15d
	movl	%edx, %esi
	orl	%r15d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%r11d, %eax
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %r11d
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ecx, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	orl	%ebx, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r15d
	movl	%edx, %esi
	orl	%r15d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%r11d, %eax
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %r11d
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ecx, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	orl	%ebx, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r15d
	movl	%edx, %esi
	orl	%r15d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%r11d, %eax
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %r11d
	subq	$1, 16(%rsp)
	movq	16(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L367
	movl	%ecx, 28(%rsp)
	movl	%esi, 32(%rsp)
	movl	%r8d, 36(%rsp)
	movl	%r9d, 40(%rsp)
	movl	%r10d, 44(%rsp)
	movl	%edx, 24(%rsp)
.L366:
	call	use_int@PLT
	movl	28(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	movl	36(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	44(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE143:
	.size	integer_mod_6, .-integer_mod_6
	.globl	integer_mod_7
	.type	integer_mod_7, @function
integer_mod_7:
.LFB144:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$56, %rsp
	.cfi_def_cfa_offset 112
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx,%rdi), %edi
	leal	63(%rcx), %esi
	movl	%esi, 4(%rsp)
	movl	16(%rdx), %ecx
	leal	(%rcx,%rax), %esi
	movl	%esi, 24(%rsp)
	leal	62(%rcx), %esi
	movl	%esi, 8(%rsp)
	movl	20(%rdx), %ecx
	leal	-1(%rcx,%rax), %r11d
	movl	%r11d, 28(%rsp)
	leal	61(%rcx), %esi
	movl	%esi, 12(%rsp)
	movl	24(%rdx), %ecx
	leal	-2(%rcx,%rax), %r10d
	movl	%r10d, 32(%rsp)
	leal	60(%rcx), %r14d
	movl	28(%rdx), %ecx
	leal	-3(%rcx,%rax), %esi
	movl	%esi, 36(%rsp)
	leal	59(%rcx), %ebp
	movl	32(%rdx), %ecx
	leal	-4(%rcx,%rax), %r8d
	movl	%r8d, 40(%rsp)
	leal	58(%rcx), %r15d
	movl	36(%rdx), %ecx
	leal	-5(%rcx,%rax), %r9d
	movl	%r9d, 44(%rsp)
	leal	57(%rcx), %r13d
	movl	40(%rdx), %edx
	leal	-6(%rdx,%rax), %ebx
	leaq	-1(%rax), %rcx
	movq	%rcx, 16(%rsp)
	testq	%rax, %rax
	je	.L371
	leal	56(%rdx), %r12d
	movl	%ebp, %eax
	movl	%r14d, %ebp
	movl	%r10d, %ecx
	movl	%eax, %r14d
.L372:
	movl	%edi, %eax
	movl	4(%rsp), %r10d
	cltd
	idivl	%r10d
	movl	%edx, %edi
	orl	%r10d, %edi
	movl	24(%rsp), %eax
	movl	8(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, 24(%rsp)
	movl	%r11d, %eax
	movl	12(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	orl	%ebp, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r8d
	orl	%r15d, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r9d
	orl	%r13d, %r9d
	movl	%ebx, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%edi, %eax
	movl	4(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %edi
	movl	24(%rsp), %eax
	movl	8(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	12(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	orl	%ebp, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%edi, %eax
	movl	4(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ebx, %eax
	movl	8(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	12(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	orl	%ebp, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%edi, %eax
	movl	4(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ebx, %eax
	movl	8(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	12(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	orl	%ebp, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%edi, %eax
	movl	4(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ebx, %eax
	movl	8(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	12(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	orl	%ebp, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%edi, %eax
	movl	4(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ebx, %eax
	movl	8(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	12(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	orl	%ebp, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%edi, %eax
	movl	4(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ebx, %eax
	movl	8(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	12(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	orl	%ebp, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%edi, %eax
	movl	4(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ebx, %eax
	movl	8(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	12(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	orl	%ebp, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%edi, %eax
	movl	4(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ebx, %eax
	movl	8(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	12(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	orl	%ebp, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r10d
	movl	%edi, %eax
	movl	4(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ebx, %eax
	movl	8(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, 24(%rsp)
	movl	%r11d, %eax
	movl	12(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %r11d
	movl	%ecx, %eax
	cltd
	idivl	%ebp
	movl	%edx, %ecx
	orl	%ebp, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r9d
	movl	%r10d, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebx
	orl	%r12d, %ebx
	subq	$1, 16(%rsp)
	movq	16(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L372
	movl	%r11d, 28(%rsp)
	movl	%ecx, 32(%rsp)
	movl	%esi, 36(%rsp)
	movl	%r8d, 40(%rsp)
	movl	%r9d, 44(%rsp)
.L371:
	call	use_int@PLT
	movl	24(%rsp), %edi
	call	use_int@PLT
	movl	28(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	movl	36(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	44(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE144:
	.size	integer_mod_7, .-integer_mod_7
	.globl	integer_mod_8
	.type	integer_mod_8, @function
integer_mod_8:
.LFB145:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$72, %rsp
	.cfi_def_cfa_offset 128
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx,%rdi), %edi
	leal	63(%rcx), %esi
	movl	%esi, 12(%rsp)
	movl	16(%rdx), %ecx
	leal	(%rcx,%rax), %esi
	movl	%esi, 40(%rsp)
	leal	62(%rcx), %esi
	movl	%esi, 16(%rsp)
	movl	20(%rdx), %ecx
	leal	-1(%rcx,%rax), %esi
	movl	%esi, 44(%rsp)
	leal	61(%rcx), %esi
	movl	%esi, 20(%rsp)
	movl	24(%rdx), %ecx
	leal	-2(%rcx,%rax), %r11d
	movl	%r11d, 48(%rsp)
	leal	60(%rcx), %esi
	movl	%esi, 24(%rsp)
	movl	28(%rdx), %ecx
	leal	-3(%rcx,%rax), %r10d
	movl	%r10d, 52(%rsp)
	leal	59(%rcx), %esi
	movl	%esi, 28(%rsp)
	movl	32(%rdx), %ecx
	leal	-4(%rcx,%rax), %r9d
	movl	%r9d, 56(%rsp)
	leal	58(%rcx), %r13d
	movl	36(%rdx), %ecx
	leal	-5(%rcx,%rax), %esi
	movl	%esi, 60(%rsp)
	leal	57(%rcx), %r12d
	movl	40(%rdx), %ecx
	leal	-6(%rcx,%rax), %ebp
	leal	56(%rcx), %r14d
	movl	44(%rdx), %edx
	leal	-7(%rdx,%rax), %ebx
	leaq	-1(%rax), %rcx
	movq	%rcx, 32(%rsp)
	testq	%rax, %rax
	je	.L376
	leal	55(%rdx), %r15d
	movl	%r12d, %eax
	movl	%r13d, %r12d
	movl	%r9d, %ecx
	movl	%eax, %r13d
.L377:
	movl	%edi, %eax
	movl	12(%rsp), %r8d
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	40(%rsp), %eax
	movl	16(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, 40(%rsp)
	movl	44(%rsp), %eax
	movl	20(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, 44(%rsp)
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	orl	%r12d, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%edx, %esi
	orl	%r13d, %esi
	movl	%ebp, %eax
	cltd
	idivl	%r14d
	movl	%edx, %r9d
	orl	%r14d, %r9d
	movl	%ebx, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	12(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %edi
	orl	%ebx, %edi
	movl	40(%rsp), %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	44(%rsp), %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	orl	%r12d, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%edx, %esi
	orl	%r13d, %esi
	movl	%r9d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	orl	%r12d, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%edx, %esi
	orl	%r13d, %esi
	movl	%r9d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	orl	%r12d, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%edx, %esi
	orl	%r13d, %esi
	movl	%r9d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	orl	%r12d, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%edx, %esi
	orl	%r13d, %esi
	movl	%r9d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	orl	%r12d, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%edx, %esi
	orl	%r13d, %esi
	movl	%r9d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	orl	%r12d, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%edx, %esi
	orl	%r13d, %esi
	movl	%r9d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	orl	%r12d, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%edx, %esi
	orl	%r13d, %esi
	movl	%r9d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	orl	%r12d, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%edx, %esi
	orl	%r13d, %esi
	movl	%r9d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	12(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, 40(%rsp)
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, 44(%rsp)
	movl	%r11d, %eax
	movl	24(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %r10d
	movl	%ecx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ecx
	orl	%r12d, %ecx
	movl	%esi, %eax
	cltd
	idivl	%r13d
	movl	%edx, %esi
	orl	%r13d, %esi
	movl	%r9d, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %ebp
	movl	%r8d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ebx
	orl	%r15d, %ebx
	subq	$1, 32(%rsp)
	movq	32(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L377
	movl	%r11d, 48(%rsp)
	movl	%r10d, 52(%rsp)
	movl	%ecx, 56(%rsp)
	movl	%esi, 60(%rsp)
.L376:
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	44(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	call	use_int@PLT
	movl	52(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	movl	60(%rsp), %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE145:
	.size	integer_mod_8, .-integer_mod_8
	.globl	integer_mod_9
	.type	integer_mod_9, @function
integer_mod_9:
.LFB146:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$72, %rsp
	.cfi_def_cfa_offset 128
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx,%rdi), %edi
	leal	63(%rcx), %esi
	movl	%esi, 4(%rsp)
	movl	16(%rdx), %ecx
	leal	(%rcx,%rax), %esi
	movl	%esi, 40(%rsp)
	leal	62(%rcx), %esi
	movl	%esi, 8(%rsp)
	movl	20(%rdx), %ecx
	leal	-1(%rcx,%rax), %esi
	movl	%esi, 44(%rsp)
	addl	$61, %ecx
	movl	%ecx, 12(%rsp)
	movl	24(%rdx), %ecx
	leal	-2(%rcx,%rax), %esi
	movl	%esi, 48(%rsp)
	addl	$60, %ecx
	movl	%ecx, 16(%rsp)
	movl	28(%rdx), %ecx
	leal	-3(%rcx,%rax), %r11d
	movl	%r11d, 52(%rsp)
	leal	59(%rcx), %esi
	movl	%esi, 20(%rsp)
	movl	32(%rdx), %ecx
	leal	-4(%rcx,%rax), %r10d
	movl	%r10d, 56(%rsp)
	addl	$58, %ecx
	movl	%ecx, 24(%rsp)
	movl	36(%rdx), %ecx
	leal	-5(%rcx,%rax), %r9d
	movl	%r9d, 60(%rsp)
	leal	57(%rcx), %esi
	movl	%esi, 28(%rsp)
	movl	40(%rdx), %ecx
	leal	-6(%rcx,%rax), %r12d
	leal	56(%rcx), %r14d
	movl	44(%rdx), %ecx
	leal	-7(%rcx,%rax), %ebp
	leal	55(%rcx), %r13d
	movl	48(%rdx), %edx
	leal	-8(%rdx,%rax), %ebx
	leaq	-1(%rax), %rcx
	movq	%rcx, 32(%rsp)
	testq	%rax, %rax
	je	.L381
	leal	54(%rdx), %r15d
	movl	%r13d, %eax
	movl	%r14d, %r13d
	movl	%eax, %r14d
.L382:
	movl	%edi, %eax
	movl	4(%rsp), %esi
	cltd
	idivl	%esi
	movl	%edx, %edi
	orl	%esi, %edi
	movl	40(%rsp), %eax
	movl	8(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, 40(%rsp)
	movl	44(%rsp), %eax
	movl	12(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, 44(%rsp)
	movl	48(%rsp), %eax
	movl	16(%rsp), %esi
	cltd
	idivl	%esi
	movl	%edx, %ecx
	orl	%esi, %ecx
	movl	%ecx, 48(%rsp)
	movl	%r11d, %eax
	movl	20(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	24(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	28(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r12d, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r8d
	orl	%r13d, %r8d
	movl	%ebp, %eax
	cltd
	idivl	%r14d
	movl	%edx, %r12d
	orl	%r14d, %r12d
	movl	%r12d, %esi
	movl	%ebx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r12d
	orl	%r15d, %r12d
	movl	%r12d, %ecx
	movl	%edi, %eax
	movl	4(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r12d
	orl	%ebx, %r12d
	movl	%r12d, %edi
	movl	40(%rsp), %eax
	movl	8(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %r12d
	movl	44(%rsp), %eax
	movl	12(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebp
	movl	48(%rsp), %eax
	movl	16(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	20(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	24(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	28(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r8d
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ecx
	orl	%r15d, %ecx
	movl	%edi, %eax
	movl	4(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%r12d, %eax
	movl	8(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	12(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	16(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	20(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	24(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	28(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r8d
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ecx
	orl	%r15d, %ecx
	movl	%edi, %eax
	movl	4(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%r12d, %eax
	movl	8(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	12(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	16(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	20(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	24(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	28(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r8d
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ecx
	orl	%r15d, %ecx
	movl	%edi, %eax
	movl	4(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%r12d, %eax
	movl	8(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	12(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	16(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	20(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	24(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	28(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r8d
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ecx
	orl	%r15d, %ecx
	movl	%edi, %eax
	movl	4(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%r12d, %eax
	movl	8(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	12(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	16(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	20(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	24(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	28(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r8d
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ecx
	orl	%r15d, %ecx
	movl	%edi, %eax
	movl	4(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%r12d, %eax
	movl	8(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	12(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	16(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	20(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	24(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	28(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r8d
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ecx
	orl	%r15d, %ecx
	movl	%edi, %eax
	movl	4(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%r12d, %eax
	movl	8(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	12(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	16(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	20(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	24(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	28(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r8d
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ecx
	orl	%r15d, %ecx
	movl	%edi, %eax
	movl	4(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%r12d, %eax
	movl	8(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	12(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	16(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	20(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	24(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	28(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r8d
	movl	%esi, %eax
	cltd
	idivl	%r14d
	movl	%edx, %esi
	orl	%r14d, %esi
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ecx
	orl	%r15d, %ecx
	movl	%edi, %eax
	movl	4(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%r12d, %eax
	movl	8(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, 40(%rsp)
	movl	%ebp, %eax
	movl	12(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, 44(%rsp)
	movl	%ebx, %eax
	movl	16(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, 48(%rsp)
	movl	%r11d, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	24(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	28(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r12d
	movl	%esi, %eax
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %ebp
	movl	%ecx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ebx
	orl	%r15d, %ebx
	subq	$1, 32(%rsp)
	movq	32(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L382
	movl	%r11d, 52(%rsp)
	movl	%r10d, 56(%rsp)
	movl	%r9d, 60(%rsp)
.L381:
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	44(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	call	use_int@PLT
	movl	52(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	movl	60(%rsp), %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE146:
	.size	integer_mod_9, .-integer_mod_9
	.globl	integer_mod_10
	.type	integer_mod_10, @function
integer_mod_10:
.LFB147:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$88, %rsp
	.cfi_def_cfa_offset 144
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx,%rdi), %edi
	leal	63(%rcx), %esi
	movl	%esi, 8(%rsp)
	movl	16(%rdx), %ecx
	leal	(%rcx,%rax), %r14d
	leal	62(%rcx), %esi
	movl	%esi, 12(%rsp)
	movl	20(%rdx), %ecx
	leal	-1(%rcx,%rax), %esi
	movl	%esi, 56(%rsp)
	leal	61(%rcx), %esi
	movl	%esi, 16(%rsp)
	movl	24(%rdx), %ecx
	leal	-2(%rcx,%rax), %esi
	movl	%esi, 60(%rsp)
	leal	60(%rcx), %esi
	movl	%esi, 20(%rsp)
	movl	28(%rdx), %ecx
	leal	-3(%rcx,%rax), %esi
	movl	%esi, 64(%rsp)
	leal	59(%rcx), %esi
	movl	%esi, 24(%rsp)
	movl	32(%rdx), %ecx
	leal	-4(%rcx,%rax), %esi
	movl	%esi, 68(%rsp)
	addl	$58, %ecx
	movl	%ecx, 28(%rsp)
	movl	36(%rdx), %ecx
	leal	-5(%rcx,%rax), %r11d
	movl	%r11d, 72(%rsp)
	addl	$57, %ecx
	movl	%ecx, 32(%rsp)
	movl	40(%rdx), %ecx
	leal	-6(%rcx,%rax), %r13d
	addl	$56, %ecx
	movl	%ecx, 36(%rsp)
	movl	44(%rdx), %ecx
	leal	-7(%rcx,%rax), %r12d
	addl	$55, %ecx
	movl	%ecx, 40(%rsp)
	movl	48(%rdx), %ecx
	leal	-8(%rcx,%rax), %ebp
	addl	$54, %ecx
	movl	%ecx, 44(%rsp)
	movl	52(%rdx), %edx
	leal	-9(%rdx,%rax), %ebx
	leaq	-1(%rax), %rcx
	movq	%rcx, 48(%rsp)
	testq	%rax, %rax
	je	.L386
	leal	53(%rdx), %r15d
	movl	%r11d, %ecx
.L387:
	movl	%edi, %eax
	movl	8(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, 68(%rsp)
	movl	%r14d, %eax
	movl	12(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r14d
	movl	56(%rsp), %eax
	movl	16(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r11d
	movl	60(%rsp), %eax
	movl	20(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	64(%rsp), %eax
	movl	24(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r9d
	movl	%esi, %eax
	movl	28(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %r8d
	movl	%ecx, %eax
	movl	32(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	%r13d, %eax
	movl	36(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	40(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	44(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %edi
	movl	%ebx, %eax
	cltd
	idivl	%r15d
	movl	%edx, %ebx
	orl	%r15d, %ebx
	movl	%ebx, %esi
	movl	68(%rsp), %eax
	movl	8(%rsp), %ebp
	cltd
	idivl	%ebp
	movl	%edx, %ebx
	orl	%ebp, %ebx
	movl	%r14d, %eax
	movl	12(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	16(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	20(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	24(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	28(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%ecx, %eax
	movl	32(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	36(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	40(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %ecx
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r12d
	movl	%ebx, %eax
	movl	8(%rsp), %esi
	cltd
	idivl	%esi
	movl	%edx, %ebx
	orl	%esi, %ebx
	movl	%ebp, %eax
	movl	12(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	16(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	20(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	24(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	28(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%r14d, %eax
	movl	32(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	36(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	40(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r12d
	movl	%ebx, %eax
	movl	8(%rsp), %edi
	cltd
	idivl	%edi
	movl	%edx, %ebx
	orl	%edi, %ebx
	movl	%ebp, %eax
	movl	12(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	16(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	20(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	24(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	28(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%r14d, %eax
	movl	32(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	36(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	40(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	44(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r12d
	movl	%ebx, %eax
	movl	8(%rsp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%ebp, %eax
	movl	12(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	16(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	20(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	24(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	28(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %ecx
	movl	%edi, %eax
	movl	32(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	36(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %r8d
	movl	%r14d, %eax
	movl	40(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	44(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r12d
	movl	%ebx, %eax
	movl	8(%rsp), %esi
	cltd
	idivl	%esi
	movl	%edx, %ebx
	orl	%esi, %ebx
	movl	%ebp, %eax
	movl	12(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	16(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	20(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	24(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	28(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	%edi, %eax
	movl	32(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	36(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%r14d, %eax
	movl	40(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	44(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r12d
	movl	%ebx, %eax
	movl	8(%rsp), %edi
	cltd
	idivl	%edi
	movl	%edx, %ebx
	orl	%edi, %ebx
	movl	%ebp, %eax
	movl	12(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	16(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	20(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	24(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	28(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	36(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%r14d, %eax
	movl	40(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	44(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r12d
	movl	%ebx, %eax
	movl	8(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ebx
	movl	%ebp, %eax
	movl	12(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	16(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %ecx
	movl	%edi, %eax
	movl	20(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	24(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	36(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%r14d, %eax
	movl	40(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	44(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r12d
	movl	%ebx, %eax
	movl	8(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %ebx
	movl	%ebp, %eax
	movl	12(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	16(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	%edi, %eax
	movl	20(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	24(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	36(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%r14d, %eax
	movl	40(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, 68(%rsp)
	movl	%r13d, %eax
	movl	44(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, 72(%rsp)
	movl	%r12d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r12d
	orl	%r15d, %r12d
	movl	%r12d, 76(%rsp)
	movl	%ebx, %eax
	movl	8(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %edi
	orl	%ebx, %edi
	movl	%esi, %eax
	movl	12(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r14d
	orl	%ebx, %r14d
	movl	%ecx, %eax
	movl	16(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %esi
	orl	%ebx, %esi
	movl	%esi, 56(%rsp)
	movl	%ebp, %eax
	movl	20(%rsp), %esi
	cltd
	idivl	%esi
	movl	%edx, %ebp
	orl	%esi, %ebp
	movl	%ebp, 60(%rsp)
	movl	%r11d, %eax
	movl	24(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %esi
	orl	%ebx, %esi
	movl	%esi, 64(%rsp)
	movl	%r10d, %eax
	movl	28(%rsp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %esi
	orl	%ecx, %esi
	movl	%r9d, %eax
	movl	32(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %ecx
	orl	%ebx, %ecx
	movl	%r8d, %eax
	movl	36(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r11d
	orl	%ebx, %r11d
	movl	%r11d, %r13d
	movl	68(%rsp), %eax
	movl	40(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %r12d
	movl	72(%rsp), %eax
	movl	44(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebp
	movl	76(%rsp), %eax
	cltd
	idivl	%r15d
	movl	%edx, %ebx
	orl	%r15d, %ebx
	subq	$1, 48(%rsp)
	movq	48(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L387
	movl	%esi, 68(%rsp)
	movl	%ecx, 72(%rsp)
.L386:
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	movl	60(%rsp), %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	call	use_int@PLT
	movl	68(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE147:
	.size	integer_mod_10, .-integer_mod_10
	.globl	integer_mod_11
	.type	integer_mod_11, @function
integer_mod_11:
.LFB148:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$88, %rsp
	.cfi_def_cfa_offset 144
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx,%rdi), %esi
	movl	%esi, 48(%rsp)
	leal	63(%rcx), %esi
	movl	%esi, (%rsp)
	movl	16(%rdx), %ecx
	leal	(%rcx,%rdi), %esi
	movl	%esi, 52(%rsp)
	leal	62(%rcx), %esi
	movl	%esi, 4(%rsp)
	movl	20(%rdx), %ecx
	leal	-1(%rcx,%rdi), %esi
	movl	%esi, 56(%rsp)
	leal	61(%rcx), %esi
	movl	%esi, 8(%rsp)
	movl	24(%rdx), %ecx
	leal	-2(%rcx,%rdi), %esi
	movl	%esi, 60(%rsp)
	leal	60(%rcx), %esi
	movl	%esi, 12(%rsp)
	movl	28(%rdx), %ecx
	leal	-3(%rcx,%rdi), %esi
	movl	%esi, 64(%rsp)
	leal	59(%rcx), %esi
	movl	%esi, 32(%rsp)
	movl	32(%rdx), %ecx
	leal	-4(%rcx,%rdi), %r8d
	movl	%r8d, %r15d
	leal	58(%rcx), %esi
	movl	%esi, 16(%rsp)
	movl	36(%rdx), %ecx
	leal	-5(%rcx,%rdi), %edi
	movl	%edi, 68(%rsp)
	leal	57(%rcx), %esi
	movl	%esi, 20(%rsp)
	movl	40(%rdx), %ecx
	leal	-6(%rcx,%rax), %r14d
	leal	56(%rcx), %esi
	movl	%esi, 24(%rsp)
	movl	44(%rdx), %ecx
	leal	-7(%rcx,%rax), %r13d
	leal	55(%rcx), %esi
	movl	%esi, 28(%rsp)
	movl	48(%rdx), %ecx
	leal	-8(%rcx,%rax), %r12d
	leal	54(%rcx), %esi
	movl	%esi, 36(%rsp)
	movl	52(%rdx), %ecx
	leal	-9(%rcx,%rax), %ebp
	leal	53(%rcx), %esi
	movl	56(%rdx), %edx
	leal	-10(%rdx,%rax), %ebx
	leal	52(%rdx), %ecx
	leaq	-1(%rax), %rdx
	movq	%rdx, 40(%rsp)
	testq	%rax, %rax
	je	.L391
	movl	%ecx, %eax
	movl	%esi, %ecx
	movl	%eax, %esi
.L392:
	movl	48(%rsp), %eax
	movl	(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r15d
	movl	52(%rsp), %eax
	movl	4(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r11d
	movl	56(%rsp), %eax
	movl	8(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r10d
	movl	60(%rsp), %eax
	movl	12(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	64(%rsp), %eax
	cltd
	idivl	32(%rsp)
	orl	32(%rsp), %edx
	movl	%edx, 48(%rsp)
	movl	%r8d, %eax
	movl	16(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	20(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%r14d, %eax
	movl	24(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, 52(%rsp)
	movl	%r13d, %eax
	movl	28(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	36(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebp
	orl	%ecx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%esi
	movl	%edx, %ebx
	orl	%esi, %ebx
	movl	%r15d, %eax
	movl	(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r14d
	movl	%r11d, %eax
	movl	4(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	8(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	12(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	48(%rsp), %eax
	movl	32(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, 48(%rsp)
	movl	%r8d, %eax
	movl	16(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	20(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	52(%rsp), %eax
	movl	24(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r13d, %eax
	movl	28(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	36(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebp
	orl	%ecx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%esi
	movl	%edx, %ebx
	orl	%esi, %ebx
	movl	%r14d, %eax
	movl	(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r11d, %eax
	movl	4(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	8(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	12(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, 52(%rsp)
	movl	48(%rsp), %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	16(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	20(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%r15d, %eax
	movl	24(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r13d, %eax
	movl	28(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	36(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebp
	orl	%ecx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%esi
	movl	%edx, %ebx
	orl	%esi, %ebx
	movl	%r14d, %eax
	movl	(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r11d, %eax
	movl	4(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	8(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, 48(%rsp)
	movl	52(%rsp), %eax
	movl	12(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	16(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	20(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%r15d, %eax
	movl	24(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r13d, %eax
	movl	28(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	36(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebp
	orl	%ecx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%esi
	movl	%edx, %ebx
	orl	%esi, %ebx
	movl	%r14d, %eax
	movl	(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r11d, %eax
	movl	4(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, 52(%rsp)
	movl	48(%rsp), %eax
	movl	8(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	12(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	16(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	20(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%r15d, %eax
	movl	24(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r13d, %eax
	movl	28(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	36(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebp
	orl	%ecx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%esi
	movl	%edx, %ebx
	orl	%esi, %ebx
	movl	%r14d, %eax
	movl	(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, 48(%rsp)
	movl	52(%rsp), %eax
	movl	4(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r11d, %eax
	movl	8(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	12(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	16(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	20(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%r15d, %eax
	movl	24(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r13d, %eax
	movl	28(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	36(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebp
	orl	%ecx, %ebp
	movl	%ebx, %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, 52(%rsp)
	movl	48(%rsp), %eax
	movl	(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r14d, %eax
	movl	4(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r11d, %eax
	movl	8(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	12(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	16(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	20(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%r15d, %eax
	movl	24(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r13d, %eax
	movl	28(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	36(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, 48(%rsp)
	movl	52(%rsp), %eax
	cltd
	idivl	%esi
	movl	%edx, %ebp
	orl	%esi, %ebp
	movl	%ebx, %eax
	movl	(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r14d, %eax
	movl	4(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r11d, %eax
	movl	8(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	12(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	16(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	20(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%r15d, %eax
	movl	24(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r13d, %eax
	movl	28(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	36(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, 52(%rsp)
	movl	48(%rsp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	cltd
	idivl	%esi
	movl	%edx, %ebp
	orl	%esi, %ebp
	movl	%ebx, %eax
	movl	(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r14d, %eax
	movl	4(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r11d, %eax
	movl	8(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	12(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	16(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	20(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%r15d, %eax
	movl	24(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r13d, %eax
	movl	28(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	52(%rsp), %eax
	cltd
	idivl	36(%rsp)
	orl	36(%rsp), %edx
	movl	%edx, 68(%rsp)
	movl	%r12d, %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, 72(%rsp)
	movl	%ebp, %eax
	cltd
	idivl	%esi
	movl	%edx, %ebp
	orl	%esi, %ebp
	movl	%ebp, 76(%rsp)
	movl	%ebx, %eax
	movl	(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r12d
	orl	%ebx, %r12d
	movl	%r12d, 48(%rsp)
	movl	%r14d, %eax
	movl	4(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r14d
	orl	%ebx, %r14d
	movl	%r14d, 52(%rsp)
	movl	%r11d, %eax
	movl	8(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r11d
	orl	%ebx, %r11d
	movl	%r11d, 56(%rsp)
	movl	%r10d, %eax
	movl	12(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r11d
	orl	%ebx, %r11d
	movl	%r11d, 60(%rsp)
	movl	%r9d, %eax
	movl	32(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r14d
	orl	%ebx, %r14d
	movl	%r14d, 64(%rsp)
	movl	%r8d, %eax
	movl	16(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r8d
	orl	%ebx, %r8d
	movl	%edi, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %edi
	orl	%ebx, %edi
	movl	%r15d, %eax
	movl	24(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r14d
	orl	%ebx, %r14d
	movl	%r13d, %eax
	movl	28(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r13d
	orl	%ebx, %r13d
	movl	68(%rsp), %eax
	movl	36(%rsp), %ebx
	cltd
	idivl	%ebx
	movl	%edx, %r12d
	orl	%ebx, %r12d
	movl	72(%rsp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ebp
	movl	76(%rsp), %eax
	cltd
	idivl	%esi
	movl	%edx, %ebx
	orl	%esi, %ebx
	subq	$1, 40(%rsp)
	movq	40(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L392
	movl	%r8d, %r15d
	movl	%edi, 68(%rsp)
.L391:
	movl	48(%rsp), %edi
	call	use_int@PLT
	movl	52(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	movl	60(%rsp), %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	68(%rsp), %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE148:
	.size	integer_mod_11, .-integer_mod_11
	.globl	integer_mod_12
	.type	integer_mod_12, @function
integer_mod_12:
.LFB149:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$104, %rsp
	.cfi_def_cfa_offset 160
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx,%rdi), %r15d
	leal	63(%rcx), %ebx
	movl	%ebx, 8(%rsp)
	movl	16(%rsi), %ecx
	leal	(%rcx,%rdi), %r14d
	leal	62(%rcx), %ebx
	movl	%ebx, 12(%rsp)
	movl	20(%rsi), %ecx
	leal	-1(%rcx,%rdi), %r13d
	leal	61(%rcx), %esi
	movl	%esi, 16(%rsp)
	movl	24(%rdx), %ecx
	leal	-2(%rcx,%rdi), %r12d
	leal	60(%rcx), %ebx
	movl	%ebx, 20(%rsp)
	movl	28(%rdx), %ecx
	leal	-3(%rcx,%rdi), %ebp
	leal	59(%rcx), %esi
	movl	%esi, 24(%rsp)
	movl	32(%rdx), %ecx
	leal	-4(%rcx,%rdi), %r11d
	movl	%r11d, 72(%rsp)
	leal	58(%rcx), %ebx
	movl	%ebx, 28(%rsp)
	movl	36(%rdx), %ecx
	leal	-5(%rcx,%rdi), %r10d
	movl	%r10d, 76(%rsp)
	leal	57(%rcx), %esi
	movl	%esi, 32(%rsp)
	movl	40(%rdx), %ecx
	leal	-6(%rcx,%rdi), %r9d
	movl	%r9d, 80(%rsp)
	leal	56(%rcx), %ebx
	movl	%ebx, 36(%rsp)
	movl	44(%rdx), %ecx
	leal	-7(%rcx,%rdi), %r8d
	movl	%r8d, 84(%rsp)
	leal	55(%rcx), %esi
	movl	%esi, 40(%rsp)
	movl	48(%rdx), %ecx
	leal	-8(%rcx,%rdi), %edi
	movl	%edi, 88(%rsp)
	leal	54(%rcx), %ebx
	movl	%ebx, 44(%rsp)
	movl	52(%rdx), %ecx
	leal	-9(%rcx,%rax), %esi
	movl	%esi, 92(%rsp)
	leal	53(%rcx), %ebx
	movl	%ebx, 48(%rsp)
	movl	56(%rdx), %ecx
	leal	-10(%rcx,%rax), %ebx
	movl	%ebx, 68(%rsp)
	addl	$52, %ecx
	movl	%ecx, 52(%rsp)
	movl	60(%rdx), %edx
	leal	-11(%rdx,%rax), %ebx
	addl	$51, %edx
	movl	%edx, 64(%rsp)
	leaq	-1(%rax), %rdx
	movq	%rdx, 56(%rsp)
	testq	%rax, %rax
	je	.L396
	movl	68(%rsp), %ecx
.L397:
	movl	%r15d, %eax
	movl	8(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	12(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	16(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	20(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	24(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	%ebx, %eax
	movl	64(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r15d, %eax
	movl	8(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	12(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	16(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	20(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	24(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	%ebx, %eax
	movl	64(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r15d, %eax
	movl	8(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	12(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	16(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	20(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	24(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	%ebx, %eax
	movl	64(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r15d, %eax
	movl	8(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	12(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	16(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	20(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	24(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	%ebx, %eax
	movl	64(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r15d, %eax
	movl	8(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	12(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	16(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	20(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	24(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	%ebx, %eax
	movl	64(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r15d, %eax
	movl	8(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	12(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	16(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	20(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	24(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	%ebx, %eax
	movl	64(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r15d, %eax
	movl	8(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	12(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	16(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	20(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	24(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	%ebx, %eax
	movl	64(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r15d, %eax
	movl	8(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	12(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	16(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	20(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	24(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	%ebx, %eax
	movl	64(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r15d, %eax
	movl	8(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	12(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	16(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	20(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	24(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	%ebx, %eax
	movl	64(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r15d, %eax
	movl	8(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	12(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	16(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	20(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	24(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	%ebx, %eax
	movl	64(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	subq	$1, 56(%rsp)
	movq	56(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L397
	movl	%r11d, 72(%rsp)
	movl	%r10d, 76(%rsp)
	movl	%r9d, 80(%rsp)
	movl	%r8d, 84(%rsp)
	movl	%edi, 88(%rsp)
	movl	%esi, 92(%rsp)
	movl	%ecx, 68(%rsp)
.L396:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	call	use_int@PLT
	movl	76(%rsp), %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	call	use_int@PLT
	movl	84(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	call	use_int@PLT
	movl	92(%rsp), %edi
	call	use_int@PLT
	movl	68(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE149:
	.size	integer_mod_12, .-integer_mod_12
	.globl	integer_mod_13
	.type	integer_mod_13, @function
integer_mod_13:
.LFB150:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$104, %rsp
	.cfi_def_cfa_offset 160
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx,%rdi), %r15d
	leal	63(%rcx), %ebx
	movl	%ebx, (%rsp)
	movl	16(%rsi), %ecx
	leal	(%rcx,%rdi), %r14d
	leal	62(%rcx), %ebx
	movl	%ebx, 4(%rsp)
	movl	20(%rsi), %ecx
	leal	-1(%rcx,%rdi), %r13d
	leal	61(%rcx), %esi
	movl	%esi, 8(%rsp)
	movl	24(%rdx), %ecx
	leal	-2(%rcx,%rdi), %r12d
	leal	60(%rcx), %ebx
	movl	%ebx, 12(%rsp)
	movl	28(%rdx), %ecx
	leal	-3(%rcx,%rdi), %ebp
	leal	59(%rcx), %esi
	movl	%esi, 16(%rsp)
	movl	32(%rdx), %ecx
	leal	-4(%rcx,%rdi), %ebx
	leal	58(%rcx), %esi
	movl	%esi, 20(%rsp)
	movl	36(%rdx), %ecx
	leal	-5(%rcx,%rdi), %r11d
	movl	%r11d, 76(%rsp)
	leal	57(%rcx), %esi
	movl	%esi, 24(%rsp)
	movl	40(%rdx), %ecx
	leal	-6(%rcx,%rdi), %r10d
	movl	%r10d, 80(%rsp)
	leal	56(%rcx), %esi
	movl	%esi, 28(%rsp)
	movl	44(%rdx), %ecx
	leal	-7(%rcx,%rdi), %r9d
	movl	%r9d, 84(%rsp)
	leal	55(%rcx), %esi
	movl	%esi, 32(%rsp)
	movl	48(%rdx), %ecx
	leal	-8(%rcx,%rdi), %r8d
	movl	%r8d, 88(%rsp)
	leal	54(%rcx), %esi
	movl	%esi, 36(%rsp)
	movl	52(%rdx), %ecx
	leal	-9(%rcx,%rdi), %edi
	movl	%edi, 92(%rsp)
	leal	53(%rcx), %esi
	movl	%esi, 40(%rsp)
	movl	56(%rdx), %ecx
	leal	-10(%rcx,%rax), %esi
	movl	%esi, 68(%rsp)
	addl	$52, %ecx
	movl	%ecx, 44(%rsp)
	movl	60(%rdx), %ecx
	leal	-11(%rcx,%rax), %esi
	movl	%esi, 72(%rsp)
	addl	$51, %ecx
	movl	%ecx, 48(%rsp)
	movl	64(%rdx), %edx
	leal	-12(%rdx,%rax), %ecx
	movl	%ecx, 64(%rsp)
	addl	$50, %edx
	movl	%edx, 52(%rsp)
	leaq	-1(%rax), %rdx
	movq	%rdx, 56(%rsp)
	testq	%rax, %rax
	je	.L401
	movl	68(%rsp), %esi
	movl	72(%rsp), %ecx
.L402:
	movl	%r15d, %eax
	movl	(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	4(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	8(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	12(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	36(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	40(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	44(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	48(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	52(%rsp)
	orl	52(%rsp), %edx
	movl	%edx, 64(%rsp)
	movl	%r15d, %eax
	movl	(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	4(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	8(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	12(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	36(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	40(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	44(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	48(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	52(%rsp)
	orl	52(%rsp), %edx
	movl	%edx, 64(%rsp)
	movl	%r15d, %eax
	movl	(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	4(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	8(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	12(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	36(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	40(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	44(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	48(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	52(%rsp)
	orl	52(%rsp), %edx
	movl	%edx, 64(%rsp)
	movl	%r15d, %eax
	movl	(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	4(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	8(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	12(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	36(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	40(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	44(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	48(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	52(%rsp)
	orl	52(%rsp), %edx
	movl	%edx, 64(%rsp)
	movl	%r15d, %eax
	movl	(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	4(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	8(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	12(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	36(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	40(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	44(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	48(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	52(%rsp)
	orl	52(%rsp), %edx
	movl	%edx, 64(%rsp)
	movl	%r15d, %eax
	movl	(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	4(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	8(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	12(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	36(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	40(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	44(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	48(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	52(%rsp)
	orl	52(%rsp), %edx
	movl	%edx, 64(%rsp)
	movl	%r15d, %eax
	movl	(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	4(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	8(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	12(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	36(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	40(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	44(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	48(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	52(%rsp)
	orl	52(%rsp), %edx
	movl	%edx, 64(%rsp)
	movl	%r15d, %eax
	movl	(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	4(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	8(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	12(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	36(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	40(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	44(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	48(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	52(%rsp)
	orl	52(%rsp), %edx
	movl	%edx, 64(%rsp)
	movl	%r15d, %eax
	movl	(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	4(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	8(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	12(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	36(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	40(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	44(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	48(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	52(%rsp)
	orl	52(%rsp), %edx
	movl	%edx, 64(%rsp)
	movl	%r15d, %eax
	movl	(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	4(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	8(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	12(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	16(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	20(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	24(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	28(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	32(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	36(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	40(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	44(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	48(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	64(%rsp), %eax
	cltd
	idivl	52(%rsp)
	orl	52(%rsp), %edx
	movl	%edx, 64(%rsp)
	subq	$1, 56(%rsp)
	movq	56(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L402
	movl	%r11d, 76(%rsp)
	movl	%r10d, 80(%rsp)
	movl	%r9d, 84(%rsp)
	movl	%r8d, 88(%rsp)
	movl	%edi, 92(%rsp)
	movl	%esi, 68(%rsp)
	movl	%ecx, 72(%rsp)
.L401:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	76(%rsp), %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	call	use_int@PLT
	movl	84(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	call	use_int@PLT
	movl	92(%rsp), %edi
	call	use_int@PLT
	movl	68(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE150:
	.size	integer_mod_13, .-integer_mod_13
	.globl	integer_mod_14
	.type	integer_mod_14, @function
integer_mod_14:
.LFB151:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$120, %rsp
	.cfi_def_cfa_offset 176
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx,%rdi), %r15d
	leal	63(%rcx), %ebx
	movl	%ebx, 4(%rsp)
	movl	16(%rsi), %ecx
	leal	(%rcx,%rdi), %r14d
	leal	62(%rcx), %ebx
	movl	%ebx, 8(%rsp)
	movl	20(%rsi), %ecx
	leal	-1(%rcx,%rdi), %r13d
	leal	61(%rcx), %ebx
	movl	%ebx, 12(%rsp)
	movl	24(%rsi), %ecx
	leal	-2(%rcx,%rdi), %r12d
	leal	60(%rcx), %esi
	movl	%esi, 16(%rsp)
	movl	28(%rdx), %ecx
	leal	-3(%rcx,%rdi), %ebp
	leal	59(%rcx), %ebx
	movl	%ebx, 20(%rsp)
	movl	32(%rdx), %ecx
	leal	-4(%rcx,%rdi), %esi
	movl	%esi, 80(%rsp)
	leal	58(%rcx), %esi
	movl	%esi, 24(%rsp)
	movl	36(%rdx), %ecx
	leal	-5(%rcx,%rdi), %r11d
	movl	%r11d, 88(%rsp)
	leal	57(%rcx), %ebx
	movl	%ebx, 28(%rsp)
	movl	40(%rdx), %ecx
	leal	-6(%rcx,%rdi), %r10d
	movl	%r10d, 92(%rsp)
	leal	56(%rcx), %esi
	movl	%esi, 32(%rsp)
	movl	44(%rdx), %ecx
	leal	-7(%rcx,%rdi), %r9d
	movl	%r9d, 96(%rsp)
	leal	55(%rcx), %ebx
	movl	%ebx, 36(%rsp)
	movl	48(%rdx), %ecx
	leal	-8(%rcx,%rdi), %r8d
	movl	%r8d, 100(%rsp)
	leal	54(%rcx), %esi
	movl	%esi, 40(%rsp)
	movl	52(%rdx), %ecx
	leal	-9(%rcx,%rdi), %edi
	movl	%edi, 104(%rsp)
	leal	53(%rcx), %ebx
	movl	%ebx, 44(%rsp)
	movl	56(%rdx), %ecx
	leal	-10(%rcx,%rax), %esi
	movl	%esi, 108(%rsp)
	leal	52(%rcx), %ebx
	movl	%ebx, 48(%rsp)
	movl	60(%rdx), %ecx
	leal	-11(%rcx,%rax), %ebx
	movl	%ebx, 84(%rsp)
	addl	$51, %ecx
	movl	%ecx, 52(%rsp)
	movl	64(%rdx), %ecx
	leal	-12(%rcx,%rax), %ebx
	movl	%ebx, 72(%rsp)
	addl	$50, %ecx
	movl	%ecx, 56(%rsp)
	movl	68(%rdx), %edx
	leal	-13(%rdx,%rax), %ecx
	movl	%ecx, 76(%rsp)
	addl	$49, %edx
	movl	%edx, 60(%rsp)
	leaq	-1(%rax), %rdx
	movq	%rdx, 64(%rsp)
	testq	%rax, %rax
	je	.L406
	movl	80(%rsp), %ebx
	movl	84(%rsp), %ecx
.L407:
	movl	%r15d, %eax
	movl	4(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	8(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	12(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	16(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	20(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	24(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	72(%rsp), %eax
	cltd
	idivl	56(%rsp)
	orl	56(%rsp), %edx
	movl	%edx, 72(%rsp)
	movl	76(%rsp), %eax
	cltd
	idivl	60(%rsp)
	orl	60(%rsp), %edx
	movl	%edx, 76(%rsp)
	movl	%r15d, %eax
	movl	4(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	8(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	12(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	16(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	20(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	24(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	72(%rsp), %eax
	cltd
	idivl	56(%rsp)
	orl	56(%rsp), %edx
	movl	%edx, 72(%rsp)
	movl	76(%rsp), %eax
	cltd
	idivl	60(%rsp)
	orl	60(%rsp), %edx
	movl	%edx, 76(%rsp)
	movl	%r15d, %eax
	movl	4(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	8(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	12(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	16(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	20(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	24(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	72(%rsp), %eax
	cltd
	idivl	56(%rsp)
	orl	56(%rsp), %edx
	movl	%edx, 72(%rsp)
	movl	76(%rsp), %eax
	cltd
	idivl	60(%rsp)
	orl	60(%rsp), %edx
	movl	%edx, 76(%rsp)
	movl	%r15d, %eax
	movl	4(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	8(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	12(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	16(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	20(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	24(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	72(%rsp), %eax
	cltd
	idivl	56(%rsp)
	orl	56(%rsp), %edx
	movl	%edx, 72(%rsp)
	movl	76(%rsp), %eax
	cltd
	idivl	60(%rsp)
	orl	60(%rsp), %edx
	movl	%edx, 76(%rsp)
	movl	%r15d, %eax
	movl	4(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	8(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	12(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	16(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	20(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	24(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	72(%rsp), %eax
	cltd
	idivl	56(%rsp)
	orl	56(%rsp), %edx
	movl	%edx, 72(%rsp)
	movl	76(%rsp), %eax
	cltd
	idivl	60(%rsp)
	orl	60(%rsp), %edx
	movl	%edx, 76(%rsp)
	movl	%r15d, %eax
	movl	4(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	8(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	12(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	16(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	20(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	24(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	72(%rsp), %eax
	cltd
	idivl	56(%rsp)
	orl	56(%rsp), %edx
	movl	%edx, 72(%rsp)
	movl	76(%rsp), %eax
	cltd
	idivl	60(%rsp)
	orl	60(%rsp), %edx
	movl	%edx, 76(%rsp)
	movl	%r15d, %eax
	movl	4(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	8(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	12(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	16(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	20(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	24(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	72(%rsp), %eax
	cltd
	idivl	56(%rsp)
	orl	56(%rsp), %edx
	movl	%edx, 72(%rsp)
	movl	76(%rsp), %eax
	cltd
	idivl	60(%rsp)
	orl	60(%rsp), %edx
	movl	%edx, 76(%rsp)
	movl	%r15d, %eax
	movl	4(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	8(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	12(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	16(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	20(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	24(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	72(%rsp), %eax
	cltd
	idivl	56(%rsp)
	orl	56(%rsp), %edx
	movl	%edx, 72(%rsp)
	movl	76(%rsp), %eax
	cltd
	idivl	60(%rsp)
	orl	60(%rsp), %edx
	movl	%edx, 76(%rsp)
	movl	%r15d, %eax
	movl	4(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	8(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	12(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	16(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	20(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	24(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	72(%rsp), %eax
	cltd
	idivl	56(%rsp)
	orl	56(%rsp), %edx
	movl	%edx, 72(%rsp)
	movl	76(%rsp), %eax
	cltd
	idivl	60(%rsp)
	orl	60(%rsp), %edx
	movl	%edx, 76(%rsp)
	movl	%r15d, %eax
	movl	4(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	8(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	12(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	16(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	20(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	24(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	28(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	32(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	36(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	40(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	44(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	48(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	52(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	72(%rsp), %eax
	cltd
	idivl	56(%rsp)
	orl	56(%rsp), %edx
	movl	%edx, 72(%rsp)
	movl	76(%rsp), %eax
	cltd
	idivl	60(%rsp)
	orl	60(%rsp), %edx
	movl	%edx, 76(%rsp)
	subq	$1, 64(%rsp)
	movq	64(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L407
	movl	%ebx, 80(%rsp)
	movl	%r11d, 88(%rsp)
	movl	%r10d, 92(%rsp)
	movl	%r9d, 96(%rsp)
	movl	%r8d, 100(%rsp)
	movl	%edi, 104(%rsp)
	movl	%esi, 108(%rsp)
	movl	%ecx, 84(%rsp)
.L406:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	call	use_int@PLT
	movl	92(%rsp), %edi
	call	use_int@PLT
	movl	96(%rsp), %edi
	call	use_int@PLT
	movl	100(%rsp), %edi
	call	use_int@PLT
	movl	104(%rsp), %edi
	call	use_int@PLT
	movl	108(%rsp), %edi
	call	use_int@PLT
	movl	84(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	call	use_int@PLT
	movl	76(%rsp), %edi
	call	use_int@PLT
	addq	$120, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE151:
	.size	integer_mod_14, .-integer_mod_14
	.globl	integer_mod_15
	.type	integer_mod_15, @function
integer_mod_15:
.LFB152:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$136, %rsp
	.cfi_def_cfa_offset 192
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx,%rdi), %r15d
	leal	63(%rcx), %ebx
	movl	%ebx, 12(%rsp)
	movl	16(%rsi), %ecx
	leal	(%rcx,%rdi), %r14d
	leal	62(%rcx), %ebx
	movl	%ebx, 16(%rsp)
	movl	20(%rsi), %ecx
	leal	-1(%rcx,%rdi), %r13d
	leal	61(%rcx), %ebx
	movl	%ebx, 20(%rsp)
	movl	24(%rsi), %ecx
	leal	-2(%rcx,%rdi), %r12d
	leal	60(%rcx), %esi
	movl	%esi, 24(%rsp)
	movl	28(%rdx), %ecx
	leal	-3(%rcx,%rdi), %ebp
	leal	59(%rcx), %ebx
	movl	%ebx, 28(%rsp)
	movl	32(%rdx), %ecx
	leal	-4(%rcx,%rdi), %esi
	movl	%esi, 96(%rsp)
	leal	58(%rcx), %esi
	movl	%esi, 32(%rsp)
	movl	36(%rdx), %ecx
	leal	-5(%rcx,%rdi), %r11d
	movl	%r11d, 104(%rsp)
	leal	57(%rcx), %ebx
	movl	%ebx, 36(%rsp)
	movl	40(%rdx), %ecx
	leal	-6(%rcx,%rdi), %r10d
	movl	%r10d, 108(%rsp)
	leal	56(%rcx), %esi
	movl	%esi, 40(%rsp)
	movl	44(%rdx), %ecx
	leal	-7(%rcx,%rdi), %r9d
	movl	%r9d, 112(%rsp)
	leal	55(%rcx), %ebx
	movl	%ebx, 44(%rsp)
	movl	48(%rdx), %ecx
	leal	-8(%rcx,%rdi), %r8d
	movl	%r8d, 116(%rsp)
	leal	54(%rcx), %esi
	movl	%esi, 48(%rsp)
	movl	52(%rdx), %ecx
	leal	-9(%rcx,%rdi), %edi
	movl	%edi, 120(%rsp)
	leal	53(%rcx), %ebx
	movl	%ebx, 52(%rsp)
	movl	56(%rdx), %ecx
	leal	-10(%rcx,%rax), %esi
	movl	%esi, 124(%rsp)
	leal	52(%rcx), %ebx
	movl	%ebx, 56(%rsp)
	movl	60(%rdx), %ecx
	leal	-11(%rcx,%rax), %ebx
	movl	%ebx, 100(%rsp)
	addl	$51, %ecx
	movl	%ecx, 60(%rsp)
	movl	64(%rdx), %ecx
	leal	-12(%rcx,%rax), %ebx
	movl	%ebx, 84(%rsp)
	addl	$50, %ecx
	movl	%ecx, 64(%rsp)
	movl	68(%rdx), %ecx
	leal	-13(%rcx,%rax), %ebx
	movl	%ebx, 88(%rsp)
	addl	$49, %ecx
	movl	%ecx, 68(%rsp)
	movl	72(%rdx), %edx
	leal	-14(%rdx,%rax), %ecx
	movl	%ecx, 92(%rsp)
	addl	$48, %edx
	movl	%edx, 80(%rsp)
	leaq	-1(%rax), %rdx
	movq	%rdx, 72(%rsp)
	testq	%rax, %rax
	je	.L411
	movl	96(%rsp), %ebx
	movl	100(%rsp), %ecx
.L412:
	movl	%r15d, %eax
	movl	12(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	16(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	20(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	24(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	28(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	32(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	36(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	40(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	44(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	48(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	52(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	56(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	60(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	84(%rsp), %eax
	cltd
	idivl	64(%rsp)
	orl	64(%rsp), %edx
	movl	%edx, 84(%rsp)
	movl	88(%rsp), %eax
	cltd
	idivl	68(%rsp)
	orl	68(%rsp), %edx
	movl	%edx, 88(%rsp)
	movl	92(%rsp), %eax
	cltd
	idivl	80(%rsp)
	orl	80(%rsp), %edx
	movl	%edx, 92(%rsp)
	movl	%r15d, %eax
	movl	12(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	16(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	20(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	24(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	28(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	32(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	36(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	40(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	44(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	48(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	52(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	56(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	60(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	84(%rsp), %eax
	cltd
	idivl	64(%rsp)
	orl	64(%rsp), %edx
	movl	%edx, 84(%rsp)
	movl	88(%rsp), %eax
	cltd
	idivl	68(%rsp)
	orl	68(%rsp), %edx
	movl	%edx, 88(%rsp)
	movl	92(%rsp), %eax
	cltd
	idivl	80(%rsp)
	orl	80(%rsp), %edx
	movl	%edx, 92(%rsp)
	movl	%r15d, %eax
	movl	12(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	16(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	20(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	24(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	28(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	32(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	36(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	40(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	44(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	48(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	52(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	56(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	60(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	84(%rsp), %eax
	cltd
	idivl	64(%rsp)
	orl	64(%rsp), %edx
	movl	%edx, 84(%rsp)
	movl	88(%rsp), %eax
	cltd
	idivl	68(%rsp)
	orl	68(%rsp), %edx
	movl	%edx, 88(%rsp)
	movl	92(%rsp), %eax
	cltd
	idivl	80(%rsp)
	orl	80(%rsp), %edx
	movl	%edx, 92(%rsp)
	movl	%r15d, %eax
	movl	12(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	16(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	20(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	24(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	28(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	32(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	36(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	40(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	44(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	48(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	52(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	56(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	60(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	84(%rsp), %eax
	cltd
	idivl	64(%rsp)
	orl	64(%rsp), %edx
	movl	%edx, 84(%rsp)
	movl	88(%rsp), %eax
	cltd
	idivl	68(%rsp)
	orl	68(%rsp), %edx
	movl	%edx, 88(%rsp)
	movl	92(%rsp), %eax
	cltd
	idivl	80(%rsp)
	orl	80(%rsp), %edx
	movl	%edx, 92(%rsp)
	movl	%r15d, %eax
	movl	12(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	16(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	20(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	24(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	28(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	32(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	36(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	40(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	44(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	48(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	52(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	56(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	60(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	84(%rsp), %eax
	cltd
	idivl	64(%rsp)
	orl	64(%rsp), %edx
	movl	%edx, 84(%rsp)
	movl	88(%rsp), %eax
	cltd
	idivl	68(%rsp)
	orl	68(%rsp), %edx
	movl	%edx, 88(%rsp)
	movl	92(%rsp), %eax
	cltd
	idivl	80(%rsp)
	orl	80(%rsp), %edx
	movl	%edx, 92(%rsp)
	movl	%r15d, %eax
	movl	12(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	16(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	20(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	24(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	28(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	32(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	36(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	40(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	44(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	48(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	52(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	56(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	60(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	84(%rsp), %eax
	cltd
	idivl	64(%rsp)
	orl	64(%rsp), %edx
	movl	%edx, 84(%rsp)
	movl	88(%rsp), %eax
	cltd
	idivl	68(%rsp)
	orl	68(%rsp), %edx
	movl	%edx, 88(%rsp)
	movl	92(%rsp), %eax
	cltd
	idivl	80(%rsp)
	orl	80(%rsp), %edx
	movl	%edx, 92(%rsp)
	movl	%r15d, %eax
	movl	12(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	16(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	20(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	24(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	28(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	32(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	36(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	40(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	44(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	48(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	52(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	56(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	60(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	84(%rsp), %eax
	cltd
	idivl	64(%rsp)
	orl	64(%rsp), %edx
	movl	%edx, 84(%rsp)
	movl	88(%rsp), %eax
	cltd
	idivl	68(%rsp)
	orl	68(%rsp), %edx
	movl	%edx, 88(%rsp)
	movl	92(%rsp), %eax
	cltd
	idivl	80(%rsp)
	orl	80(%rsp), %edx
	movl	%edx, 92(%rsp)
	movl	%r15d, %eax
	movl	12(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	16(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	20(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	24(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	28(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	32(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	36(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	40(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	44(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	48(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	52(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	56(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	60(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	84(%rsp), %eax
	cltd
	idivl	64(%rsp)
	orl	64(%rsp), %edx
	movl	%edx, 84(%rsp)
	movl	88(%rsp), %eax
	cltd
	idivl	68(%rsp)
	orl	68(%rsp), %edx
	movl	%edx, 88(%rsp)
	movl	92(%rsp), %eax
	cltd
	idivl	80(%rsp)
	orl	80(%rsp), %edx
	movl	%edx, 92(%rsp)
	movl	%r15d, %eax
	movl	12(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	16(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	20(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	24(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	28(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	32(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	36(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	40(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	44(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	48(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	52(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	56(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	60(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	84(%rsp), %eax
	cltd
	idivl	64(%rsp)
	orl	64(%rsp), %edx
	movl	%edx, 84(%rsp)
	movl	88(%rsp), %eax
	cltd
	idivl	68(%rsp)
	orl	68(%rsp), %edx
	movl	%edx, 88(%rsp)
	movl	92(%rsp), %eax
	cltd
	idivl	80(%rsp)
	orl	80(%rsp), %edx
	movl	%edx, 92(%rsp)
	movl	%r15d, %eax
	movl	12(%rsp), %r15d
	cltd
	idivl	%r15d
	orl	%r15d, %edx
	movl	%edx, %r15d
	movl	%r14d, %eax
	movl	16(%rsp), %r14d
	cltd
	idivl	%r14d
	orl	%r14d, %edx
	movl	%edx, %r14d
	movl	%r13d, %eax
	movl	20(%rsp), %r13d
	cltd
	idivl	%r13d
	orl	%r13d, %edx
	movl	%edx, %r13d
	movl	%r12d, %eax
	movl	24(%rsp), %r12d
	cltd
	idivl	%r12d
	orl	%r12d, %edx
	movl	%edx, %r12d
	movl	%ebp, %eax
	movl	28(%rsp), %ebp
	cltd
	idivl	%ebp
	orl	%ebp, %edx
	movl	%edx, %ebp
	movl	%ebx, %eax
	movl	32(%rsp), %ebx
	cltd
	idivl	%ebx
	orl	%ebx, %edx
	movl	%edx, %ebx
	movl	%r11d, %eax
	movl	36(%rsp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, %r11d
	movl	%r10d, %eax
	movl	40(%rsp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, %r10d
	movl	%r9d, %eax
	movl	44(%rsp), %r9d
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, %r9d
	movl	%r8d, %eax
	movl	48(%rsp), %r8d
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, %r8d
	movl	%edi, %eax
	movl	52(%rsp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, %edi
	movl	%esi, %eax
	movl	56(%rsp), %esi
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, %esi
	movl	%ecx, %eax
	movl	60(%rsp), %ecx
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, %ecx
	movl	84(%rsp), %eax
	cltd
	idivl	64(%rsp)
	orl	64(%rsp), %edx
	movl	%edx, 84(%rsp)
	movl	88(%rsp), %eax
	cltd
	idivl	68(%rsp)
	orl	68(%rsp), %edx
	movl	%edx, 88(%rsp)
	movl	92(%rsp), %eax
	cltd
	idivl	80(%rsp)
	orl	80(%rsp), %edx
	movl	%edx, 92(%rsp)
	subq	$1, 72(%rsp)
	movq	72(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L412
	movl	%ebx, 96(%rsp)
	movl	%r11d, 104(%rsp)
	movl	%r10d, 108(%rsp)
	movl	%r9d, 112(%rsp)
	movl	%r8d, 116(%rsp)
	movl	%edi, 120(%rsp)
	movl	%esi, 124(%rsp)
	movl	%ecx, 100(%rsp)
.L411:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	96(%rsp), %edi
	call	use_int@PLT
	movl	104(%rsp), %edi
	call	use_int@PLT
	movl	108(%rsp), %edi
	call	use_int@PLT
	movl	112(%rsp), %edi
	call	use_int@PLT
	movl	116(%rsp), %edi
	call	use_int@PLT
	movl	120(%rsp), %edi
	call	use_int@PLT
	movl	124(%rsp), %edi
	call	use_int@PLT
	movl	100(%rsp), %edi
	call	use_int@PLT
	movl	84(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	call	use_int@PLT
	movl	92(%rsp), %edi
	call	use_int@PLT
	addq	$136, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE152:
	.size	integer_mod_15, .-integer_mod_15
	.globl	int64_bit_0
	.type	int64_bit_0, @function
int64_bit_0:
.LFB153:
	.cfi_startproc
	endbr64
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movq	%rdi, %rdx
	movl	12(%rsi), %eax
	leal	1(%rax), %edi
	movslq	%edi, %rcx
	salq	$32, %rdi
	orq	%rcx, %rdi
	cltq
	leaq	1(%rdx,%rax), %rax
	movq	%rax, %rsi
	salq	$32, %rsi
	orq	%rax, %rsi
	testq	%rdx, %rdx
	je	.L416
	leaq	0(,%rsi,4), %r8
	leaq	-1(%r8), %rax
	subq	%rdx, %r8
	subq	$1, %r8
.L417:
	subq	$1, %rax
	xorq	%rax, %rdi
	movq	%rdi, %rcx
	xorq	%rsi, %rcx
	orq	%rsi, %rdi
	xorq	%rax, %rdi
	movq	%rcx, %rdx
	xorq	%rdi, %rdx
	orq	%rdi, %rcx
	xorq	%rax, %rcx
	movq	%rdx, %rsi
	xorq	%rcx, %rsi
	orq	%rcx, %rdx
	xorq	%rax, %rdx
	movq	%rsi, %rcx
	xorq	%rdx, %rcx
	orq	%rdx, %rsi
	xorq	%rax, %rsi
	movq	%rcx, %rdx
	xorq	%rsi, %rdx
	orq	%rsi, %rcx
	xorq	%rax, %rcx
	movq	%rdx, %rsi
	xorq	%rcx, %rsi
	orq	%rcx, %rdx
	xorq	%rax, %rdx
	movq	%rsi, %rcx
	xorq	%rdx, %rcx
	orq	%rdx, %rsi
	xorq	%rax, %rsi
	movq	%rcx, %rdx
	xorq	%rsi, %rdx
	orq	%rsi, %rcx
	xorq	%rax, %rcx
	movq	%rdx, %rdi
	xorq	%rcx, %rdi
	orq	%rcx, %rdx
	xorq	%rax, %rdx
	movq	%rdi, %rsi
	xorq	%rdx, %rsi
	orq	%rdx, %rdi
	cmpq	%r8, %rax
	jne	.L417
.L416:
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE153:
	.size	int64_bit_0, .-int64_bit_0
	.globl	int64_bit_1
	.type	int64_bit_1, @function
int64_bit_1:
.LFB154:
	.cfi_startproc
	endbr64
	pushq	%r12
	.cfi_def_cfa_offset 16
	.cfi_offset 12, -16
	pushq	%rbp
	.cfi_def_cfa_offset 24
	.cfi_offset 6, -24
	pushq	%rbx
	.cfi_def_cfa_offset 32
	.cfi_offset 3, -32
	movl	12(%rsi), %eax
	leal	1(%rax), %edx
	movslq	%edx, %rcx
	salq	$32, %rdx
	orq	%rcx, %rdx
	cltq
	leaq	1(%rdi,%rax), %rax
	movq	%rax, %r8
	salq	$32, %r8
	orq	%rax, %r8
	movslq	16(%rsi), %rax
	movq	%rax, %rbp
	salq	$32, %rbp
	orq	%rax, %rbp
	addq	%rdi, %rax
	movq	%rax, %r9
	salq	$32, %r9
	orq	%rax, %r9
	testq	%rdi, %rdi
	je	.L421
	leaq	0(,%r8,4), %r10
	leaq	-1(%r10), %rcx
	leaq	0(,%r9,4), %rbx
	movq	%r10, %r11
	subq	%rdi, %r11
	subq	$1, %r11
.L422:
	subq	$1, %rcx
	leaq	(%rcx,%rbx), %rsi
	subq	%r10, %rsi
	xorq	%rcx, %rdx
	movq	%rdx, %rax
	xorq	%r8, %rax
	orq	%rdx, %r8
	movq	%rbp, %rdx
	xorq	%rsi, %rdx
	movq	%rdx, %rdi
	xorq	%r9, %rdi
	orq	%r9, %rdx
	xorq	%rcx, %r8
	movq	%rax, %r9
	xorq	%r8, %r9
	orq	%r8, %rax
	xorq	%rsi, %rdx
	movq	%rdi, %r8
	xorq	%rdx, %r8
	orq	%rdx, %rdi
	xorq	%rcx, %rax
	movq	%r9, %rdx
	xorq	%rax, %rdx
	orq	%rax, %r9
	xorq	%rsi, %rdi
	movq	%r8, %rax
	xorq	%rdi, %rax
	orq	%rdi, %r8
	xorq	%rcx, %r9
	movq	%rdx, %rdi
	xorq	%r9, %rdi
	orq	%r9, %rdx
	xorq	%rsi, %r8
	movq	%rax, %r9
	xorq	%r8, %r9
	orq	%r8, %rax
	xorq	%rcx, %rdx
	movq	%rdi, %r8
	xorq	%rdx, %r8
	orq	%rdx, %rdi
	xorq	%rsi, %rax
	movq	%r9, %rdx
	xorq	%rax, %rdx
	orq	%rax, %r9
	xorq	%rcx, %rdi
	movq	%r8, %rax
	xorq	%rdi, %rax
	orq	%rdi, %r8
	xorq	%rsi, %r9
	movq	%rdx, %rdi
	xorq	%r9, %rdi
	orq	%r9, %rdx
	xorq	%rcx, %r8
	movq	%rax, %r9
	xorq	%r8, %r9
	orq	%r8, %rax
	xorq	%rsi, %rdx
	movq	%rdi, %r8
	xorq	%rdx, %r8
	orq	%rdx, %rdi
	xorq	%rcx, %rax
	movq	%r9, %rdx
	xorq	%rax, %rdx
	orq	%rax, %r9
	xorq	%rsi, %rdi
	movq	%r8, %rax
	xorq	%rdi, %rax
	orq	%rdi, %r8
	xorq	%rcx, %r9
	movq	%rdx, %r12
	xorq	%r9, %r12
	orq	%r9, %rdx
	xorq	%rsi, %r8
	movq	%rax, %rbp
	xorq	%r8, %rbp
	orq	%r8, %rax
	xorq	%rcx, %rdx
	movq	%r12, %r8
	xorq	%rdx, %r8
	orq	%r12, %rdx
	xorq	%rsi, %rax
	movq	%rbp, %r9
	xorq	%rax, %r9
	orq	%rax, %rbp
	cmpq	%r11, %rcx
	jne	.L422
.L421:
	movl	%edx, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%rbp
	.cfi_def_cfa_offset 16
	popq	%r12
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE154:
	.size	int64_bit_1, .-int64_bit_1
	.globl	int64_bit_2
	.type	int64_bit_2, @function
int64_bit_2:
.LFB155:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$24, %rsp
	.cfi_def_cfa_offset 80
	movq	%rdi, %rdx
	movq	%rsi, %r9
	movl	12(%rsi), %ecx
	leal	1(%rcx), %eax
	movslq	%eax, %rsi
	salq	$32, %rax
	orq	%rsi, %rax
	movslq	%ecx, %rcx
	leaq	1(%rdi,%rcx), %rcx
	movq	%rcx, %r11
	salq	$32, %r11
	orq	%rcx, %r11
	movslq	16(%r9), %rsi
	movq	%rsi, %rbp
	salq	$32, %rbp
	orq	%rsi, %rbp
	addq	%rdi, %rsi
	movq	%rsi, %r12
	salq	$32, %r12
	orq	%rsi, %r12
	leaq	0(,%r12,4), %rsi
	movq	%rsi, (%rsp)
	movl	20(%r9), %esi
	leal	-1(%rsi), %ebx
	movslq	%ebx, %rdi
	salq	$32, %rbx
	orq	%rdi, %rbx
	movslq	%esi, %rsi
	leaq	-1(%rdx,%rsi), %rsi
	movq	%rsi, %r13
	salq	$32, %r13
	orq	%rsi, %r13
	leaq	0(,%r13,4), %rsi
	movq	%rsi, 8(%rsp)
	testq	%rdx, %rdx
	je	.L426
	leaq	0(,%r11,4), %r8
	leaq	-1(%r8), %rcx
	movq	%r8, %r9
	subq	%rdx, %r9
	subq	$1, %r9
.L427:
	subq	$1, %rcx
	movq	(%rsp), %rsi
	leaq	(%rsi,%rcx), %rdx
	subq	%r8, %rdx
	movq	8(%rsp), %rsi
	addq	%rcx, %rsi
	subq	%r8, %rsi
	xorq	%rcx, %rax
	movq	%rax, %r15
	xorq	%r11, %r15
	orq	%r11, %rax
	movq	%rbp, %r10
	xorq	%rdx, %r10
	movq	%r10, %r14
	xorq	%r12, %r14
	orq	%r12, %r10
	xorq	%rsi, %rbx
	movq	%rbx, %r12
	xorq	%r13, %r12
	orq	%r13, %rbx
	xorq	%rcx, %rax
	movq	%r15, %r11
	xorq	%rax, %r11
	orq	%r15, %rax
	xorq	%rdx, %r10
	movq	%r14, %r13
	xorq	%r10, %r13
	orq	%r14, %r10
	xorq	%rsi, %rbx
	movq	%r12, %r14
	xorq	%rbx, %r14
	orq	%r12, %rbx
	xorq	%rcx, %rax
	movq	%r11, %r12
	xorq	%rax, %r12
	orq	%r11, %rax
	xorq	%rdx, %r10
	movq	%r13, %r11
	xorq	%r10, %r11
	orq	%r13, %r10
	xorq	%rsi, %rbx
	movq	%r14, %r13
	xorq	%rbx, %r13
	orq	%r14, %rbx
	xorq	%rcx, %rax
	movq	%r12, %rdi
	xorq	%rax, %rdi
	orq	%r12, %rax
	xorq	%rdx, %r10
	movq	%r11, %r12
	xorq	%r10, %r12
	orq	%r11, %r10
	xorq	%rsi, %rbx
	movq	%r13, %r11
	xorq	%rbx, %r11
	orq	%r13, %rbx
	xorq	%rcx, %rax
	movq	%rdi, %rbp
	xorq	%rax, %rbp
	orq	%rdi, %rax
	xorq	%rdx, %r10
	movq	%r12, %rdi
	xorq	%r10, %rdi
	orq	%r12, %r10
	xorq	%rsi, %rbx
	movq	%r11, %r12
	xorq	%rbx, %r12
	orq	%r11, %rbx
	xorq	%rcx, %rax
	movq	%rbp, %r11
	xorq	%rax, %r11
	orq	%rbp, %rax
	xorq	%rdx, %r10
	movq	%rdi, %rbp
	xorq	%r10, %rbp
	orq	%rdi, %r10
	xorq	%rsi, %rbx
	movq	%r12, %rdi
	xorq	%rbx, %rdi
	orq	%r12, %rbx
	xorq	%rcx, %rax
	movq	%r11, %r13
	xorq	%rax, %r13
	orq	%r11, %rax
	xorq	%rdx, %r10
	movq	%rbp, %r11
	xorq	%r10, %r11
	orq	%rbp, %r10
	xorq	%rsi, %rbx
	movq	%rdi, %r12
	xorq	%rbx, %r12
	orq	%rdi, %rbx
	xorq	%rcx, %rax
	movq	%r13, %rbp
	xorq	%rax, %rbp
	orq	%r13, %rax
	xorq	%rdx, %r10
	movq	%r11, %r13
	xorq	%r10, %r13
	orq	%r11, %r10
	xorq	%rsi, %rbx
	movq	%r12, %r11
	xorq	%rbx, %r11
	orq	%r12, %rbx
	xorq	%rcx, %rax
	movq	%rbp, %r12
	xorq	%rax, %r12
	orq	%rbp, %rax
	xorq	%rdx, %r10
	movq	%r13, %rbp
	xorq	%r10, %rbp
	orq	%r13, %r10
	xorq	%rsi, %rbx
	movq	%r11, %r14
	xorq	%rbx, %r14
	orq	%r11, %rbx
	xorq	%rcx, %rax
	movq	%r12, %r11
	xorq	%rax, %r11
	orq	%r12, %rax
	xorq	%r10, %rdx
	movq	%rbp, %r12
	xorq	%rdx, %r12
	orq	%rdx, %rbp
	xorq	%rsi, %rbx
	movq	%r14, %r13
	xorq	%rbx, %r13
	orq	%r14, %rbx
	cmpq	%r9, %rcx
	jne	.L427
.L426:
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE155:
	.size	int64_bit_2, .-int64_bit_2
	.globl	int64_bit_3
	.type	int64_bit_3, @function
int64_bit_3:
.LFB156:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$56, %rsp
	.cfi_def_cfa_offset 112
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx), %edi
	movslq	%edi, %rsi
	salq	$32, %rdi
	orq	%rsi, %rdi
	movslq	%ecx, %rcx
	leaq	1(%rax,%rcx), %rcx
	movq	%rcx, %r10
	salq	$32, %r10
	orq	%rcx, %r10
	leaq	0(,%r10,4), %r11
	movq	%r11, 8(%rsp)
	movslq	16(%rdx), %rcx
	movq	%rcx, %r12
	salq	$32, %r12
	orq	%rcx, %r12
	addq	%rax, %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 16(%rsp)
	movl	20(%rdx), %ecx
	leal	-1(%rcx), %ebp
	movslq	%ebp, %r8
	salq	$32, %rbp
	orq	%r8, %rbp
	movslq	%ecx, %rcx
	leaq	-1(%rax,%rcx), %rcx
	movq	%rcx, %r8
	salq	$32, %r8
	orq	%rcx, %r8
	leaq	0(,%r8,4), %rbx
	movq	%rbx, 24(%rsp)
	movl	24(%rdx), %edx
	leal	-2(%rdx), %ebx
	movslq	%ebx, %rcx
	salq	$32, %rbx
	orq	%rcx, %rbx
	movslq	%edx, %rdx
	leaq	-2(%rax,%rdx), %rdx
	movq	%rdx, %rcx
	salq	$32, %rcx
	orq	%rdx, %rcx
	leaq	0(,%rcx,4), %rdx
	movq	%rdx, 32(%rsp)
	testq	%rax, %rax
	je	.L431
	leaq	-1(%r11), %r15
	movq	%rcx, %r14
	movq	%r11, %rdx
	subq	%rax, %rdx
	leaq	-1(%rdx), %rax
	movq	%rax, 40(%rsp)
	movq	%r10, %r9
.L432:
	subq	$1, %r15
	movq	16(%rsp), %rax
	addq	%r15, %rax
	movq	8(%rsp), %rcx
	subq	%rcx, %rax
	movq	%rax, %r11
	movq	24(%rsp), %rax
	addq	%r15, %rax
	subq	%rcx, %rax
	movq	%rax, %r13
	movq	32(%rsp), %rax
	addq	%r15, %rax
	subq	%rcx, %rax
	movq	%rax, %r10
	xorq	%r15, %rdi
	movq	%rdi, %rdx
	xorq	%r9, %rdx
	orq	%r9, %rdi
	movq	%r12, %rax
	xorq	%r11, %rax
	movq	%rax, %r9
	xorq	%rsi, %r9
	orq	%rsi, %rax
	xorq	%r13, %rbp
	movq	%rbp, %rsi
	xorq	%r8, %rsi
	orq	%r8, %rbp
	xorq	%r10, %rbx
	movq	%rbx, %r8
	xorq	%r14, %r8
	orq	%r14, %rbx
	xorq	%r15, %rdi
	movq	%rdx, %rcx
	xorq	%rdi, %rcx
	orq	%rdx, %rdi
	xorq	%r11, %rax
	movq	%r9, %rdx
	xorq	%rax, %rdx
	orq	%r9, %rax
	xorq	%r13, %rbp
	movq	%rsi, %r9
	xorq	%rbp, %r9
	orq	%rsi, %rbp
	xorq	%r10, %rbx
	movq	%r8, %rsi
	xorq	%rbx, %rsi
	orq	%r8, %rbx
	xorq	%r15, %rdi
	movq	%rcx, %r8
	xorq	%rdi, %r8
	orq	%rcx, %rdi
	xorq	%r11, %rax
	movq	%rdx, %rcx
	xorq	%rax, %rcx
	orq	%rdx, %rax
	xorq	%r13, %rbp
	movq	%r9, %rdx
	xorq	%rbp, %rdx
	orq	%r9, %rbp
	xorq	%r10, %rbx
	movq	%rsi, %r9
	xorq	%rbx, %r9
	orq	%rsi, %rbx
	xorq	%r15, %rdi
	movq	%r8, %rsi
	xorq	%rdi, %rsi
	orq	%r8, %rdi
	xorq	%r11, %rax
	movq	%rcx, %r8
	xorq	%rax, %r8
	orq	%rcx, %rax
	xorq	%r13, %rbp
	movq	%rdx, %rcx
	xorq	%rbp, %rcx
	orq	%rdx, %rbp
	xorq	%r10, %rbx
	movq	%r9, %rdx
	xorq	%rbx, %rdx
	orq	%r9, %rbx
	xorq	%r15, %rdi
	movq	%rsi, %r9
	xorq	%rdi, %r9
	orq	%rsi, %rdi
	xorq	%r11, %rax
	movq	%r8, %rsi
	xorq	%rax, %rsi
	orq	%r8, %rax
	xorq	%r13, %rbp
	movq	%rcx, %r8
	xorq	%rbp, %r8
	orq	%rcx, %rbp
	xorq	%r10, %rbx
	movq	%rdx, %rcx
	xorq	%rbx, %rcx
	orq	%rdx, %rbx
	xorq	%r15, %rdi
	movq	%r9, %rdx
	xorq	%rdi, %rdx
	orq	%r9, %rdi
	xorq	%r11, %rax
	movq	%rsi, %r9
	xorq	%rax, %r9
	orq	%rsi, %rax
	xorq	%r13, %rbp
	movq	%r8, %rsi
	xorq	%rbp, %rsi
	orq	%r8, %rbp
	xorq	%r10, %rbx
	movq	%rcx, %r8
	xorq	%rbx, %r8
	orq	%rcx, %rbx
	xorq	%r15, %rdi
	movq	%rdx, %rcx
	xorq	%rdi, %rcx
	orq	%rdx, %rdi
	xorq	%r11, %rax
	movq	%r9, %rdx
	xorq	%rax, %rdx
	orq	%r9, %rax
	xorq	%r13, %rbp
	movq	%rsi, %r9
	xorq	%rbp, %r9
	orq	%rsi, %rbp
	xorq	%r10, %rbx
	movq	%r8, %rsi
	xorq	%rbx, %rsi
	orq	%r8, %rbx
	xorq	%r15, %rdi
	movq	%rcx, %r8
	xorq	%rdi, %r8
	orq	%rcx, %rdi
	xorq	%r11, %rax
	movq	%rdx, %rcx
	xorq	%rax, %rcx
	orq	%rdx, %rax
	xorq	%r13, %rbp
	movq	%r9, %rdx
	xorq	%rbp, %rdx
	orq	%r9, %rbp
	xorq	%r10, %rbx
	movq	%rsi, %r9
	xorq	%rbx, %r9
	orq	%rsi, %rbx
	xorq	%r15, %rdi
	movq	%r8, %rsi
	xorq	%rdi, %rsi
	orq	%r8, %rdi
	xorq	%r11, %rax
	movq	%rcx, %r8
	xorq	%rax, %r8
	orq	%rcx, %rax
	xorq	%r13, %rbp
	movq	%rdx, %rcx
	xorq	%rbp, %rcx
	orq	%rdx, %rbp
	xorq	%r10, %rbx
	movq	%r9, %rdx
	xorq	%rbx, %rdx
	orq	%r9, %rbx
	xorq	%r15, %rdi
	movq	%rsi, %r9
	xorq	%rdi, %r9
	orq	%rsi, %rdi
	xorq	%rax, %r11
	movq	%r8, %rsi
	xorq	%r11, %rsi
	orq	%r11, %r8
	movq	%r8, %r12
	xorq	%r13, %rbp
	movq	%rcx, %r8
	xorq	%rbp, %r8
	orq	%rcx, %rbp
	xorq	%r10, %rbx
	movq	%rdx, %r14
	xorq	%rbx, %r14
	orq	%rdx, %rbx
	cmpq	40(%rsp), %r15
	jne	.L432
.L431:
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE156:
	.size	int64_bit_3, .-int64_bit_3
	.globl	int64_bit_4
	.type	int64_bit_4, @function
int64_bit_4:
.LFB157:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$88, %rsp
	.cfi_def_cfa_offset 144
	movq	%rdi, %rdx
	movq	%rsi, %rcx
	movl	12(%rsi), %esi
	leal	1(%rsi), %eax
	movslq	%eax, %rdi
	salq	$32, %rax
	orq	%rdi, %rax
	movslq	%esi, %rsi
	leaq	1(%rdx,%rsi), %rsi
	movq	%rsi, %rdi
	salq	$32, %rdi
	orq	%rsi, %rdi
	movq	%rdi, %r12
	leaq	0(,%rdi,4), %r14
	movq	%r14, 32(%rsp)
	movslq	16(%rcx), %rsi
	movq	%rsi, %rbp
	salq	$32, %rbp
	orq	%rsi, %rbp
	addq	%rdx, %rsi
	movq	%rsi, %rdi
	salq	$32, %rdi
	orq	%rsi, %rdi
	movq	%rdi, (%rsp)
	leaq	0(,%rdi,4), %rbx
	movq	%rbx, 40(%rsp)
	movl	20(%rcx), %esi
	leal	-1(%rsi), %edi
	movslq	%edi, %r9
	salq	$32, %rdi
	movq	%rdi, %r15
	orq	%r9, %r15
	movslq	%esi, %rsi
	leaq	-1(%rdx,%rsi), %rsi
	movq	%rsi, %rbx
	salq	$32, %rbx
	movq	%rbx, %r11
	orq	%rsi, %r11
	leaq	0(,%r11,4), %rbx
	movq	%rbx, 48(%rsp)
	movl	24(%rcx), %esi
	leal	-2(%rsi), %edi
	movslq	%edi, %r9
	salq	$32, %rdi
	movq	%rdi, %r13
	orq	%r9, %r13
	movslq	%esi, %rsi
	leaq	-2(%rdx,%rsi), %rsi
	movq	%rsi, %rbx
	salq	$32, %rbx
	movq	%rbx, %r9
	orq	%rsi, %r9
	leaq	0(,%r9,4), %rbx
	movq	%rbx, 56(%rsp)
	movl	28(%rcx), %ecx
	leal	-3(%rcx), %ebx
	movslq	%ebx, %rsi
	salq	$32, %rbx
	orq	%rsi, %rbx
	movslq	%ecx, %rcx
	leaq	-3(%rdx,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	leaq	0(,%rsi,4), %rcx
	movq	%rcx, 64(%rsp)
	testq	%rdx, %rdx
	je	.L436
	leaq	-1(%r14), %r8
	movq	%r14, %rcx
	subq	%rdx, %rcx
	leaq	-1(%rcx), %rdx
	movq	%rdx, 72(%rsp)
	movq	%r8, %r14
	movq	%rbp, %rdx
	movq	%r11, %rbp
	movq	%r9, %r10
	movq	(%rsp), %r9
	movq	%rsi, %r11
.L437:
	subq	$1, %r14
	movq	40(%rsp), %rcx
	addq	%r14, %rcx
	movq	%rcx, %rdi
	movq	32(%rsp), %rsi
	subq	%rsi, %rdi
	movq	48(%rsp), %rcx
	addq	%r14, %rcx
	subq	%rsi, %rcx
	movq	%rcx, %r8
	movq	56(%rsp), %rcx
	addq	%r14, %rcx
	subq	%rsi, %rcx
	movq	%rcx, 8(%rsp)
	movq	64(%rsp), %rsi
	leaq	(%rsi,%r14), %rcx
	movq	%rcx, %rsi
	subq	32(%rsp), %rsi
	movq	%rsi, (%rsp)
	xorq	%r14, %rax
	movq	%rax, %rsi
	xorq	%r12, %rsi
	orq	%r12, %rax
	movq	%rdi, %r12
	xorq	%rdi, %rdx
	movq	%rdx, %rdi
	xorq	%r9, %rdi
	movq	%rdi, %rcx
	orq	%r9, %rdx
	movq	%r8, 16(%rsp)
	xorq	%r8, %r15
	movq	%r15, %r9
	xorq	%rbp, %r9
	orq	%rbp, %r15
	xorq	8(%rsp), %r13
	movq	%r13, %rdi
	xorq	%r10, %rdi
	movq	%rdi, %r8
	orq	%r10, %r13
	xorq	(%rsp), %rbx
	movq	%rbx, %rdi
	xorq	%r11, %rdi
	orq	%r11, %rbx
	xorq	%r14, %rax
	movq	%rax, %r10
	movq	%rsi, %rax
	xorq	%r10, %rax
	orq	%r10, %rsi
	movq	%r12, 24(%rsp)
	xorq	%r12, %rdx
	movq	%rcx, %rbp
	xorq	%rdx, %rbp
	orq	%rdx, %rcx
	xorq	16(%rsp), %r15
	movq	%r9, %r11
	xorq	%r15, %r11
	orq	%r15, %r9
	xorq	8(%rsp), %r13
	movq	%r8, %r10
	xorq	%r13, %r10
	orq	%r13, %r8
	xorq	(%rsp), %rbx
	movq	%rdi, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, %r12
	orq	%rbx, %rdi
	movq	%rsi, %rdx
	xorq	%r14, %rdx
	movq	%rax, %rsi
	xorq	%rdx, %rsi
	orq	%rdx, %rax
	movq	%rcx, %rdx
	xorq	24(%rsp), %rdx
	movq	%rbp, %rcx
	xorq	%rdx, %rcx
	orq	%rbp, %rdx
	xorq	16(%rsp), %r9
	movq	%r11, %rbp
	xorq	%r9, %rbp
	orq	%r9, %r11
	movq	%r11, %r15
	xorq	8(%rsp), %r8
	movq	%r10, %r11
	xorq	%r8, %r11
	orq	%r8, %r10
	movq	%r10, %r13
	xorq	(%rsp), %rdi
	movq	%r12, %r10
	xorq	%rdi, %r10
	movq	%r12, %rbx
	orq	%rdi, %rbx
	xorq	%r14, %rax
	movq	%rax, %rdi
	movq	%rsi, %rax
	xorq	%rdi, %rax
	orq	%rdi, %rsi
	movq	24(%rsp), %r12
	xorq	%r12, %rdx
	movq	%rdx, %rdi
	movq	%rcx, %rdx
	xorq	%rdi, %rdx
	orq	%rdi, %rcx
	xorq	16(%rsp), %r15
	movq	%rbp, %r9
	xorq	%r15, %r9
	orq	%r15, %rbp
	xorq	8(%rsp), %r13
	movq	%r11, %r8
	xorq	%r13, %r8
	orq	%r13, %r11
	movq	(%rsp), %r13
	xorq	%r13, %rbx
	movq	%r10, %rdi
	xorq	%rbx, %rdi
	orq	%rbx, %r10
	xorq	%r14, %rsi
	movq	%rsi, %rbx
	movq	%rax, %rsi
	xorq	%rbx, %rsi
	orq	%rbx, %rax
	movq	%rcx, %rbx
	movq	%r12, %r15
	xorq	%r12, %rbx
	movq	%rdx, %rcx
	xorq	%rbx, %rcx
	orq	%rbx, %rdx
	movq	16(%rsp), %r12
	xorq	%r12, %rbp
	movq	%r9, %rbx
	xorq	%rbp, %rbx
	orq	%rbp, %r9
	xorq	8(%rsp), %r11
	movq	%r11, %rbp
	movq	%r8, %r11
	xorq	%rbp, %r11
	orq	%rbp, %r8
	xorq	%r13, %r10
	movq	%r10, %rbp
	movq	%rdi, %r10
	xorq	%rbp, %r10
	orq	%rbp, %rdi
	xorq	%r14, %rax
	movq	%rax, %rbp
	movq	%rsi, %rax
	xorq	%rbp, %rax
	orq	%rbp, %rsi
	xorq	%r15, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rdx
	xorq	%rbp, %rdx
	orq	%rbp, %rcx
	movq	%r12, %r13
	xorq	%r12, %r9
	movq	%r9, %rbp
	movq	%rbx, %r9
	xorq	%rbp, %r9
	orq	%rbp, %rbx
	xorq	8(%rsp), %r8
	movq	%r8, %rbp
	movq	%r11, %r8
	xorq	%rbp, %r8
	orq	%rbp, %r11
	xorq	(%rsp), %rdi
	movq	%rdi, %rbp
	movq	%r10, %rdi
	xorq	%rbp, %rdi
	orq	%rbp, %r10
	xorq	%r14, %rsi
	movq	%rsi, %rbp
	movq	%rax, %rsi
	xorq	%rbp, %rsi
	orq	%rbp, %rax
	movq	%r15, %r12
	xorq	%r15, %rcx
	movq	%rcx, %rbp
	movq	%rdx, %rcx
	xorq	%rbp, %rcx
	orq	%rbp, %rdx
	movq	%r13, %rbp
	xorq	%r13, %rbx
	movq	%r9, %r15
	xorq	%rbx, %r15
	orq	%rbx, %r9
	xorq	8(%rsp), %r11
	movq	%r8, %r13
	xorq	%r11, %r13
	orq	%r11, %r8
	xorq	(%rsp), %r10
	movq	%rdi, %rbx
	xorq	%r10, %rbx
	orq	%r10, %rdi
	xorq	%r14, %rax
	movq	%rax, %r10
	movq	%rsi, %rax
	xorq	%r10, %rax
	orq	%r10, %rsi
	xorq	%r12, %rdx
	movq	%rcx, %r10
	xorq	%rdx, %r10
	movq	%r10, %r12
	orq	%rdx, %rcx
	xorq	%rbp, %r9
	movq	%r15, %rdx
	xorq	%r9, %rdx
	movq	%rdx, %rbp
	orq	%r9, %r15
	xorq	8(%rsp), %r8
	movq	%r13, %r11
	xorq	%r8, %r11
	orq	%r8, %r13
	xorq	(%rsp), %rdi
	movq	%rbx, %r10
	xorq	%rdi, %r10
	orq	%rdi, %rbx
	xorq	%r14, %rsi
	movq	%rax, %r9
	xorq	%rsi, %r9
	orq	%rsi, %rax
	xorq	24(%rsp), %rcx
	movq	%r12, %rdx
	xorq	%rcx, %rdx
	orq	%r12, %rcx
	xorq	16(%rsp), %r15
	movq	%rbp, %r8
	xorq	%r15, %r8
	orq	%rbp, %r15
	xorq	8(%rsp), %r13
	movq	%r11, %rdi
	xorq	%r13, %rdi
	orq	%r11, %r13
	movq	(%rsp), %r11
	xorq	%r11, %rbx
	movq	%r10, %rsi
	xorq	%rbx, %rsi
	orq	%r10, %rbx
	xorq	%r14, %rax
	movq	%r9, %r12
	xorq	%rax, %r12
	orq	%r9, %rax
	xorq	24(%rsp), %rcx
	movq	%rdx, %r9
	xorq	%rcx, %r9
	orq	%rcx, %rdx
	xorq	16(%rsp), %r15
	movq	%r8, %rbp
	xorq	%r15, %rbp
	orq	%r8, %r15
	xorq	8(%rsp), %r13
	movq	%rdi, %r10
	xorq	%r13, %r10
	orq	%rdi, %r13
	xorq	%r11, %rbx
	movq	%rsi, %r11
	xorq	%rbx, %r11
	orq	%rsi, %rbx
	cmpq	72(%rsp), %r14
	jne	.L437
	movq	%rdx, %rbp
.L436:
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE157:
	.size	int64_bit_4, .-int64_bit_4
	.globl	int64_bit_5
	.type	int64_bit_5, @function
int64_bit_5:
.LFB158:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$120, %rsp
	.cfi_def_cfa_offset 176
	movq	%rdi, %rdx
	movq	%rsi, %rcx
	movl	12(%rsi), %esi
	leal	1(%rsi), %eax
	movslq	%eax, %rdi
	salq	$32, %rax
	orq	%rdi, %rax
	movslq	%esi, %rsi
	leaq	1(%rdx,%rsi), %rsi
	movq	%rsi, %rdi
	salq	$32, %rdi
	orq	%rsi, %rdi
	movq	%rdi, 56(%rsp)
	leaq	0(,%rdi,4), %rbx
	movq	%rbx, 48(%rsp)
	movslq	16(%rcx), %rsi
	movq	%rsi, %rdi
	salq	$32, %rdi
	orq	%rsi, %rdi
	movq	%rdi, %r14
	addq	%rdx, %rsi
	movq	%rsi, %rdi
	salq	$32, %rdi
	orq	%rsi, %rdi
	movq	%rdi, %rbp
	leaq	0(,%rdi,4), %rsi
	movq	%rsi, 64(%rsp)
	movl	20(%rcx), %esi
	leal	-1(%rsi), %edi
	movslq	%edi, %r8
	salq	$32, %rdi
	orq	%r8, %rdi
	movq	%rdi, %r13
	movslq	%esi, %rsi
	leaq	-1(%rdx,%rsi), %rsi
	movq	%rsi, %rdi
	salq	$32, %rdi
	orq	%rsi, %rdi
	movq	%rdi, %r10
	leaq	0(,%rdi,4), %rsi
	movq	%rsi, 72(%rsp)
	movl	24(%rcx), %esi
	leal	-2(%rsi), %edi
	movslq	%edi, %r8
	salq	$32, %rdi
	orq	%r8, %rdi
	movq	%rdi, %r12
	movslq	%esi, %rsi
	leaq	-2(%rdx,%rsi), %rsi
	movq	%rsi, %rdi
	salq	$32, %rdi
	orq	%rsi, %rdi
	movq	%rdi, %r9
	leaq	0(,%rdi,4), %rsi
	movq	%rsi, 80(%rsp)
	movl	28(%rcx), %esi
	leal	-3(%rsi), %edi
	movslq	%edi, %r8
	salq	$32, %rdi
	orq	%r8, %rdi
	movq	%rdi, %r8
	movq	%rdi, 8(%rsp)
	movslq	%esi, %rsi
	leaq	-3(%rdx,%rsi), %rsi
	movq	%rsi, %rdi
	salq	$32, %rdi
	orq	%rsi, %rdi
	movq	%rdi, 16(%rsp)
	leaq	0(,%rdi,4), %rsi
	movq	%rsi, 88(%rsp)
	movl	32(%rcx), %ecx
	leal	-4(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r15
	movslq	%ecx, %rcx
	leaq	-4(%rdx,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	leaq	0(,%rsi,4), %rcx
	movq	%rcx, 96(%rsp)
	testq	%rdx, %rdx
	je	.L441
	leaq	-1(%rbx), %r11
	movq	%rbx, %rcx
	subq	%rdx, %rcx
	leaq	-1(%rcx), %rbx
	movq	%rbx, 104(%rsp)
	movq	%r14, %rcx
	movq	%r11, %r14
	movq	%r10, %rbx
	movq	%r9, %r10
	movq	16(%rsp), %r9
	movq	%r8, %rdx
	movq	%rsi, %r8
	movq	%rcx, %r11
.L442:
	subq	$1, %r14
	movq	64(%rsp), %rsi
	leaq	(%rsi,%r14), %rcx
	movq	%rcx, %rsi
	movq	48(%rsp), %rdi
	subq	%rdi, %rsi
	movq	72(%rsp), %rcx
	addq	%r14, %rcx
	subq	%rdi, %rcx
	movq	%rcx, 40(%rsp)
	movq	80(%rsp), %rcx
	addq	%r14, %rcx
	movq	%rcx, %rdi
	subq	48(%rsp), %rdi
	movq	%rdi, 32(%rsp)
	movq	88(%rsp), %rcx
	addq	%r14, %rcx
	subq	48(%rsp), %rcx
	movq	%rcx, 24(%rsp)
	movq	96(%rsp), %rdi
	leaq	(%rdi,%r14), %rcx
	movq	%rcx, %rdi
	subq	48(%rsp), %rdi
	movq	%rdi, 16(%rsp)
	xorq	%r14, %rax
	movq	56(%rsp), %rcx
	movq	%rcx, %rdi
	xorq	%rax, %rdi
	orq	%rcx, %rax
	movq	%rsi, 8(%rsp)
	xorq	%rsi, %r11
	movq	%r11, %rsi
	xorq	%rbp, %rsi
	orq	%r11, %rbp
	xorq	40(%rsp), %r13
	movq	%r13, %r11
	xorq	%rbx, %r11
	movq	%r11, %rcx
	orq	%rbx, %r13
	xorq	32(%rsp), %r12
	movq	%r12, %rbx
	xorq	%r10, %rbx
	orq	%r10, %r12
	xorq	24(%rsp), %rdx
	movq	%rdx, %r11
	xorq	%r9, %r11
	orq	%r9, %rdx
	xorq	16(%rsp), %r15
	movq	%r15, %r10
	xorq	%r8, %r10
	orq	%r8, %r15
	xorq	%r14, %rax
	movq	%rdi, %r9
	xorq	%rax, %r9
	orq	%rdi, %rax
	xorq	8(%rsp), %rbp
	movq	%rsi, %r8
	xorq	%rbp, %r8
	orq	%rbp, %rsi
	xorq	40(%rsp), %r13
	movq	%rcx, %rdi
	xorq	%r13, %rdi
	orq	%r13, %rcx
	xorq	32(%rsp), %r12
	movq	%rbx, %rbp
	xorq	%r12, %rbp
	movq	%rbp, 56(%rsp)
	orq	%r12, %rbx
	xorq	24(%rsp), %rdx
	movq	%rdx, %rbp
	movq	%r11, %rdx
	xorq	%rbp, %rdx
	orq	%rbp, %r11
	xorq	16(%rsp), %r15
	movq	%r10, %r13
	xorq	%r15, %r13
	orq	%r15, %r10
	xorq	%r14, %rax
	movq	%rax, %rbp
	movq	%r9, %rax
	xorq	%rbp, %rax
	orq	%rbp, %r9
	xorq	8(%rsp), %rsi
	movq	%rsi, %rbp
	movq	%r8, %rsi
	xorq	%rbp, %rsi
	orq	%rbp, %r8
	xorq	40(%rsp), %rcx
	movq	%rcx, %rbp
	movq	%rdi, %rcx
	xorq	%rbp, %rcx
	orq	%rbp, %rdi
	xorq	32(%rsp), %rbx
	movq	56(%rsp), %r12
	movq	%r12, %rbp
	xorq	%rbx, %rbp
	orq	%rbx, %r12
	xorq	24(%rsp), %r11
	movq	%rdx, %rbx
	xorq	%r11, %rbx
	orq	%r11, %rdx
	xorq	16(%rsp), %r10
	movq	%r13, %r11
	xorq	%r10, %r11
	orq	%r10, %r13
	movq	%r13, %r15
	xorq	%r14, %r9
	movq	%rax, %r10
	xorq	%r9, %r10
	orq	%r9, %rax
	xorq	8(%rsp), %r8
	movq	%rsi, %r9
	xorq	%r8, %r9
	orq	%r8, %rsi
	xorq	40(%rsp), %rdi
	movq	%rcx, %r8
	xorq	%rdi, %r8
	orq	%rdi, %rcx
	xorq	32(%rsp), %r12
	movq	%rbp, %rdi
	xorq	%r12, %rdi
	orq	%r12, %rbp
	xorq	24(%rsp), %rdx
	movq	%rdx, %r12
	movq	%rbx, %rdx
	xorq	%r12, %rdx
	orq	%r12, %rbx
	xorq	16(%rsp), %r15
	movq	%r11, %r12
	xorq	%r15, %r12
	orq	%r15, %r11
	xorq	%r14, %rax
	movq	%rax, %r13
	movq	%r10, %rax
	xorq	%r13, %rax
	orq	%r13, %r10
	movq	%rsi, %r13
	movq	8(%rsp), %r15
	xorq	%r15, %r13
	movq	%r9, %rsi
	xorq	%r13, %rsi
	orq	%r13, %r9
	movq	%rcx, %r13
	xorq	40(%rsp), %r13
	movq	%r8, %rcx
	xorq	%r13, %rcx
	orq	%r13, %r8
	xorq	32(%rsp), %rbp
	movq	%rbp, %r13
	movq	%rdi, %rbp
	xorq	%r13, %rbp
	orq	%r13, %rdi
	xorq	24(%rsp), %rbx
	movq	%rbx, %r13
	movq	%rdx, %rbx
	xorq	%r13, %rbx
	orq	%r13, %rdx
	xorq	16(%rsp), %r11
	movq	%r11, %r13
	movq	%r12, %r11
	xorq	%r13, %r11
	orq	%r13, %r12
	movq	%r10, %r13
	xorq	%r14, %r13
	movq	%rax, %r10
	xorq	%r13, %r10
	orq	%r13, %rax
	xorq	%r15, %r9
	movq	%r9, %r13
	movq	%rsi, %r9
	xorq	%r13, %r9
	orq	%r13, %rsi
	xorq	40(%rsp), %r8
	movq	%r8, %r13
	movq	%rcx, %r8
	xorq	%r13, %r8
	orq	%r13, %rcx
	xorq	32(%rsp), %rdi
	movq	%rdi, %r13
	movq	%rbp, %rdi
	xorq	%r13, %rdi
	orq	%r13, %rbp
	xorq	24(%rsp), %rdx
	movq	%rdx, %r13
	movq	%rbx, %rdx
	xorq	%r13, %rdx
	orq	%r13, %rbx
	xorq	16(%rsp), %r12
	movq	%r11, %r15
	xorq	%r12, %r15
	orq	%r12, %r11
	xorq	%r14, %rax
	movq	%rax, %r12
	movq	%r10, %rax
	xorq	%r12, %rax
	orq	%r12, %r10
	movq	8(%rsp), %r13
	xorq	%r13, %rsi
	movq	%rsi, %r12
	movq	%r9, %rsi
	xorq	%r12, %rsi
	orq	%r12, %r9
	xorq	40(%rsp), %rcx
	movq	%rcx, %r12
	movq	%r8, %rcx
	xorq	%r12, %rcx
	orq	%r12, %r8
	xorq	32(%rsp), %rbp
	movq	%rdi, %r12
	xorq	%rbp, %r12
	orq	%rbp, %rdi
	xorq	24(%rsp), %rbx
	movq	%rdx, %rbp
	xorq	%rbx, %rbp
	orq	%rbx, %rdx
	xorq	16(%rsp), %r11
	movq	%r15, %rbx
	xorq	%r11, %rbx
	orq	%r11, %r15
	xorq	%r14, %r10
	movq	%rax, %r11
	xorq	%r10, %r11
	orq	%r10, %rax
	xorq	%r13, %r9
	movq	%rsi, %r10
	xorq	%r9, %r10
	orq	%r9, %rsi
	xorq	40(%rsp), %r8
	movq	%rcx, %r9
	xorq	%r8, %r9
	orq	%r8, %rcx
	xorq	32(%rsp), %rdi
	movq	%r12, %r8
	xorq	%rdi, %r8
	orq	%rdi, %r12
	xorq	24(%rsp), %rdx
	movq	%rbp, %rdi
	xorq	%rdx, %rdi
	orq	%rbp, %rdx
	xorq	16(%rsp), %r15
	movq	%rbx, %rbp
	xorq	%r15, %rbp
	orq	%rbx, %r15
	xorq	%r14, %rax
	movq	%r11, %rbx
	xorq	%rax, %rbx
	orq	%r11, %rax
	xorq	%r13, %rsi
	movq	%r10, %r11
	xorq	%rsi, %r11
	orq	%r10, %rsi
	movq	40(%rsp), %r10
	xorq	%r10, %rcx
	movq	%r9, %r13
	xorq	%rcx, %r13
	orq	%r9, %rcx
	xorq	32(%rsp), %r12
	movq	%r8, %r9
	xorq	%r12, %r9
	orq	%r8, %r12
	xorq	24(%rsp), %rdx
	movq	%rdi, %r8
	xorq	%rdx, %r8
	orq	%rdi, %rdx
	xorq	16(%rsp), %r15
	movq	%rbp, %rdi
	xorq	%r15, %rdi
	orq	%rbp, %r15
	xorq	%r14, %rax
	movq	%rbx, %rbp
	xorq	%rax, %rbp
	movq	%rbp, 56(%rsp)
	orq	%rbx, %rax
	xorq	8(%rsp), %rsi
	movq	%r11, %rbp
	xorq	%rsi, %rbp
	orq	%rsi, %r11
	xorq	%r10, %rcx
	movq	%r13, %rbx
	xorq	%rcx, %rbx
	orq	%rcx, %r13
	xorq	32(%rsp), %r12
	movq	%r9, %r10
	xorq	%r12, %r10
	orq	%r9, %r12
	xorq	24(%rsp), %rdx
	movq	%r8, %r9
	xorq	%rdx, %r9
	orq	%r8, %rdx
	xorq	16(%rsp), %r15
	movq	%rdi, %r8
	xorq	%r15, %r8
	orq	%rdi, %r15
	cmpq	104(%rsp), %r14
	jne	.L442
	movq	%r11, %r14
	movq	%rdx, 8(%rsp)
.L441:
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	addq	$120, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE158:
	.size	int64_bit_5, .-int64_bit_5
	.globl	int64_bit_6
	.type	int64_bit_6, @function
int64_bit_6:
.LFB159:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$168, %rsp
	.cfi_def_cfa_offset 224
	movq	%rdi, %rdx
	movq	%rsi, %rcx
	movl	12(%rsi), %esi
	leal	1(%rsi), %eax
	movslq	%eax, %rdi
	salq	$32, %rax
	orq	%rdi, %rax
	movslq	%esi, %rsi
	leaq	1(%rdx,%rsi), %rsi
	movq	%rsi, %rdi
	salq	$32, %rdi
	orq	%rsi, %rdi
	movq	%rdi, 72(%rsp)
	leaq	0(,%rdi,4), %r11
	movq	%r11, 64(%rsp)
	movslq	16(%rcx), %rsi
	movq	%rsi, %rdi
	salq	$32, %rdi
	orq	%rsi, %rdi
	movq	%rdi, %r14
	addq	%rdx, %rsi
	movq	%rsi, %rdi
	salq	$32, %rdi
	orq	%rsi, %rdi
	movq	%rdi, %r9
	leaq	0(,%rdi,4), %rsi
	movq	%rsi, 104(%rsp)
	movl	20(%rcx), %esi
	leal	-1(%rsi), %edi
	movslq	%edi, %r8
	salq	$32, %rdi
	orq	%r8, %rdi
	movq	%rdi, %rbp
	movslq	%esi, %rsi
	leaq	-1(%rdx,%rsi), %rsi
	movq	%rsi, %rdi
	salq	$32, %rdi
	orq	%rsi, %rdi
	movq	%rdi, 80(%rsp)
	leaq	0(,%rdi,4), %rsi
	movq	%rsi, 112(%rsp)
	movl	24(%rcx), %esi
	leal	-2(%rsi), %edi
	movslq	%edi, %r8
	salq	$32, %rdi
	orq	%r8, %rdi
	movq	%rdi, %r12
	movslq	%esi, %rsi
	leaq	-2(%rdx,%rsi), %rsi
	movq	%rsi, %rdi
	salq	$32, %rdi
	orq	%rsi, %rdi
	movq	%rdi, %rsi
	movq	%rdi, 88(%rsp)
	salq	$2, %rsi
	movq	%rsi, 120(%rsp)
	movl	28(%rcx), %esi
	leal	-3(%rsi), %edi
	movslq	%edi, %r8
	salq	$32, %rdi
	orq	%r8, %rdi
	movq	%rdi, %r13
	movslq	%esi, %rsi
	leaq	-3(%rdx,%rsi), %rsi
	movq	%rsi, %rdi
	salq	$32, %rdi
	orq	%rsi, %rdi
	movq	%rdi, 96(%rsp)
	leaq	0(,%rdi,4), %rsi
	movq	%rsi, 128(%rsp)
	movl	32(%rcx), %esi
	leal	-4(%rsi), %edi
	movslq	%edi, %r8
	salq	$32, %rdi
	orq	%r8, %rdi
	movq	%rdi, %r10
	movq	%rdi, 8(%rsp)
	movslq	%esi, %rsi
	leaq	-4(%rdx,%rsi), %rsi
	movq	%rsi, %rdi
	salq	$32, %rdi
	orq	%rsi, %rdi
	movq	%rdi, 16(%rsp)
	leaq	0(,%rdi,4), %rsi
	movq	%rsi, 136(%rsp)
	movl	36(%rcx), %ecx
	leal	-5(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r15
	movslq	%ecx, %rcx
	leaq	-5(%rdx,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 24(%rsp)
	leaq	0(,%rsi,4), %rcx
	movq	%rcx, 144(%rsp)
	testq	%rdx, %rdx
	je	.L446
	leaq	-1(%r11), %rbx
	subq	%rdx, %r11
	leaq	-1(%r11), %rdx
	movq	%rdx, 152(%rsp)
	movq	%rbx, 8(%rsp)
	movq	%r14, %rdx
	movq	%r9, %r14
	movq	%rbp, %r8
	movq	%r12, %rdi
	movq	%r13, %rsi
	movq	16(%rsp), %r13
	movq	%r10, %rcx
	movq	24(%rsp), %r12
.L447:
	subq	$1, 8(%rsp)
	movq	8(%rsp), %rbx
	movq	104(%rsp), %r9
	addq	%rbx, %r9
	movq	%r9, %r10
	movq	64(%rsp), %rbp
	subq	%rbp, %r10
	movq	112(%rsp), %r9
	addq	%rbx, %r9
	movq	%r9, %r11
	subq	%rbp, %r11
	movq	120(%rsp), %r9
	addq	%rbx, %r9
	subq	%rbp, %r9
	movq	%r9, 56(%rsp)
	movq	128(%rsp), %r9
	addq	%rbx, %r9
	subq	%rbp, %r9
	movq	%r9, 48(%rsp)
	movq	136(%rsp), %r9
	addq	%rbx, %r9
	movq	%r9, %rbp
	subq	64(%rsp), %rbp
	movq	%rbp, 40(%rsp)
	movq	144(%rsp), %r9
	addq	%rbx, %r9
	subq	64(%rsp), %r9
	movq	%r9, 32(%rsp)
	xorq	%rbx, %rax
	movq	72(%rsp), %rbx
	movq	%rbx, %rbp
	xorq	%rax, %rbp
	orq	%rbx, %rax
	movq	%r10, 24(%rsp)
	xorq	%r10, %rdx
	movq	%rdx, %r9
	xorq	%r14, %rdx
	orq	%r9, %r14
	movq	%r11, 16(%rsp)
	xorq	%r11, %r8
	movq	80(%rsp), %r10
	movq	%r10, %rbx
	xorq	%r8, %rbx
	orq	%r10, %r8
	xorq	56(%rsp), %rdi
	movq	88(%rsp), %r10
	movq	%r10, %r11
	xorq	%rdi, %r11
	orq	%r10, %rdi
	xorq	48(%rsp), %rsi
	movq	96(%rsp), %r9
	movq	%r9, %r10
	xorq	%rsi, %r10
	orq	%r9, %rsi
	xorq	40(%rsp), %rcx
	movq	%rcx, %r9
	xorq	%r13, %r9
	orq	%r13, %rcx
	xorq	32(%rsp), %r15
	movq	%r15, %r13
	xorq	%r12, %r13
	orq	%r12, %r15
	xorq	8(%rsp), %rax
	movq	%rax, %r12
	movq	%rbp, %rax
	xorq	%r12, %rax
	orq	%r12, %rbp
	xorq	24(%rsp), %r14
	movq	%rdx, %r12
	xorq	%r14, %r12
	orq	%r14, %rdx
	xorq	16(%rsp), %r8
	movq	%r8, %r14
	movq	%rbx, %r8
	xorq	%r14, %r8
	orq	%r14, %rbx
	xorq	56(%rsp), %rdi
	movq	%rdi, %r14
	movq	%r11, %rdi
	xorq	%r14, %rdi
	orq	%r14, %r11
	xorq	48(%rsp), %rsi
	movq	%rsi, %r14
	movq	%r10, %rsi
	xorq	%r14, %rsi
	orq	%r14, %r10
	xorq	40(%rsp), %rcx
	movq	%rcx, %r14
	movq	%r9, %rcx
	xorq	%r14, %rcx
	orq	%r14, %r9
	movq	%r15, %r14
	xorq	32(%rsp), %r14
	movq	%r13, %r15
	xorq	%r14, %r15
	orq	%r14, %r13
	movq	%rbp, %r14
	xorq	8(%rsp), %r14
	movq	%rax, %rbp
	xorq	%r14, %rbp
	orq	%r14, %rax
	xorq	24(%rsp), %rdx
	movq	%rdx, %r14
	movq	%r12, %rdx
	xorq	%r14, %rdx
	orq	%r14, %r12
	movq	%rbx, %r14
	xorq	16(%rsp), %r14
	movq	%r8, %rbx
	xorq	%r14, %rbx
	orq	%r14, %r8
	movq	%r11, %r14
	xorq	56(%rsp), %r14
	movq	%rdi, %r11
	xorq	%r14, %r11
	orq	%r14, %rdi
	movq	%r10, %r14
	xorq	48(%rsp), %r14
	movq	%rsi, %r10
	xorq	%r14, %r10
	orq	%r14, %rsi
	movq	%r9, %r14
	xorq	40(%rsp), %r14
	movq	%rcx, %r9
	xorq	%r14, %r9
	orq	%r14, %rcx
	xorq	32(%rsp), %r13
	movq	%r15, %r14
	xorq	%r13, %r14
	orq	%r13, %r15
	xorq	8(%rsp), %rax
	movq	%rax, %r13
	movq	%rbp, %rax
	xorq	%r13, %rax
	orq	%r13, %rbp
	xorq	24(%rsp), %r12
	movq	%rdx, %r13
	xorq	%r12, %r13
	orq	%r12, %rdx
	xorq	16(%rsp), %r8
	movq	%r8, %r12
	movq	%rbx, %r8
	xorq	%r12, %r8
	orq	%r12, %rbx
	xorq	56(%rsp), %rdi
	movq	%rdi, %r12
	movq	%r11, %rdi
	xorq	%r12, %rdi
	orq	%r12, %r11
	xorq	48(%rsp), %rsi
	movq	%rsi, %r12
	movq	%r10, %rsi
	xorq	%r12, %rsi
	orq	%r12, %r10
	xorq	40(%rsp), %rcx
	movq	%rcx, %r12
	movq	%r9, %rcx
	xorq	%r12, %rcx
	orq	%r12, %r9
	xorq	32(%rsp), %r15
	movq	%r14, %r12
	xorq	%r15, %r12
	orq	%r15, %r14
	movq	%rbp, %r15
	xorq	8(%rsp), %r15
	movq	%rax, %rbp
	xorq	%r15, %rbp
	orq	%r15, %rax
	xorq	24(%rsp), %rdx
	movq	%rdx, %r15
	movq	%r13, %rdx
	xorq	%r15, %rdx
	orq	%r15, %r13
	movq	%rbx, %r15
	xorq	16(%rsp), %r15
	movq	%r8, %rbx
	xorq	%r15, %rbx
	orq	%r15, %r8
	movq	%r11, %r15
	xorq	56(%rsp), %r15
	movq	%rdi, %r11
	xorq	%r15, %r11
	orq	%r15, %rdi
	movq	%r10, %r15
	xorq	48(%rsp), %r15
	movq	%rsi, %r10
	xorq	%r15, %r10
	orq	%r15, %rsi
	movq	%r9, %r15
	xorq	40(%rsp), %r15
	movq	%rcx, %r9
	xorq	%r15, %r9
	orq	%r15, %rcx
	xorq	32(%rsp), %r14
	movq	%r14, %r15
	movq	%r12, %r14
	xorq	%r15, %r14
	orq	%r15, %r12
	xorq	8(%rsp), %rax
	movq	%rax, %r15
	movq	%rbp, %rax
	xorq	%r15, %rax
	orq	%r15, %rbp
	movq	%r13, %r15
	xorq	24(%rsp), %r15
	movq	%rdx, %r13
	xorq	%r15, %r13
	orq	%r15, %rdx
	xorq	16(%rsp), %r8
	movq	%r8, %r15
	movq	%rbx, %r8
	xorq	%r15, %r8
	orq	%r15, %rbx
	xorq	56(%rsp), %rdi
	movq	%rdi, %r15
	movq	%r11, %rdi
	xorq	%r15, %rdi
	orq	%r15, %r11
	xorq	48(%rsp), %rsi
	movq	%rsi, %r15
	movq	%r10, %rsi
	xorq	%r15, %rsi
	orq	%r15, %r10
	xorq	40(%rsp), %rcx
	movq	%rcx, %r15
	movq	%r9, %rcx
	xorq	%r15, %rcx
	orq	%r15, %r9
	movq	%r12, %r15
	xorq	32(%rsp), %r15
	movq	%r14, %r12
	xorq	%r15, %r12
	orq	%r15, %r14
	movq	%rbp, %r15
	xorq	8(%rsp), %r15
	movq	%rax, %rbp
	xorq	%r15, %rbp
	orq	%r15, %rax
	xorq	24(%rsp), %rdx
	movq	%rdx, %r15
	movq	%r13, %rdx
	xorq	%r15, %rdx
	orq	%r15, %r13
	movq	%rbx, %r15
	xorq	16(%rsp), %r15
	movq	%r8, %rbx
	xorq	%r15, %rbx
	orq	%r15, %r8
	xorq	56(%rsp), %r11
	movq	%r11, %r15
	movq	%rdi, %r11
	xorq	%r15, %r11
	orq	%r15, %rdi
	xorq	48(%rsp), %r10
	movq	%r10, %r15
	movq	%rsi, %r10
	xorq	%r15, %r10
	orq	%r15, %rsi
	xorq	40(%rsp), %r9
	movq	%r9, %r15
	movq	%rcx, %r9
	xorq	%r15, %r9
	orq	%r15, %rcx
	xorq	32(%rsp), %r14
	movq	%r12, %r15
	xorq	%r14, %r15
	orq	%r14, %r12
	xorq	8(%rsp), %rax
	movq	%rax, %r14
	movq	%rbp, %rax
	xorq	%r14, %rax
	orq	%r14, %rbp
	xorq	24(%rsp), %r13
	movq	%rdx, %r14
	xorq	%r13, %r14
	orq	%r13, %rdx
	xorq	16(%rsp), %r8
	movq	%r8, %r13
	movq	%rbx, %r8
	xorq	%r13, %r8
	orq	%r13, %rbx
	xorq	56(%rsp), %rdi
	movq	%rdi, %r13
	movq	%r11, %rdi
	xorq	%r13, %rdi
	orq	%r13, %r11
	xorq	48(%rsp), %rsi
	movq	%rsi, %r13
	movq	%r10, %rsi
	xorq	%r13, %rsi
	orq	%r13, %r10
	xorq	40(%rsp), %rcx
	movq	%rcx, %r13
	movq	%r9, %rcx
	xorq	%r13, %rcx
	orq	%r13, %r9
	xorq	32(%rsp), %r12
	movq	%r15, %r13
	xorq	%r12, %r13
	orq	%r12, %r15
	xorq	8(%rsp), %rbp
	movq	%rax, %r12
	xorq	%rbp, %r12
	orq	%rbp, %rax
	xorq	24(%rsp), %rdx
	movq	%rdx, %rbp
	movq	%r14, %rdx
	xorq	%rbp, %rdx
	orq	%rbp, %r14
	xorq	16(%rsp), %rbx
	movq	%r8, %rbp
	xorq	%rbx, %rbp
	orq	%rbx, %r8
	xorq	56(%rsp), %r11
	movq	%rdi, %rbx
	xorq	%r11, %rbx
	orq	%r11, %rdi
	xorq	48(%rsp), %r10
	movq	%rsi, %r11
	xorq	%r10, %r11
	orq	%r10, %rsi
	xorq	40(%rsp), %r9
	movq	%rcx, %r10
	xorq	%r9, %r10
	orq	%r9, %rcx
	xorq	32(%rsp), %r15
	movq	%r13, %r9
	xorq	%r15, %r9
	orq	%r13, %r15
	xorq	8(%rsp), %rax
	movq	%r12, %r13
	xorq	%rax, %r13
	movq	%r13, 72(%rsp)
	orq	%r12, %rax
	movq	24(%rsp), %r12
	xorq	%r14, %r12
	movq	%rdx, %r14
	xorq	%r12, %r14
	orq	%r12, %rdx
	xorq	16(%rsp), %r8
	movq	%rbp, %r13
	xorq	%r8, %r13
	movq	%r13, 80(%rsp)
	orq	%rbp, %r8
	xorq	56(%rsp), %rdi
	movq	%rbx, %rbp
	xorq	%rdi, %rbp
	movq	%rbp, 88(%rsp)
	orq	%rbx, %rdi
	xorq	48(%rsp), %rsi
	movq	%r11, %rbp
	xorq	%rsi, %rbp
	movq	%rbp, 96(%rsp)
	orq	%r11, %rsi
	xorq	40(%rsp), %rcx
	movq	%r10, %r13
	xorq	%rcx, %r13
	orq	%r10, %rcx
	xorq	32(%rsp), %r15
	movq	%r9, %r12
	xorq	%r15, %r12
	orq	%r9, %r15
	movq	8(%rsp), %rbx
	cmpq	152(%rsp), %rbx
	jne	.L447
	movq	%rdx, %r14
	movq	%r8, %rbp
	movq	%rdi, %r12
	movq	%rsi, %r13
	movq	%rcx, 8(%rsp)
.L446:
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	addq	$168, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE159:
	.size	int64_bit_6, .-int64_bit_6
	.globl	int64_bit_7
	.type	int64_bit_7, @function
int64_bit_7:
.LFB160:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$200, %rsp
	.cfi_def_cfa_offset 256
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r15
	movslq	%ecx, %rcx
	leaq	1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %r11
	leaq	0(,%rsi,4), %r9
	movq	%r9, 120(%rsp)
	movslq	16(%rdx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %r14
	addq	%rax, %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 64(%rsp)
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 128(%rsp)
	movl	20(%rdx), %ecx
	leal	-1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r13
	movslq	%ecx, %rcx
	leaq	-1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 72(%rsp)
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 136(%rsp)
	movl	24(%rdx), %ecx
	leal	-2(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r12
	movslq	%ecx, %rcx
	leaq	-2(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 80(%rsp)
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 144(%rsp)
	movl	28(%rdx), %ecx
	leal	-3(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %rbp
	movslq	%ecx, %rcx
	leaq	-3(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %r10
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 152(%rsp)
	movl	32(%rdx), %ecx
	leal	-4(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	movq	%rsi, %rbx
	orq	%rdi, %rbx
	movslq	%ecx, %rcx
	leaq	-4(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 88(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 160(%rsp)
	movl	36(%rdx), %edi
	leal	-5(%rdi), %ecx
	movslq	%ecx, %rsi
	salq	$32, %rcx
	orq	%rsi, %rcx
	movq	%rcx, 8(%rsp)
	movslq	%edi, %rcx
	leaq	-5(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 96(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 168(%rsp)
	movl	40(%rdx), %edx
	leal	-6(%rdx), %ecx
	movslq	%ecx, %rsi
	salq	$32, %rcx
	movq	%rcx, %rdi
	orq	%rsi, %rdi
	movq	%rdi, (%rsp)
	movslq	%edx, %rdx
	leaq	-6(%rax,%rdx), %rdx
	movq	%rdx, %rcx
	salq	$32, %rcx
	orq	%rdx, %rcx
	movq	%rcx, 104(%rsp)
	leaq	0(,%rcx,4), %rdx
	movq	%rdx, 176(%rsp)
	testq	%rax, %rax
	je	.L451
	leaq	-1(%r9), %r8
	subq	%rax, %r9
	leaq	-1(%r9), %rax
	movq	%rax, 184(%rsp)
	movq	%r8, (%rsp)
	movq	%r11, 112(%rsp)
	movq	%r10, %r11
	movq	8(%rsp), %r10
	movq	%rdi, %rdx
.L452:
	subq	$1, (%rsp)
	movq	(%rsp), %rsi
	movq	128(%rsp), %rax
	addq	%rsi, %rax
	movq	120(%rsp), %r8
	subq	%r8, %rax
	movq	%rax, %rdi
	movq	136(%rsp), %rax
	addq	%rsi, %rax
	subq	%r8, %rax
	movq	%rax, %rcx
	movq	144(%rsp), %rax
	addq	%rsi, %rax
	subq	%r8, %rax
	movq	%rax, 56(%rsp)
	movq	152(%rsp), %rax
	addq	%rsi, %rax
	subq	%r8, %rax
	movq	%rax, 48(%rsp)
	movq	160(%rsp), %r9
	addq	%rsi, %r9
	subq	%r8, %r9
	movq	%r9, 40(%rsp)
	movq	168(%rsp), %rax
	addq	%rsi, %rax
	movq	%r8, %r9
	subq	%r8, %rax
	movq	%rax, 32(%rsp)
	movq	176(%rsp), %r8
	addq	%rsi, %r8
	subq	%r9, %r8
	movq	%r8, 24(%rsp)
	xorq	%rsi, %r15
	movq	%r15, %rax
	movq	112(%rsp), %rsi
	movq	%rsi, %r15
	xorq	%rax, %r15
	orq	%rsi, %rax
	movq	%rdi, 16(%rsp)
	xorq	%rdi, %r14
	movq	%r14, %r9
	movq	64(%rsp), %rdi
	movq	%rdi, %r14
	xorq	%r9, %r14
	orq	%rdi, %r9
	movq	%rcx, 8(%rsp)
	xorq	%rcx, %r13
	movq	%r13, %r8
	movq	72(%rsp), %rcx
	movq	%rcx, %r13
	xorq	%r8, %r13
	orq	%rcx, %r8
	movq	%r8, 112(%rsp)
	xorq	56(%rsp), %r12
	movq	%r12, %rdi
	movq	80(%rsp), %rcx
	movq	%rcx, %r12
	xorq	%rdi, %r12
	orq	%rcx, %rdi
	xorq	48(%rsp), %rbp
	movq	%rbp, %rsi
	xorq	%r11, %rbp
	orq	%r11, %rsi
	xorq	40(%rsp), %rbx
	movq	88(%rsp), %rcx
	movq	%rcx, %r11
	xorq	%rbx, %r11
	orq	%rcx, %rbx
	movq	%rbx, 80(%rsp)
	xorq	32(%rsp), %r10
	movq	%r10, %rcx
	movq	96(%rsp), %rbx
	movq	%rbx, %r10
	xorq	%rcx, %r10
	orq	%rbx, %rcx
	xorq	24(%rsp), %rdx
	movq	104(%rsp), %rbx
	movq	%rbx, %r8
	xorq	%rdx, %r8
	movq	%r8, 88(%rsp)
	orq	%rbx, %rdx
	xorq	(%rsp), %rax
	movq	%r15, %rbx
	xorq	%rax, %rbx
	movq	%rbx, 64(%rsp)
	orq	%r15, %rax
	xorq	16(%rsp), %r9
	movq	%r14, %r15
	xorq	%r9, %r15
	movq	%r15, 72(%rsp)
	orq	%r14, %r9
	movq	112(%rsp), %r8
	xorq	8(%rsp), %r8
	movq	%r13, %r14
	xorq	%r8, %r14
	orq	%r13, %r8
	xorq	56(%rsp), %rdi
	movq	%r12, %r13
	xorq	%rdi, %r13
	orq	%r12, %rdi
	xorq	48(%rsp), %rsi
	movq	%rbp, %r12
	xorq	%rsi, %r12
	orq	%rbp, %rsi
	movq	80(%rsp), %rbx
	xorq	40(%rsp), %rbx
	movq	%r11, %rbp
	xorq	%rbx, %rbp
	orq	%r11, %rbx
	xorq	32(%rsp), %rcx
	movq	%r10, %r11
	xorq	%rcx, %r11
	orq	%r10, %rcx
	xorq	24(%rsp), %rdx
	movq	88(%rsp), %r15
	movq	%r15, %r10
	xorq	%rdx, %r10
	orq	%r15, %rdx
	xorq	(%rsp), %rax
	movq	64(%rsp), %r15
	xorq	%rax, %r15
	movq	%r15, 80(%rsp)
	orq	64(%rsp), %rax
	xorq	16(%rsp), %r9
	movq	72(%rsp), %r15
	xorq	%r9, %r15
	movq	%r15, 64(%rsp)
	orq	72(%rsp), %r9
	xorq	8(%rsp), %r8
	movq	%r14, %r15
	xorq	%r8, %r15
	orq	%r14, %r8
	xorq	56(%rsp), %rdi
	movq	%r13, %r14
	xorq	%rdi, %r14
	orq	%r13, %rdi
	xorq	48(%rsp), %rsi
	movq	%r12, %r13
	xorq	%rsi, %r13
	orq	%r12, %rsi
	xorq	40(%rsp), %rbx
	movq	%rbp, %r12
	xorq	%rbx, %r12
	orq	%rbp, %rbx
	xorq	32(%rsp), %rcx
	movq	%r11, %rbp
	xorq	%rcx, %rbp
	orq	%r11, %rcx
	xorq	24(%rsp), %rdx
	movq	%r10, %r11
	xorq	%rdx, %r11
	orq	%r10, %rdx
	movq	%rdx, 72(%rsp)
	xorq	(%rsp), %rax
	movq	80(%rsp), %rdx
	movq	%rdx, %r10
	xorq	%rax, %r10
	orq	%rdx, %rax
	xorq	16(%rsp), %r9
	movq	64(%rsp), %rdx
	xorq	%r9, %rdx
	movq	%rdx, 80(%rsp)
	orq	64(%rsp), %r9
	xorq	8(%rsp), %r8
	movq	%r15, %rdx
	xorq	%r8, %rdx
	movq	%rdx, 64(%rsp)
	orq	%r15, %r8
	xorq	56(%rsp), %rdi
	movq	%r14, %r15
	xorq	%rdi, %r15
	orq	%r14, %rdi
	xorq	48(%rsp), %rsi
	movq	%r13, %r14
	xorq	%rsi, %r14
	orq	%r13, %rsi
	xorq	40(%rsp), %rbx
	movq	%r12, %r13
	xorq	%rbx, %r13
	orq	%r12, %rbx
	movq	%rbx, 88(%rsp)
	xorq	32(%rsp), %rcx
	movq	%rbp, %r12
	xorq	%rcx, %r12
	orq	%rbp, %rcx
	movq	72(%rsp), %rdx
	xorq	24(%rsp), %rdx
	movq	%r11, %rbp
	xorq	%rdx, %rbp
	orq	%r11, %rdx
	xorq	(%rsp), %rax
	movq	%r10, %r11
	xorq	%rax, %r11
	orq	%r10, %rax
	xorq	16(%rsp), %r9
	movq	80(%rsp), %rbx
	movq	%rbx, %r10
	xorq	%r9, %r10
	orq	%rbx, %r9
	xorq	8(%rsp), %r8
	movq	64(%rsp), %rbx
	xorq	%r8, %rbx
	movq	%rbx, 80(%rsp)
	orq	64(%rsp), %r8
	xorq	56(%rsp), %rdi
	movq	%r15, %rbx
	xorq	%rdi, %rbx
	movq	%rbx, 64(%rsp)
	orq	%r15, %rdi
	xorq	48(%rsp), %rsi
	movq	%r14, %r15
	xorq	%rsi, %r15
	movq	%r15, 72(%rsp)
	orq	%r14, %rsi
	movq	88(%rsp), %rbx
	xorq	40(%rsp), %rbx
	movq	%r13, %r14
	xorq	%rbx, %r14
	orq	%r13, %rbx
	xorq	32(%rsp), %rcx
	movq	%r12, %r13
	xorq	%rcx, %r13
	orq	%r12, %rcx
	xorq	24(%rsp), %rdx
	movq	%rbp, %r12
	xorq	%rdx, %r12
	orq	%rbp, %rdx
	xorq	(%rsp), %rax
	movq	%r11, %rbp
	xorq	%rax, %rbp
	orq	%r11, %rax
	xorq	16(%rsp), %r9
	movq	%r10, %r11
	xorq	%r9, %r11
	orq	%r10, %r9
	xorq	8(%rsp), %r8
	movq	80(%rsp), %r15
	movq	%r15, %r10
	xorq	%r8, %r10
	orq	%r15, %r8
	xorq	56(%rsp), %rdi
	movq	64(%rsp), %r15
	xorq	%rdi, %r15
	movq	%r15, 80(%rsp)
	orq	64(%rsp), %rdi
	xorq	48(%rsp), %rsi
	movq	72(%rsp), %r15
	xorq	%rsi, %r15
	movq	%r15, 64(%rsp)
	orq	72(%rsp), %rsi
	xorq	40(%rsp), %rbx
	movq	%r14, %r15
	xorq	%rbx, %r15
	orq	%r14, %rbx
	xorq	32(%rsp), %rcx
	movq	%r13, %r14
	xorq	%rcx, %r14
	orq	%r13, %rcx
	xorq	24(%rsp), %rdx
	movq	%r12, %r13
	xorq	%rdx, %r13
	orq	%r12, %rdx
	movq	%rdx, 72(%rsp)
	xorq	(%rsp), %rax
	movq	%rbp, %r12
	xorq	%rax, %r12
	orq	%rbp, %rax
	xorq	16(%rsp), %r9
	movq	%r11, %rbp
	xorq	%r9, %rbp
	orq	%r11, %r9
	xorq	8(%rsp), %r8
	movq	%r10, %r11
	xorq	%r8, %r11
	orq	%r10, %r8
	movq	%r8, 88(%rsp)
	xorq	56(%rsp), %rdi
	movq	80(%rsp), %rdx
	movq	%rdx, %r10
	xorq	%rdi, %r10
	orq	%rdx, %rdi
	xorq	48(%rsp), %rsi
	movq	64(%rsp), %rdx
	movq	%rdx, %r8
	xorq	%rsi, %r8
	movq	%r8, 80(%rsp)
	orq	%rdx, %rsi
	xorq	40(%rsp), %rbx
	movq	%r15, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 64(%rsp)
	orq	%r15, %rbx
	xorq	32(%rsp), %rcx
	movq	%r14, %r15
	xorq	%rcx, %r15
	orq	%r14, %rcx
	movq	72(%rsp), %rdx
	xorq	24(%rsp), %rdx
	movq	%r13, %r14
	xorq	%rdx, %r14
	orq	%r13, %rdx
	xorq	(%rsp), %rax
	movq	%r12, %r13
	xorq	%rax, %r13
	orq	%r12, %rax
	xorq	16(%rsp), %r9
	movq	%rbp, %r12
	xorq	%r9, %r12
	orq	%rbp, %r9
	movq	88(%rsp), %r8
	xorq	8(%rsp), %r8
	movq	%r11, %rbp
	xorq	%r8, %rbp
	orq	%r11, %r8
	movq	%r8, 72(%rsp)
	xorq	56(%rsp), %rdi
	movq	%r10, %r11
	xorq	%rdi, %r11
	orq	%r10, %rdi
	xorq	48(%rsp), %rsi
	movq	80(%rsp), %r8
	movq	%r8, %r10
	xorq	%rsi, %r10
	orq	%r8, %rsi
	xorq	40(%rsp), %rbx
	movq	64(%rsp), %r8
	xorq	%rbx, %r8
	movq	%r8, 80(%rsp)
	orq	64(%rsp), %rbx
	xorq	32(%rsp), %rcx
	movq	%r15, %r8
	xorq	%rcx, %r8
	movq	%r8, 64(%rsp)
	orq	%r15, %rcx
	xorq	24(%rsp), %rdx
	movq	%r14, %r15
	xorq	%rdx, %r15
	orq	%r14, %rdx
	xorq	(%rsp), %rax
	movq	%r13, %r14
	xorq	%rax, %r14
	orq	%r13, %rax
	xorq	16(%rsp), %r9
	movq	%r12, %r13
	xorq	%r9, %r13
	orq	%r9, %r12
	movq	72(%rsp), %r8
	xorq	8(%rsp), %r8
	movq	%rbp, %r9
	xorq	%r8, %r9
	orq	%r8, %rbp
	xorq	56(%rsp), %rdi
	movq	%r11, %r8
	xorq	%rdi, %r8
	orq	%rdi, %r11
	xorq	48(%rsp), %rsi
	movq	%r10, %rdi
	xorq	%rsi, %rdi
	orq	%r10, %rsi
	movq	%rsi, 88(%rsp)
	xorq	40(%rsp), %rbx
	movq	80(%rsp), %r10
	movq	%r10, %rsi
	xorq	%rbx, %rsi
	movq	%rsi, 96(%rsp)
	orq	%r10, %rbx
	xorq	32(%rsp), %rcx
	movq	64(%rsp), %rsi
	movq	%rsi, %r10
	xorq	%rcx, %r10
	orq	%rsi, %rcx
	xorq	24(%rsp), %rdx
	movq	%r15, %rsi
	xorq	%rdx, %rsi
	movq	%rsi, 104(%rsp)
	orq	%r15, %rdx
	xorq	(%rsp), %rax
	movq	%r14, %r15
	xorq	%rax, %r15
	movq	%r15, 112(%rsp)
	orq	%rax, %r14
	movq	%r14, %r15
	movq	16(%rsp), %rax
	xorq	%r12, %rax
	movq	%r13, %r14
	xorq	%rax, %r14
	movq	%r14, 64(%rsp)
	movq	%r13, %r14
	orq	%rax, %r14
	movq	8(%rsp), %rax
	xorq	%rbp, %rax
	movq	%r9, %rbp
	xorq	%rax, %rbp
	movq	%rbp, 72(%rsp)
	orq	%rax, %r9
	movq	%r9, %r13
	xorq	56(%rsp), %r11
	movq	%r8, %r9
	xorq	%r11, %r9
	movq	%r9, 80(%rsp)
	orq	%r11, %r8
	movq	%r8, %r12
	movq	48(%rsp), %rax
	xorq	88(%rsp), %rax
	movq	%rdi, %r11
	xorq	%rax, %r11
	movq	%rdi, %rbp
	orq	%rax, %rbp
	xorq	40(%rsp), %rbx
	movq	96(%rsp), %rdi
	movq	%rdi, %rsi
	xorq	%rbx, %rsi
	movq	%rsi, 88(%rsp)
	orq	%rdi, %rbx
	movq	32(%rsp), %rax
	xorq	%rcx, %rax
	movq	%r10, %rdi
	xorq	%rax, %rdi
	movq	%rdi, 96(%rsp)
	orq	%rax, %r10
	xorq	24(%rsp), %rdx
	movq	104(%rsp), %rcx
	movq	%rcx, %rdi
	xorq	%rdx, %rdi
	movq	%rdi, 104(%rsp)
	orq	%rcx, %rdx
	movq	(%rsp), %rax
	cmpq	184(%rsp), %rax
	jne	.L452
	movq	%r10, 8(%rsp)
	movq	%rdx, (%rsp)
.L451:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	(%rsp), %edi
	call	use_int@PLT
	addq	$200, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE160:
	.size	int64_bit_7, .-int64_bit_7
	.globl	int64_bit_8
	.type	int64_bit_8, @function
int64_bit_8:
.LFB161:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$248, %rsp
	.cfi_def_cfa_offset 304
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r15
	movslq	%ecx, %rcx
	leaq	1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 88(%rsp)
	leaq	0(,%rsi,4), %r10
	movq	%r10, 72(%rsp)
	movslq	16(%rdx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %r14
	addq	%rax, %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 96(%rsp)
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 168(%rsp)
	movl	20(%rdx), %ecx
	leal	-1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r13
	movslq	%ecx, %rcx
	leaq	-1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 104(%rsp)
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 176(%rsp)
	movl	24(%rdx), %ecx
	leal	-2(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r12
	movslq	%ecx, %rcx
	leaq	-2(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 112(%rsp)
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 184(%rsp)
	movl	28(%rdx), %ecx
	leal	-3(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %rbp
	movslq	%ecx, %rcx
	leaq	-3(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 120(%rsp)
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 192(%rsp)
	movl	32(%rdx), %ecx
	leal	-4(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	movq	%rsi, %rbx
	orq	%rdi, %rbx
	movslq	%ecx, %rcx
	leaq	-4(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 128(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 200(%rsp)
	movl	36(%rdx), %ecx
	leal	-5(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r9
	movq	%rsi, (%rsp)
	movslq	%ecx, %rcx
	leaq	-5(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %rdi
	movq	%rsi, 136(%rsp)
	salq	$2, %rdi
	movq	%rdi, 208(%rsp)
	movl	40(%rdx), %ecx
	leal	-6(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r11
	movq	%rsi, 8(%rsp)
	movslq	%ecx, %rcx
	leaq	-6(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %rdi
	movq	%rsi, 144(%rsp)
	salq	$2, %rdi
	movq	%rdi, 216(%rsp)
	movl	44(%rdx), %edx
	leal	-7(%rdx), %ecx
	movslq	%ecx, %rsi
	salq	$32, %rcx
	movq	%rcx, %rdi
	orq	%rsi, %rdi
	movq	%rdi, 80(%rsp)
	movslq	%edx, %rdx
	leaq	-7(%rax,%rdx), %rdx
	movq	%rdx, %rcx
	salq	$32, %rcx
	orq	%rdx, %rcx
	movq	%rcx, 152(%rsp)
	leaq	0(,%rcx,4), %rsi
	movq	%rsi, 224(%rsp)
	testq	%rax, %rax
	je	.L456
	leaq	-1(%r10), %r8
	subq	%rax, %r10
	leaq	-1(%r10), %rax
	movq	%rax, 232(%rsp)
	movq	%r8, (%rsp)
	movq	%r15, %rcx
	movq	%r9, %rsi
	movq	%r11, %r15
.L457:
	subq	$1, (%rsp)
	movq	(%rsp), %rdi
	movq	168(%rsp), %rax
	addq	%rdi, %rax
	movq	72(%rsp), %r9
	subq	%r9, %rax
	movq	%rax, %r10
	movq	176(%rsp), %rax
	addq	%rdi, %rax
	subq	%r9, %rax
	movq	%rax, %r8
	movq	184(%rsp), %rax
	addq	%rdi, %rax
	subq	%r9, %rax
	movq	%rax, 8(%rsp)
	movq	192(%rsp), %rax
	addq	%rdi, %rax
	subq	%r9, %rax
	movq	%rax, 56(%rsp)
	movq	200(%rsp), %rdx
	addq	%rdi, %rdx
	movq	%rdx, %r11
	subq	%r9, %r11
	movq	%r11, 48(%rsp)
	movq	208(%rsp), %rdx
	addq	%rdi, %rdx
	movq	%rdx, %r9
	movq	72(%rsp), %r11
	subq	%r11, %r9
	movq	%r9, 40(%rsp)
	movq	216(%rsp), %rdx
	addq	%rdi, %rdx
	movq	%rdx, %rax
	movq	%r11, %r9
	subq	%r11, %rax
	movq	%rax, 32(%rsp)
	movq	224(%rsp), %rdx
	addq	%rdi, %rdx
	movq	%rdx, %r11
	subq	%r9, %r11
	movq	%r11, 64(%rsp)
	xorq	%rdi, %rcx
	movq	%rcx, %rax
	movq	88(%rsp), %rcx
	movq	%rcx, %r9
	xorq	%rax, %r9
	movq	%r9, 88(%rsp)
	orq	%rcx, %rax
	movq	%r10, 24(%rsp)
	xorq	%r10, %r14
	movq	%r14, %r11
	movq	96(%rsp), %rdx
	movq	%rdx, %r14
	xorq	%r11, %r14
	orq	%rdx, %r11
	movq	%r8, 16(%rsp)
	xorq	%r8, %r13
	movq	%r13, %r10
	movq	104(%rsp), %rdx
	movq	%rdx, %r13
	xorq	%r10, %r13
	orq	%rdx, %r10
	xorq	8(%rsp), %r12
	movq	%r12, %r9
	movq	112(%rsp), %rdx
	movq	%rdx, %r12
	xorq	%r9, %r12
	orq	%rdx, %r9
	xorq	56(%rsp), %rbp
	movq	%rbp, %r8
	movq	120(%rsp), %rdx
	movq	%rdx, %rbp
	xorq	%r8, %rbp
	orq	%rdx, %r8
	xorq	48(%rsp), %rbx
	movq	%rbx, %rdi
	movq	128(%rsp), %rcx
	movq	%rcx, %rbx
	xorq	%rdi, %rbx
	orq	%rcx, %rdi
	xorq	40(%rsp), %rsi
	movq	136(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%rsi, %rcx
	movq	%rcx, 96(%rsp)
	orq	%rdx, %rsi
	movq	%r15, %rcx
	xorq	32(%rsp), %rcx
	movq	144(%rsp), %r15
	movq	%r15, %rdx
	xorq	%rcx, %rdx
	movq	%rdx, 104(%rsp)
	orq	%r15, %rcx
	movq	%rcx, 112(%rsp)
	movq	80(%rsp), %rdx
	xorq	64(%rsp), %rdx
	movq	152(%rsp), %r15
	movq	%r15, %rcx
	xorq	%rdx, %rcx
	movq	%rcx, 80(%rsp)
	orq	%r15, %rdx
	movq	%rdx, 120(%rsp)
	xorq	(%rsp), %rax
	movq	88(%rsp), %r15
	movq	%r15, %rdx
	xorq	%rax, %rdx
	movq	%rdx, 88(%rsp)
	orq	%r15, %rax
	xorq	24(%rsp), %r11
	movq	%r14, %r15
	xorq	%r11, %r15
	orq	%r14, %r11
	xorq	16(%rsp), %r10
	movq	%r13, %r14
	xorq	%r10, %r14
	orq	%r13, %r10
	xorq	8(%rsp), %r9
	movq	%r12, %r13
	xorq	%r9, %r13
	orq	%r12, %r9
	movq	%r9, 128(%rsp)
	xorq	56(%rsp), %r8
	movq	%rbp, %r12
	xorq	%r8, %r12
	orq	%rbp, %r8
	xorq	48(%rsp), %rdi
	movq	%rbx, %rbp
	xorq	%rdi, %rbp
	orq	%rbx, %rdi
	xorq	40(%rsp), %rsi
	movq	96(%rsp), %r9
	movq	%r9, %rbx
	xorq	%rsi, %rbx
	orq	%r9, %rsi
	movq	112(%rsp), %rcx
	xorq	32(%rsp), %rcx
	movq	104(%rsp), %r9
	movq	%r9, %rdx
	xorq	%rcx, %rdx
	movq	%rdx, 96(%rsp)
	orq	%r9, %rcx
	movq	%rcx, 104(%rsp)
	movq	120(%rsp), %rdx
	xorq	64(%rsp), %rdx
	movq	80(%rsp), %rcx
	movq	%rcx, %r9
	xorq	%rdx, %r9
	movq	%r9, 80(%rsp)
	orq	%rcx, %rdx
	xorq	(%rsp), %rax
	movq	88(%rsp), %r9
	movq	%r9, %rcx
	xorq	%rax, %rcx
	movq	%rcx, 88(%rsp)
	orq	%r9, %rax
	xorq	24(%rsp), %r11
	movq	%r15, %rcx
	xorq	%r11, %rcx
	movq	%rcx, 112(%rsp)
	orq	%r15, %r11
	xorq	16(%rsp), %r10
	movq	%r14, %r15
	xorq	%r10, %r15
	orq	%r14, %r10
	movq	128(%rsp), %r9
	xorq	8(%rsp), %r9
	movq	%r13, %r14
	xorq	%r9, %r14
	orq	%r13, %r9
	xorq	56(%rsp), %r8
	movq	%r12, %r13
	xorq	%r8, %r13
	orq	%r12, %r8
	movq	%r8, 120(%rsp)
	xorq	48(%rsp), %rdi
	movq	%rbp, %r12
	xorq	%rdi, %r12
	orq	%rbp, %rdi
	xorq	40(%rsp), %rsi
	movq	%rbx, %rbp
	xorq	%rsi, %rbp
	orq	%rbx, %rsi
	movq	%rsi, 128(%rsp)
	movq	104(%rsp), %rcx
	xorq	32(%rsp), %rcx
	movq	96(%rsp), %r8
	movq	%r8, %rbx
	xorq	%rcx, %rbx
	orq	%r8, %rcx
	xorq	64(%rsp), %rdx
	movq	80(%rsp), %rsi
	movq	%rsi, %r8
	xorq	%rdx, %r8
	movq	%r8, 80(%rsp)
	orq	%rsi, %rdx
	movq	%rdx, 96(%rsp)
	xorq	(%rsp), %rax
	movq	88(%rsp), %rdx
	movq	%rdx, %rsi
	xorq	%rax, %rsi
	movq	%rsi, 88(%rsp)
	orq	%rdx, %rax
	xorq	24(%rsp), %r11
	movq	112(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r11, %rdx
	movq	%rdx, 104(%rsp)
	orq	%rsi, %r11
	xorq	16(%rsp), %r10
	movq	%r15, %rsi
	xorq	%r10, %rsi
	movq	%rsi, 112(%rsp)
	orq	%r15, %r10
	xorq	8(%rsp), %r9
	movq	%r14, %r15
	xorq	%r9, %r15
	orq	%r14, %r9
	movq	120(%rsp), %r8
	xorq	56(%rsp), %r8
	movq	%r13, %r14
	xorq	%r8, %r14
	orq	%r13, %r8
	movq	%r8, 120(%rsp)
	xorq	48(%rsp), %rdi
	movq	%r12, %r13
	xorq	%rdi, %r13
	orq	%r12, %rdi
	movq	128(%rsp), %rsi
	xorq	40(%rsp), %rsi
	movq	%rbp, %r12
	xorq	%rsi, %r12
	orq	%rbp, %rsi
	movq	%rsi, 128(%rsp)
	xorq	32(%rsp), %rcx
	movq	%rbx, %rbp
	xorq	%rcx, %rbp
	orq	%rbx, %rcx
	movq	96(%rsp), %rdx
	xorq	64(%rsp), %rdx
	movq	80(%rsp), %r8
	movq	%r8, %rbx
	xorq	%rdx, %rbx
	orq	%r8, %rdx
	xorq	(%rsp), %rax
	movq	88(%rsp), %rsi
	movq	%rsi, %r8
	xorq	%rax, %r8
	movq	%r8, 80(%rsp)
	orq	%rsi, %rax
	xorq	24(%rsp), %r11
	movq	104(%rsp), %r8
	movq	%r8, %rsi
	xorq	%r11, %rsi
	movq	%rsi, 88(%rsp)
	orq	%r8, %r11
	xorq	16(%rsp), %r10
	movq	112(%rsp), %rsi
	movq	%rsi, %r8
	xorq	%r10, %r8
	movq	%r8, 96(%rsp)
	orq	%rsi, %r10
	xorq	8(%rsp), %r9
	movq	%r15, %rsi
	xorq	%r9, %rsi
	movq	%rsi, 104(%rsp)
	orq	%r15, %r9
	movq	120(%rsp), %r8
	xorq	56(%rsp), %r8
	movq	%r14, %r15
	xorq	%r8, %r15
	orq	%r14, %r8
	xorq	48(%rsp), %rdi
	movq	%r13, %r14
	xorq	%rdi, %r14
	orq	%r13, %rdi
	movq	128(%rsp), %rsi
	xorq	40(%rsp), %rsi
	movq	%r12, %r13
	xorq	%rsi, %r13
	orq	%r12, %rsi
	xorq	32(%rsp), %rcx
	movq	%rbp, %r12
	xorq	%rcx, %r12
	orq	%rbp, %rcx
	movq	%rcx, 112(%rsp)
	xorq	64(%rsp), %rdx
	movq	%rbx, %rbp
	xorq	%rdx, %rbp
	orq	%rbx, %rdx
	movq	%rdx, 120(%rsp)
	xorq	(%rsp), %rax
	movq	80(%rsp), %rcx
	movq	%rcx, %rbx
	xorq	%rax, %rbx
	orq	%rcx, %rax
	xorq	24(%rsp), %r11
	movq	88(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r11, %rcx
	movq	%rcx, 80(%rsp)
	orq	%rdx, %r11
	xorq	16(%rsp), %r10
	movq	96(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r10, %rcx
	movq	%rcx, 88(%rsp)
	orq	%rdx, %r10
	xorq	8(%rsp), %r9
	movq	104(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r9, %rcx
	movq	%rcx, 96(%rsp)
	orq	%rdx, %r9
	xorq	56(%rsp), %r8
	movq	%r15, %rdx
	xorq	%r8, %rdx
	movq	%rdx, 104(%rsp)
	orq	%r15, %r8
	xorq	48(%rsp), %rdi
	movq	%r14, %r15
	xorq	%rdi, %r15
	orq	%r14, %rdi
	xorq	40(%rsp), %rsi
	movq	%r13, %r14
	xorq	%rsi, %r14
	orq	%r13, %rsi
	movq	%rsi, 128(%rsp)
	movq	112(%rsp), %rcx
	xorq	32(%rsp), %rcx
	movq	%r12, %r13
	xorq	%rcx, %r13
	orq	%r12, %rcx
	movq	%rcx, 112(%rsp)
	movq	120(%rsp), %rdx
	xorq	64(%rsp), %rdx
	movq	%rbp, %r12
	xorq	%rdx, %r12
	orq	%rbp, %rdx
	xorq	(%rsp), %rax
	movq	%rbx, %rbp
	xorq	%rax, %rbp
	orq	%rbx, %rax
	xorq	24(%rsp), %r11
	movq	80(%rsp), %rcx
	movq	%rcx, %rbx
	xorq	%r11, %rbx
	orq	%rcx, %r11
	xorq	16(%rsp), %r10
	movq	88(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%r10, %rsi
	movq	%rsi, 80(%rsp)
	orq	%rcx, %r10
	xorq	8(%rsp), %r9
	movq	96(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%r9, %rsi
	movq	%rsi, 88(%rsp)
	orq	%rcx, %r9
	xorq	56(%rsp), %r8
	movq	104(%rsp), %rsi
	movq	%rsi, %rcx
	xorq	%r8, %rcx
	movq	%rcx, 96(%rsp)
	orq	%rsi, %r8
	xorq	48(%rsp), %rdi
	movq	%r15, %rcx
	xorq	%rdi, %rcx
	movq	%rcx, 104(%rsp)
	orq	%r15, %rdi
	movq	128(%rsp), %rsi
	xorq	40(%rsp), %rsi
	movq	%r14, %r15
	xorq	%rsi, %r15
	orq	%r14, %rsi
	movq	%rsi, 120(%rsp)
	movq	112(%rsp), %rcx
	xorq	32(%rsp), %rcx
	movq	%r13, %r14
	xorq	%rcx, %r14
	orq	%r13, %rcx
	xorq	64(%rsp), %rdx
	movq	%r12, %r13
	xorq	%rdx, %r13
	orq	%r12, %rdx
	xorq	(%rsp), %rax
	movq	%rbp, %r12
	xorq	%rax, %r12
	orq	%rbp, %rax
	xorq	24(%rsp), %r11
	movq	%rbx, %rbp
	xorq	%r11, %rbp
	orq	%rbx, %r11
	movq	%r11, 112(%rsp)
	xorq	16(%rsp), %r10
	movq	80(%rsp), %r11
	movq	%r11, %rbx
	xorq	%r10, %rbx
	orq	%r11, %r10
	xorq	8(%rsp), %r9
	movq	88(%rsp), %r11
	movq	%r11, %rsi
	xorq	%r9, %rsi
	movq	%rsi, 80(%rsp)
	orq	%r11, %r9
	xorq	56(%rsp), %r8
	movq	96(%rsp), %rsi
	movq	%rsi, %r11
	xorq	%r8, %r11
	movq	%r11, 88(%rsp)
	orq	%rsi, %r8
	xorq	48(%rsp), %rdi
	movq	104(%rsp), %rsi
	movq	%rsi, %r11
	xorq	%rdi, %r11
	movq	%r11, 96(%rsp)
	orq	%rsi, %rdi
	movq	120(%rsp), %rsi
	xorq	40(%rsp), %rsi
	movq	%r15, %r11
	xorq	%rsi, %r11
	movq	%r11, 104(%rsp)
	orq	%r15, %rsi
	xorq	32(%rsp), %rcx
	movq	%r14, %r15
	xorq	%rcx, %r15
	orq	%r14, %rcx
	movq	%rcx, 120(%rsp)
	xorq	64(%rsp), %rdx
	movq	%r13, %r14
	xorq	%rdx, %r14
	orq	%r13, %rdx
	movq	%rdx, 128(%rsp)
	xorq	(%rsp), %rax
	movq	%r12, %r13
	xorq	%rax, %r13
	orq	%r12, %rax
	movq	112(%rsp), %r11
	xorq	24(%rsp), %r11
	movq	%rbp, %r12
	xorq	%r11, %r12
	orq	%r11, %rbp
	xorq	16(%rsp), %r10
	movq	%rbx, %rcx
	xorq	%r10, %rcx
	movq	%rcx, %r11
	orq	%r10, %rbx
	xorq	8(%rsp), %r9
	movq	80(%rsp), %rdx
	movq	%rdx, %r10
	xorq	%r9, %r10
	orq	%rdx, %r9
	xorq	56(%rsp), %r8
	movq	88(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r8, %rdx
	movq	%rdx, 80(%rsp)
	orq	%rcx, %r8
	xorq	48(%rsp), %rdi
	movq	96(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%rdi, %rcx
	movq	%rcx, 136(%rsp)
	orq	%rdx, %rdi
	xorq	40(%rsp), %rsi
	movq	104(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%rsi, %rcx
	movq	%rcx, 144(%rsp)
	orq	%rdx, %rsi
	movq	120(%rsp), %rcx
	xorq	32(%rsp), %rcx
	movq	%r15, %rdx
	xorq	%rcx, %rdx
	movq	%rdx, 152(%rsp)
	orq	%rcx, %r15
	movq	128(%rsp), %rdx
	xorq	64(%rsp), %rdx
	movq	%r14, %rcx
	xorq	%rdx, %rcx
	movq	%rcx, 160(%rsp)
	orq	%r14, %rdx
	xorq	(%rsp), %rax
	movq	%r13, %r14
	xorq	%rax, %r14
	movq	%r14, 88(%rsp)
	orq	%rax, %r13
	movq	%r13, %rcx
	movq	24(%rsp), %rax
	xorq	%rbp, %rax
	movq	%r12, %r14
	xorq	%rax, %r14
	movq	%r14, 96(%rsp)
	movq	%r12, %r14
	orq	%rax, %r14
	movq	16(%rsp), %rax
	xorq	%rbx, %rax
	movq	%r11, %rbp
	xorq	%rax, %rbp
	movq	%rbp, 104(%rsp)
	orq	%rax, %r11
	movq	%r11, %r13
	movq	8(%rsp), %rax
	xorq	%r9, %rax
	movq	%r10, %r11
	xorq	%rax, %r11
	movq	%r11, 112(%rsp)
	orq	%rax, %r10
	movq	%r10, %r12
	movq	56(%rsp), %rax
	xorq	%r8, %rax
	movq	80(%rsp), %rbx
	movq	%rbx, %r10
	xorq	%rax, %r10
	movq	%r10, 120(%rsp)
	orq	%rax, %rbx
	movq	%rbx, %rbp
	movq	48(%rsp), %rax
	xorq	%rdi, %rax
	movq	136(%rsp), %rbx
	movq	%rbx, %rdi
	xorq	%rax, %rdi
	movq	%rdi, 128(%rsp)
	orq	%rax, %rbx
	movq	40(%rsp), %rax
	xorq	%rsi, %rax
	movq	144(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%rax, %rdi
	movq	%rdi, 136(%rsp)
	orq	%rax, %rsi
	xorq	32(%rsp), %r15
	movq	152(%rsp), %rdi
	movq	%rdi, %r10
	xorq	%r15, %r10
	movq	%r10, 144(%rsp)
	orq	%rdi, %r15
	xorq	64(%rsp), %rdx
	movq	160(%rsp), %rdi
	movq	%rdi, %r10
	xorq	%rdx, %r10
	movq	%r10, 152(%rsp)
	orq	%rdi, %rdx
	movq	%rdx, 80(%rsp)
	movq	(%rsp), %rax
	cmpq	232(%rsp), %rax
	jne	.L457
	movq	%rsi, (%rsp)
	movq	%r15, 8(%rsp)
	movq	%rcx, %r15
.L456:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	(%rsp), %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	call	use_int@PLT
	addq	$248, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE161:
	.size	int64_bit_8, .-int64_bit_8
	.globl	int64_bit_9
	.type	int64_bit_9, @function
int64_bit_9:
.LFB162:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$264, %rsp
	.cfi_def_cfa_offset 320
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r15
	movslq	%ecx, %rcx
	leaq	1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 96(%rsp)
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 88(%rsp)
	movslq	16(%rdx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %r14
	addq	%rax, %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 104(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 176(%rsp)
	movl	20(%rdx), %ecx
	leal	-1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r13
	movslq	%ecx, %rcx
	leaq	-1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 112(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 184(%rsp)
	movl	24(%rdx), %ecx
	leal	-2(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r12
	movslq	%ecx, %rcx
	leaq	-2(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 120(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 192(%rsp)
	movl	28(%rdx), %ecx
	leal	-3(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %rbp
	movslq	%ecx, %rcx
	leaq	-3(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 128(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 200(%rsp)
	movl	32(%rdx), %ecx
	leal	-4(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r11
	movq	%rsi, 8(%rsp)
	movslq	%ecx, %rcx
	leaq	-4(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 136(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 208(%rsp)
	movl	36(%rdx), %ecx
	leal	-5(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r10
	movq	%rsi, 32(%rsp)
	movslq	%ecx, %rcx
	leaq	-5(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 144(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 216(%rsp)
	movl	40(%rdx), %ecx
	leal	-6(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r9
	movq	%rsi, 40(%rsp)
	movslq	%ecx, %rcx
	leaq	-6(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 152(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 224(%rsp)
	movl	44(%rdx), %ecx
	leal	-7(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 16(%rsp)
	movslq	%ecx, %rcx
	leaq	-7(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 160(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 232(%rsp)
	movl	48(%rdx), %edx
	leal	-8(%rdx), %ecx
	movslq	%ecx, %rsi
	salq	$32, %rcx
	orq	%rsi, %rcx
	movq	%rcx, 24(%rsp)
	movslq	%edx, %rdx
	leaq	-8(%rax,%rdx), %rdx
	movq	%rdx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rcx
	orq	%rdx, %rcx
	movq	%rcx, 168(%rsp)
	leaq	0(,%rcx,4), %rdi
	movq	%rdi, 240(%rsp)
	testq	%rax, %rax
	je	.L461
	leaq	-1(%rbx), %r8
	subq	%rax, %rbx
	leaq	-1(%rbx), %rax
	movq	%rax, 248(%rsp)
	movq	%r8, 8(%rsp)
	movq	%r15, %rcx
	movq	%r14, %rbx
	movq	%r11, %r8
	movq	%r10, %rdi
	movq	%r9, %rsi
	movq	16(%rsp), %r14
	movq	24(%rsp), %rdx
.L462:
	subq	$1, 8(%rsp)
	movq	8(%rsp), %r15
	movq	176(%rsp), %rax
	addq	%r15, %rax
	movq	88(%rsp), %r9
	subq	%r9, %rax
	movq	%rax, %r11
	movq	184(%rsp), %rax
	addq	%r15, %rax
	subq	%r9, %rax
	movq	%rax, %r10
	movq	192(%rsp), %rax
	addq	%r15, %rax
	subq	%r9, %rax
	movq	%rax, %r9
	movq	200(%rsp), %rax
	addq	%r15, %rax
	subq	88(%rsp), %rax
	movq	%rax, 72(%rsp)
	movq	208(%rsp), %rax
	addq	%r15, %rax
	subq	88(%rsp), %rax
	movq	%rax, 64(%rsp)
	movq	216(%rsp), %rax
	addq	%r15, %rax
	subq	88(%rsp), %rax
	movq	%rax, 56(%rsp)
	movq	224(%rsp), %rax
	addq	%r15, %rax
	subq	88(%rsp), %rax
	movq	%rax, 48(%rsp)
	movq	232(%rsp), %rax
	addq	%r15, %rax
	subq	88(%rsp), %rax
	movq	%rax, 40(%rsp)
	movq	240(%rsp), %rax
	addq	%r15, %rax
	subq	88(%rsp), %rax
	movq	%rax, 80(%rsp)
	xorq	%r15, %rcx
	movq	%rcx, %rax
	movq	96(%rsp), %rcx
	movq	%rcx, %r15
	xorq	%rax, %r15
	orq	%rcx, %rax
	movq	%r11, 32(%rsp)
	xorq	%r11, %rbx
	movq	104(%rsp), %r11
	movq	%r11, %rcx
	xorq	%rbx, %rcx
	movq	%rcx, 96(%rsp)
	orq	%r11, %rbx
	movq	%r10, 24(%rsp)
	xorq	%r10, %r13
	movq	%r13, %r11
	movq	112(%rsp), %r10
	movq	%r10, %r13
	xorq	%r11, %r13
	orq	%r10, %r11
	movq	%r9, 16(%rsp)
	xorq	%r9, %r12
	movq	%r12, %r10
	movq	120(%rsp), %r9
	movq	%r9, %r12
	xorq	%r10, %r12
	orq	%r9, %r10
	xorq	72(%rsp), %rbp
	movq	%rbp, %r9
	movq	128(%rsp), %rcx
	movq	%rcx, %rbp
	xorq	%r9, %rbp
	orq	%rcx, %r9
	movq	%r9, 104(%rsp)
	xorq	64(%rsp), %r8
	movq	136(%rsp), %r9
	movq	%r9, %rcx
	xorq	%r8, %rcx
	movq	%rcx, 112(%rsp)
	orq	%r9, %r8
	xorq	56(%rsp), %rdi
	movq	144(%rsp), %rcx
	movq	%rcx, %r9
	xorq	%rdi, %r9
	movq	%r9, 120(%rsp)
	orq	%rcx, %rdi
	xorq	48(%rsp), %rsi
	movq	152(%rsp), %rcx
	movq	%rcx, %r9
	xorq	%rsi, %r9
	movq	%r9, 128(%rsp)
	orq	%rcx, %rsi
	xorq	40(%rsp), %r14
	movq	%r14, %rcx
	movq	160(%rsp), %r14
	movq	%r14, %r9
	xorq	%rcx, %r9
	movq	%r9, 136(%rsp)
	movq	%rcx, %r9
	orq	%r14, %r9
	movq	%r9, 144(%rsp)
	xorq	80(%rsp), %rdx
	movq	168(%rsp), %r14
	movq	%r14, %r9
	xorq	%rdx, %r9
	movq	%r9, 152(%rsp)
	orq	%r14, %rdx
	xorq	8(%rsp), %rax
	movq	%r15, %r14
	xorq	%rax, %r14
	movq	%r14, 160(%rsp)
	orq	%r15, %rax
	xorq	32(%rsp), %rbx
	movq	96(%rsp), %rcx
	movq	%rcx, %r15
	xorq	%rbx, %r15
	orq	%rcx, %rbx
	xorq	24(%rsp), %r11
	movq	%r13, %r14
	xorq	%r11, %r14
	orq	%r13, %r11
	xorq	16(%rsp), %r10
	movq	%r12, %r13
	xorq	%r10, %r13
	orq	%r12, %r10
	movq	104(%rsp), %r9
	xorq	72(%rsp), %r9
	movq	%rbp, %r12
	xorq	%r9, %r12
	orq	%rbp, %r9
	movq	%r9, 96(%rsp)
	xorq	64(%rsp), %r8
	movq	112(%rsp), %rcx
	movq	%rcx, %rbp
	xorq	%r8, %rbp
	orq	%rcx, %r8
	xorq	56(%rsp), %rdi
	movq	120(%rsp), %r9
	movq	%r9, %rcx
	xorq	%rdi, %rcx
	movq	%rcx, 104(%rsp)
	orq	%r9, %rdi
	movq	%rdi, 112(%rsp)
	xorq	48(%rsp), %rsi
	movq	128(%rsp), %rdi
	movq	%rdi, %r9
	xorq	%rsi, %r9
	movq	%r9, 120(%rsp)
	orq	%rdi, %rsi
	movq	144(%rsp), %rcx
	xorq	40(%rsp), %rcx
	movq	136(%rsp), %r9
	movq	%r9, %rdi
	xorq	%rcx, %rdi
	movq	%rdi, 128(%rsp)
	movq	%rcx, %rdi
	orq	%r9, %rdi
	movq	%rdi, 136(%rsp)
	xorq	80(%rsp), %rdx
	movq	152(%rsp), %r9
	movq	%r9, %rcx
	xorq	%rdx, %rcx
	movq	%rcx, 144(%rsp)
	orq	%r9, %rdx
	xorq	8(%rsp), %rax
	movq	160(%rsp), %rdi
	movq	%rdi, %r9
	xorq	%rax, %r9
	movq	%r9, 152(%rsp)
	orq	%rdi, %rax
	xorq	32(%rsp), %rbx
	movq	%r15, %r9
	xorq	%rbx, %r9
	movq	%r9, 160(%rsp)
	orq	%r15, %rbx
	xorq	24(%rsp), %r11
	movq	%r14, %r15
	xorq	%r11, %r15
	orq	%r14, %r11
	xorq	16(%rsp), %r10
	movq	%r13, %r14
	xorq	%r10, %r14
	orq	%r13, %r10
	movq	96(%rsp), %r9
	xorq	72(%rsp), %r9
	movq	%r12, %r13
	xorq	%r9, %r13
	orq	%r12, %r9
	xorq	64(%rsp), %r8
	movq	%rbp, %r12
	xorq	%r8, %r12
	orq	%rbp, %r8
	movq	%r8, 96(%rsp)
	movq	112(%rsp), %rdi
	xorq	56(%rsp), %rdi
	movq	104(%rsp), %rcx
	movq	%rcx, %rbp
	xorq	%rdi, %rbp
	orq	%rcx, %rdi
	movq	%rdi, 104(%rsp)
	xorq	48(%rsp), %rsi
	movq	120(%rsp), %rcx
	movq	%rcx, %rdi
	xorq	%rsi, %rdi
	movq	%rdi, 112(%rsp)
	orq	%rcx, %rsi
	movq	136(%rsp), %rcx
	xorq	40(%rsp), %rcx
	movq	128(%rsp), %rdi
	movq	%rdi, %r8
	xorq	%rcx, %r8
	movq	%r8, 120(%rsp)
	orq	%rdi, %rcx
	xorq	80(%rsp), %rdx
	movq	144(%rsp), %r8
	movq	%r8, %rdi
	xorq	%rdx, %rdi
	movq	%rdi, 128(%rsp)
	orq	%r8, %rdx
	movq	%rdx, 136(%rsp)
	xorq	8(%rsp), %rax
	movq	152(%rsp), %rdx
	movq	%rdx, %r8
	xorq	%rax, %r8
	movq	%r8, 144(%rsp)
	orq	%rdx, %rax
	xorq	32(%rsp), %rbx
	movq	160(%rsp), %r8
	movq	%r8, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 152(%rsp)
	orq	%r8, %rbx
	xorq	24(%rsp), %r11
	movq	%r15, %rdx
	xorq	%r11, %rdx
	movq	%rdx, 160(%rsp)
	orq	%r15, %r11
	xorq	16(%rsp), %r10
	movq	%r14, %r15
	xorq	%r10, %r15
	orq	%r14, %r10
	xorq	72(%rsp), %r9
	movq	%r13, %r14
	xorq	%r9, %r14
	orq	%r13, %r9
	movq	96(%rsp), %r8
	xorq	64(%rsp), %r8
	movq	%r12, %r13
	xorq	%r8, %r13
	orq	%r12, %r8
	movq	%r8, 96(%rsp)
	movq	104(%rsp), %rdi
	xorq	56(%rsp), %rdi
	movq	%rbp, %r12
	xorq	%rdi, %r12
	orq	%rbp, %rdi
	movq	%rdi, 104(%rsp)
	xorq	48(%rsp), %rsi
	movq	112(%rsp), %rdi
	movq	%rdi, %rbp
	xorq	%rsi, %rbp
	orq	%rdi, %rsi
	xorq	40(%rsp), %rcx
	movq	120(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%rcx, %rdi
	movq	%rdi, 112(%rsp)
	orq	%rdx, %rcx
	movq	136(%rsp), %rdx
	xorq	80(%rsp), %rdx
	movq	128(%rsp), %r8
	movq	%r8, %rdi
	xorq	%rdx, %rdi
	movq	%rdi, 120(%rsp)
	orq	%r8, %rdx
	xorq	8(%rsp), %rax
	movq	144(%rsp), %r8
	movq	%r8, %rdi
	xorq	%rax, %rdi
	movq	%rdi, 128(%rsp)
	orq	%r8, %rax
	xorq	32(%rsp), %rbx
	movq	152(%rsp), %rdi
	movq	%rdi, %r8
	xorq	%rbx, %r8
	movq	%r8, 136(%rsp)
	orq	%rdi, %rbx
	xorq	24(%rsp), %r11
	movq	160(%rsp), %r8
	movq	%r8, %rdi
	xorq	%r11, %rdi
	movq	%rdi, 144(%rsp)
	orq	%r8, %r11
	xorq	16(%rsp), %r10
	movq	%r15, %r8
	xorq	%r10, %r8
	movq	%r8, 152(%rsp)
	orq	%r15, %r10
	xorq	72(%rsp), %r9
	movq	%r14, %r15
	xorq	%r9, %r15
	orq	%r14, %r9
	movq	96(%rsp), %r8
	xorq	64(%rsp), %r8
	movq	%r13, %r14
	xorq	%r8, %r14
	orq	%r13, %r8
	movq	104(%rsp), %rdi
	xorq	56(%rsp), %rdi
	movq	%r12, %r13
	xorq	%rdi, %r13
	orq	%r12, %rdi
	movq	%rdi, 96(%rsp)
	xorq	48(%rsp), %rsi
	movq	%rbp, %r12
	xorq	%rsi, %r12
	orq	%rbp, %rsi
	movq	%rsi, 104(%rsp)
	xorq	40(%rsp), %rcx
	movq	112(%rsp), %rsi
	movq	%rsi, %rbp
	xorq	%rcx, %rbp
	orq	%rsi, %rcx
	xorq	80(%rsp), %rdx
	movq	120(%rsp), %rdi
	movq	%rdi, %rsi
	xorq	%rdx, %rsi
	movq	%rsi, 112(%rsp)
	orq	%rdi, %rdx
	xorq	8(%rsp), %rax
	movq	128(%rsp), %rdi
	movq	%rdi, %rsi
	xorq	%rax, %rsi
	movq	%rsi, 120(%rsp)
	orq	%rdi, %rax
	xorq	32(%rsp), %rbx
	movq	136(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%rbx, %rdi
	movq	%rdi, 128(%rsp)
	orq	%rsi, %rbx
	xorq	24(%rsp), %r11
	movq	144(%rsp), %rdi
	movq	%rdi, %rsi
	xorq	%r11, %rsi
	movq	%rsi, 136(%rsp)
	orq	%rdi, %r11
	xorq	16(%rsp), %r10
	movq	152(%rsp), %rdi
	movq	%rdi, %rsi
	xorq	%r10, %rsi
	movq	%rsi, 144(%rsp)
	orq	%rdi, %r10
	xorq	72(%rsp), %r9
	movq	%r15, %rdi
	xorq	%r9, %rdi
	movq	%rdi, 152(%rsp)
	orq	%r15, %r9
	xorq	64(%rsp), %r8
	movq	%r14, %r15
	xorq	%r8, %r15
	orq	%r14, %r8
	movq	96(%rsp), %rdi
	xorq	56(%rsp), %rdi
	movq	%r13, %r14
	xorq	%rdi, %r14
	orq	%r13, %rdi
	movq	104(%rsp), %rsi
	xorq	48(%rsp), %rsi
	movq	%r12, %r13
	xorq	%rsi, %r13
	orq	%r12, %rsi
	movq	%rsi, 96(%rsp)
	xorq	40(%rsp), %rcx
	movq	%rbp, %r12
	xorq	%rcx, %r12
	orq	%rbp, %rcx
	movq	%rcx, 104(%rsp)
	xorq	80(%rsp), %rdx
	movq	112(%rsp), %rsi
	movq	%rsi, %rbp
	xorq	%rdx, %rbp
	orq	%rsi, %rdx
	xorq	8(%rsp), %rax
	movq	120(%rsp), %rsi
	movq	%rsi, %rcx
	xorq	%rax, %rcx
	movq	%rcx, 112(%rsp)
	orq	%rsi, %rax
	xorq	32(%rsp), %rbx
	movq	128(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%rbx, %rsi
	movq	%rsi, 120(%rsp)
	orq	%rcx, %rbx
	xorq	24(%rsp), %r11
	movq	136(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%r11, %rsi
	movq	%rsi, 128(%rsp)
	orq	%rcx, %r11
	xorq	16(%rsp), %r10
	movq	144(%rsp), %rsi
	movq	%rsi, %rcx
	xorq	%r10, %rcx
	movq	%rcx, 136(%rsp)
	orq	%rsi, %r10
	xorq	72(%rsp), %r9
	movq	152(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%r9, %rsi
	movq	%rsi, 144(%rsp)
	orq	%rcx, %r9
	xorq	64(%rsp), %r8
	movq	%r15, %rcx
	xorq	%r8, %rcx
	movq	%rcx, 152(%rsp)
	orq	%r15, %r8
	xorq	56(%rsp), %rdi
	movq	%r14, %r15
	xorq	%rdi, %r15
	orq	%r14, %rdi
	movq	96(%rsp), %rsi
	xorq	48(%rsp), %rsi
	movq	%r13, %r14
	xorq	%rsi, %r14
	orq	%r13, %rsi
	movq	%rsi, 96(%rsp)
	movq	104(%rsp), %rcx
	xorq	40(%rsp), %rcx
	movq	%r12, %r13
	xorq	%rcx, %r13
	orq	%r12, %rcx
	xorq	80(%rsp), %rdx
	movq	%rbp, %r12
	xorq	%rdx, %r12
	orq	%rbp, %rdx
	movq	%rdx, 104(%rsp)
	xorq	8(%rsp), %rax
	movq	112(%rsp), %rdx
	movq	%rdx, %rbp
	xorq	%rax, %rbp
	orq	%rdx, %rax
	xorq	32(%rsp), %rbx
	movq	120(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 112(%rsp)
	orq	%rsi, %rbx
	xorq	24(%rsp), %r11
	movq	128(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r11, %rdx
	movq	%rdx, 120(%rsp)
	orq	%rsi, %r11
	xorq	16(%rsp), %r10
	movq	136(%rsp), %rdx
	movq	%rdx, %rsi
	xorq	%r10, %rsi
	movq	%rsi, 128(%rsp)
	orq	%rdx, %r10
	xorq	72(%rsp), %r9
	movq	144(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r9, %rdx
	movq	%rdx, 136(%rsp)
	orq	%rsi, %r9
	xorq	64(%rsp), %r8
	movq	152(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r8, %rdx
	movq	%rdx, 144(%rsp)
	orq	%rsi, %r8
	xorq	56(%rsp), %rdi
	movq	%r15, %rsi
	xorq	%rdi, %rsi
	movq	%rsi, 152(%rsp)
	orq	%r15, %rdi
	movq	%rdi, 160(%rsp)
	movq	96(%rsp), %rsi
	xorq	48(%rsp), %rsi
	movq	%r14, %r15
	xorq	%rsi, %r15
	orq	%r14, %rsi
	xorq	40(%rsp), %rcx
	movq	%r13, %r14
	xorq	%rcx, %r14
	orq	%r13, %rcx
	movq	%rcx, 96(%rsp)
	movq	104(%rsp), %rdx
	xorq	80(%rsp), %rdx
	movq	%r12, %rdi
	xorq	%rdx, %rdi
	movq	%rdi, 104(%rsp)
	orq	%r12, %rdx
	movq	%rdx, 168(%rsp)
	xorq	8(%rsp), %rax
	movq	%rbp, %r12
	xorq	%rax, %r12
	orq	%rbp, %rax
	xorq	32(%rsp), %rbx
	movq	112(%rsp), %r13
	movq	%r13, %rbp
	xorq	%rbx, %rbp
	orq	%r13, %rbx
	xorq	24(%rsp), %r11
	movq	120(%rsp), %rcx
	movq	%rcx, %r13
	xorq	%r11, %r13
	orq	%rcx, %r11
	xorq	16(%rsp), %r10
	movq	128(%rsp), %rdi
	movq	%rdi, %rcx
	xorq	%r10, %rcx
	movq	%rcx, 120(%rsp)
	orq	%rdi, %r10
	xorq	72(%rsp), %r9
	movq	136(%rsp), %rcx
	movq	%rcx, %rdi
	xorq	%r9, %rdi
	movq	%rdi, 128(%rsp)
	orq	%rcx, %r9
	xorq	64(%rsp), %r8
	movq	144(%rsp), %rcx
	movq	%rcx, %rdi
	xorq	%r8, %rdi
	movq	%rdi, 136(%rsp)
	orq	%rcx, %r8
	movq	160(%rsp), %rdi
	xorq	56(%rsp), %rdi
	movq	152(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%rdi, %rdx
	movq	%rdx, 144(%rsp)
	orq	%rcx, %rdi
	xorq	48(%rsp), %rsi
	movq	%r15, %rdx
	xorq	%rsi, %rdx
	movq	%rdx, 152(%rsp)
	orq	%rsi, %r15
	movq	96(%rsp), %rcx
	xorq	40(%rsp), %rcx
	movq	%r14, %rdx
	xorq	%rcx, %rdx
	movq	%rdx, 160(%rsp)
	orq	%rcx, %r14
	movq	168(%rsp), %rdx
	xorq	80(%rsp), %rdx
	movq	104(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%rdx, %rsi
	movq	%rsi, 168(%rsp)
	orq	%rcx, %rdx
	xorq	8(%rsp), %rax
	movq	%r12, %rcx
	xorq	%rax, %rcx
	movq	%rcx, 96(%rsp)
	orq	%rax, %r12
	movq	%r12, %rcx
	movq	32(%rsp), %rax
	xorq	%rbx, %rax
	movq	%rbp, %rbx
	xorq	%rax, %rbx
	movq	%rbx, 104(%rsp)
	movq	%rbp, %rbx
	orq	%rax, %rbx
	movq	24(%rsp), %rax
	xorq	%r11, %rax
	movq	%r13, %rbp
	xorq	%rax, %rbp
	movq	%rbp, 112(%rsp)
	orq	%rax, %r13
	movq	16(%rsp), %rax
	xorq	%r10, %rax
	movq	120(%rsp), %rsi
	movq	%rsi, %r10
	xorq	%rax, %r10
	movq	%r10, 120(%rsp)
	orq	%rax, %rsi
	movq	%rsi, %r12
	movq	72(%rsp), %rax
	xorq	%r9, %rax
	movq	128(%rsp), %rsi
	movq	%rsi, %r11
	xorq	%rax, %r11
	movq	%r11, 128(%rsp)
	orq	%rax, %rsi
	movq	%rsi, %rbp
	movq	64(%rsp), %rax
	xorq	%r8, %rax
	movq	136(%rsp), %rsi
	movq	%rsi, %r10
	xorq	%rax, %r10
	movq	%r10, 136(%rsp)
	orq	%rax, %rsi
	movq	%rsi, %r8
	movq	56(%rsp), %rax
	xorq	%rdi, %rax
	movq	144(%rsp), %rdi
	movq	%rdi, %rsi
	xorq	%rax, %rsi
	movq	%rsi, 144(%rsp)
	orq	%rax, %rdi
	xorq	48(%rsp), %r15
	movq	152(%rsp), %r10
	movq	%r10, %r9
	xorq	%r15, %r9
	movq	%r9, 152(%rsp)
	orq	%r15, %r10
	movq	%r10, %rsi
	xorq	40(%rsp), %r14
	movq	160(%rsp), %r10
	movq	%r10, %r15
	xorq	%r14, %r15
	movq	%r15, 160(%rsp)
	orq	%r10, %r14
	xorq	80(%rsp), %rdx
	movq	168(%rsp), %r11
	movq	%r11, %r10
	xorq	%rdx, %r10
	movq	%r10, 168(%rsp)
	orq	%r11, %rdx
	movq	8(%rsp), %rax
	cmpq	248(%rsp), %rax
	jne	.L462
	movq	%rcx, %r15
	movq	%r8, 8(%rsp)
	movq	%rdi, 32(%rsp)
	movq	%rsi, 40(%rsp)
	movq	%r14, 16(%rsp)
	movq	%rbx, %r14
	movq	%rdx, 24(%rsp)
.L461:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	call	use_int@PLT
	addq	$264, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE162:
	.size	int64_bit_9, .-int64_bit_9
	.globl	int64_bit_10
	.type	int64_bit_10, @function
int64_bit_10:
.LFB163:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$296, %rsp
	.cfi_def_cfa_offset 352
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r15
	movslq	%ecx, %rcx
	leaq	1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 104(%rsp)
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 88(%rsp)
	movslq	16(%rdx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %r14
	addq	%rax, %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 112(%rsp)
	salq	$2, %rsi
	movq	%rsi, 200(%rsp)
	movl	20(%rdx), %ecx
	leal	-1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r13
	movslq	%ecx, %rcx
	leaq	-1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 120(%rsp)
	salq	$2, %rsi
	movq	%rsi, 208(%rsp)
	movl	24(%rdx), %ecx
	leal	-2(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r12
	movslq	%ecx, %rcx
	leaq	-2(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 128(%rsp)
	salq	$2, %rsi
	movq	%rsi, 216(%rsp)
	movl	28(%rdx), %ecx
	leal	-3(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r10
	movq	%rsi, (%rsp)
	movslq	%ecx, %rcx
	leaq	-3(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 136(%rsp)
	salq	$2, %rsi
	movq	%rsi, 224(%rsp)
	movl	32(%rdx), %ecx
	leal	-4(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r9
	movq	%rsi, 32(%rsp)
	movslq	%ecx, %rcx
	leaq	-4(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 144(%rsp)
	salq	$2, %rsi
	movq	%rsi, 232(%rsp)
	movl	36(%rdx), %ecx
	leal	-5(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r11
	movq	%rsi, 40(%rsp)
	movslq	%ecx, %rcx
	leaq	-5(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 152(%rsp)
	salq	$2, %rsi
	movq	%rsi, 240(%rsp)
	movl	40(%rdx), %ecx
	leal	-6(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 8(%rsp)
	movslq	%ecx, %rcx
	leaq	-6(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 160(%rsp)
	leaq	0(,%rsi,4), %rcx
	movq	%rcx, 248(%rsp)
	movl	44(%rdx), %ecx
	leal	-7(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 16(%rsp)
	movslq	%ecx, %rcx
	leaq	-7(%rax,%rcx), %rcx
	movq	%rcx, %rbp
	salq	$32, %rbp
	movq	%rbp, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %rcx
	movq	%rsi, 168(%rsp)
	salq	$2, %rcx
	movq	%rcx, 256(%rsp)
	movl	48(%rdx), %ecx
	leal	-8(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	movq	%rsi, %rbp
	orq	%rdi, %rbp
	movq	%rbp, 24(%rsp)
	movslq	%ecx, %rcx
	leaq	-8(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %rcx
	movq	%rsi, 176(%rsp)
	salq	$2, %rcx
	movq	%rcx, 264(%rsp)
	movl	52(%rdx), %edx
	leal	-9(%rdx), %ecx
	movslq	%ecx, %rsi
	salq	$32, %rcx
	orq	%rsi, %rcx
	movq	%rcx, 96(%rsp)
	movslq	%edx, %rdx
	leaq	-9(%rax,%rdx), %rdx
	movq	%rdx, %rcx
	salq	$32, %rcx
	orq	%rdx, %rcx
	movq	%rcx, 184(%rsp)
	leaq	0(,%rcx,4), %rdx
	movq	%rdx, 272(%rsp)
	testq	%rax, %rax
	je	.L466
	leaq	-1(%rbx), %r8
	subq	%rax, %rbx
	leaq	-1(%rbx), %rax
	movq	%rax, 280(%rsp)
	movq	%r8, (%rsp)
	movq	%r15, %rcx
	movq	%r14, %rbp
	movq	%r13, %rbx
	movq	%r11, %r8
	movq	8(%rsp), %rdi
	movq	16(%rsp), %rsi
	movq	24(%rsp), %r13
.L467:
	subq	$1, (%rsp)
	movq	(%rsp), %r14
	movq	200(%rsp), %rax
	addq	%r14, %rax
	movq	88(%rsp), %r15
	subq	%r15, %rax
	movq	%rax, %rdx
	movq	208(%rsp), %rax
	addq	%r14, %rax
	subq	%r15, %rax
	movq	%rax, %r11
	movq	216(%rsp), %rax
	addq	%r14, %rax
	subq	%r15, %rax
	movq	%rax, 8(%rsp)
	movq	224(%rsp), %rax
	addq	%r14, %rax
	subq	%r15, %rax
	movq	%rax, 72(%rsp)
	movq	232(%rsp), %rax
	addq	%r14, %rax
	movq	%rax, %r15
	subq	88(%rsp), %r15
	movq	%r15, 64(%rsp)
	movq	240(%rsp), %rax
	addq	%r14, %rax
	subq	88(%rsp), %rax
	movq	%rax, 56(%rsp)
	movq	248(%rsp), %r15
	addq	%r14, %r15
	subq	88(%rsp), %r15
	movq	%r15, 48(%rsp)
	movq	256(%rsp), %rax
	addq	%r14, %rax
	subq	88(%rsp), %rax
	movq	%rax, 40(%rsp)
	movq	264(%rsp), %r15
	addq	%r14, %r15
	subq	88(%rsp), %r15
	movq	%r15, 32(%rsp)
	movq	272(%rsp), %rax
	addq	%r14, %rax
	subq	88(%rsp), %rax
	movq	%rax, 80(%rsp)
	xorq	%r14, %rcx
	movq	%rcx, %rax
	movq	104(%rsp), %rcx
	movq	%rcx, %r14
	xorq	%rax, %r14
	movq	%r14, %r15
	orq	%rcx, %rax
	movq	%rdx, 24(%rsp)
	xorq	%rdx, %rbp
	movq	112(%rsp), %rcx
	movq	%rcx, %r14
	xorq	%rbp, %r14
	orq	%rcx, %rbp
	movq	%r11, 16(%rsp)
	xorq	%r11, %rbx
	movq	120(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 104(%rsp)
	orq	%rcx, %rbx
	xorq	8(%rsp), %r12
	movq	%r12, %r11
	movq	128(%rsp), %rcx
	movq	%rcx, %r12
	xorq	%r11, %r12
	orq	%rcx, %r11
	xorq	72(%rsp), %r10
	movq	136(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r10, %rdx
	movq	%rdx, 112(%rsp)
	orq	%rcx, %r10
	xorq	64(%rsp), %r9
	movq	144(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r9, %rcx
	movq	%rcx, 120(%rsp)
	orq	%rdx, %r9
	xorq	56(%rsp), %r8
	movq	152(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r8, %rcx
	movq	%rcx, 128(%rsp)
	orq	%rdx, %r8
	movq	%r8, 136(%rsp)
	xorq	48(%rsp), %rdi
	movq	160(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%rdi, %rcx
	movq	%rcx, 144(%rsp)
	orq	%rdx, %rdi
	xorq	40(%rsp), %rsi
	movq	168(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%rsi, %rcx
	movq	%rcx, 152(%rsp)
	orq	%rdx, %rsi
	xorq	32(%rsp), %r13
	movq	%r13, %rcx
	movq	176(%rsp), %r13
	movq	%r13, %rdx
	xorq	%rcx, %rdx
	movq	%rdx, 160(%rsp)
	orq	%r13, %rcx
	movq	%rcx, 168(%rsp)
	movq	96(%rsp), %r13
	xorq	80(%rsp), %r13
	movq	%r13, %rdx
	movq	184(%rsp), %r13
	movq	%r13, %rcx
	xorq	%rdx, %rcx
	movq	%rcx, 96(%rsp)
	orq	%r13, %rdx
	xorq	(%rsp), %rax
	movq	%r15, %r13
	xorq	%rax, %r13
	movq	%r13, 176(%rsp)
	orq	%r15, %rax
	xorq	24(%rsp), %rbp
	movq	%r14, %r15
	xorq	%rbp, %r15
	orq	%r14, %rbp
	xorq	16(%rsp), %rbx
	movq	104(%rsp), %r8
	movq	%r8, %r14
	xorq	%rbx, %r14
	orq	%r8, %rbx
	xorq	8(%rsp), %r11
	movq	%r12, %r13
	xorq	%r11, %r13
	orq	%r12, %r11
	xorq	72(%rsp), %r10
	movq	112(%rsp), %r8
	movq	%r8, %r12
	xorq	%r10, %r12
	orq	%r8, %r10
	xorq	64(%rsp), %r9
	movq	120(%rsp), %rcx
	movq	%rcx, %r8
	xorq	%r9, %r8
	movq	%r8, 104(%rsp)
	orq	%rcx, %r9
	movq	%r9, 112(%rsp)
	movq	136(%rsp), %r8
	xorq	56(%rsp), %r8
	movq	128(%rsp), %rcx
	movq	%rcx, %r9
	xorq	%r8, %r9
	movq	%r9, 120(%rsp)
	orq	%rcx, %r8
	movq	%r8, 128(%rsp)
	xorq	48(%rsp), %rdi
	movq	144(%rsp), %rcx
	movq	%rcx, %r8
	xorq	%rdi, %r8
	movq	%r8, 136(%rsp)
	orq	%rcx, %rdi
	movq	%rdi, 144(%rsp)
	xorq	40(%rsp), %rsi
	movq	152(%rsp), %rdi
	movq	%rdi, %rcx
	xorq	%rsi, %rcx
	movq	%rcx, 152(%rsp)
	orq	%rdi, %rsi
	movq	%rsi, 184(%rsp)
	movq	168(%rsp), %rcx
	xorq	32(%rsp), %rcx
	movq	160(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%rcx, %rdi
	movq	%rdi, 160(%rsp)
	orq	%rsi, %rcx
	movq	%rcx, 168(%rsp)
	xorq	80(%rsp), %rdx
	movq	96(%rsp), %r9
	movq	%r9, %rsi
	xorq	%rdx, %rsi
	movq	%rsi, 96(%rsp)
	orq	%r9, %rdx
	xorq	(%rsp), %rax
	movq	176(%rsp), %rsi
	movq	%rsi, %rcx
	xorq	%rax, %rcx
	movq	%rcx, 176(%rsp)
	orq	%rsi, %rax
	xorq	24(%rsp), %rbp
	movq	%r15, %rsi
	xorq	%rbp, %rsi
	movq	%rsi, 192(%rsp)
	orq	%r15, %rbp
	xorq	16(%rsp), %rbx
	movq	%r14, %r15
	xorq	%rbx, %r15
	orq	%r14, %rbx
	xorq	8(%rsp), %r11
	movq	%r13, %r14
	xorq	%r11, %r14
	orq	%r13, %r11
	xorq	72(%rsp), %r10
	movq	%r12, %r13
	xorq	%r10, %r13
	orq	%r12, %r10
	movq	112(%rsp), %r9
	xorq	64(%rsp), %r9
	movq	104(%rsp), %r8
	movq	%r8, %r12
	xorq	%r9, %r12
	orq	%r8, %r9
	movq	128(%rsp), %r8
	xorq	56(%rsp), %r8
	movq	120(%rsp), %rdi
	movq	%rdi, %rsi
	xorq	%r8, %rsi
	movq	%rsi, 104(%rsp)
	orq	%rdi, %r8
	movq	144(%rsp), %rdi
	xorq	48(%rsp), %rdi
	movq	136(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%rdi, %rsi
	movq	%rsi, 112(%rsp)
	orq	%rcx, %rdi
	movq	%rdi, 120(%rsp)
	movq	184(%rsp), %rsi
	xorq	40(%rsp), %rsi
	movq	152(%rsp), %rcx
	movq	%rcx, %rdi
	xorq	%rsi, %rdi
	movq	%rdi, 128(%rsp)
	orq	%rcx, %rsi
	movq	%rsi, 136(%rsp)
	movq	168(%rsp), %rcx
	xorq	32(%rsp), %rcx
	movq	160(%rsp), %rdi
	movq	%rdi, %rsi
	xorq	%rcx, %rsi
	movq	%rsi, 144(%rsp)
	orq	%rdi, %rcx
	xorq	80(%rsp), %rdx
	movq	96(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%rdx, %rdi
	movq	%rdi, 96(%rsp)
	orq	%rsi, %rdx
	xorq	(%rsp), %rax
	movq	176(%rsp), %rdi
	movq	%rdi, %rsi
	xorq	%rax, %rsi
	movq	%rsi, 152(%rsp)
	orq	%rdi, %rax
	xorq	24(%rsp), %rbp
	movq	192(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%rbp, %rdi
	movq	%rdi, 160(%rsp)
	orq	%rsi, %rbp
	xorq	16(%rsp), %rbx
	movq	%r15, %rsi
	xorq	%rbx, %rsi
	movq	%rsi, 168(%rsp)
	orq	%r15, %rbx
	xorq	8(%rsp), %r11
	movq	%r14, %r15
	xorq	%r11, %r15
	orq	%r14, %r11
	xorq	72(%rsp), %r10
	movq	%r13, %r14
	xorq	%r10, %r14
	orq	%r13, %r10
	xorq	64(%rsp), %r9
	movq	%r12, %r13
	xorq	%r9, %r13
	orq	%r12, %r9
	xorq	56(%rsp), %r8
	movq	104(%rsp), %rsi
	movq	%rsi, %r12
	xorq	%r8, %r12
	orq	%rsi, %r8
	movq	%r8, 104(%rsp)
	movq	120(%rsp), %rdi
	xorq	48(%rsp), %rdi
	movq	112(%rsp), %rsi
	movq	%rsi, %r8
	xorq	%rdi, %r8
	movq	%r8, 112(%rsp)
	orq	%rsi, %rdi
	movq	%rdi, 120(%rsp)
	movq	136(%rsp), %rsi
	xorq	40(%rsp), %rsi
	movq	128(%rsp), %r8
	movq	%r8, %rdi
	xorq	%rsi, %rdi
	movq	%rdi, 128(%rsp)
	orq	%r8, %rsi
	xorq	32(%rsp), %rcx
	movq	144(%rsp), %rdi
	movq	%rdi, %r8
	xorq	%rcx, %r8
	movq	%r8, 136(%rsp)
	orq	%rdi, %rcx
	xorq	80(%rsp), %rdx
	movq	96(%rsp), %rdi
	movq	%rdi, %r8
	xorq	%rdx, %r8
	movq	%r8, 96(%rsp)
	orq	%rdi, %rdx
	xorq	(%rsp), %rax
	movq	152(%rsp), %r8
	movq	%r8, %rdi
	xorq	%rax, %rdi
	movq	%rdi, 144(%rsp)
	orq	%r8, %rax
	xorq	24(%rsp), %rbp
	movq	160(%rsp), %r8
	movq	%r8, %rdi
	xorq	%rbp, %rdi
	movq	%rdi, 152(%rsp)
	orq	%r8, %rbp
	xorq	16(%rsp), %rbx
	movq	168(%rsp), %rdi
	movq	%rdi, %r8
	xorq	%rbx, %r8
	movq	%r8, 160(%rsp)
	orq	%rdi, %rbx
	xorq	8(%rsp), %r11
	movq	%r15, %rdi
	xorq	%r11, %rdi
	movq	%rdi, 168(%rsp)
	orq	%r15, %r11
	xorq	72(%rsp), %r10
	movq	%r14, %r15
	xorq	%r10, %r15
	orq	%r14, %r10
	xorq	64(%rsp), %r9
	movq	%r13, %r14
	xorq	%r9, %r14
	orq	%r13, %r9
	movq	%r9, 176(%rsp)
	movq	104(%rsp), %r8
	xorq	56(%rsp), %r8
	movq	%r12, %r13
	xorq	%r8, %r13
	orq	%r12, %r8
	movq	%r8, 104(%rsp)
	movq	120(%rsp), %rdi
	xorq	48(%rsp), %rdi
	movq	112(%rsp), %r9
	movq	%r9, %r12
	xorq	%rdi, %r12
	orq	%r9, %rdi
	xorq	40(%rsp), %rsi
	movq	128(%rsp), %r9
	movq	%r9, %r8
	xorq	%rsi, %r8
	movq	%r8, 112(%rsp)
	orq	%r9, %rsi
	xorq	32(%rsp), %rcx
	movq	136(%rsp), %r8
	movq	%r8, %r9
	xorq	%rcx, %r9
	movq	%r9, 120(%rsp)
	orq	%r8, %rcx
	xorq	80(%rsp), %rdx
	movq	96(%rsp), %r8
	movq	%r8, %r9
	xorq	%rdx, %r9
	movq	%r9, 96(%rsp)
	orq	%r8, %rdx
	xorq	(%rsp), %rax
	movq	144(%rsp), %r9
	movq	%r9, %r8
	xorq	%rax, %r8
	movq	%r8, 128(%rsp)
	orq	%r9, %rax
	xorq	24(%rsp), %rbp
	movq	152(%rsp), %r9
	movq	%r9, %r8
	xorq	%rbp, %r8
	movq	%r8, 136(%rsp)
	orq	%r9, %rbp
	xorq	16(%rsp), %rbx
	movq	160(%rsp), %r8
	movq	%r8, %r9
	xorq	%rbx, %r9
	movq	%r9, 144(%rsp)
	orq	%r8, %rbx
	xorq	8(%rsp), %r11
	movq	168(%rsp), %r8
	movq	%r8, %r9
	xorq	%r11, %r9
	movq	%r9, 152(%rsp)
	orq	%r8, %r11
	xorq	72(%rsp), %r10
	movq	%r15, %r8
	xorq	%r10, %r8
	movq	%r8, 160(%rsp)
	orq	%r15, %r10
	movq	176(%rsp), %r9
	xorq	64(%rsp), %r9
	movq	%r14, %r15
	xorq	%r9, %r15
	orq	%r14, %r9
	movq	%r9, 168(%rsp)
	movq	104(%rsp), %r8
	xorq	56(%rsp), %r8
	movq	%r13, %r14
	xorq	%r8, %r14
	orq	%r13, %r8
	xorq	48(%rsp), %rdi
	movq	%r12, %r13
	xorq	%rdi, %r13
	orq	%r12, %rdi
	movq	%rdi, 104(%rsp)
	xorq	40(%rsp), %rsi
	movq	112(%rsp), %rdi
	movq	%rdi, %r12
	xorq	%rsi, %r12
	orq	%rdi, %rsi
	xorq	32(%rsp), %rcx
	movq	120(%rsp), %r9
	movq	%r9, %rdi
	xorq	%rcx, %rdi
	movq	%rdi, 112(%rsp)
	orq	%r9, %rcx
	xorq	80(%rsp), %rdx
	movq	96(%rsp), %r9
	movq	%r9, %rdi
	xorq	%rdx, %rdi
	movq	%rdi, 96(%rsp)
	orq	%r9, %rdx
	xorq	(%rsp), %rax
	movq	128(%rsp), %rdi
	movq	%rdi, %r9
	xorq	%rax, %r9
	movq	%r9, 120(%rsp)
	orq	%rdi, %rax
	xorq	24(%rsp), %rbp
	movq	136(%rsp), %rdi
	movq	%rdi, %r9
	xorq	%rbp, %r9
	movq	%r9, 128(%rsp)
	orq	%rdi, %rbp
	xorq	16(%rsp), %rbx
	movq	144(%rsp), %r9
	movq	%r9, %rdi
	xorq	%rbx, %rdi
	movq	%rdi, 136(%rsp)
	orq	%r9, %rbx
	xorq	8(%rsp), %r11
	movq	152(%rsp), %r9
	movq	%r9, %rdi
	xorq	%r11, %rdi
	movq	%rdi, 144(%rsp)
	orq	%r9, %r11
	xorq	72(%rsp), %r10
	movq	160(%rsp), %r9
	movq	%r9, %rdi
	xorq	%r10, %rdi
	movq	%rdi, 152(%rsp)
	orq	%r9, %r10
	movq	168(%rsp), %r9
	xorq	64(%rsp), %r9
	movq	%r15, %rdi
	xorq	%r9, %rdi
	movq	%rdi, 160(%rsp)
	orq	%r15, %r9
	xorq	56(%rsp), %r8
	movq	%r14, %r15
	xorq	%r8, %r15
	orq	%r14, %r8
	movq	104(%rsp), %rdi
	xorq	48(%rsp), %rdi
	movq	%r13, %r14
	xorq	%rdi, %r14
	orq	%r13, %rdi
	movq	%rdi, 104(%rsp)
	xorq	40(%rsp), %rsi
	movq	%r12, %r13
	xorq	%rsi, %r13
	orq	%r12, %rsi
	movq	%rsi, 168(%rsp)
	xorq	32(%rsp), %rcx
	movq	112(%rsp), %rdi
	movq	%rdi, %r12
	xorq	%rcx, %r12
	orq	%rdi, %rcx
	xorq	80(%rsp), %rdx
	movq	96(%rsp), %rdi
	movq	%rdi, %rsi
	xorq	%rdx, %rsi
	movq	%rsi, 96(%rsp)
	orq	%rdi, %rdx
	xorq	(%rsp), %rax
	movq	120(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%rax, %rdi
	movq	%rdi, 112(%rsp)
	orq	%rsi, %rax
	xorq	24(%rsp), %rbp
	movq	128(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%rbp, %rdi
	movq	%rdi, 120(%rsp)
	orq	%rsi, %rbp
	xorq	16(%rsp), %rbx
	movq	136(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%rbx, %rdi
	movq	%rdi, 128(%rsp)
	orq	%rsi, %rbx
	xorq	8(%rsp), %r11
	movq	144(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%r11, %rdi
	movq	%rdi, 136(%rsp)
	orq	%rsi, %r11
	xorq	72(%rsp), %r10
	movq	152(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%r10, %rdi
	movq	%rdi, 144(%rsp)
	orq	%rsi, %r10
	xorq	64(%rsp), %r9
	movq	160(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%r9, %rdi
	movq	%rdi, 152(%rsp)
	orq	%rsi, %r9
	xorq	56(%rsp), %r8
	movq	%r15, %rsi
	xorq	%r8, %rsi
	movq	%rsi, 160(%rsp)
	orq	%r15, %r8
	movq	104(%rsp), %rdi
	xorq	48(%rsp), %rdi
	movq	%r14, %r15
	xorq	%rdi, %r15
	orq	%r14, %rdi
	movq	168(%rsp), %rsi
	xorq	40(%rsp), %rsi
	movq	%r13, %r14
	xorq	%rsi, %r14
	orq	%r13, %rsi
	xorq	32(%rsp), %rcx
	movq	%r12, %r13
	xorq	%rcx, %r13
	orq	%r12, %rcx
	movq	%rcx, 104(%rsp)
	xorq	80(%rsp), %rdx
	movq	96(%rsp), %r12
	movq	%r12, %rcx
	xorq	%rdx, %rcx
	movq	%rcx, 96(%rsp)
	orq	%r12, %rdx
	movq	%rdx, 168(%rsp)
	xorq	(%rsp), %rax
	movq	112(%rsp), %r12
	movq	%r12, %rcx
	xorq	%rax, %rcx
	movq	%rcx, 112(%rsp)
	orq	%r12, %rax
	xorq	24(%rsp), %rbp
	movq	120(%rsp), %r12
	movq	%r12, %rcx
	xorq	%rbp, %rcx
	movq	%rcx, 120(%rsp)
	orq	%r12, %rbp
	xorq	16(%rsp), %rbx
	movq	128(%rsp), %r12
	movq	%r12, %rcx
	xorq	%rbx, %rcx
	movq	%rcx, 128(%rsp)
	orq	%r12, %rbx
	xorq	8(%rsp), %r11
	movq	136(%rsp), %rdx
	movq	%rdx, %r12
	xorq	%r11, %r12
	orq	%rdx, %r11
	xorq	72(%rsp), %r10
	movq	144(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r10, %rdx
	movq	%rdx, 136(%rsp)
	orq	%rcx, %r10
	xorq	64(%rsp), %r9
	movq	152(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r9, %rcx
	movq	%rcx, 144(%rsp)
	orq	%rdx, %r9
	xorq	56(%rsp), %r8
	movq	160(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r8, %rcx
	movq	%rcx, 152(%rsp)
	orq	%rdx, %r8
	xorq	48(%rsp), %rdi
	movq	%r15, %rdx
	xorq	%rdi, %rdx
	movq	%rdx, 160(%rsp)
	orq	%rdi, %r15
	xorq	40(%rsp), %rsi
	movq	%r14, %rdx
	xorq	%rsi, %rdx
	movq	%rdx, 176(%rsp)
	orq	%rsi, %r14
	movq	104(%rsp), %rcx
	xorq	32(%rsp), %rcx
	movq	%r13, %rsi
	xorq	%rcx, %rsi
	movq	%rsi, 184(%rsp)
	orq	%rcx, %r13
	movq	168(%rsp), %rdx
	xorq	80(%rsp), %rdx
	movq	96(%rsp), %rdi
	movq	%rdi, %rsi
	xorq	%rdx, %rsi
	movq	%rsi, 96(%rsp)
	orq	%rdi, %rdx
	xorq	(%rsp), %rax
	movq	112(%rsp), %rdi
	movq	%rdi, %rcx
	xorq	%rax, %rcx
	movq	%rcx, 104(%rsp)
	movq	%rdi, %rcx
	orq	%rax, %rcx
	movq	24(%rsp), %rax
	xorq	%rbp, %rax
	movq	120(%rsp), %rdi
	movq	%rdi, %rbp
	xorq	%rax, %rbp
	movq	%rbp, 112(%rsp)
	orq	%rax, %rdi
	movq	%rdi, %rbp
	movq	16(%rsp), %rax
	xorq	%rbx, %rax
	movq	128(%rsp), %rbx
	movq	%rbx, %rdi
	xorq	%rax, %rdi
	movq	%rdi, 120(%rsp)
	orq	%rax, %rbx
	movq	8(%rsp), %rax
	xorq	%r11, %rax
	movq	%r12, %rsi
	xorq	%rax, %rsi
	movq	%rsi, 128(%rsp)
	orq	%rax, %r12
	movq	72(%rsp), %rax
	xorq	%r10, %rax
	movq	136(%rsp), %rdi
	movq	%rdi, %r10
	xorq	%rax, %r10
	movq	%r10, 136(%rsp)
	orq	%rax, %rdi
	movq	%rdi, %r10
	movq	64(%rsp), %rax
	xorq	%r9, %rax
	movq	144(%rsp), %rdi
	movq	%rdi, %r11
	xorq	%rax, %r11
	movq	%r11, 144(%rsp)
	orq	%rax, %rdi
	movq	%rdi, %r9
	movq	56(%rsp), %rax
	xorq	%r8, %rax
	movq	152(%rsp), %rdi
	movq	%rdi, %r11
	xorq	%rax, %r11
	movq	%r11, 152(%rsp)
	orq	%rax, %rdi
	movq	%rdi, %r8
	xorq	48(%rsp), %r15
	movq	160(%rsp), %rdi
	movq	%rdi, %r11
	xorq	%r15, %r11
	movq	%r11, 160(%rsp)
	orq	%r15, %rdi
	xorq	40(%rsp), %r14
	movq	176(%rsp), %r11
	movq	%r11, %rax
	xorq	%r14, %rax
	movq	%rax, 168(%rsp)
	orq	%r14, %r11
	movq	%r11, %rsi
	xorq	32(%rsp), %r13
	movq	184(%rsp), %r11
	movq	%r11, %r14
	xorq	%r13, %r14
	movq	%r14, 176(%rsp)
	orq	%r11, %r13
	xorq	80(%rsp), %rdx
	movq	96(%rsp), %r15
	movq	%r15, %r11
	xorq	%rdx, %r11
	movq	%r11, 184(%rsp)
	orq	%r15, %rdx
	movq	%rdx, 96(%rsp)
	movq	(%rsp), %rax
	cmpq	280(%rsp), %rax
	jne	.L467
	movq	%rcx, %r15
	movq	%rbp, %r14
	movq	%r10, (%rsp)
	movq	%r9, 32(%rsp)
	movq	%r8, 40(%rsp)
	movq	%rdi, 8(%rsp)
	movq	%rsi, 16(%rsp)
	movq	%r13, 24(%rsp)
	movq	%rbx, %r13
.L466:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	call	use_int@PLT
	movl	96(%rsp), %edi
	call	use_int@PLT
	addq	$296, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE163:
	.size	int64_bit_10, .-int64_bit_10
	.globl	int64_bit_11
	.type	int64_bit_11, @function
int64_bit_11:
.LFB164:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$344, %rsp
	.cfi_def_cfa_offset 400
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r15
	movslq	%ecx, %rcx
	leaq	1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 120(%rsp)
	leaq	0(,%rsi,4), %rbp
	movq	%rbp, 16(%rsp)
	movslq	16(%rdx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %r14
	addq	%rax, %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 128(%rsp)
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 240(%rsp)
	movl	20(%rdx), %ecx
	leal	-1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r13
	movslq	%ecx, %rcx
	leaq	-1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 136(%rsp)
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 248(%rsp)
	movl	24(%rdx), %ecx
	leal	-2(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	movq	%rsi, %rbx
	orq	%rdi, %rbx
	movslq	%ecx, %rcx
	leaq	-2(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 144(%rsp)
	leaq	0(,%rsi,4), %rcx
	movq	%rcx, 256(%rsp)
	movl	28(%rdx), %ecx
	leal	-3(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r11
	movq	%rsi, 8(%rsp)
	movslq	%ecx, %rcx
	leaq	-3(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 152(%rsp)
	leaq	0(,%rsi,4), %rcx
	movq	%rcx, 264(%rsp)
	movl	32(%rdx), %ecx
	leal	-4(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r10
	movq	%rsi, 56(%rsp)
	movslq	%ecx, %rcx
	leaq	-4(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 160(%rsp)
	leaq	0(,%rsi,4), %rcx
	movq	%rcx, 272(%rsp)
	movl	36(%rdx), %ecx
	leal	-5(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r9
	movq	%rsi, 64(%rsp)
	movslq	%ecx, %rcx
	leaq	-5(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 168(%rsp)
	leaq	0(,%rsi,4), %rcx
	movq	%rcx, 280(%rsp)
	movl	40(%rdx), %ecx
	leal	-6(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 24(%rsp)
	movslq	%ecx, %rcx
	leaq	-6(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 176(%rsp)
	leaq	0(,%rsi,4), %rcx
	movq	%rcx, 288(%rsp)
	movl	44(%rdx), %ecx
	leal	-7(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 32(%rsp)
	movslq	%ecx, %rcx
	leaq	-7(%rax,%rcx), %rcx
	movq	%rcx, %r12
	salq	$32, %r12
	movq	%r12, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 184(%rsp)
	leaq	0(,%rsi,4), %rcx
	movq	%rcx, 296(%rsp)
	movl	48(%rdx), %ecx
	leal	-8(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	movq	%rsi, %r12
	orq	%rdi, %r12
	movq	%r12, 40(%rsp)
	movslq	%ecx, %rcx
	leaq	-8(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %rcx
	movq	%rsi, 192(%rsp)
	salq	$2, %rcx
	movq	%rcx, 304(%rsp)
	movl	52(%rdx), %ecx
	leal	-9(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	movq	%rsi, %r12
	orq	%rdi, %r12
	movq	%r12, 112(%rsp)
	movslq	%ecx, %rcx
	leaq	-9(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %rcx
	movq	%rsi, 200(%rsp)
	salq	$2, %rcx
	movq	%rcx, 312(%rsp)
	movl	56(%rdx), %edx
	leal	-10(%rdx), %ecx
	movslq	%ecx, %rsi
	salq	$32, %rcx
	orq	%rsi, %rcx
	movq	%rcx, 48(%rsp)
	movslq	%edx, %rdx
	leaq	-10(%rax,%rdx), %rdx
	movq	%rdx, %rcx
	salq	$32, %rcx
	orq	%rdx, %rcx
	movq	%rcx, 208(%rsp)
	leaq	0(,%rcx,4), %rdx
	movq	%rdx, 320(%rsp)
	testq	%rax, %rax
	je	.L471
	leaq	-1(%rbp), %r8
	subq	%rax, %rbp
	leaq	-1(%rbp), %rax
	movq	%rax, 328(%rsp)
	movq	%r8, 8(%rsp)
	movq	%r15, %rcx
	movq	%r14, %r12
	movq	%r13, %rbp
	movq	24(%rsp), %r8
	movq	32(%rsp), %rdi
	movq	40(%rsp), %rsi
	movq	48(%rsp), %rdx
.L472:
	subq	$1, 8(%rsp)
	movq	8(%rsp), %r14
	movq	240(%rsp), %rax
	addq	%r14, %rax
	movq	16(%rsp), %r15
	subq	%r15, %rax
	movq	%rax, %r13
	movq	248(%rsp), %rax
	addq	%r14, %rax
	subq	%r15, %rax
	movq	%rax, 40(%rsp)
	movq	256(%rsp), %rax
	addq	%r14, %rax
	subq	%r15, %rax
	movq	%rax, 32(%rsp)
	movq	264(%rsp), %rax
	addq	%r14, %rax
	movq	%rax, %r15
	subq	16(%rsp), %r15
	movq	%r15, 24(%rsp)
	movq	272(%rsp), %rax
	addq	%r14, %rax
	subq	16(%rsp), %rax
	movq	%rax, 96(%rsp)
	movq	280(%rsp), %r15
	addq	%r14, %r15
	subq	16(%rsp), %r15
	movq	%r15, 88(%rsp)
	movq	288(%rsp), %rax
	addq	%r14, %rax
	subq	16(%rsp), %rax
	movq	%rax, 80(%rsp)
	movq	296(%rsp), %r15
	addq	%r14, %r15
	subq	16(%rsp), %r15
	movq	%r15, 72(%rsp)
	movq	304(%rsp), %rax
	addq	%r14, %rax
	subq	16(%rsp), %rax
	movq	%rax, 64(%rsp)
	movq	312(%rsp), %r15
	addq	%r14, %r15
	subq	16(%rsp), %r15
	movq	%r15, 56(%rsp)
	movq	320(%rsp), %rax
	addq	%r14, %rax
	subq	16(%rsp), %rax
	movq	%rax, 104(%rsp)
	xorq	%r14, %rcx
	movq	%rcx, %rax
	movq	120(%rsp), %rcx
	movq	%rcx, %r14
	xorq	%rax, %r14
	movq	%r14, %r15
	orq	%rcx, %rax
	movq	%rax, 120(%rsp)
	movq	%r13, 48(%rsp)
	xorq	%r13, %r12
	movq	128(%rsp), %rcx
	movq	%rcx, %r14
	xorq	%r12, %r14
	orq	%rcx, %r12
	xorq	40(%rsp), %rbp
	movq	136(%rsp), %rcx
	movq	%rcx, %r13
	xorq	%rbp, %r13
	orq	%rcx, %rbp
	xorq	32(%rsp), %rbx
	movq	144(%rsp), %rcx
	movq	%rcx, %rax
	xorq	%rbx, %rax
	movq	%rax, 128(%rsp)
	orq	%rcx, %rbx
	xorq	24(%rsp), %r11
	movq	152(%rsp), %rcx
	movq	%rcx, %rax
	xorq	%r11, %rax
	movq	%rax, 136(%rsp)
	orq	%rcx, %r11
	xorq	96(%rsp), %r10
	movq	160(%rsp), %rcx
	movq	%rcx, %rax
	xorq	%r10, %rax
	movq	%rax, 144(%rsp)
	orq	%rcx, %r10
	xorq	88(%rsp), %r9
	movq	168(%rsp), %rcx
	movq	%rcx, %rax
	xorq	%r9, %rax
	movq	%rax, 152(%rsp)
	orq	%rcx, %r9
	xorq	80(%rsp), %r8
	movq	176(%rsp), %rax
	movq	%rax, %rcx
	xorq	%r8, %rcx
	movq	%rcx, 160(%rsp)
	orq	%rax, %r8
	xorq	72(%rsp), %rdi
	movq	184(%rsp), %rcx
	movq	%rcx, %rax
	xorq	%rdi, %rax
	movq	%rax, 168(%rsp)
	orq	%rcx, %rdi
	xorq	64(%rsp), %rsi
	movq	192(%rsp), %rax
	movq	%rax, %rcx
	xorq	%rsi, %rcx
	movq	%rcx, 176(%rsp)
	orq	%rax, %rsi
	movq	%rsi, 184(%rsp)
	movq	112(%rsp), %rcx
	xorq	56(%rsp), %rcx
	movq	200(%rsp), %rax
	movq	%rax, %rsi
	xorq	%rcx, %rsi
	movq	%rsi, 112(%rsp)
	orq	%rax, %rcx
	xorq	104(%rsp), %rdx
	movq	208(%rsp), %rsi
	movq	%rsi, %rax
	xorq	%rdx, %rax
	movq	%rax, 192(%rsp)
	orq	%rsi, %rdx
	movq	%rdx, 200(%rsp)
	movq	120(%rsp), %rax
	xorq	8(%rsp), %rax
	movq	%r15, %rsi
	xorq	%rax, %rsi
	movq	%rsi, 120(%rsp)
	orq	%r15, %rax
	xorq	48(%rsp), %r12
	movq	%r14, %r15
	xorq	%r12, %r15
	orq	%r14, %r12
	xorq	40(%rsp), %rbp
	movq	%r13, %r14
	xorq	%rbp, %r14
	orq	%r13, %rbp
	xorq	32(%rsp), %rbx
	movq	128(%rsp), %rdx
	movq	%rdx, %r13
	xorq	%rbx, %r13
	orq	%rdx, %rbx
	xorq	24(%rsp), %r11
	movq	136(%rsp), %rdx
	movq	%rdx, %rsi
	xorq	%r11, %rsi
	movq	%rsi, 128(%rsp)
	orq	%rdx, %r11
	xorq	96(%rsp), %r10
	movq	144(%rsp), %rdx
	movq	%rdx, %rsi
	xorq	%r10, %rsi
	movq	%rsi, 136(%rsp)
	orq	%rdx, %r10
	xorq	88(%rsp), %r9
	movq	152(%rsp), %rdx
	movq	%rdx, %rsi
	xorq	%r9, %rsi
	movq	%rsi, 144(%rsp)
	orq	%rdx, %r9
	xorq	80(%rsp), %r8
	movq	160(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r8, %rdx
	movq	%rdx, 152(%rsp)
	orq	%rsi, %r8
	movq	%r8, 160(%rsp)
	xorq	72(%rsp), %rdi
	movq	168(%rsp), %r8
	movq	%r8, %rsi
	xorq	%rdi, %rsi
	movq	%rsi, 168(%rsp)
	orq	%r8, %rdi
	movq	%rdi, 208(%rsp)
	movq	184(%rsp), %rsi
	xorq	64(%rsp), %rsi
	movq	176(%rsp), %rdi
	movq	%rdi, %r8
	xorq	%rsi, %r8
	movq	%r8, 176(%rsp)
	orq	%rdi, %rsi
	movq	%rsi, 184(%rsp)
	xorq	56(%rsp), %rcx
	movq	112(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%rcx, %rdi
	movq	%rdi, 112(%rsp)
	orq	%rdx, %rcx
	movq	200(%rsp), %rdx
	xorq	104(%rsp), %rdx
	movq	192(%rsp), %rdi
	movq	%rdi, %rsi
	xorq	%rdx, %rsi
	movq	%rsi, 192(%rsp)
	orq	%rdi, %rdx
	xorq	8(%rsp), %rax
	movq	120(%rsp), %rdi
	movq	%rdi, %rsi
	xorq	%rax, %rsi
	movq	%rsi, 120(%rsp)
	orq	%rdi, %rax
	movq	%rax, 200(%rsp)
	xorq	48(%rsp), %r12
	movq	%r15, %rax
	xorq	%r12, %rax
	movq	%rax, 216(%rsp)
	orq	%r15, %r12
	xorq	40(%rsp), %rbp
	movq	%r14, %r15
	xorq	%rbp, %r15
	orq	%r14, %rbp
	xorq	32(%rsp), %rbx
	movq	%r13, %r14
	xorq	%rbx, %r14
	orq	%r13, %rbx
	xorq	24(%rsp), %r11
	movq	128(%rsp), %rdi
	movq	%rdi, %r13
	xorq	%r11, %r13
	orq	%rdi, %r11
	xorq	96(%rsp), %r10
	movq	136(%rsp), %rax
	movq	%rax, %rdi
	xorq	%r10, %rdi
	movq	%rdi, 128(%rsp)
	orq	%rax, %r10
	xorq	88(%rsp), %r9
	movq	144(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%r9, %rdi
	movq	%rdi, 136(%rsp)
	orq	%rsi, %r9
	movq	160(%rsp), %r8
	xorq	80(%rsp), %r8
	movq	152(%rsp), %rdi
	movq	%rdi, %rax
	xorq	%r8, %rax
	movq	%rax, 144(%rsp)
	orq	%rdi, %r8
	movq	%r8, 152(%rsp)
	movq	208(%rsp), %rdi
	xorq	72(%rsp), %rdi
	movq	168(%rsp), %rax
	movq	%rax, %rsi
	xorq	%rdi, %rsi
	movq	%rsi, 160(%rsp)
	orq	%rax, %rdi
	movq	%rdi, 168(%rsp)
	movq	184(%rsp), %rsi
	xorq	64(%rsp), %rsi
	movq	176(%rsp), %r8
	movq	%r8, %rax
	xorq	%rsi, %rax
	movq	%rax, 176(%rsp)
	orq	%r8, %rsi
	xorq	56(%rsp), %rcx
	movq	112(%rsp), %rdi
	movq	%rdi, %r8
	xorq	%rcx, %r8
	movq	%r8, 112(%rsp)
	orq	%rdi, %rcx
	movq	%rcx, 184(%rsp)
	xorq	104(%rsp), %rdx
	movq	192(%rsp), %r8
	movq	%r8, %rcx
	xorq	%rdx, %rcx
	movq	%rcx, 192(%rsp)
	orq	%r8, %rdx
	movq	200(%rsp), %rax
	xorq	8(%rsp), %rax
	movq	120(%rsp), %rcx
	movq	%rcx, %r8
	xorq	%rax, %r8
	movq	%r8, 120(%rsp)
	orq	%rcx, %rax
	xorq	48(%rsp), %r12
	movq	216(%rsp), %rcx
	movq	%rcx, %r8
	xorq	%r12, %r8
	movq	%r8, 200(%rsp)
	orq	%rcx, %r12
	xorq	40(%rsp), %rbp
	movq	%r15, %rcx
	xorq	%rbp, %rcx
	movq	%rcx, 208(%rsp)
	orq	%r15, %rbp
	xorq	32(%rsp), %rbx
	movq	%r14, %r15
	xorq	%rbx, %r15
	orq	%r14, %rbx
	xorq	24(%rsp), %r11
	movq	%r13, %r14
	xorq	%r11, %r14
	orq	%r13, %r11
	xorq	96(%rsp), %r10
	movq	128(%rsp), %rdi
	movq	%rdi, %r13
	xorq	%r10, %r13
	orq	%rdi, %r10
	xorq	88(%rsp), %r9
	movq	136(%rsp), %rdi
	movq	%rdi, %r8
	xorq	%r9, %r8
	movq	%r8, 128(%rsp)
	orq	%rdi, %r9
	movq	152(%rsp), %r8
	xorq	80(%rsp), %r8
	movq	144(%rsp), %rcx
	movq	%rcx, %rdi
	xorq	%r8, %rdi
	movq	%rdi, 136(%rsp)
	orq	%rcx, %r8
	movq	%r8, 144(%rsp)
	movq	168(%rsp), %rdi
	xorq	72(%rsp), %rdi
	movq	160(%rsp), %rcx
	movq	%rcx, %r8
	xorq	%rdi, %r8
	movq	%r8, 152(%rsp)
	orq	%rcx, %rdi
	xorq	64(%rsp), %rsi
	movq	176(%rsp), %rcx
	movq	%rcx, %r8
	xorq	%rsi, %r8
	movq	%r8, 160(%rsp)
	orq	%rcx, %rsi
	movq	%rsi, 168(%rsp)
	movq	184(%rsp), %rcx
	xorq	56(%rsp), %rcx
	movq	112(%rsp), %r8
	movq	%r8, %rsi
	xorq	%rcx, %rsi
	movq	%rsi, 112(%rsp)
	orq	%r8, %rcx
	xorq	104(%rsp), %rdx
	movq	192(%rsp), %rsi
	movq	%rsi, %r8
	xorq	%rdx, %r8
	movq	%r8, 176(%rsp)
	orq	%rsi, %rdx
	movq	%rdx, 184(%rsp)
	xorq	8(%rsp), %rax
	movq	120(%rsp), %r8
	movq	%r8, %rsi
	xorq	%rax, %rsi
	movq	%rsi, 120(%rsp)
	orq	%r8, %rax
	xorq	48(%rsp), %r12
	movq	200(%rsp), %rdx
	movq	%rdx, %rsi
	xorq	%r12, %rsi
	movq	%rsi, 192(%rsp)
	orq	%rdx, %r12
	xorq	40(%rsp), %rbp
	movq	208(%rsp), %rdx
	movq	%rdx, %r8
	xorq	%rbp, %r8
	movq	%r8, 200(%rsp)
	orq	%rdx, %rbp
	xorq	32(%rsp), %rbx
	movq	%r15, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 208(%rsp)
	orq	%r15, %rbx
	xorq	24(%rsp), %r11
	movq	%r14, %r15
	xorq	%r11, %r15
	orq	%r14, %r11
	xorq	96(%rsp), %r10
	movq	%r13, %r14
	xorq	%r10, %r14
	orq	%r13, %r10
	xorq	88(%rsp), %r9
	movq	128(%rsp), %rdx
	movq	%rdx, %r13
	xorq	%r9, %r13
	orq	%rdx, %r9
	movq	144(%rsp), %r8
	xorq	80(%rsp), %r8
	movq	136(%rsp), %rdx
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rsi, 128(%rsp)
	orq	%rdx, %r8
	xorq	72(%rsp), %rdi
	movq	152(%rsp), %rdx
	movq	%rdx, %rsi
	xorq	%rdi, %rsi
	movq	%rsi, 136(%rsp)
	orq	%rdx, %rdi
	movq	%rdi, 144(%rsp)
	movq	168(%rsp), %rsi
	xorq	64(%rsp), %rsi
	movq	160(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%rsi, %rdx
	movq	%rdx, 152(%rsp)
	orq	%rdi, %rsi
	movq	%rsi, 160(%rsp)
	xorq	56(%rsp), %rcx
	movq	112(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%rcx, %rdi
	movq	%rdi, 112(%rsp)
	orq	%rdx, %rcx
	movq	184(%rsp), %rdx
	xorq	104(%rsp), %rdx
	movq	176(%rsp), %rdi
	movq	%rdi, %rsi
	xorq	%rdx, %rsi
	movq	%rsi, 168(%rsp)
	orq	%rdi, %rdx
	xorq	8(%rsp), %rax
	movq	120(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%rax, %rdi
	movq	%rdi, 120(%rsp)
	orq	%rsi, %rax
	xorq	48(%rsp), %r12
	movq	192(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%r12, %rdi
	movq	%rdi, 176(%rsp)
	orq	%rsi, %r12
	xorq	40(%rsp), %rbp
	movq	200(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%rbp, %rdi
	movq	%rdi, 184(%rsp)
	orq	%rsi, %rbp
	xorq	32(%rsp), %rbx
	movq	208(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%rbx, %rdi
	movq	%rdi, 192(%rsp)
	orq	%rsi, %rbx
	xorq	24(%rsp), %r11
	movq	%r15, %rsi
	xorq	%r11, %rsi
	movq	%rsi, 200(%rsp)
	orq	%r15, %r11
	xorq	96(%rsp), %r10
	movq	%r14, %r15
	xorq	%r10, %r15
	orq	%r14, %r10
	xorq	88(%rsp), %r9
	movq	%r13, %r14
	xorq	%r9, %r14
	orq	%r13, %r9
	xorq	80(%rsp), %r8
	movq	128(%rsp), %rsi
	movq	%rsi, %r13
	xorq	%r8, %r13
	orq	%rsi, %r8
	movq	%r8, 128(%rsp)
	movq	144(%rsp), %rdi
	xorq	72(%rsp), %rdi
	movq	136(%rsp), %rsi
	movq	%rsi, %r8
	xorq	%rdi, %r8
	movq	%r8, 136(%rsp)
	orq	%rsi, %rdi
	movq	%rdi, 144(%rsp)
	movq	160(%rsp), %rsi
	xorq	64(%rsp), %rsi
	movq	152(%rsp), %r8
	movq	%r8, %rdi
	xorq	%rsi, %rdi
	movq	%rdi, 152(%rsp)
	orq	%r8, %rsi
	xorq	56(%rsp), %rcx
	movq	112(%rsp), %rdi
	movq	%rdi, %r8
	xorq	%rcx, %r8
	movq	%r8, 112(%rsp)
	orq	%rdi, %rcx
	xorq	104(%rsp), %rdx
	movq	168(%rsp), %r8
	movq	%r8, %rdi
	xorq	%rdx, %rdi
	movq	%rdi, 160(%rsp)
	orq	%r8, %rdx
	xorq	8(%rsp), %rax
	movq	120(%rsp), %rdi
	movq	%rdi, %r8
	xorq	%rax, %r8
	movq	%r8, 120(%rsp)
	orq	%rdi, %rax
	xorq	48(%rsp), %r12
	movq	176(%rsp), %rdi
	movq	%rdi, %r8
	xorq	%r12, %r8
	movq	%r8, 168(%rsp)
	orq	%rdi, %r12
	xorq	40(%rsp), %rbp
	movq	184(%rsp), %rdi
	movq	%rdi, %r8
	xorq	%rbp, %r8
	movq	%r8, 176(%rsp)
	orq	%rdi, %rbp
	xorq	32(%rsp), %rbx
	movq	192(%rsp), %rdi
	movq	%rdi, %r8
	xorq	%rbx, %r8
	movq	%r8, 184(%rsp)
	orq	%rdi, %rbx
	xorq	24(%rsp), %r11
	movq	200(%rsp), %rdi
	movq	%rdi, %r8
	xorq	%r11, %r8
	movq	%r8, 192(%rsp)
	orq	%rdi, %r11
	xorq	96(%rsp), %r10
	movq	%r15, %rdi
	xorq	%r10, %rdi
	movq	%rdi, 200(%rsp)
	orq	%r15, %r10
	xorq	88(%rsp), %r9
	movq	%r14, %rdi
	xorq	%r9, %rdi
	movq	%rdi, 208(%rsp)
	orq	%r14, %r9
	movq	128(%rsp), %r8
	xorq	80(%rsp), %r8
	movq	%r13, %r14
	xorq	%r8, %r14
	orq	%r13, %r8
	movq	%r8, 128(%rsp)
	movq	144(%rsp), %rdi
	xorq	72(%rsp), %rdi
	movq	136(%rsp), %r15
	movq	%r15, %r13
	xorq	%rdi, %r13
	orq	%r15, %rdi
	xorq	64(%rsp), %rsi
	movq	152(%rsp), %r8
	movq	%r8, %r15
	xorq	%rsi, %r15
	movq	%r15, 136(%rsp)
	orq	%r8, %rsi
	xorq	56(%rsp), %rcx
	movq	112(%rsp), %r8
	movq	%r8, %r15
	xorq	%rcx, %r15
	movq	%r15, 112(%rsp)
	orq	%r8, %rcx
	xorq	104(%rsp), %rdx
	movq	160(%rsp), %r15
	movq	%r15, %r8
	xorq	%rdx, %r8
	movq	%r8, 144(%rsp)
	orq	%r15, %rdx
	xorq	8(%rsp), %rax
	movq	120(%rsp), %r15
	movq	%r15, %r8
	xorq	%rax, %r8
	movq	%r8, 120(%rsp)
	orq	%r15, %rax
	xorq	48(%rsp), %r12
	movq	168(%rsp), %r15
	movq	%r15, %r8
	xorq	%r12, %r8
	movq	%r8, 152(%rsp)
	orq	%r15, %r12
	xorq	40(%rsp), %rbp
	movq	176(%rsp), %r15
	movq	%r15, %r8
	xorq	%rbp, %r8
	movq	%r8, 160(%rsp)
	orq	%r15, %rbp
	xorq	32(%rsp), %rbx
	movq	184(%rsp), %r15
	movq	%r15, %r8
	xorq	%rbx, %r8
	movq	%r8, 168(%rsp)
	orq	%r15, %rbx
	xorq	24(%rsp), %r11
	movq	192(%rsp), %r15
	movq	%r15, %r8
	xorq	%r11, %r8
	movq	%r8, 176(%rsp)
	orq	%r15, %r11
	xorq	96(%rsp), %r10
	movq	200(%rsp), %r15
	movq	%r15, %r8
	xorq	%r10, %r8
	movq	%r8, 184(%rsp)
	orq	%r15, %r10
	xorq	88(%rsp), %r9
	movq	208(%rsp), %r8
	movq	%r8, %r15
	xorq	%r9, %r15
	movq	%r15, 192(%rsp)
	orq	%r8, %r9
	movq	128(%rsp), %r8
	xorq	80(%rsp), %r8
	movq	%r14, %r15
	xorq	%r8, %r15
	orq	%r14, %r8
	xorq	72(%rsp), %rdi
	movq	%r13, %r14
	xorq	%rdi, %r14
	movq	%r14, 128(%rsp)
	orq	%r13, %rdi
	xorq	64(%rsp), %rsi
	movq	136(%rsp), %r14
	movq	%r14, %r13
	xorq	%rsi, %r13
	orq	%r14, %rsi
	movq	%rsi, 136(%rsp)
	xorq	56(%rsp), %rcx
	movq	112(%rsp), %rsi
	movq	%rsi, %r14
	xorq	%rcx, %r14
	movq	%r14, 112(%rsp)
	orq	%rsi, %rcx
	xorq	104(%rsp), %rdx
	movq	144(%rsp), %r14
	movq	%r14, %rsi
	xorq	%rdx, %rsi
	movq	%rsi, 144(%rsp)
	orq	%r14, %rdx
	xorq	8(%rsp), %rax
	movq	120(%rsp), %rsi
	movq	%rsi, %r14
	xorq	%rax, %r14
	movq	%r14, 120(%rsp)
	orq	%rsi, %rax
	xorq	48(%rsp), %r12
	movq	152(%rsp), %rsi
	movq	%rsi, %r14
	xorq	%r12, %r14
	movq	%r14, 152(%rsp)
	orq	%rsi, %r12
	xorq	40(%rsp), %rbp
	movq	160(%rsp), %rsi
	movq	%rsi, %r14
	xorq	%rbp, %r14
	movq	%r14, 160(%rsp)
	orq	%rsi, %rbp
	xorq	32(%rsp), %rbx
	movq	168(%rsp), %rsi
	movq	%rsi, %r14
	xorq	%rbx, %r14
	movq	%r14, 168(%rsp)
	orq	%rsi, %rbx
	xorq	24(%rsp), %r11
	movq	176(%rsp), %rsi
	movq	%rsi, %r14
	xorq	%r11, %r14
	movq	%r14, 176(%rsp)
	orq	%rsi, %r11
	xorq	96(%rsp), %r10
	movq	184(%rsp), %rsi
	movq	%rsi, %r14
	xorq	%r10, %r14
	movq	%r14, 184(%rsp)
	orq	%rsi, %r10
	xorq	88(%rsp), %r9
	movq	192(%rsp), %rsi
	movq	%rsi, %r14
	xorq	%r9, %r14
	movq	%r14, 192(%rsp)
	orq	%rsi, %r9
	xorq	80(%rsp), %r8
	movq	%r15, %rsi
	xorq	%r8, %rsi
	movq	%rsi, 200(%rsp)
	orq	%r8, %r15
	xorq	72(%rsp), %rdi
	movq	128(%rsp), %r14
	movq	%r14, %rsi
	xorq	%rdi, %rsi
	movq	%rsi, 208(%rsp)
	orq	%rdi, %r14
	movq	136(%rsp), %rsi
	xorq	64(%rsp), %rsi
	movq	%r13, %rdi
	xorq	%rsi, %rdi
	movq	%rdi, 216(%rsp)
	orq	%rsi, %r13
	xorq	56(%rsp), %rcx
	movq	112(%rsp), %rdi
	movq	%rdi, %rsi
	xorq	%rcx, %rsi
	movq	%rsi, 112(%rsp)
	orq	%rdi, %rcx
	movq	%rcx, 224(%rsp)
	xorq	104(%rsp), %rdx
	movq	144(%rsp), %rcx
	movq	%rcx, %rdi
	xorq	%rdx, %rdi
	movq	%rdi, 232(%rsp)
	orq	%rcx, %rdx
	xorq	8(%rsp), %rax
	movq	120(%rsp), %rcx
	movq	%rcx, %r8
	xorq	%rax, %r8
	movq	%r8, 120(%rsp)
	orq	%rax, %rcx
	movq	48(%rsp), %rax
	xorq	%r12, %rax
	movq	152(%rsp), %r8
	movq	%r8, %r12
	xorq	%rax, %r12
	movq	%r12, 128(%rsp)
	orq	%rax, %r8
	movq	%r8, %r12
	movq	40(%rsp), %rax
	xorq	%rbp, %rax
	movq	160(%rsp), %r8
	movq	%r8, %rbp
	xorq	%rax, %rbp
	movq	%rbp, 136(%rsp)
	orq	%rax, %r8
	movq	%r8, %rbp
	movq	32(%rsp), %rax
	xorq	%rbx, %rax
	movq	168(%rsp), %rbx
	movq	%rbx, %r8
	xorq	%rax, %r8
	movq	%r8, 144(%rsp)
	orq	%rax, %rbx
	movq	24(%rsp), %rax
	xorq	%r11, %rax
	movq	176(%rsp), %r11
	movq	%r11, %r8
	xorq	%rax, %r8
	movq	%r8, 152(%rsp)
	orq	%rax, %r11
	movq	96(%rsp), %rax
	xorq	%r10, %rax
	movq	184(%rsp), %r10
	movq	%r10, %r8
	xorq	%rax, %r8
	movq	%r8, 160(%rsp)
	orq	%rax, %r10
	movq	88(%rsp), %rax
	xorq	%r9, %rax
	movq	192(%rsp), %r9
	movq	%r9, %r8
	xorq	%rax, %r8
	movq	%r8, 168(%rsp)
	orq	%rax, %r9
	xorq	80(%rsp), %r15
	movq	200(%rsp), %r8
	movq	%r8, %rax
	xorq	%r15, %rax
	movq	%rax, 176(%rsp)
	orq	%r15, %r8
	xorq	72(%rsp), %r14
	movq	208(%rsp), %rax
	movq	%rax, %rdi
	xorq	%r14, %rdi
	movq	%rdi, 184(%rsp)
	orq	%r14, %rax
	movq	%rax, %rdi
	xorq	64(%rsp), %r13
	movq	216(%rsp), %r14
	movq	%r14, %rax
	xorq	%r13, %rax
	movq	%rax, 192(%rsp)
	orq	%r13, %r14
	movq	%r14, %rsi
	movq	224(%rsp), %rax
	xorq	56(%rsp), %rax
	movq	112(%rsp), %r15
	movq	%r15, %r14
	xorq	%rax, %r14
	movq	%r14, 200(%rsp)
	orq	%r15, %rax
	movq	%rax, 112(%rsp)
	xorq	104(%rsp), %rdx
	movq	232(%rsp), %r15
	movq	%r15, %r14
	xorq	%rdx, %r14
	movq	%r14, 208(%rsp)
	orq	%r15, %rdx
	movq	8(%rsp), %rax
	cmpq	328(%rsp), %rax
	jne	.L472
	movq	%rcx, %r15
	movq	%r12, %r14
	movq	%rbp, %r13
	movq	%r11, 8(%rsp)
	movq	%r10, 56(%rsp)
	movq	%r9, 64(%rsp)
	movq	%r8, 24(%rsp)
	movq	%rdi, 32(%rsp)
	movq	%rsi, 40(%rsp)
	movq	%rdx, 48(%rsp)
.L471:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	112(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	call	use_int@PLT
	addq	$344, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE164:
	.size	int64_bit_11, .-int64_bit_11
	.globl	int64_bit_12
	.type	int64_bit_12, @function
int64_bit_12:
.LFB165:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$376, %rsp
	.cfi_def_cfa_offset 432
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r15
	movslq	%ecx, %rcx
	leaq	1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 136(%rsp)
	leaq	0(,%rsi,4), %r13
	movq	%r13, 16(%rsp)
	movslq	16(%rdx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %r14
	addq	%rax, %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 144(%rsp)
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 264(%rsp)
	movl	20(%rdx), %ecx
	leal	-1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r12
	movslq	%ecx, %rcx
	leaq	-1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 152(%rsp)
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 272(%rsp)
	movl	24(%rdx), %ecx
	leal	-2(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %rbp
	movslq	%ecx, %rcx
	leaq	-2(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 160(%rsp)
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 280(%rsp)
	movl	28(%rdx), %ecx
	leal	-3(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	movq	%rsi, %rbx
	orq	%rdi, %rbx
	movslq	%ecx, %rcx
	leaq	-3(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 168(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 288(%rsp)
	movl	32(%rdx), %ecx
	leal	-4(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r11
	movq	%rsi, 8(%rsp)
	movslq	%ecx, %rcx
	leaq	-4(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 176(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 296(%rsp)
	movl	36(%rdx), %ecx
	leal	-5(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r10
	movq	%rsi, 48(%rsp)
	movslq	%ecx, %rcx
	leaq	-5(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 184(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 304(%rsp)
	movl	40(%rdx), %ecx
	leal	-6(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r9
	movq	%rsi, 56(%rsp)
	movslq	%ecx, %rcx
	leaq	-6(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 192(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 312(%rsp)
	movl	44(%rdx), %ecx
	leal	-7(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 24(%rsp)
	movslq	%ecx, %rcx
	leaq	-7(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 200(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 320(%rsp)
	movl	48(%rdx), %ecx
	leal	-8(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 32(%rsp)
	movslq	%ecx, %rcx
	leaq	-8(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 208(%rsp)
	leaq	0(,%rsi,4), %rcx
	movq	%rcx, 328(%rsp)
	movl	52(%rdx), %ecx
	leal	-9(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 40(%rsp)
	movslq	%ecx, %rcx
	leaq	-9(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %rcx
	movq	%rsi, 216(%rsp)
	salq	$2, %rcx
	movq	%rcx, 336(%rsp)
	movl	56(%rdx), %ecx
	leal	-10(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 120(%rsp)
	movslq	%ecx, %rcx
	leaq	-10(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %rcx
	movq	%rsi, 224(%rsp)
	salq	$2, %rcx
	movq	%rcx, 344(%rsp)
	movl	60(%rdx), %edx
	leal	-11(%rdx), %ecx
	movslq	%ecx, %rsi
	salq	$32, %rcx
	orq	%rsi, %rcx
	movq	%rcx, 128(%rsp)
	movslq	%edx, %rdx
	leaq	-11(%rax,%rdx), %rdx
	movq	%rdx, %rcx
	salq	$32, %rcx
	orq	%rdx, %rcx
	movq	%rcx, 232(%rsp)
	leaq	0(,%rcx,4), %rdi
	movq	%rdi, 352(%rsp)
	testq	%rax, %rax
	je	.L476
	leaq	-1(%r13), %r8
	subq	%rax, %r13
	leaq	-1(%r13), %rax
	movq	%rax, 360(%rsp)
	movq	%r8, 8(%rsp)
	movq	%r15, %rcx
	movq	%r14, %r13
	movq	24(%rsp), %r8
	movq	32(%rsp), %rdi
	movq	40(%rsp), %rsi
.L477:
	subq	$1, 8(%rsp)
	movq	8(%rsp), %r14
	movq	264(%rsp), %rax
	addq	%r14, %rax
	movq	16(%rsp), %r15
	subq	%r15, %rax
	movq	%rax, %rdx
	movq	272(%rsp), %rax
	addq	%r14, %rax
	subq	%r15, %rax
	movq	%rax, 40(%rsp)
	movq	280(%rsp), %rax
	addq	%r14, %rax
	subq	%r15, %rax
	movq	%rax, 32(%rsp)
	movq	288(%rsp), %rax
	addq	%r14, %rax
	movq	%rax, %r15
	subq	16(%rsp), %r15
	movq	%r15, 24(%rsp)
	movq	296(%rsp), %rax
	addq	%r14, %rax
	subq	16(%rsp), %rax
	movq	%rax, 104(%rsp)
	movq	304(%rsp), %rax
	addq	%r14, %rax
	movq	%rax, %r15
	subq	16(%rsp), %r15
	movq	%r15, 96(%rsp)
	movq	312(%rsp), %rax
	addq	%r14, %rax
	movq	16(%rsp), %r15
	subq	%r15, %rax
	movq	%rax, 88(%rsp)
	movq	320(%rsp), %rax
	addq	%r14, %rax
	subq	%r15, %rax
	movq	%rax, 80(%rsp)
	movq	328(%rsp), %rax
	addq	%r14, %rax
	subq	%r15, %rax
	movq	%rax, 72(%rsp)
	movq	336(%rsp), %r15
	addq	%r14, %r15
	subq	16(%rsp), %r15
	movq	%r15, 64(%rsp)
	movq	344(%rsp), %r15
	addq	%r14, %r15
	movq	%r15, %rax
	movq	16(%rsp), %r15
	subq	%r15, %rax
	movq	%rax, 56(%rsp)
	movq	352(%rsp), %rax
	addq	%r14, %rax
	subq	%r15, %rax
	movq	%rax, 112(%rsp)
	xorq	%r14, %rcx
	movq	%rcx, %rax
	movq	136(%rsp), %rcx
	movq	%rcx, %r14
	xorq	%rax, %r14
	movq	%r14, %r15
	orq	%rcx, %rax
	movq	%rdx, 48(%rsp)
	xorq	%rdx, %r13
	movq	144(%rsp), %rcx
	movq	%rcx, %r14
	xorq	%r13, %r14
	orq	%rcx, %r13
	xorq	40(%rsp), %r12
	movq	152(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r12, %rdx
	movq	%rdx, 136(%rsp)
	orq	%rcx, %r12
	xorq	32(%rsp), %rbp
	movq	160(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%rbp, %rdx
	movq	%rdx, 144(%rsp)
	orq	%rcx, %rbp
	xorq	24(%rsp), %rbx
	movq	168(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 152(%rsp)
	orq	%rcx, %rbx
	xorq	104(%rsp), %r11
	movq	176(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r11, %rdx
	movq	%rdx, 160(%rsp)
	orq	%rcx, %r11
	xorq	96(%rsp), %r10
	movq	184(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r10, %rdx
	movq	%rdx, 168(%rsp)
	orq	%rcx, %r10
	xorq	88(%rsp), %r9
	movq	192(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r9, %rdx
	movq	%rdx, 176(%rsp)
	orq	%rcx, %r9
	xorq	80(%rsp), %r8
	movq	200(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r8, %rdx
	movq	%rdx, 184(%rsp)
	orq	%rcx, %r8
	movq	%r8, 192(%rsp)
	xorq	72(%rsp), %rdi
	movq	208(%rsp), %rcx
	movq	%rcx, %r8
	xorq	%rdi, %r8
	movq	%r8, 200(%rsp)
	orq	%rcx, %rdi
	xorq	64(%rsp), %rsi
	movq	216(%rsp), %rcx
	movq	%rcx, %r8
	xorq	%rsi, %r8
	movq	%r8, 208(%rsp)
	orq	%rcx, %rsi
	movq	%rsi, 216(%rsp)
	movq	120(%rsp), %rcx
	xorq	56(%rsp), %rcx
	movq	224(%rsp), %r8
	movq	%r8, %rdx
	xorq	%rcx, %rdx
	movq	%rdx, 120(%rsp)
	orq	%r8, %rcx
	movq	128(%rsp), %rdx
	xorq	112(%rsp), %rdx
	movq	232(%rsp), %r8
	movq	%r8, %rsi
	xorq	%rdx, %rsi
	movq	%rsi, 128(%rsp)
	orq	%r8, %rdx
	xorq	8(%rsp), %rax
	movq	%r15, %rsi
	xorq	%rax, %rsi
	movq	%rsi, 224(%rsp)
	orq	%r15, %rax
	movq	%rax, 232(%rsp)
	xorq	48(%rsp), %r13
	movq	%r14, %r15
	xorq	%r13, %r15
	orq	%r14, %r13
	xorq	40(%rsp), %r12
	movq	136(%rsp), %r8
	movq	%r8, %r14
	xorq	%r12, %r14
	orq	%r8, %r12
	xorq	32(%rsp), %rbp
	movq	144(%rsp), %rsi
	movq	%rsi, %r8
	xorq	%rbp, %r8
	movq	%r8, 136(%rsp)
	orq	%rsi, %rbp
	xorq	24(%rsp), %rbx
	movq	152(%rsp), %r8
	movq	%r8, %rsi
	xorq	%rbx, %rsi
	movq	%rsi, 144(%rsp)
	orq	%r8, %rbx
	xorq	104(%rsp), %r11
	movq	160(%rsp), %rsi
	movq	%rsi, %r8
	xorq	%r11, %r8
	movq	%r8, 152(%rsp)
	orq	%rsi, %r11
	xorq	96(%rsp), %r10
	movq	168(%rsp), %r8
	movq	%r8, %rax
	xorq	%r10, %rax
	movq	%rax, 160(%rsp)
	orq	%r8, %r10
	xorq	88(%rsp), %r9
	movq	176(%rsp), %rsi
	movq	%rsi, %r8
	xorq	%r9, %r8
	movq	%r8, 168(%rsp)
	orq	%rsi, %r9
	movq	%r9, 176(%rsp)
	movq	192(%rsp), %r8
	xorq	80(%rsp), %r8
	movq	184(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r8, %rsi
	movq	%rsi, 184(%rsp)
	orq	%rax, %r8
	xorq	72(%rsp), %rdi
	movq	200(%rsp), %rsi
	movq	%rsi, %r9
	xorq	%rdi, %r9
	movq	%r9, 192(%rsp)
	orq	%rsi, %rdi
	movq	216(%rsp), %rsi
	xorq	64(%rsp), %rsi
	movq	208(%rsp), %r9
	movq	%r9, %rax
	xorq	%rsi, %rax
	movq	%rax, 200(%rsp)
	orq	%r9, %rsi
	xorq	56(%rsp), %rcx
	movq	120(%rsp), %rax
	movq	%rax, %r9
	xorq	%rcx, %r9
	movq	%r9, 120(%rsp)
	orq	%rax, %rcx
	xorq	112(%rsp), %rdx
	movq	128(%rsp), %rax
	movq	%rax, %r9
	xorq	%rdx, %r9
	movq	%r9, 128(%rsp)
	orq	%rax, %rdx
	movq	%rdx, 208(%rsp)
	movq	232(%rsp), %rax
	xorq	8(%rsp), %rax
	movq	224(%rsp), %r9
	movq	%r9, %rdx
	xorq	%rax, %rdx
	movq	%rdx, 216(%rsp)
	orq	%r9, %rax
	xorq	48(%rsp), %r13
	movq	%r15, %rdx
	xorq	%r13, %rdx
	movq	%rdx, 224(%rsp)
	orq	%r15, %r13
	xorq	40(%rsp), %r12
	movq	%r14, %r15
	xorq	%r12, %r15
	orq	%r14, %r12
	xorq	32(%rsp), %rbp
	movq	136(%rsp), %rdx
	movq	%rdx, %r14
	xorq	%rbp, %r14
	orq	%rdx, %rbp
	xorq	24(%rsp), %rbx
	movq	144(%rsp), %r9
	movq	%r9, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 136(%rsp)
	orq	%r9, %rbx
	xorq	104(%rsp), %r11
	movq	152(%rsp), %rdx
	movq	%rdx, %r9
	xorq	%r11, %r9
	movq	%r9, 144(%rsp)
	orq	%rdx, %r11
	xorq	96(%rsp), %r10
	movq	160(%rsp), %r9
	movq	%r9, %rdx
	xorq	%r10, %rdx
	movq	%rdx, 152(%rsp)
	orq	%r9, %r10
	movq	%r10, 160(%rsp)
	movq	176(%rsp), %r9
	xorq	88(%rsp), %r9
	movq	168(%rsp), %r10
	movq	%r10, %rdx
	xorq	%r9, %rdx
	movq	%rdx, 168(%rsp)
	orq	%r10, %r9
	xorq	80(%rsp), %r8
	movq	184(%rsp), %rdx
	movq	%rdx, %r10
	xorq	%r8, %r10
	movq	%r10, 176(%rsp)
	orq	%rdx, %r8
	xorq	72(%rsp), %rdi
	movq	192(%rsp), %rdx
	movq	%rdx, %r10
	xorq	%rdi, %r10
	movq	%r10, 184(%rsp)
	orq	%rdx, %rdi
	movq	%rdi, 192(%rsp)
	xorq	64(%rsp), %rsi
	movq	200(%rsp), %r10
	movq	%r10, %rdi
	xorq	%rsi, %rdi
	movq	%rdi, 200(%rsp)
	orq	%r10, %rsi
	xorq	56(%rsp), %rcx
	movq	120(%rsp), %rdi
	movq	%rdi, %r10
	xorq	%rcx, %r10
	movq	%r10, 120(%rsp)
	orq	%rdi, %rcx
	movq	208(%rsp), %rdx
	xorq	112(%rsp), %rdx
	movq	128(%rsp), %rdi
	movq	%rdi, %r10
	xorq	%rdx, %r10
	movq	%r10, 128(%rsp)
	orq	%rdi, %rdx
	xorq	8(%rsp), %rax
	movq	216(%rsp), %rdi
	movq	%rdi, %r10
	xorq	%rax, %r10
	movq	%r10, 208(%rsp)
	orq	%rdi, %rax
	movq	%rax, 216(%rsp)
	xorq	48(%rsp), %r13
	movq	224(%rsp), %rdi
	movq	%rdi, %r10
	xorq	%r13, %r10
	movq	%r10, 224(%rsp)
	orq	%rdi, %r13
	xorq	40(%rsp), %r12
	movq	%r15, %rdi
	xorq	%r12, %rdi
	movq	%rdi, 232(%rsp)
	orq	%r15, %r12
	xorq	32(%rsp), %rbp
	movq	%r14, %r15
	xorq	%rbp, %r15
	orq	%r14, %rbp
	xorq	24(%rsp), %rbx
	movq	136(%rsp), %rax
	movq	%rax, %r14
	xorq	%rbx, %r14
	orq	%rax, %rbx
	xorq	104(%rsp), %r11
	movq	144(%rsp), %rax
	movq	%rax, %rdi
	xorq	%r11, %rdi
	movq	%rdi, 136(%rsp)
	orq	%rax, %r11
	movq	160(%rsp), %r10
	xorq	96(%rsp), %r10
	movq	152(%rsp), %rdi
	movq	%rdi, %rax
	xorq	%r10, %rax
	movq	%rax, 144(%rsp)
	orq	%rdi, %r10
	movq	%r10, 152(%rsp)
	xorq	88(%rsp), %r9
	movq	168(%rsp), %rdi
	movq	%rdi, %r10
	xorq	%r9, %r10
	movq	%r10, 160(%rsp)
	orq	%rdi, %r9
	xorq	80(%rsp), %r8
	movq	176(%rsp), %rdi
	movq	%rdi, %r10
	xorq	%r8, %r10
	movq	%r10, 168(%rsp)
	orq	%rdi, %r8
	movq	%r8, 176(%rsp)
	movq	192(%rsp), %rdi
	xorq	72(%rsp), %rdi
	movq	184(%rsp), %rax
	movq	%rax, %r8
	xorq	%rdi, %r8
	movq	%r8, 184(%rsp)
	orq	%rax, %rdi
	xorq	64(%rsp), %rsi
	movq	200(%rsp), %r8
	movq	%r8, %r10
	xorq	%rsi, %r10
	movq	%r10, 192(%rsp)
	orq	%r8, %rsi
	xorq	56(%rsp), %rcx
	movq	120(%rsp), %r10
	movq	%r10, %r8
	xorq	%rcx, %r8
	movq	%r8, 120(%rsp)
	orq	%r10, %rcx
	xorq	112(%rsp), %rdx
	movq	128(%rsp), %r10
	movq	%r10, %r8
	xorq	%rdx, %r8
	movq	%r8, 128(%rsp)
	orq	%r10, %rdx
	movq	%rdx, 200(%rsp)
	movq	216(%rsp), %rax
	xorq	8(%rsp), %rax
	movq	208(%rsp), %r10
	movq	%r10, %r8
	xorq	%rax, %r8
	movq	%r8, 208(%rsp)
	orq	%r10, %rax
	xorq	48(%rsp), %r13
	movq	224(%rsp), %rdx
	movq	%rdx, %r10
	xorq	%r13, %r10
	movq	%r10, 216(%rsp)
	orq	%rdx, %r13
	xorq	40(%rsp), %r12
	movq	232(%rsp), %rdx
	movq	%rdx, %r10
	xorq	%r12, %r10
	movq	%r10, 224(%rsp)
	orq	%rdx, %r12
	xorq	32(%rsp), %rbp
	movq	%r15, %rdx
	xorq	%rbp, %rdx
	movq	%rdx, 232(%rsp)
	orq	%r15, %rbp
	xorq	24(%rsp), %rbx
	movq	%r14, %r15
	xorq	%rbx, %r15
	orq	%r14, %rbx
	xorq	104(%rsp), %r11
	movq	136(%rsp), %rdx
	movq	%rdx, %r14
	xorq	%r11, %r14
	orq	%rdx, %r11
	movq	152(%rsp), %r10
	xorq	96(%rsp), %r10
	movq	144(%rsp), %rdx
	movq	%rdx, %r8
	xorq	%r10, %r8
	movq	%r8, 136(%rsp)
	orq	%rdx, %r10
	xorq	88(%rsp), %r9
	movq	160(%rsp), %rdx
	movq	%rdx, %r8
	xorq	%r9, %r8
	movq	%r8, 144(%rsp)
	orq	%rdx, %r9
	movq	%r9, 152(%rsp)
	movq	176(%rsp), %r8
	xorq	80(%rsp), %r8
	movq	168(%rsp), %r9
	movq	%r9, %rdx
	xorq	%r8, %rdx
	movq	%rdx, 160(%rsp)
	orq	%r9, %r8
	movq	%r8, 168(%rsp)
	xorq	72(%rsp), %rdi
	movq	184(%rsp), %rdx
	movq	%rdx, %r9
	xorq	%rdi, %r9
	movq	%r9, 176(%rsp)
	orq	%rdx, %rdi
	xorq	64(%rsp), %rsi
	movq	192(%rsp), %r9
	movq	%r9, %r8
	xorq	%rsi, %r8
	movq	%r8, 184(%rsp)
	orq	%r9, %rsi
	xorq	56(%rsp), %rcx
	movq	120(%rsp), %r8
	movq	%r8, %r9
	xorq	%rcx, %r9
	movq	%r9, 120(%rsp)
	orq	%r8, %rcx
	movq	200(%rsp), %rdx
	xorq	112(%rsp), %rdx
	movq	128(%rsp), %r8
	movq	%r8, %r9
	xorq	%rdx, %r9
	movq	%r9, 128(%rsp)
	orq	%r8, %rdx
	xorq	8(%rsp), %rax
	movq	208(%rsp), %r8
	movq	%r8, %r9
	xorq	%rax, %r9
	movq	%r9, 192(%rsp)
	orq	%r8, %rax
	xorq	48(%rsp), %r13
	movq	216(%rsp), %r8
	movq	%r8, %r9
	xorq	%r13, %r9
	movq	%r9, 200(%rsp)
	orq	%r8, %r13
	xorq	40(%rsp), %r12
	movq	224(%rsp), %r8
	movq	%r8, %r9
	xorq	%r12, %r9
	movq	%r9, 208(%rsp)
	orq	%r8, %r12
	xorq	32(%rsp), %rbp
	movq	232(%rsp), %r8
	movq	%r8, %r9
	xorq	%rbp, %r9
	movq	%r9, 216(%rsp)
	orq	%r8, %rbp
	xorq	24(%rsp), %rbx
	movq	%r15, %r8
	xorq	%rbx, %r8
	movq	%r8, 224(%rsp)
	orq	%r15, %rbx
	xorq	104(%rsp), %r11
	movq	%r14, %r15
	xorq	%r11, %r15
	orq	%r14, %r11
	xorq	96(%rsp), %r10
	movq	136(%rsp), %r8
	movq	%r8, %r14
	xorq	%r10, %r14
	orq	%r8, %r10
	movq	%r10, 136(%rsp)
	movq	152(%rsp), %r9
	xorq	88(%rsp), %r9
	movq	144(%rsp), %r8
	movq	%r8, %r10
	xorq	%r9, %r10
	movq	%r10, 144(%rsp)
	orq	%r8, %r9
	movq	%r9, 152(%rsp)
	movq	168(%rsp), %r8
	xorq	80(%rsp), %r8
	movq	160(%rsp), %r9
	movq	%r9, %r10
	xorq	%r8, %r10
	movq	%r10, 160(%rsp)
	orq	%r9, %r8
	xorq	72(%rsp), %rdi
	movq	176(%rsp), %r9
	movq	%r9, %r10
	xorq	%rdi, %r10
	movq	%r10, 168(%rsp)
	orq	%r9, %rdi
	xorq	64(%rsp), %rsi
	movq	184(%rsp), %r10
	movq	%r10, %r9
	xorq	%rsi, %r9
	movq	%r9, 176(%rsp)
	orq	%r10, %rsi
	xorq	56(%rsp), %rcx
	movq	120(%rsp), %r10
	movq	%r10, %r9
	xorq	%rcx, %r9
	movq	%r9, 120(%rsp)
	orq	%r10, %rcx
	xorq	112(%rsp), %rdx
	movq	128(%rsp), %r10
	movq	%r10, %r9
	xorq	%rdx, %r9
	movq	%r9, 128(%rsp)
	orq	%r10, %rdx
	xorq	8(%rsp), %rax
	movq	192(%rsp), %r10
	movq	%r10, %r9
	xorq	%rax, %r9
	movq	%r9, 184(%rsp)
	orq	%r10, %rax
	xorq	48(%rsp), %r13
	movq	200(%rsp), %r10
	movq	%r10, %r9
	xorq	%r13, %r9
	movq	%r9, 192(%rsp)
	orq	%r10, %r13
	xorq	40(%rsp), %r12
	movq	208(%rsp), %r10
	movq	%r10, %r9
	xorq	%r12, %r9
	movq	%r9, 200(%rsp)
	orq	%r10, %r12
	xorq	32(%rsp), %rbp
	movq	216(%rsp), %r10
	movq	%r10, %r9
	xorq	%rbp, %r9
	movq	%r9, 208(%rsp)
	orq	%r10, %rbp
	xorq	24(%rsp), %rbx
	movq	224(%rsp), %r10
	movq	%r10, %r9
	xorq	%rbx, %r9
	movq	%r9, 216(%rsp)
	orq	%r10, %rbx
	xorq	104(%rsp), %r11
	movq	%r15, %r10
	xorq	%r11, %r10
	movq	%r10, 224(%rsp)
	orq	%r15, %r11
	movq	136(%rsp), %r10
	xorq	96(%rsp), %r10
	movq	%r14, %r15
	xorq	%r10, %r15
	movq	%r15, 136(%rsp)
	orq	%r14, %r10
	movq	%r10, 232(%rsp)
	movq	152(%rsp), %r9
	xorq	88(%rsp), %r9
	movq	144(%rsp), %r10
	movq	%r10, %r14
	xorq	%r9, %r14
	orq	%r10, %r9
	xorq	80(%rsp), %r8
	movq	160(%rsp), %r10
	movq	%r10, %r15
	xorq	%r8, %r15
	movq	%r15, 144(%rsp)
	orq	%r10, %r8
	xorq	72(%rsp), %rdi
	movq	168(%rsp), %r10
	movq	%r10, %r15
	xorq	%rdi, %r15
	movq	%r15, 152(%rsp)
	orq	%r10, %rdi
	xorq	64(%rsp), %rsi
	movq	176(%rsp), %r15
	movq	%r15, %r10
	xorq	%rsi, %r10
	movq	%r10, 160(%rsp)
	orq	%r15, %rsi
	xorq	56(%rsp), %rcx
	movq	120(%rsp), %r15
	movq	%r15, %r10
	xorq	%rcx, %r10
	movq	%r10, 120(%rsp)
	orq	%r15, %rcx
	xorq	112(%rsp), %rdx
	movq	128(%rsp), %r15
	movq	%r15, %r10
	xorq	%rdx, %r10
	movq	%r10, 128(%rsp)
	orq	%r15, %rdx
	movq	%rdx, 168(%rsp)
	xorq	8(%rsp), %rax
	movq	184(%rsp), %rdx
	movq	%rdx, %r15
	xorq	%rax, %r15
	movq	%r15, 176(%rsp)
	orq	%rdx, %rax
	xorq	48(%rsp), %r13
	movq	192(%rsp), %rdx
	movq	%rdx, %r15
	xorq	%r13, %r15
	movq	%r15, 184(%rsp)
	orq	%rdx, %r13
	xorq	40(%rsp), %r12
	movq	200(%rsp), %rdx
	movq	%rdx, %r10
	xorq	%r12, %r10
	movq	%r10, 192(%rsp)
	orq	%rdx, %r12
	xorq	32(%rsp), %rbp
	movq	208(%rsp), %rdx
	movq	%rdx, %r15
	xorq	%rbp, %r15
	movq	%r15, 200(%rsp)
	orq	%rdx, %rbp
	xorq	24(%rsp), %rbx
	movq	216(%rsp), %rdx
	movq	%rdx, %r10
	xorq	%rbx, %r10
	movq	%r10, 208(%rsp)
	orq	%rdx, %rbx
	xorq	104(%rsp), %r11
	movq	224(%rsp), %rdx
	movq	%rdx, %r15
	xorq	%r11, %r15
	movq	%r15, 216(%rsp)
	orq	%rdx, %r11
	movq	232(%rsp), %r10
	xorq	96(%rsp), %r10
	movq	136(%rsp), %r15
	movq	%r15, %rdx
	xorq	%r10, %rdx
	movq	%rdx, 136(%rsp)
	orq	%r15, %r10
	xorq	88(%rsp), %r9
	movq	%r14, %rdx
	xorq	%r9, %rdx
	movq	%rdx, %r15
	orq	%r14, %r9
	xorq	80(%rsp), %r8
	movq	144(%rsp), %rdx
	movq	%rdx, %r14
	xorq	%r8, %r14
	movq	%r14, 144(%rsp)
	orq	%rdx, %r8
	xorq	72(%rsp), %rdi
	movq	152(%rsp), %rdx
	movq	%rdx, %r14
	xorq	%rdi, %r14
	movq	%r14, 152(%rsp)
	orq	%rdx, %rdi
	xorq	64(%rsp), %rsi
	movq	160(%rsp), %r14
	movq	%r14, %rdx
	xorq	%rsi, %rdx
	movq	%rdx, 160(%rsp)
	orq	%r14, %rsi
	xorq	56(%rsp), %rcx
	movq	120(%rsp), %rdx
	movq	%rdx, %r14
	xorq	%rcx, %r14
	movq	%r14, 120(%rsp)
	orq	%rdx, %rcx
	movq	%rcx, 224(%rsp)
	movq	168(%rsp), %rdx
	xorq	112(%rsp), %rdx
	movq	128(%rsp), %r14
	movq	%r14, %rcx
	xorq	%rdx, %rcx
	movq	%rcx, 128(%rsp)
	orq	%r14, %rdx
	xorq	8(%rsp), %rax
	movq	176(%rsp), %r14
	movq	%r14, %rcx
	xorq	%rax, %rcx
	movq	%rcx, 168(%rsp)
	orq	%r14, %rax
	xorq	48(%rsp), %r13
	movq	184(%rsp), %r14
	movq	%r14, %rcx
	xorq	%r13, %rcx
	movq	%rcx, 176(%rsp)
	orq	%r14, %r13
	xorq	40(%rsp), %r12
	movq	192(%rsp), %r14
	movq	%r14, %rcx
	xorq	%r12, %rcx
	movq	%rcx, 184(%rsp)
	orq	%r14, %r12
	xorq	32(%rsp), %rbp
	movq	200(%rsp), %r14
	movq	%r14, %rcx
	xorq	%rbp, %rcx
	movq	%rcx, 192(%rsp)
	orq	%r14, %rbp
	xorq	24(%rsp), %rbx
	movq	208(%rsp), %r14
	movq	%r14, %rcx
	xorq	%rbx, %rcx
	movq	%rcx, 200(%rsp)
	orq	%r14, %rbx
	xorq	104(%rsp), %r11
	movq	216(%rsp), %r14
	movq	%r14, %rcx
	xorq	%r11, %rcx
	movq	%rcx, 208(%rsp)
	orq	%r14, %r11
	xorq	96(%rsp), %r10
	movq	136(%rsp), %r14
	movq	%r14, %rcx
	xorq	%r10, %rcx
	movq	%rcx, 216(%rsp)
	orq	%r14, %r10
	xorq	88(%rsp), %r9
	movq	%r15, %r14
	xorq	%r9, %r14
	movq	%r14, 232(%rsp)
	orq	%r9, %r15
	xorq	80(%rsp), %r8
	movq	144(%rsp), %rcx
	movq	%rcx, %r14
	xorq	%r8, %r14
	movq	%r14, 240(%rsp)
	orq	%r8, %rcx
	movq	%rcx, %r14
	xorq	72(%rsp), %rdi
	movq	152(%rsp), %r9
	movq	%r9, %r8
	xorq	%rdi, %r8
	movq	%r8, 248(%rsp)
	orq	%r9, %rdi
	xorq	64(%rsp), %rsi
	movq	160(%rsp), %r9
	movq	%r9, %r8
	xorq	%rsi, %r8
	movq	%r8, 256(%rsp)
	orq	%r9, %rsi
	movq	224(%rsp), %rcx
	xorq	56(%rsp), %rcx
	movq	120(%rsp), %r9
	movq	%r9, %r8
	xorq	%rcx, %r8
	movq	%r8, 120(%rsp)
	movq	%rcx, %r8
	orq	%r9, %r8
	movq	%r8, 224(%rsp)
	xorq	112(%rsp), %rdx
	movq	128(%rsp), %rcx
	movq	%rcx, %r9
	xorq	%rdx, %r9
	movq	%r9, 128(%rsp)
	orq	%rcx, %rdx
	xorq	8(%rsp), %rax
	movq	168(%rsp), %rcx
	movq	%rcx, %r9
	xorq	%rax, %r9
	movq	%r9, 136(%rsp)
	orq	%rax, %rcx
	movq	48(%rsp), %rax
	xorq	%r13, %rax
	movq	176(%rsp), %r13
	movq	%r13, %r9
	xorq	%rax, %r9
	movq	%r9, 144(%rsp)
	orq	%rax, %r13
	movq	40(%rsp), %rax
	xorq	%r12, %rax
	movq	184(%rsp), %r12
	movq	%r12, %r9
	xorq	%rax, %r9
	movq	%r9, 152(%rsp)
	orq	%rax, %r12
	movq	32(%rsp), %rax
	xorq	%rbp, %rax
	movq	192(%rsp), %rbp
	movq	%rbp, %r9
	xorq	%rax, %r9
	movq	%r9, 160(%rsp)
	orq	%rax, %rbp
	movq	24(%rsp), %rax
	xorq	%rbx, %rax
	movq	200(%rsp), %rbx
	movq	%rbx, %r9
	xorq	%rax, %r9
	movq	%r9, 168(%rsp)
	orq	%rax, %rbx
	movq	104(%rsp), %rax
	xorq	%r11, %rax
	movq	208(%rsp), %r11
	movq	%r11, %r9
	xorq	%rax, %r9
	movq	%r9, 176(%rsp)
	orq	%rax, %r11
	movq	96(%rsp), %rax
	xorq	%r10, %rax
	movq	216(%rsp), %r10
	movq	%r10, %r9
	xorq	%rax, %r9
	movq	%r9, 184(%rsp)
	orq	%rax, %r10
	xorq	88(%rsp), %r15
	movq	232(%rsp), %r9
	movq	%r9, %rax
	xorq	%r15, %rax
	movq	%rax, 192(%rsp)
	orq	%r15, %r9
	xorq	80(%rsp), %r14
	movq	240(%rsp), %r15
	movq	%r15, %rax
	xorq	%r14, %rax
	movq	%rax, 200(%rsp)
	orq	%r14, %r15
	movq	%r15, %r8
	movq	72(%rsp), %rax
	xorq	%rdi, %rax
	movq	248(%rsp), %rdi
	movq	%rdi, %r15
	xorq	%rax, %r15
	movq	%r15, 208(%rsp)
	orq	%rax, %rdi
	movq	64(%rsp), %rax
	xorq	%rsi, %rax
	movq	256(%rsp), %rsi
	movq	%rsi, %r15
	xorq	%rax, %r15
	movq	%r15, 216(%rsp)
	orq	%rax, %rsi
	movq	224(%rsp), %rax
	xorq	56(%rsp), %rax
	movq	120(%rsp), %r15
	movq	%r15, %r14
	xorq	%rax, %r14
	movq	%r14, 224(%rsp)
	orq	%r15, %rax
	movq	%rax, 120(%rsp)
	xorq	112(%rsp), %rdx
	movq	128(%rsp), %r15
	movq	%r15, %r14
	xorq	%rdx, %r14
	movq	%r14, 232(%rsp)
	orq	%r15, %rdx
	movq	%rdx, 128(%rsp)
	movq	8(%rsp), %rax
	cmpq	360(%rsp), %rax
	jne	.L477
	movq	%rcx, %r15
	movq	%r13, %r14
	movq	%r11, 8(%rsp)
	movq	%r10, 48(%rsp)
	movq	%r9, 56(%rsp)
	movq	%r8, 24(%rsp)
	movq	%rdi, 32(%rsp)
	movq	%rsi, 40(%rsp)
.L476:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	120(%rsp), %edi
	call	use_int@PLT
	movl	128(%rsp), %edi
	call	use_int@PLT
	addq	$376, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE165:
	.size	int64_bit_12, .-int64_bit_12
	.globl	int64_bit_13
	.type	int64_bit_13, @function
int64_bit_13:
.LFB166:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$424, %rsp
	.cfi_def_cfa_offset 480
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r15
	movslq	%ecx, %rcx
	leaq	1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 136(%rsp)
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 296(%rsp)
	movslq	16(%rdx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %r14
	addq	%rax, %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 144(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 304(%rsp)
	movl	20(%rdx), %ecx
	leal	-1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r13
	movslq	%ecx, %rcx
	leaq	-1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 152(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 312(%rsp)
	movl	24(%rdx), %ecx
	leal	-2(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r12
	movslq	%ecx, %rcx
	leaq	-2(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 160(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 320(%rsp)
	movl	28(%rdx), %ecx
	leal	-3(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %rbp
	movslq	%ecx, %rcx
	leaq	-3(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 168(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 328(%rsp)
	movl	32(%rdx), %ecx
	leal	-4(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r9
	movq	%rsi, (%rsp)
	movslq	%ecx, %rcx
	leaq	-4(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 176(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 336(%rsp)
	movl	36(%rdx), %ecx
	leal	-5(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r11
	movq	%rsi, 32(%rsp)
	movslq	%ecx, %rcx
	leaq	-5(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 184(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 344(%rsp)
	movl	40(%rdx), %ecx
	leal	-6(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r10
	movq	%rsi, 40(%rsp)
	movslq	%ecx, %rcx
	leaq	-6(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 192(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 352(%rsp)
	movl	44(%rdx), %ecx
	leal	-7(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 8(%rsp)
	movslq	%ecx, %rcx
	leaq	-7(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 200(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 360(%rsp)
	movl	48(%rdx), %ecx
	leal	-8(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 16(%rsp)
	movslq	%ecx, %rcx
	leaq	-8(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 208(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 368(%rsp)
	movl	52(%rdx), %ecx
	leal	-9(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 24(%rsp)
	movslq	%ecx, %rcx
	leaq	-9(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 216(%rsp)
	leaq	0(,%rsi,4), %rcx
	movq	%rcx, 376(%rsp)
	movl	56(%rdx), %ecx
	leal	-10(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 120(%rsp)
	movslq	%ecx, %rcx
	leaq	-10(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %rcx
	movq	%rsi, 224(%rsp)
	salq	$2, %rcx
	movq	%rcx, 384(%rsp)
	movl	60(%rdx), %ecx
	leal	-11(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 128(%rsp)
	movslq	%ecx, %rcx
	leaq	-11(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %rcx
	movq	%rsi, 232(%rsp)
	salq	$2, %rcx
	movq	%rcx, 392(%rsp)
	movl	64(%rdx), %edx
	leal	-12(%rdx), %ecx
	movslq	%ecx, %rsi
	salq	$32, %rcx
	orq	%rsi, %rcx
	movq	%rcx, 112(%rsp)
	movslq	%edx, %rdx
	leaq	-12(%rax,%rdx), %rdx
	movq	%rdx, %rcx
	salq	$32, %rcx
	orq	%rdx, %rcx
	movq	%rcx, 240(%rsp)
	salq	$2, %rcx
	movq	%rcx, 400(%rsp)
	testq	%rax, %rax
	je	.L481
	leaq	-1(%rbx), %r8
	movq	%rbx, %rcx
	subq	%rax, %rcx
	leaq	-1(%rcx), %rax
	movq	%rax, 408(%rsp)
	movq	%r8, (%rsp)
	movq	%r15, %rcx
	movq	%r9, %rbx
	movq	8(%rsp), %r9
	movq	16(%rsp), %r8
	movq	24(%rsp), %rdi
.L482:
	subq	$1, (%rsp)
	movq	(%rsp), %r15
	movq	304(%rsp), %rax
	addq	%r15, %rax
	movq	296(%rsp), %rsi
	subq	%rsi, %rax
	movq	%rax, %rdx
	movq	312(%rsp), %rax
	addq	%r15, %rax
	subq	%rsi, %rax
	movq	%rax, 24(%rsp)
	movq	320(%rsp), %rax
	addq	%r15, %rax
	subq	%rsi, %rax
	movq	%rax, 16(%rsp)
	movq	328(%rsp), %rax
	addq	%r15, %rax
	subq	%rsi, %rax
	movq	%rax, 8(%rsp)
	movq	336(%rsp), %rax
	addq	%r15, %rax
	subq	%rsi, %rax
	movq	%rax, 96(%rsp)
	movq	344(%rsp), %rax
	addq	%r15, %rax
	subq	%rsi, %rax
	movq	%rax, 88(%rsp)
	movq	352(%rsp), %rax
	addq	%r15, %rax
	subq	%rsi, %rax
	movq	%rax, 80(%rsp)
	movq	360(%rsp), %rax
	addq	%r15, %rax
	subq	%rsi, %rax
	movq	%rax, 72(%rsp)
	movq	368(%rsp), %rax
	addq	%r15, %rax
	subq	%rsi, %rax
	movq	%rax, 64(%rsp)
	movq	376(%rsp), %rax
	addq	%r15, %rax
	subq	%rsi, %rax
	movq	%rax, 56(%rsp)
	movq	384(%rsp), %rax
	addq	%r15, %rax
	subq	%rsi, %rax
	movq	%rax, 48(%rsp)
	movq	392(%rsp), %rax
	addq	%r15, %rax
	subq	%rsi, %rax
	movq	%rax, 40(%rsp)
	movq	400(%rsp), %rax
	addq	%r15, %rax
	subq	%rsi, %rax
	movq	%rax, 104(%rsp)
	xorq	%r15, %rcx
	movq	%rcx, %rax
	movq	136(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%rax, %rsi
	movq	%rsi, %r15
	orq	%rcx, %rax
	movq	%rdx, 32(%rsp)
	xorq	%rdx, %r14
	movq	144(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%r14, %rsi
	movq	%rsi, 136(%rsp)
	orq	%rcx, %r14
	xorq	24(%rsp), %r13
	movq	152(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%r13, %rsi
	movq	%rsi, 144(%rsp)
	orq	%rcx, %r13
	xorq	16(%rsp), %r12
	movq	160(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%r12, %rsi
	movq	%rsi, 152(%rsp)
	orq	%rcx, %r12
	xorq	8(%rsp), %rbp
	movq	168(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%rbp, %rsi
	movq	%rsi, 160(%rsp)
	orq	%rcx, %rbp
	xorq	96(%rsp), %rbx
	movq	176(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%rbx, %rsi
	movq	%rsi, 168(%rsp)
	orq	%rcx, %rbx
	xorq	88(%rsp), %r11
	movq	184(%rsp), %rsi
	movq	%rsi, %rcx
	xorq	%r11, %rcx
	movq	%rcx, 176(%rsp)
	orq	%rsi, %r11
	xorq	80(%rsp), %r10
	movq	192(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r10, %rcx
	movq	%rcx, 184(%rsp)
	orq	%rdx, %r10
	xorq	72(%rsp), %r9
	movq	200(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r9, %rdx
	movq	%rdx, 192(%rsp)
	orq	%rcx, %r9
	xorq	64(%rsp), %r8
	movq	208(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r8, %rdx
	movq	%rdx, 200(%rsp)
	orq	%rcx, %r8
	movq	%r8, 208(%rsp)
	xorq	56(%rsp), %rdi
	movq	216(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%rdi, %rdx
	movq	%rdx, 216(%rsp)
	orq	%rcx, %rdi
	movq	120(%rsp), %rsi
	xorq	48(%rsp), %rsi
	movq	224(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%rsi, %rdx
	movq	%rdx, 120(%rsp)
	orq	%rcx, %rsi
	movq	%rsi, 224(%rsp)
	movq	128(%rsp), %rcx
	xorq	40(%rsp), %rcx
	movq	232(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%rcx, %rdx
	movq	%rdx, 128(%rsp)
	orq	%rsi, %rcx
	movq	112(%rsp), %rdx
	xorq	104(%rsp), %rdx
	movq	240(%rsp), %rsi
	movq	%rsi, %r8
	xorq	%rdx, %r8
	movq	%r8, 112(%rsp)
	orq	%rsi, %rdx
	xorq	(%rsp), %rax
	movq	%r15, %rsi
	xorq	%rax, %rsi
	movq	%rsi, 232(%rsp)
	orq	%r15, %rax
	movq	%rax, 240(%rsp)
	xorq	32(%rsp), %r14
	movq	136(%rsp), %r8
	movq	%r8, %r15
	xorq	%r14, %r15
	orq	%r8, %r14
	xorq	24(%rsp), %r13
	movq	144(%rsp), %r8
	movq	%r8, %rsi
	xorq	%r13, %rsi
	movq	%rsi, 136(%rsp)
	orq	%r8, %r13
	xorq	16(%rsp), %r12
	movq	152(%rsp), %r8
	movq	%r8, %rsi
	xorq	%r12, %rsi
	movq	%rsi, 144(%rsp)
	orq	%r8, %r12
	xorq	8(%rsp), %rbp
	movq	160(%rsp), %r8
	movq	%r8, %rsi
	xorq	%rbp, %rsi
	movq	%rsi, 152(%rsp)
	orq	%r8, %rbp
	xorq	96(%rsp), %rbx
	movq	168(%rsp), %rsi
	movq	%rsi, %r8
	xorq	%rbx, %r8
	movq	%r8, 160(%rsp)
	orq	%rsi, %rbx
	xorq	88(%rsp), %r11
	movq	176(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r11, %rsi
	movq	%rsi, 168(%rsp)
	orq	%rax, %r11
	xorq	80(%rsp), %r10
	movq	184(%rsp), %r8
	movq	%r8, %rax
	xorq	%r10, %rax
	movq	%rax, 176(%rsp)
	orq	%r8, %r10
	xorq	72(%rsp), %r9
	movq	192(%rsp), %r8
	movq	%r8, %rax
	xorq	%r9, %rax
	movq	%rax, 184(%rsp)
	orq	%r8, %r9
	movq	208(%rsp), %r8
	xorq	64(%rsp), %r8
	movq	200(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r8, %rsi
	movq	%rsi, 192(%rsp)
	orq	%rax, %r8
	xorq	56(%rsp), %rdi
	movq	216(%rsp), %rsi
	movq	%rsi, %rax
	xorq	%rdi, %rax
	movq	%rax, 200(%rsp)
	orq	%rsi, %rdi
	movq	%rdi, 208(%rsp)
	movq	224(%rsp), %rsi
	xorq	48(%rsp), %rsi
	movq	120(%rsp), %rax
	movq	%rax, %rdi
	xorq	%rsi, %rdi
	movq	%rdi, 120(%rsp)
	orq	%rax, %rsi
	xorq	40(%rsp), %rcx
	movq	128(%rsp), %rdi
	movq	%rdi, %rax
	xorq	%rcx, %rax
	movq	%rax, 128(%rsp)
	orq	%rdi, %rcx
	xorq	104(%rsp), %rdx
	movq	112(%rsp), %rdi
	movq	%rdi, %rax
	xorq	%rdx, %rax
	movq	%rax, 112(%rsp)
	orq	%rdi, %rdx
	movq	%rdx, 216(%rsp)
	movq	240(%rsp), %rax
	xorq	(%rsp), %rax
	movq	232(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%rax, %rdx
	movq	%rdx, 224(%rsp)
	orq	%rdi, %rax
	xorq	32(%rsp), %r14
	movq	%r15, %rdx
	xorq	%r14, %rdx
	movq	%rdx, 232(%rsp)
	orq	%r15, %r14
	xorq	24(%rsp), %r13
	movq	136(%rsp), %rdi
	movq	%rdi, %r15
	xorq	%r13, %r15
	movq	%r15, 136(%rsp)
	orq	%rdi, %r13
	xorq	16(%rsp), %r12
	movq	144(%rsp), %r15
	movq	%r15, %rdi
	xorq	%r12, %rdi
	movq	%rdi, 144(%rsp)
	orq	%r15, %r12
	xorq	8(%rsp), %rbp
	movq	152(%rsp), %rdx
	movq	%rdx, %r15
	xorq	%rbp, %r15
	movq	%r15, 152(%rsp)
	orq	%rdx, %rbp
	xorq	96(%rsp), %rbx
	movq	160(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 160(%rsp)
	orq	%rdi, %rbx
	xorq	88(%rsp), %r11
	movq	168(%rsp), %r15
	movq	%r15, %rdi
	xorq	%r11, %rdi
	movq	%rdi, 168(%rsp)
	orq	%r15, %r11
	xorq	80(%rsp), %r10
	movq	176(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r10, %rdx
	movq	%rdx, 176(%rsp)
	orq	%rdi, %r10
	xorq	72(%rsp), %r9
	movq	184(%rsp), %rdi
	movq	%rdi, %r15
	xorq	%r9, %r15
	movq	%r15, 184(%rsp)
	orq	%rdi, %r9
	movq	%r9, 240(%rsp)
	xorq	64(%rsp), %r8
	movq	192(%rsp), %r15
	movq	%r15, %r9
	xorq	%r8, %r9
	movq	%r9, 192(%rsp)
	orq	%r15, %r8
	movq	208(%rsp), %rdi
	xorq	56(%rsp), %rdi
	movq	200(%rsp), %r9
	movq	%r9, %r15
	xorq	%rdi, %r15
	movq	%r15, 200(%rsp)
	orq	%r9, %rdi
	xorq	48(%rsp), %rsi
	movq	120(%rsp), %r9
	movq	%r9, %r15
	xorq	%rsi, %r15
	movq	%r15, 120(%rsp)
	orq	%r9, %rsi
	xorq	40(%rsp), %rcx
	movq	128(%rsp), %r9
	movq	%r9, %r15
	xorq	%rcx, %r15
	movq	%r15, 128(%rsp)
	orq	%r9, %rcx
	movq	216(%rsp), %rdx
	xorq	104(%rsp), %rdx
	movq	112(%rsp), %r9
	movq	%r9, %r15
	xorq	%rdx, %r15
	movq	%r15, 112(%rsp)
	orq	%r9, %rdx
	xorq	(%rsp), %rax
	movq	224(%rsp), %r9
	movq	%r9, %r15
	xorq	%rax, %r15
	movq	%r15, 208(%rsp)
	orq	%r9, %rax
	movq	%rax, 216(%rsp)
	xorq	32(%rsp), %r14
	movq	232(%rsp), %r9
	movq	%r9, %rax
	xorq	%r14, %rax
	movq	%rax, 224(%rsp)
	orq	%r9, %r14
	xorq	24(%rsp), %r13
	movq	136(%rsp), %r15
	movq	%r15, %r9
	xorq	%r13, %r9
	movq	%r9, 136(%rsp)
	orq	%r15, %r13
	xorq	16(%rsp), %r12
	movq	144(%rsp), %r9
	movq	%r9, %r15
	xorq	%r12, %r15
	orq	%r9, %r12
	xorq	8(%rsp), %rbp
	movq	152(%rsp), %r9
	movq	%r9, %rax
	xorq	%rbp, %rax
	movq	%rax, 144(%rsp)
	orq	%r9, %rbp
	xorq	96(%rsp), %rbx
	movq	160(%rsp), %r9
	movq	%r9, %rax
	xorq	%rbx, %rax
	movq	%rax, 152(%rsp)
	orq	%r9, %rbx
	xorq	88(%rsp), %r11
	movq	168(%rsp), %r9
	movq	%r9, %rax
	xorq	%r11, %rax
	movq	%rax, 160(%rsp)
	orq	%r9, %r11
	xorq	80(%rsp), %r10
	movq	176(%rsp), %r9
	movq	%r9, %rax
	xorq	%r10, %rax
	movq	%rax, 168(%rsp)
	orq	%r9, %r10
	movq	%r10, 176(%rsp)
	movq	240(%rsp), %r9
	xorq	72(%rsp), %r9
	movq	184(%rsp), %rax
	movq	%rax, %r10
	xorq	%r9, %r10
	movq	%r10, 184(%rsp)
	orq	%rax, %r9
	xorq	64(%rsp), %r8
	movq	192(%rsp), %r10
	movq	%r10, %rax
	xorq	%r8, %rax
	movq	%rax, 192(%rsp)
	orq	%r10, %r8
	xorq	56(%rsp), %rdi
	movq	200(%rsp), %rax
	movq	%rax, %r10
	xorq	%rdi, %r10
	movq	%r10, 200(%rsp)
	orq	%rax, %rdi
	xorq	48(%rsp), %rsi
	movq	120(%rsp), %rax
	movq	%rax, %r10
	xorq	%rsi, %r10
	movq	%r10, 120(%rsp)
	orq	%rax, %rsi
	xorq	40(%rsp), %rcx
	movq	128(%rsp), %rax
	movq	%rax, %r10
	xorq	%rcx, %r10
	movq	%r10, 128(%rsp)
	orq	%rax, %rcx
	xorq	104(%rsp), %rdx
	movq	112(%rsp), %rax
	movq	%rax, %r10
	xorq	%rdx, %r10
	movq	%r10, 112(%rsp)
	orq	%rax, %rdx
	movq	%rdx, 232(%rsp)
	movq	216(%rsp), %rax
	xorq	(%rsp), %rax
	movq	208(%rsp), %r10
	movq	%r10, %rdx
	xorq	%rax, %rdx
	movq	%rdx, 208(%rsp)
	orq	%r10, %rax
	xorq	32(%rsp), %r14
	movq	224(%rsp), %rdx
	movq	%rdx, %r10
	xorq	%r14, %r10
	movq	%r10, 216(%rsp)
	orq	%rdx, %r14
	xorq	24(%rsp), %r13
	movq	136(%rsp), %rdx
	movq	%rdx, %r10
	xorq	%r13, %r10
	movq	%r10, 136(%rsp)
	orq	%rdx, %r13
	xorq	16(%rsp), %r12
	movq	%r15, %rdx
	xorq	%r12, %rdx
	movq	%rdx, 224(%rsp)
	orq	%r15, %r12
	xorq	8(%rsp), %rbp
	movq	144(%rsp), %rdx
	movq	%rdx, %r15
	xorq	%rbp, %r15
	orq	%rdx, %rbp
	xorq	96(%rsp), %rbx
	movq	152(%rsp), %rdx
	movq	%rdx, %r10
	xorq	%rbx, %r10
	movq	%r10, 144(%rsp)
	orq	%rdx, %rbx
	xorq	88(%rsp), %r11
	movq	160(%rsp), %rdx
	movq	%rdx, %r10
	xorq	%r11, %r10
	movq	%r10, 152(%rsp)
	orq	%rdx, %r11
	movq	%r11, 160(%rsp)
	movq	176(%rsp), %r10
	xorq	80(%rsp), %r10
	movq	168(%rsp), %rdx
	movq	%rdx, %r11
	xorq	%r10, %r11
	movq	%r11, 168(%rsp)
	orq	%rdx, %r10
	movq	%r10, 176(%rsp)
	xorq	72(%rsp), %r9
	movq	184(%rsp), %rdx
	movq	%rdx, %r11
	xorq	%r9, %r11
	movq	%r11, 184(%rsp)
	orq	%rdx, %r9
	xorq	64(%rsp), %r8
	movq	192(%rsp), %r11
	movq	%r11, %r10
	xorq	%r8, %r10
	movq	%r10, 192(%rsp)
	orq	%r11, %r8
	xorq	56(%rsp), %rdi
	movq	200(%rsp), %r10
	movq	%r10, %r11
	xorq	%rdi, %r11
	movq	%r11, 200(%rsp)
	orq	%r10, %rdi
	xorq	48(%rsp), %rsi
	movq	120(%rsp), %r10
	movq	%r10, %r11
	xorq	%rsi, %r11
	movq	%r11, 120(%rsp)
	orq	%r10, %rsi
	xorq	40(%rsp), %rcx
	movq	128(%rsp), %r10
	movq	%r10, %r11
	xorq	%rcx, %r11
	movq	%r11, 128(%rsp)
	orq	%r10, %rcx
	movq	232(%rsp), %rdx
	xorq	104(%rsp), %rdx
	movq	112(%rsp), %r10
	movq	%r10, %r11
	xorq	%rdx, %r11
	movq	%r11, 112(%rsp)
	orq	%r10, %rdx
	xorq	(%rsp), %rax
	movq	208(%rsp), %r10
	movq	%r10, %r11
	xorq	%rax, %r11
	movq	%r11, 208(%rsp)
	orq	%r10, %rax
	xorq	32(%rsp), %r14
	movq	216(%rsp), %r10
	movq	%r10, %r11
	xorq	%r14, %r11
	movq	%r11, 216(%rsp)
	orq	%r10, %r14
	xorq	24(%rsp), %r13
	movq	136(%rsp), %r10
	movq	%r10, %r11
	xorq	%r13, %r11
	movq	%r11, 136(%rsp)
	orq	%r10, %r13
	xorq	16(%rsp), %r12
	movq	224(%rsp), %r10
	movq	%r10, %r11
	xorq	%r12, %r11
	movq	%r11, 224(%rsp)
	orq	%r10, %r12
	xorq	8(%rsp), %rbp
	movq	%r15, %r10
	xorq	%rbp, %r10
	movq	%r10, 232(%rsp)
	orq	%r15, %rbp
	xorq	96(%rsp), %rbx
	movq	144(%rsp), %r15
	movq	%r15, %r10
	xorq	%rbx, %r10
	movq	%r10, 144(%rsp)
	orq	%r15, %rbx
	movq	160(%rsp), %r11
	xorq	88(%rsp), %r11
	movq	152(%rsp), %r15
	movq	%r15, %r10
	xorq	%r11, %r10
	movq	%r10, 152(%rsp)
	orq	%r15, %r11
	movq	%r11, 160(%rsp)
	movq	176(%rsp), %r10
	xorq	80(%rsp), %r10
	movq	168(%rsp), %r15
	movq	%r15, %r11
	xorq	%r10, %r11
	movq	%r11, 168(%rsp)
	orq	%r15, %r10
	xorq	72(%rsp), %r9
	movq	184(%rsp), %r11
	movq	%r11, %r15
	xorq	%r9, %r15
	movq	%r15, 176(%rsp)
	orq	%r11, %r9
	xorq	64(%rsp), %r8
	movq	192(%rsp), %r15
	movq	%r15, %r11
	xorq	%r8, %r11
	movq	%r11, 184(%rsp)
	orq	%r15, %r8
	xorq	56(%rsp), %rdi
	movq	200(%rsp), %r11
	movq	%r11, %r15
	xorq	%rdi, %r15
	movq	%r15, 192(%rsp)
	orq	%r11, %rdi
	xorq	48(%rsp), %rsi
	movq	120(%rsp), %r11
	movq	%r11, %r15
	xorq	%rsi, %r15
	movq	%r15, 120(%rsp)
	orq	%r11, %rsi
	xorq	40(%rsp), %rcx
	movq	128(%rsp), %r11
	movq	%r11, %r15
	xorq	%rcx, %r15
	movq	%r15, 128(%rsp)
	orq	%r11, %rcx
	xorq	104(%rsp), %rdx
	movq	112(%rsp), %r11
	movq	%r11, %r15
	xorq	%rdx, %r15
	movq	%r15, 112(%rsp)
	orq	%r11, %rdx
	movq	%rdx, 200(%rsp)
	xorq	(%rsp), %rax
	movq	208(%rsp), %r11
	movq	%r11, %r15
	xorq	%rax, %r15
	movq	%r15, 208(%rsp)
	orq	%r11, %rax
	xorq	32(%rsp), %r14
	movq	216(%rsp), %rdx
	movq	%rdx, %r11
	xorq	%r14, %r11
	movq	%r11, 216(%rsp)
	orq	%rdx, %r14
	xorq	24(%rsp), %r13
	movq	136(%rsp), %rdx
	movq	%rdx, %r11
	xorq	%r13, %r11
	movq	%r11, 136(%rsp)
	orq	%rdx, %r13
	xorq	16(%rsp), %r12
	movq	224(%rsp), %rdx
	movq	%rdx, %r11
	xorq	%r12, %r11
	movq	%r11, 224(%rsp)
	orq	%rdx, %r12
	xorq	8(%rsp), %rbp
	movq	232(%rsp), %rdx
	movq	%rdx, %r11
	xorq	%rbp, %r11
	movq	%r11, 232(%rsp)
	orq	%rdx, %rbp
	xorq	96(%rsp), %rbx
	movq	144(%rsp), %r15
	movq	%r15, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 144(%rsp)
	orq	%r15, %rbx
	movq	%rbx, 240(%rsp)
	movq	160(%rsp), %r11
	xorq	88(%rsp), %r11
	movq	152(%rsp), %rdx
	movq	%rdx, %rbx
	xorq	%r11, %rbx
	movq	%rbx, %r15
	orq	%rdx, %r11
	xorq	80(%rsp), %r10
	movq	168(%rsp), %rbx
	movq	%rbx, %rdx
	xorq	%r10, %rdx
	movq	%rdx, 152(%rsp)
	orq	%rbx, %r10
	movq	%r10, 160(%rsp)
	xorq	72(%rsp), %r9
	movq	176(%rsp), %rdx
	movq	%rdx, %rbx
	xorq	%r9, %rbx
	movq	%rbx, 168(%rsp)
	orq	%rdx, %r9
	xorq	64(%rsp), %r8
	movq	184(%rsp), %rbx
	movq	%rbx, %r10
	xorq	%r8, %r10
	movq	%r10, 176(%rsp)
	orq	%rbx, %r8
	xorq	56(%rsp), %rdi
	movq	192(%rsp), %r10
	movq	%r10, %rbx
	xorq	%rdi, %rbx
	movq	%rbx, 184(%rsp)
	orq	%r10, %rdi
	xorq	48(%rsp), %rsi
	movq	120(%rsp), %r10
	movq	%r10, %rbx
	xorq	%rsi, %rbx
	movq	%rbx, 120(%rsp)
	orq	%r10, %rsi
	xorq	40(%rsp), %rcx
	movq	128(%rsp), %r10
	movq	%r10, %rbx
	xorq	%rcx, %rbx
	movq	%rbx, 128(%rsp)
	orq	%r10, %rcx
	movq	%rcx, 192(%rsp)
	movq	200(%rsp), %rdx
	xorq	104(%rsp), %rdx
	movq	112(%rsp), %r10
	movq	%r10, %rbx
	xorq	%rdx, %rbx
	movq	%rbx, 112(%rsp)
	orq	%r10, %rdx
	xorq	(%rsp), %rax
	movq	208(%rsp), %r10
	movq	%r10, %rbx
	xorq	%rax, %rbx
	movq	%rbx, 200(%rsp)
	orq	%r10, %rax
	xorq	32(%rsp), %r14
	movq	216(%rsp), %r10
	movq	%r10, %rbx
	xorq	%r14, %rbx
	movq	%rbx, 208(%rsp)
	orq	%r10, %r14
	xorq	24(%rsp), %r13
	movq	136(%rsp), %r10
	movq	%r10, %rbx
	xorq	%r13, %rbx
	movq	%rbx, 136(%rsp)
	orq	%r10, %r13
	xorq	16(%rsp), %r12
	movq	224(%rsp), %r10
	movq	%r10, %rbx
	xorq	%r12, %rbx
	movq	%rbx, 216(%rsp)
	orq	%r10, %r12
	xorq	8(%rsp), %rbp
	movq	232(%rsp), %r10
	movq	%r10, %rbx
	xorq	%rbp, %rbx
	movq	%rbx, 224(%rsp)
	orq	%r10, %rbp
	movq	240(%rsp), %rbx
	xorq	96(%rsp), %rbx
	movq	144(%rsp), %r10
	movq	%r10, %rcx
	xorq	%rbx, %rcx
	movq	%rcx, 144(%rsp)
	orq	%r10, %rbx
	movq	%rbx, 232(%rsp)
	xorq	88(%rsp), %r11
	movq	%r15, %rcx
	xorq	%r11, %rcx
	movq	%rcx, 240(%rsp)
	orq	%r15, %r11
	movq	160(%rsp), %r10
	xorq	80(%rsp), %r10
	movq	152(%rsp), %r15
	movq	%r15, %rbx
	xorq	%r10, %rbx
	movq	%rbx, 152(%rsp)
	orq	%r15, %r10
	xorq	72(%rsp), %r9
	movq	168(%rsp), %rcx
	movq	%rcx, %r15
	xorq	%r9, %r15
	movq	%r15, 160(%rsp)
	orq	%rcx, %r9
	xorq	64(%rsp), %r8
	movq	176(%rsp), %r15
	movq	%r15, %rbx
	xorq	%r8, %rbx
	movq	%rbx, 168(%rsp)
	orq	%r15, %r8
	xorq	56(%rsp), %rdi
	movq	184(%rsp), %rbx
	movq	%rbx, %r15
	xorq	%rdi, %r15
	movq	%r15, 176(%rsp)
	orq	%rbx, %rdi
	xorq	48(%rsp), %rsi
	movq	120(%rsp), %rbx
	movq	%rbx, %r15
	xorq	%rsi, %r15
	movq	%r15, 120(%rsp)
	orq	%rbx, %rsi
	movq	192(%rsp), %rcx
	xorq	40(%rsp), %rcx
	movq	128(%rsp), %rbx
	movq	%rbx, %r15
	xorq	%rcx, %r15
	movq	%r15, 128(%rsp)
	orq	%rbx, %rcx
	xorq	104(%rsp), %rdx
	movq	112(%rsp), %rbx
	movq	%rbx, %r15
	xorq	%rdx, %r15
	movq	%r15, 112(%rsp)
	orq	%rbx, %rdx
	movq	%rdx, 184(%rsp)
	xorq	(%rsp), %rax
	movq	200(%rsp), %rbx
	movq	%rbx, %r15
	xorq	%rax, %r15
	movq	%r15, 192(%rsp)
	orq	%rbx, %rax
	xorq	32(%rsp), %r14
	movq	208(%rsp), %rbx
	movq	%rbx, %r15
	xorq	%r14, %r15
	movq	%r15, 200(%rsp)
	orq	%rbx, %r14
	xorq	24(%rsp), %r13
	movq	136(%rsp), %rbx
	movq	%rbx, %r15
	xorq	%r13, %r15
	movq	%r15, 208(%rsp)
	orq	%rbx, %r13
	xorq	16(%rsp), %r12
	movq	216(%rsp), %rbx
	movq	%rbx, %r15
	xorq	%r12, %r15
	movq	%r15, 216(%rsp)
	orq	%rbx, %r12
	xorq	8(%rsp), %rbp
	movq	224(%rsp), %rbx
	movq	%rbx, %r15
	xorq	%rbp, %r15
	movq	%r15, 224(%rsp)
	orq	%rbx, %rbp
	movq	232(%rsp), %rbx
	xorq	96(%rsp), %rbx
	movq	144(%rsp), %r15
	movq	%r15, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 232(%rsp)
	orq	%r15, %rbx
	xorq	88(%rsp), %r11
	movq	240(%rsp), %rdx
	movq	%rdx, %r15
	xorq	%r11, %r15
	movq	%r15, 240(%rsp)
	orq	%rdx, %r11
	xorq	80(%rsp), %r10
	movq	152(%rsp), %r15
	movq	%r15, %rdx
	xorq	%r10, %rdx
	movq	%rdx, 248(%rsp)
	orq	%r15, %r10
	movq	%r10, 256(%rsp)
	xorq	72(%rsp), %r9
	movq	160(%rsp), %r10
	movq	%r10, %r15
	xorq	%r9, %r15
	movq	%r15, 264(%rsp)
	orq	%r10, %r9
	xorq	64(%rsp), %r8
	movq	168(%rsp), %r10
	movq	%r10, %r15
	xorq	%r8, %r15
	movq	%r15, 272(%rsp)
	orq	%r10, %r8
	xorq	56(%rsp), %rdi
	movq	176(%rsp), %r10
	movq	%r10, %r15
	xorq	%rdi, %r15
	movq	%r15, 280(%rsp)
	orq	%r10, %rdi
	xorq	48(%rsp), %rsi
	movq	120(%rsp), %r10
	movq	%r10, %r15
	xorq	%rsi, %r15
	movq	%r15, 120(%rsp)
	orq	%r10, %rsi
	xorq	40(%rsp), %rcx
	movq	128(%rsp), %r10
	movq	%r10, %r15
	xorq	%rcx, %r15
	movq	%r15, 128(%rsp)
	orq	%r10, %rcx
	movq	%rcx, 288(%rsp)
	movq	184(%rsp), %rdx
	xorq	104(%rsp), %rdx
	movq	112(%rsp), %rcx
	movq	%rcx, %r10
	xorq	%rdx, %r10
	movq	%r10, 112(%rsp)
	orq	%rcx, %rdx
	xorq	(%rsp), %rax
	movq	192(%rsp), %rcx
	movq	%rcx, %r10
	xorq	%rax, %r10
	movq	%r10, 136(%rsp)
	orq	%rax, %rcx
	movq	32(%rsp), %rax
	xorq	%r14, %rax
	movq	200(%rsp), %r14
	movq	%r14, %r10
	xorq	%rax, %r10
	movq	%r10, 144(%rsp)
	orq	%rax, %r14
	movq	24(%rsp), %rax
	xorq	%r13, %rax
	movq	208(%rsp), %r13
	movq	%r13, %r10
	xorq	%rax, %r10
	movq	%r10, 152(%rsp)
	orq	%rax, %r13
	movq	16(%rsp), %rax
	xorq	%r12, %rax
	movq	216(%rsp), %r12
	movq	%r12, %r10
	xorq	%rax, %r10
	movq	%r10, 160(%rsp)
	orq	%rax, %r12
	movq	8(%rsp), %rax
	xorq	%rbp, %rax
	movq	224(%rsp), %rbp
	movq	%rbp, %r10
	xorq	%rax, %r10
	movq	%r10, 168(%rsp)
	orq	%rax, %rbp
	movq	96(%rsp), %rax
	xorq	%rbx, %rax
	movq	232(%rsp), %rbx
	movq	%rbx, %r10
	xorq	%rax, %r10
	movq	%r10, 176(%rsp)
	orq	%rax, %rbx
	movq	88(%rsp), %rax
	xorq	%r11, %rax
	movq	240(%rsp), %r11
	movq	%r11, %r10
	xorq	%rax, %r10
	movq	%r10, 184(%rsp)
	orq	%rax, %r11
	movq	256(%rsp), %r15
	xorq	80(%rsp), %r15
	movq	248(%rsp), %r10
	movq	%r10, %rax
	xorq	%r15, %rax
	movq	%rax, 192(%rsp)
	orq	%r15, %r10
	movq	72(%rsp), %rax
	xorq	%r9, %rax
	movq	264(%rsp), %r9
	movq	%r9, %r15
	xorq	%rax, %r15
	movq	%r15, 200(%rsp)
	orq	%rax, %r9
	movq	64(%rsp), %rax
	xorq	%r8, %rax
	movq	272(%rsp), %r8
	movq	%r8, %r15
	xorq	%rax, %r15
	movq	%r15, 208(%rsp)
	orq	%rax, %r8
	movq	56(%rsp), %rax
	xorq	%rdi, %rax
	movq	280(%rsp), %rdi
	movq	%rdi, %r15
	xorq	%rax, %r15
	movq	%r15, 216(%rsp)
	orq	%rax, %rdi
	movq	48(%rsp), %rax
	xorq	%rsi, %rax
	movq	120(%rsp), %rsi
	movq	%rsi, %r15
	xorq	%rax, %r15
	movq	%r15, 224(%rsp)
	orq	%rax, %rsi
	movq	%rsi, 120(%rsp)
	movq	288(%rsp), %rax
	xorq	40(%rsp), %rax
	movq	128(%rsp), %r15
	movq	%r15, %rsi
	xorq	%rax, %rsi
	movq	%rsi, 232(%rsp)
	orq	%r15, %rax
	movq	%rax, 128(%rsp)
	xorq	104(%rsp), %rdx
	movq	112(%rsp), %rsi
	movq	%rsi, %r15
	xorq	%rdx, %r15
	movq	%r15, 240(%rsp)
	orq	%rsi, %rdx
	movq	%rdx, 112(%rsp)
	movq	(%rsp), %rax
	cmpq	408(%rsp), %rax
	jne	.L482
	movq	%rcx, %r15
	movq	%rbx, (%rsp)
	movq	%r11, 32(%rsp)
	movq	%r10, 40(%rsp)
	movq	%r9, 8(%rsp)
	movq	%r8, 16(%rsp)
	movq	%rdi, 24(%rsp)
.L481:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	call	use_int@PLT
	movl	120(%rsp), %edi
	call	use_int@PLT
	movl	128(%rsp), %edi
	call	use_int@PLT
	movl	112(%rsp), %edi
	call	use_int@PLT
	addq	$424, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE166:
	.size	int64_bit_13, .-int64_bit_13
	.globl	int64_bit_14
	.type	int64_bit_14, @function
int64_bit_14:
.LFB167:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$408, %rsp
	.cfi_def_cfa_offset 464
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r15
	movslq	%ecx, %rcx
	leaq	1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 136(%rsp)
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 272(%rsp)
	movslq	16(%rdx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %r9
	movq	%rsi, (%rsp)
	addq	%rax, %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 144(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 280(%rsp)
	movl	20(%rdx), %ecx
	leal	-1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r14
	movslq	%ecx, %rcx
	leaq	-1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 152(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 288(%rsp)
	movl	24(%rdx), %ecx
	leal	-2(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r13
	movslq	%ecx, %rcx
	leaq	-2(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 160(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 296(%rsp)
	movl	28(%rdx), %ecx
	leal	-3(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r12
	movslq	%ecx, %rcx
	leaq	-3(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 168(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 304(%rsp)
	movl	32(%rdx), %ecx
	leal	-4(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %rbp
	movslq	%ecx, %rcx
	leaq	-4(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 176(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 312(%rsp)
	movl	36(%rdx), %ecx
	leal	-5(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r10
	movq	%rsi, 40(%rsp)
	movslq	%ecx, %rcx
	leaq	-5(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 184(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 320(%rsp)
	movl	40(%rdx), %ecx
	leal	-6(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r11
	movq	%rsi, 48(%rsp)
	movslq	%ecx, %rcx
	leaq	-6(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 192(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 328(%rsp)
	movl	44(%rdx), %ecx
	leal	-7(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 8(%rsp)
	movslq	%ecx, %rcx
	leaq	-7(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 200(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 336(%rsp)
	movl	48(%rdx), %ecx
	leal	-8(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 16(%rsp)
	movslq	%ecx, %rcx
	leaq	-8(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 208(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 344(%rsp)
	movl	52(%rdx), %ecx
	leal	-9(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 24(%rsp)
	movslq	%ecx, %rcx
	leaq	-9(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 216(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 352(%rsp)
	movl	56(%rdx), %ecx
	leal	-10(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 32(%rsp)
	movslq	%ecx, %rcx
	leaq	-10(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 224(%rsp)
	leaq	0(,%rsi,4), %rcx
	movq	%rcx, 360(%rsp)
	movl	60(%rdx), %ecx
	leal	-11(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 120(%rsp)
	movslq	%ecx, %rcx
	leaq	-11(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %rcx
	movq	%rsi, 232(%rsp)
	salq	$2, %rcx
	movq	%rcx, 368(%rsp)
	movl	64(%rdx), %ecx
	leal	-12(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 128(%rsp)
	movslq	%ecx, %rcx
	leaq	-12(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %rcx
	movq	%rsi, 240(%rsp)
	salq	$2, %rcx
	movq	%rcx, 376(%rsp)
	movl	68(%rdx), %edx
	leal	-13(%rdx), %ecx
	movslq	%ecx, %rsi
	salq	$32, %rcx
	orq	%rsi, %rcx
	movq	%rcx, %rsi
	movq	%rcx, 56(%rsp)
	movslq	%edx, %rdx
	leaq	-13(%rax,%rdx), %rdx
	movq	%rdx, %rcx
	salq	$32, %rcx
	orq	%rdx, %rcx
	movq	%rcx, 248(%rsp)
	salq	$2, %rcx
	movq	%rcx, 384(%rsp)
	testq	%rax, %rax
	je	.L486
	leaq	-1(%rbx), %r8
	subq	%rax, %rbx
	leaq	-1(%rbx), %rax
	movq	%rax, 392(%rsp)
	movq	%r8, (%rsp)
	movq	%r15, %rcx
	movq	%r9, %r15
	movq	%r10, %rbx
	movq	8(%rsp), %r10
	movq	16(%rsp), %r9
	movq	24(%rsp), %r8
	movq	32(%rsp), %rdi
	movq	%rsi, %rdx
.L487:
	subq	$1, (%rsp)
	movq	(%rsp), %rax
	movq	280(%rsp), %rsi
	addq	%rax, %rsi
	movq	%rsi, %rax
	movq	272(%rsp), %rsi
	subq	%rsi, %rax
	movq	%rax, 40(%rsp)
	movq	288(%rsp), %rax
	addq	(%rsp), %rax
	subq	%rsi, %rax
	movq	%rax, 32(%rsp)
	movq	296(%rsp), %rax
	addq	(%rsp), %rax
	subq	%rsi, %rax
	movq	%rax, 24(%rsp)
	movq	304(%rsp), %rax
	addq	(%rsp), %rax
	subq	%rsi, %rax
	movq	%rax, 16(%rsp)
	movq	312(%rsp), %rax
	addq	(%rsp), %rax
	subq	%rsi, %rax
	movq	%rax, 8(%rsp)
	movq	320(%rsp), %rax
	addq	(%rsp), %rax
	subq	%rsi, %rax
	movq	%rax, 96(%rsp)
	movq	328(%rsp), %rax
	addq	(%rsp), %rax
	subq	%rsi, %rax
	movq	%rax, 88(%rsp)
	movq	336(%rsp), %rax
	addq	(%rsp), %rax
	subq	%rsi, %rax
	movq	%rax, 80(%rsp)
	movq	344(%rsp), %rax
	addq	(%rsp), %rax
	subq	%rsi, %rax
	movq	%rax, 72(%rsp)
	movq	352(%rsp), %rax
	addq	(%rsp), %rax
	subq	%rsi, %rax
	movq	%rax, 64(%rsp)
	movq	360(%rsp), %rax
	addq	(%rsp), %rax
	subq	%rsi, %rax
	movq	%rax, 56(%rsp)
	movq	368(%rsp), %rax
	addq	(%rsp), %rax
	subq	%rsi, %rax
	movq	%rax, 48(%rsp)
	movq	376(%rsp), %rax
	addq	(%rsp), %rax
	subq	%rsi, %rax
	movq	%rax, 104(%rsp)
	movq	384(%rsp), %rax
	addq	(%rsp), %rax
	subq	%rsi, %rax
	movq	%rax, 112(%rsp)
	xorq	(%rsp), %rcx
	movq	%rcx, %rax
	movq	136(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%rax, %rsi
	movq	%rsi, 136(%rsp)
	orq	%rcx, %rax
	xorq	40(%rsp), %r15
	movq	144(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%r15, %rsi
	movq	%rsi, 144(%rsp)
	orq	%rcx, %r15
	xorq	32(%rsp), %r14
	movq	152(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%r14, %rsi
	movq	%rsi, 152(%rsp)
	orq	%rcx, %r14
	xorq	24(%rsp), %r13
	movq	160(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%r13, %rsi
	movq	%rsi, 160(%rsp)
	orq	%rcx, %r13
	xorq	16(%rsp), %r12
	movq	168(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%r12, %rsi
	movq	%rsi, 168(%rsp)
	orq	%rcx, %r12
	xorq	8(%rsp), %rbp
	movq	176(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%rbp, %rsi
	movq	%rsi, 176(%rsp)
	orq	%rcx, %rbp
	xorq	96(%rsp), %rbx
	movq	184(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%rbx, %rsi
	movq	%rsi, 184(%rsp)
	orq	%rcx, %rbx
	xorq	88(%rsp), %r11
	movq	192(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%r11, %rsi
	movq	%rsi, 192(%rsp)
	orq	%rcx, %r11
	xorq	80(%rsp), %r10
	movq	200(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%r10, %rsi
	movq	%rsi, 200(%rsp)
	orq	%rcx, %r10
	xorq	72(%rsp), %r9
	movq	208(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%r9, %rsi
	movq	%rsi, 208(%rsp)
	orq	%rcx, %r9
	xorq	64(%rsp), %r8
	movq	216(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%r8, %rsi
	movq	%rsi, 216(%rsp)
	orq	%rcx, %r8
	movq	%r8, 256(%rsp)
	xorq	56(%rsp), %rdi
	movq	224(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%rdi, %rsi
	movq	%rsi, 224(%rsp)
	orq	%rcx, %rdi
	movq	%rdi, 264(%rsp)
	movq	120(%rsp), %rsi
	xorq	48(%rsp), %rsi
	movq	232(%rsp), %rcx
	movq	%rcx, %rdi
	xorq	%rsi, %rdi
	movq	%rdi, 120(%rsp)
	orq	%rcx, %rsi
	movq	128(%rsp), %rcx
	xorq	104(%rsp), %rcx
	movq	240(%rsp), %rdi
	movq	%rdi, %r8
	xorq	%rcx, %r8
	movq	%r8, 128(%rsp)
	orq	%rdi, %rcx
	xorq	112(%rsp), %rdx
	movq	248(%rsp), %r8
	movq	%r8, %rdi
	xorq	%rdx, %rdi
	movq	%rdi, 232(%rsp)
	orq	%r8, %rdx
	movq	%rdx, 240(%rsp)
	xorq	(%rsp), %rax
	movq	136(%rsp), %r8
	movq	%r8, %rdx
	xorq	%rax, %rdx
	movq	%rdx, 136(%rsp)
	orq	%r8, %rax
	movq	%rax, 248(%rsp)
	xorq	40(%rsp), %r15
	movq	144(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%r15, %rdi
	movq	%rdi, 144(%rsp)
	orq	%rdx, %r15
	xorq	32(%rsp), %r14
	movq	152(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r14, %rdx
	movq	%rdx, 152(%rsp)
	orq	%rdi, %r14
	xorq	24(%rsp), %r13
	movq	160(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r13, %rdx
	movq	%rdx, 160(%rsp)
	orq	%rdi, %r13
	xorq	16(%rsp), %r12
	movq	168(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r12, %rdx
	movq	%rdx, 168(%rsp)
	orq	%rdi, %r12
	xorq	8(%rsp), %rbp
	movq	176(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%rbp, %rdx
	movq	%rdx, 176(%rsp)
	orq	%rdi, %rbp
	xorq	96(%rsp), %rbx
	movq	184(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 184(%rsp)
	orq	%rdi, %rbx
	xorq	88(%rsp), %r11
	movq	192(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r11, %rdx
	movq	%rdx, 192(%rsp)
	orq	%rdi, %r11
	xorq	80(%rsp), %r10
	movq	200(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r10, %rdx
	movq	%rdx, 200(%rsp)
	orq	%rdi, %r10
	xorq	72(%rsp), %r9
	movq	208(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r9, %rdx
	movq	%rdx, 208(%rsp)
	orq	%rdi, %r9
	movq	256(%rsp), %r8
	xorq	64(%rsp), %r8
	movq	216(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r8, %rdx
	movq	%rdx, 216(%rsp)
	orq	%rdi, %r8
	movq	264(%rsp), %rdi
	xorq	56(%rsp), %rdi
	movq	224(%rsp), %rdx
	movq	%rdx, %rax
	xorq	%rdi, %rax
	movq	%rax, 224(%rsp)
	orq	%rdx, %rdi
	xorq	48(%rsp), %rsi
	movq	120(%rsp), %rax
	movq	%rax, %rdx
	xorq	%rsi, %rdx
	movq	%rdx, 120(%rsp)
	orq	%rax, %rsi
	xorq	104(%rsp), %rcx
	movq	128(%rsp), %rax
	movq	%rax, %rdx
	xorq	%rcx, %rdx
	movq	%rdx, 128(%rsp)
	orq	%rax, %rcx
	movq	%rcx, 256(%rsp)
	movq	240(%rsp), %rdx
	xorq	112(%rsp), %rdx
	movq	232(%rsp), %rcx
	movq	%rcx, %rax
	xorq	%rdx, %rax
	movq	%rax, 232(%rsp)
	orq	%rcx, %rdx
	movq	%rdx, 240(%rsp)
	movq	248(%rsp), %rax
	xorq	(%rsp), %rax
	movq	136(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%rax, %rcx
	movq	%rcx, 136(%rsp)
	orq	%rdx, %rax
	xorq	40(%rsp), %r15
	movq	144(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r15, %rdx
	movq	%rdx, 144(%rsp)
	orq	%rcx, %r15
	xorq	32(%rsp), %r14
	movq	152(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r14, %rcx
	movq	%rcx, 152(%rsp)
	orq	%rdx, %r14
	xorq	24(%rsp), %r13
	movq	160(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r13, %rcx
	movq	%rcx, 160(%rsp)
	orq	%rdx, %r13
	xorq	16(%rsp), %r12
	movq	168(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r12, %rcx
	movq	%rcx, 168(%rsp)
	orq	%rdx, %r12
	xorq	8(%rsp), %rbp
	movq	176(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%rbp, %rcx
	movq	%rcx, 176(%rsp)
	orq	%rdx, %rbp
	xorq	96(%rsp), %rbx
	movq	184(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%rbx, %rcx
	movq	%rcx, 184(%rsp)
	orq	%rdx, %rbx
	xorq	88(%rsp), %r11
	movq	192(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r11, %rcx
	movq	%rcx, 192(%rsp)
	orq	%rdx, %r11
	xorq	80(%rsp), %r10
	movq	200(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r10, %rcx
	movq	%rcx, 200(%rsp)
	orq	%rdx, %r10
	xorq	72(%rsp), %r9
	movq	208(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r9, %rcx
	movq	%rcx, 208(%rsp)
	orq	%rdx, %r9
	xorq	64(%rsp), %r8
	movq	216(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r8, %rcx
	movq	%rcx, 216(%rsp)
	orq	%rdx, %r8
	xorq	56(%rsp), %rdi
	movq	224(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%rdi, %rcx
	movq	%rcx, 224(%rsp)
	orq	%rdx, %rdi
	movq	%rdi, 248(%rsp)
	xorq	48(%rsp), %rsi
	movq	120(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%rsi, %rcx
	movq	%rcx, 120(%rsp)
	orq	%rdx, %rsi
	movq	%rsi, 264(%rsp)
	movq	256(%rsp), %rcx
	xorq	104(%rsp), %rcx
	movq	128(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%rcx, %rdx
	movq	%rdx, 128(%rsp)
	orq	%rsi, %rcx
	movq	240(%rsp), %rdx
	xorq	112(%rsp), %rdx
	movq	232(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%rdx, %rdi
	movq	%rdi, 232(%rsp)
	orq	%rsi, %rdx
	movq	%rdx, 240(%rsp)
	xorq	(%rsp), %rax
	movq	136(%rsp), %rdi
	movq	%rdi, %rsi
	xorq	%rax, %rsi
	movq	%rsi, 136(%rsp)
	orq	%rdi, %rax
	movq	%rax, 256(%rsp)
	xorq	40(%rsp), %r15
	movq	144(%rsp), %rsi
	movq	%rsi, %rax
	xorq	%r15, %rax
	movq	%rax, 144(%rsp)
	orq	%rsi, %r15
	xorq	32(%rsp), %r14
	movq	152(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r14, %rsi
	movq	%rsi, 152(%rsp)
	orq	%rax, %r14
	xorq	24(%rsp), %r13
	movq	160(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r13, %rsi
	movq	%rsi, 160(%rsp)
	orq	%rax, %r13
	xorq	16(%rsp), %r12
	movq	168(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r12, %rsi
	movq	%rsi, 168(%rsp)
	orq	%rax, %r12
	xorq	8(%rsp), %rbp
	movq	176(%rsp), %rax
	movq	%rax, %rsi
	xorq	%rbp, %rsi
	movq	%rsi, 176(%rsp)
	orq	%rax, %rbp
	xorq	96(%rsp), %rbx
	movq	184(%rsp), %rax
	movq	%rax, %rsi
	xorq	%rbx, %rsi
	movq	%rsi, 184(%rsp)
	orq	%rax, %rbx
	xorq	88(%rsp), %r11
	movq	192(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r11, %rsi
	movq	%rsi, 192(%rsp)
	orq	%rax, %r11
	xorq	80(%rsp), %r10
	movq	200(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r10, %rsi
	movq	%rsi, 200(%rsp)
	orq	%rax, %r10
	xorq	72(%rsp), %r9
	movq	208(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r9, %rsi
	movq	%rsi, 208(%rsp)
	orq	%rax, %r9
	xorq	64(%rsp), %r8
	movq	216(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r8, %rsi
	movq	%rsi, 216(%rsp)
	orq	%rax, %r8
	movq	248(%rsp), %rdi
	xorq	56(%rsp), %rdi
	movq	224(%rsp), %rax
	movq	%rax, %rsi
	xorq	%rdi, %rsi
	movq	%rsi, 224(%rsp)
	orq	%rax, %rdi
	movq	264(%rsp), %rsi
	xorq	48(%rsp), %rsi
	movq	120(%rsp), %rax
	movq	%rax, %rdx
	xorq	%rsi, %rdx
	movq	%rdx, 120(%rsp)
	orq	%rax, %rsi
	movq	%rsi, 248(%rsp)
	xorq	104(%rsp), %rcx
	movq	128(%rsp), %rdx
	movq	%rdx, %rax
	xorq	%rcx, %rax
	movq	%rax, 128(%rsp)
	orq	%rdx, %rcx
	movq	240(%rsp), %rdx
	xorq	112(%rsp), %rdx
	movq	232(%rsp), %rax
	movq	%rax, %rsi
	xorq	%rdx, %rsi
	movq	%rsi, 232(%rsp)
	orq	%rax, %rdx
	movq	%rdx, 240(%rsp)
	movq	256(%rsp), %rax
	xorq	(%rsp), %rax
	movq	136(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%rax, %rdx
	movq	%rdx, 136(%rsp)
	orq	%rsi, %rax
	xorq	40(%rsp), %r15
	movq	144(%rsp), %rdx
	movq	%rdx, %rsi
	xorq	%r15, %rsi
	movq	%rsi, 144(%rsp)
	orq	%rdx, %r15
	xorq	32(%rsp), %r14
	movq	152(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r14, %rdx
	movq	%rdx, 152(%rsp)
	orq	%rsi, %r14
	xorq	24(%rsp), %r13
	movq	160(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r13, %rdx
	movq	%rdx, 160(%rsp)
	orq	%rsi, %r13
	xorq	16(%rsp), %r12
	movq	168(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r12, %rdx
	movq	%rdx, 168(%rsp)
	orq	%rsi, %r12
	xorq	8(%rsp), %rbp
	movq	176(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%rbp, %rdx
	movq	%rdx, 176(%rsp)
	orq	%rsi, %rbp
	xorq	96(%rsp), %rbx
	movq	184(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 184(%rsp)
	orq	%rsi, %rbx
	xorq	88(%rsp), %r11
	movq	192(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r11, %rdx
	movq	%rdx, 192(%rsp)
	orq	%rsi, %r11
	xorq	80(%rsp), %r10
	movq	200(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r10, %rdx
	movq	%rdx, 200(%rsp)
	orq	%rsi, %r10
	xorq	72(%rsp), %r9
	movq	208(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r9, %rdx
	movq	%rdx, 208(%rsp)
	orq	%rsi, %r9
	xorq	64(%rsp), %r8
	movq	216(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r8, %rdx
	movq	%rdx, 216(%rsp)
	orq	%rsi, %r8
	xorq	56(%rsp), %rdi
	movq	224(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%rdi, %rdx
	movq	%rdx, 224(%rsp)
	orq	%rsi, %rdi
	movq	%rdi, 256(%rsp)
	movq	248(%rsp), %rsi
	xorq	48(%rsp), %rsi
	movq	120(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%rsi, %rdi
	movq	%rdi, 120(%rsp)
	orq	%rdx, %rsi
	xorq	104(%rsp), %rcx
	movq	128(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%rcx, %rdx
	movq	%rdx, 128(%rsp)
	orq	%rdi, %rcx
	movq	%rcx, 248(%rsp)
	movq	240(%rsp), %rdx
	xorq	112(%rsp), %rdx
	movq	232(%rsp), %rdi
	movq	%rdi, %rcx
	xorq	%rdx, %rcx
	movq	%rcx, 232(%rsp)
	orq	%rdi, %rdx
	movq	%rdx, 240(%rsp)
	xorq	(%rsp), %rax
	movq	136(%rsp), %rdi
	movq	%rdi, %rcx
	xorq	%rax, %rcx
	movq	%rcx, 136(%rsp)
	orq	%rdi, %rax
	xorq	40(%rsp), %r15
	movq	144(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r15, %rdx
	movq	%rdx, 144(%rsp)
	orq	%rcx, %r15
	xorq	32(%rsp), %r14
	movq	152(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r14, %rdx
	movq	%rdx, 152(%rsp)
	orq	%rcx, %r14
	xorq	24(%rsp), %r13
	movq	160(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r13, %rdx
	movq	%rdx, 160(%rsp)
	orq	%rcx, %r13
	xorq	16(%rsp), %r12
	movq	168(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r12, %rdx
	movq	%rdx, 168(%rsp)
	orq	%rcx, %r12
	xorq	8(%rsp), %rbp
	movq	176(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%rbp, %rdx
	movq	%rdx, 176(%rsp)
	orq	%rcx, %rbp
	xorq	96(%rsp), %rbx
	movq	184(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 184(%rsp)
	orq	%rcx, %rbx
	xorq	88(%rsp), %r11
	movq	192(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r11, %rdx
	movq	%rdx, 192(%rsp)
	orq	%rcx, %r11
	xorq	80(%rsp), %r10
	movq	200(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r10, %rdx
	movq	%rdx, 200(%rsp)
	orq	%rcx, %r10
	xorq	72(%rsp), %r9
	movq	208(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r9, %rdx
	movq	%rdx, 208(%rsp)
	orq	%rcx, %r9
	xorq	64(%rsp), %r8
	movq	216(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%r8, %rdx
	movq	%rdx, 216(%rsp)
	orq	%rcx, %r8
	movq	256(%rsp), %rdi
	xorq	56(%rsp), %rdi
	movq	224(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%rdi, %rdx
	movq	%rdx, 224(%rsp)
	orq	%rcx, %rdi
	xorq	48(%rsp), %rsi
	movq	120(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%rsi, %rdx
	movq	%rdx, 120(%rsp)
	orq	%rcx, %rsi
	movq	%rsi, 256(%rsp)
	movq	248(%rsp), %rcx
	xorq	104(%rsp), %rcx
	movq	128(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%rcx, %rdx
	movq	%rdx, 128(%rsp)
	orq	%rsi, %rcx
	movq	%rcx, 248(%rsp)
	movq	240(%rsp), %rdx
	xorq	112(%rsp), %rdx
	movq	232(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%rdx, %rsi
	movq	%rsi, 232(%rsp)
	orq	%rcx, %rdx
	xorq	(%rsp), %rax
	movq	136(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%rax, %rsi
	movq	%rsi, 136(%rsp)
	orq	%rcx, %rax
	movq	%rax, 240(%rsp)
	xorq	40(%rsp), %r15
	movq	144(%rsp), %rsi
	movq	%rsi, %rax
	xorq	%r15, %rax
	movq	%rax, 144(%rsp)
	orq	%rsi, %r15
	xorq	32(%rsp), %r14
	movq	152(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r14, %rsi
	movq	%rsi, 152(%rsp)
	orq	%rax, %r14
	xorq	24(%rsp), %r13
	movq	160(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r13, %rsi
	movq	%rsi, 160(%rsp)
	orq	%rax, %r13
	xorq	16(%rsp), %r12
	movq	168(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r12, %rsi
	movq	%rsi, 168(%rsp)
	orq	%rax, %r12
	xorq	8(%rsp), %rbp
	movq	176(%rsp), %rax
	movq	%rax, %rsi
	xorq	%rbp, %rsi
	movq	%rsi, 176(%rsp)
	orq	%rax, %rbp
	xorq	96(%rsp), %rbx
	movq	184(%rsp), %rax
	movq	%rax, %rsi
	xorq	%rbx, %rsi
	movq	%rsi, 184(%rsp)
	orq	%rax, %rbx
	xorq	88(%rsp), %r11
	movq	192(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r11, %rsi
	movq	%rsi, 192(%rsp)
	orq	%rax, %r11
	xorq	80(%rsp), %r10
	movq	200(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r10, %rsi
	movq	%rsi, 200(%rsp)
	orq	%rax, %r10
	xorq	72(%rsp), %r9
	movq	208(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r9, %rsi
	movq	%rsi, 208(%rsp)
	orq	%rax, %r9
	xorq	64(%rsp), %r8
	movq	216(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r8, %rsi
	movq	%rsi, 216(%rsp)
	orq	%rax, %r8
	xorq	56(%rsp), %rdi
	movq	224(%rsp), %rax
	movq	%rax, %rsi
	xorq	%rdi, %rsi
	movq	%rsi, 224(%rsp)
	orq	%rax, %rdi
	movq	256(%rsp), %rsi
	xorq	48(%rsp), %rsi
	movq	120(%rsp), %rax
	movq	%rax, %rcx
	xorq	%rsi, %rcx
	movq	%rcx, 120(%rsp)
	orq	%rax, %rsi
	movq	%rsi, 256(%rsp)
	movq	248(%rsp), %rcx
	xorq	104(%rsp), %rcx
	movq	128(%rsp), %rax
	movq	%rax, %rsi
	xorq	%rcx, %rsi
	movq	%rsi, 128(%rsp)
	orq	%rax, %rcx
	xorq	112(%rsp), %rdx
	movq	232(%rsp), %rax
	movq	%rax, %rsi
	xorq	%rdx, %rsi
	movq	%rsi, 232(%rsp)
	orq	%rax, %rdx
	movq	%rdx, 248(%rsp)
	movq	240(%rsp), %rax
	xorq	(%rsp), %rax
	movq	136(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%rax, %rdx
	movq	%rdx, 136(%rsp)
	orq	%rsi, %rax
	xorq	40(%rsp), %r15
	movq	144(%rsp), %rdx
	movq	%rdx, %rsi
	xorq	%r15, %rsi
	movq	%rsi, 144(%rsp)
	orq	%rdx, %r15
	xorq	32(%rsp), %r14
	movq	152(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r14, %rdx
	movq	%rdx, 152(%rsp)
	orq	%rsi, %r14
	xorq	24(%rsp), %r13
	movq	160(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r13, %rdx
	movq	%rdx, 160(%rsp)
	orq	%rsi, %r13
	xorq	16(%rsp), %r12
	movq	168(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r12, %rdx
	movq	%rdx, 168(%rsp)
	orq	%rsi, %r12
	xorq	8(%rsp), %rbp
	movq	176(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%rbp, %rdx
	movq	%rdx, 176(%rsp)
	orq	%rsi, %rbp
	xorq	96(%rsp), %rbx
	movq	184(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 184(%rsp)
	orq	%rsi, %rbx
	xorq	88(%rsp), %r11
	movq	192(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r11, %rdx
	movq	%rdx, 192(%rsp)
	orq	%rsi, %r11
	xorq	80(%rsp), %r10
	movq	200(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r10, %rdx
	movq	%rdx, 200(%rsp)
	orq	%rsi, %r10
	xorq	72(%rsp), %r9
	movq	208(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r9, %rdx
	movq	%rdx, 208(%rsp)
	orq	%rsi, %r9
	xorq	64(%rsp), %r8
	movq	216(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%r8, %rdx
	movq	%rdx, 216(%rsp)
	orq	%rsi, %r8
	xorq	56(%rsp), %rdi
	movq	224(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%rdi, %rdx
	movq	%rdx, 224(%rsp)
	orq	%rsi, %rdi
	movq	%rdi, 240(%rsp)
	movq	256(%rsp), %rsi
	xorq	48(%rsp), %rsi
	movq	120(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%rsi, %rdx
	movq	%rdx, 120(%rsp)
	orq	%rdi, %rsi
	movq	%rsi, 256(%rsp)
	xorq	104(%rsp), %rcx
	movq	128(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%rcx, %rdi
	movq	%rdi, 128(%rsp)
	orq	%rsi, %rcx
	movq	248(%rsp), %rdx
	xorq	112(%rsp), %rdx
	movq	232(%rsp), %rsi
	movq	%rsi, %rdi
	xorq	%rdx, %rdi
	movq	%rdi, 232(%rsp)
	orq	%rsi, %rdx
	movq	%rdx, 248(%rsp)
	xorq	(%rsp), %rax
	movq	136(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%rax, %rdx
	movq	%rdx, 136(%rsp)
	orq	%rsi, %rax
	xorq	40(%rsp), %r15
	movq	144(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%r15, %rdi
	movq	%rdi, 144(%rsp)
	orq	%rdx, %r15
	xorq	32(%rsp), %r14
	movq	152(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r14, %rdx
	movq	%rdx, 152(%rsp)
	orq	%rdi, %r14
	xorq	24(%rsp), %r13
	movq	160(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r13, %rdx
	movq	%rdx, 160(%rsp)
	orq	%rdi, %r13
	xorq	16(%rsp), %r12
	movq	168(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r12, %rdx
	movq	%rdx, 168(%rsp)
	orq	%rdi, %r12
	xorq	8(%rsp), %rbp
	movq	176(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%rbp, %rdx
	movq	%rdx, 176(%rsp)
	orq	%rdi, %rbp
	xorq	96(%rsp), %rbx
	movq	184(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 184(%rsp)
	orq	%rdi, %rbx
	xorq	88(%rsp), %r11
	movq	192(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r11, %rdx
	movq	%rdx, 192(%rsp)
	orq	%rdi, %r11
	xorq	80(%rsp), %r10
	movq	200(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r10, %rdx
	movq	%rdx, 200(%rsp)
	orq	%rdi, %r10
	xorq	72(%rsp), %r9
	movq	208(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r9, %rdx
	movq	%rdx, 208(%rsp)
	orq	%rdi, %r9
	xorq	64(%rsp), %r8
	movq	216(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r8, %rdx
	movq	%rdx, 216(%rsp)
	orq	%rdi, %r8
	movq	%r8, 264(%rsp)
	movq	240(%rsp), %rdi
	xorq	56(%rsp), %rdi
	movq	224(%rsp), %rdx
	movq	%rdx, %rsi
	xorq	%rdi, %rsi
	movq	%rsi, 224(%rsp)
	orq	%rdx, %rdi
	movq	256(%rsp), %rsi
	xorq	48(%rsp), %rsi
	movq	120(%rsp), %rdx
	movq	%rdx, %r8
	xorq	%rsi, %r8
	movq	%r8, 120(%rsp)
	orq	%rdx, %rsi
	xorq	104(%rsp), %rcx
	movq	128(%rsp), %r8
	movq	%r8, %rdx
	xorq	%rcx, %rdx
	movq	%rdx, 128(%rsp)
	orq	%r8, %rcx
	movq	%rcx, 240(%rsp)
	movq	248(%rsp), %rdx
	xorq	112(%rsp), %rdx
	movq	232(%rsp), %rcx
	movq	%rcx, %r8
	xorq	%rdx, %r8
	movq	%r8, 248(%rsp)
	orq	%rcx, %rdx
	movq	%rdx, 256(%rsp)
	xorq	(%rsp), %rax
	movq	136(%rsp), %rcx
	movq	%rcx, %rdx
	xorq	%rax, %rdx
	movq	%rdx, 136(%rsp)
	orq	%rax, %rcx
	movq	40(%rsp), %rax
	xorq	%r15, %rax
	movq	144(%rsp), %rdx
	movq	%rdx, %r15
	xorq	%rax, %r15
	movq	%r15, 144(%rsp)
	orq	%rax, %rdx
	movq	%rdx, %r15
	movq	32(%rsp), %rax
	xorq	%r14, %rax
	movq	152(%rsp), %rdx
	movq	%rdx, %r14
	xorq	%rax, %r14
	movq	%r14, 152(%rsp)
	orq	%rax, %rdx
	movq	%rdx, %r14
	movq	24(%rsp), %rax
	xorq	%r13, %rax
	movq	160(%rsp), %rdx
	movq	%rdx, %r13
	xorq	%rax, %r13
	movq	%r13, 160(%rsp)
	orq	%rax, %rdx
	movq	%rdx, %r13
	movq	16(%rsp), %rax
	xorq	%r12, %rax
	movq	168(%rsp), %rdx
	movq	%rdx, %r12
	xorq	%rax, %r12
	movq	%r12, 168(%rsp)
	orq	%rax, %rdx
	movq	%rdx, %r12
	movq	8(%rsp), %rax
	xorq	%rbp, %rax
	movq	176(%rsp), %rdx
	movq	%rdx, %rbp
	xorq	%rax, %rbp
	movq	%rbp, 176(%rsp)
	orq	%rax, %rdx
	movq	%rdx, %rbp
	movq	96(%rsp), %rax
	xorq	%rbx, %rax
	movq	184(%rsp), %rbx
	movq	%rbx, %rdx
	xorq	%rax, %rdx
	movq	%rdx, 184(%rsp)
	orq	%rax, %rbx
	movq	88(%rsp), %rax
	xorq	%r11, %rax
	movq	192(%rsp), %rdx
	movq	%rdx, %r11
	xorq	%rax, %r11
	movq	%r11, 192(%rsp)
	orq	%rax, %rdx
	movq	%rdx, %r11
	movq	80(%rsp), %rax
	xorq	%r10, %rax
	movq	200(%rsp), %rdx
	movq	%rdx, %r10
	xorq	%rax, %r10
	movq	%r10, 200(%rsp)
	orq	%rax, %rdx
	movq	%rdx, %r10
	movq	72(%rsp), %rax
	xorq	%r9, %rax
	movq	208(%rsp), %rdx
	movq	%rdx, %r9
	xorq	%rax, %r9
	movq	%r9, 208(%rsp)
	orq	%rax, %rdx
	movq	%rdx, %r9
	movq	64(%rsp), %rax
	xorq	264(%rsp), %rax
	movq	216(%rsp), %rdx
	movq	%rdx, %r8
	xorq	%rax, %r8
	movq	%r8, 216(%rsp)
	orq	%rax, %rdx
	movq	%rdx, %r8
	movq	56(%rsp), %rax
	xorq	%rdi, %rax
	movq	224(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%rax, %rdx
	movq	%rdx, 224(%rsp)
	orq	%rax, %rdi
	movq	48(%rsp), %rax
	xorq	%rsi, %rax
	movq	120(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%rax, %rdx
	movq	%rdx, 232(%rsp)
	orq	%rax, %rsi
	movq	%rsi, 120(%rsp)
	movq	240(%rsp), %rax
	xorq	104(%rsp), %rax
	movq	128(%rsp), %rsi
	movq	%rsi, %rdx
	xorq	%rax, %rdx
	movq	%rdx, 240(%rsp)
	orq	%rsi, %rax
	movq	%rax, 128(%rsp)
	movq	256(%rsp), %rdx
	xorq	112(%rsp), %rdx
	movq	248(%rsp), %rsi
	movq	%rsi, %rax
	xorq	%rdx, %rax
	movq	%rax, 248(%rsp)
	orq	%rsi, %rdx
	movq	(%rsp), %rax
	cmpq	392(%rsp), %rax
	jne	.L487
	movq	%r15, (%rsp)
	movq	%rcx, %r15
	movq	%rbx, 40(%rsp)
	movq	%r11, 48(%rsp)
	movq	%r10, 8(%rsp)
	movq	%r9, 16(%rsp)
	movq	%r8, 24(%rsp)
	movq	%rdi, 32(%rsp)
	movq	%rdx, 56(%rsp)
.L486:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	(%rsp), %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	movl	120(%rsp), %edi
	call	use_int@PLT
	movl	128(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	addq	$408, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE167:
	.size	int64_bit_14, .-int64_bit_14
	.globl	int64_bit_15
	.type	int64_bit_15, @function
int64_bit_15:
.LFB168:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$472, %rsp
	.cfi_def_cfa_offset 528
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ecx
	leal	1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r14
	movslq	%ecx, %rcx
	leaq	1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 192(%rsp)
	leaq	0(,%rsi,4), %rbx
	movq	%rbx, 328(%rsp)
	movslq	16(%rdx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, %r13
	addq	%rax, %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 200(%rsp)
	salq	$2, %rsi
	movq	%rsi, 336(%rsp)
	movl	20(%rdx), %ecx
	leal	-1(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r12
	movslq	%ecx, %rcx
	leaq	-1(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 208(%rsp)
	salq	$2, %rsi
	movq	%rsi, 344(%rsp)
	movl	24(%rdx), %ecx
	leal	-2(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %rbp
	movslq	%ecx, %rcx
	leaq	-2(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 216(%rsp)
	salq	$2, %rsi
	movq	%rsi, 352(%rsp)
	movl	28(%rdx), %ecx
	leal	-3(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r11
	movq	%rsi, (%rsp)
	movslq	%ecx, %rcx
	leaq	-3(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 224(%rsp)
	salq	$2, %rsi
	movq	%rsi, 360(%rsp)
	movl	32(%rdx), %ecx
	leal	-4(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r15
	movslq	%ecx, %rcx
	leaq	-4(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 232(%rsp)
	salq	$2, %rsi
	movq	%rsi, 368(%rsp)
	movl	36(%rdx), %ecx
	leal	-5(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 128(%rsp)
	movslq	%ecx, %rcx
	leaq	-5(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 240(%rsp)
	salq	$2, %rsi
	movq	%rsi, 376(%rsp)
	movl	40(%rdx), %ecx
	leal	-6(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, %r9
	movq	%rsi, 16(%rsp)
	movslq	%ecx, %rcx
	leaq	-6(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 248(%rsp)
	salq	$2, %rsi
	movq	%rsi, 384(%rsp)
	movl	44(%rdx), %ecx
	leal	-7(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 144(%rsp)
	movslq	%ecx, %rcx
	leaq	-7(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 256(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 392(%rsp)
	movl	48(%rdx), %ecx
	leal	-8(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 152(%rsp)
	movslq	%ecx, %rcx
	leaq	-8(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 264(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 400(%rsp)
	movl	52(%rdx), %ecx
	leal	-9(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 160(%rsp)
	movslq	%ecx, %rcx
	leaq	-9(%rax,%rcx), %rcx
	movq	%rcx, %rsi
	salq	$32, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 272(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 408(%rsp)
	movl	56(%rdx), %ecx
	leal	-10(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 8(%rsp)
	movslq	%ecx, %rcx
	leaq	-10(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 280(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 416(%rsp)
	movl	60(%rdx), %ecx
	leal	-11(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 168(%rsp)
	movslq	%ecx, %rcx
	leaq	-11(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 288(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 424(%rsp)
	movl	64(%rdx), %ecx
	leal	-12(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 176(%rsp)
	movslq	%ecx, %rcx
	leaq	-12(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 296(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 432(%rsp)
	movl	68(%rdx), %ecx
	leal	-13(%rcx), %esi
	movslq	%esi, %rdi
	salq	$32, %rsi
	orq	%rdi, %rsi
	movq	%rsi, 184(%rsp)
	movslq	%ecx, %rcx
	leaq	-13(%rax,%rcx), %rcx
	movq	%rcx, %rdi
	salq	$32, %rdi
	movq	%rdi, %rsi
	orq	%rcx, %rsi
	movq	%rsi, 304(%rsp)
	leaq	0(,%rsi,4), %rdi
	movq	%rdi, 440(%rsp)
	movl	72(%rdx), %edx
	leal	-14(%rdx), %ecx
	movslq	%ecx, %rsi
	salq	$32, %rcx
	movq	%rcx, %rdi
	orq	%rsi, %rdi
	movq	%rdi, 136(%rsp)
	movslq	%edx, %rdx
	leaq	-14(%rax,%rdx), %rdx
	movq	%rdx, %rcx
	salq	$32, %rcx
	orq	%rdx, %rcx
	movq	%rcx, 312(%rsp)
	salq	$2, %rcx
	movq	%rcx, 448(%rsp)
	testq	%rax, %rax
	je	.L491
	leaq	-1(%rbx), %r8
	movq	%rbx, %rdx
	subq	%rax, %rdx
	leaq	-1(%rdx), %rbx
	movq	%rbx, 456(%rsp)
	movq	%r8, (%rsp)
	movq	%r11, %rbx
	movq	%r15, %r11
	movq	8(%rsp), %rax
.L492:
	subq	$1, (%rsp)
	movq	(%rsp), %rdi
	movq	336(%rsp), %rdx
	addq	%rdi, %rdx
	movq	328(%rsp), %rsi
	subq	%rsi, %rdx
	movq	%rdx, %r15
	movq	344(%rsp), %rdx
	addq	%rdi, %rdx
	subq	%rsi, %rdx
	movq	%rdx, %r8
	movq	352(%rsp), %rdx
	addq	%rdi, %rdx
	subq	%rsi, %rdx
	movq	%rdx, %r10
	movq	360(%rsp), %rdx
	addq	%rdi, %rdx
	subq	%rsi, %rdx
	movq	%rdx, 16(%rsp)
	movq	368(%rsp), %rcx
	addq	%rdi, %rcx
	subq	%rsi, %rcx
	movq	%rcx, 8(%rsp)
	movq	376(%rsp), %rdx
	addq	%rdi, %rdx
	subq	%rsi, %rdx
	movq	%rdx, 104(%rsp)
	movq	384(%rsp), %rcx
	addq	%rdi, %rcx
	subq	%rsi, %rcx
	movq	%rcx, 96(%rsp)
	movq	392(%rsp), %rdx
	addq	%rdi, %rdx
	subq	%rsi, %rdx
	movq	%rdx, 88(%rsp)
	movq	400(%rsp), %rcx
	addq	%rdi, %rcx
	subq	%rsi, %rcx
	movq	%rcx, 80(%rsp)
	movq	408(%rsp), %rdx
	addq	%rdi, %rdx
	subq	%rsi, %rdx
	movq	%rdx, 72(%rsp)
	movq	416(%rsp), %rcx
	addq	%rdi, %rcx
	subq	%rsi, %rcx
	movq	%rcx, 64(%rsp)
	movq	424(%rsp), %rdx
	addq	%rdi, %rdx
	subq	%rsi, %rdx
	movq	%rdx, 56(%rsp)
	movq	432(%rsp), %rcx
	addq	%rdi, %rcx
	movq	%rcx, %rdx
	subq	%rsi, %rdx
	movq	%rdx, 48(%rsp)
	movq	440(%rsp), %rcx
	addq	%rdi, %rcx
	subq	%rsi, %rcx
	movq	%rcx, 112(%rsp)
	movq	448(%rsp), %rcx
	addq	%rdi, %rcx
	subq	%rsi, %rcx
	movq	%rcx, 120(%rsp)
	xorq	%rdi, %r14
	movq	192(%rsp), %rsi
	movq	%rsi, %rcx
	xorq	%r14, %rcx
	movq	%rcx, 192(%rsp)
	orq	%rsi, %r14
	movq	%r15, 40(%rsp)
	xorq	%r15, %r13
	movq	200(%rsp), %rcx
	movq	%rcx, %rsi
	xorq	%r13, %rsi
	movq	%rsi, 200(%rsp)
	orq	%rcx, %r13
	movq	%r8, 32(%rsp)
	xorq	%r8, %r12
	movq	208(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r12, %rcx
	movq	%rcx, 208(%rsp)
	orq	%rdx, %r12
	movq	%r10, 24(%rsp)
	xorq	%r10, %rbp
	movq	216(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%rbp, %rcx
	movq	%rcx, 216(%rsp)
	orq	%rdx, %rbp
	xorq	16(%rsp), %rbx
	movq	224(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%rbx, %rdi
	movq	%rdi, 224(%rsp)
	orq	%rdx, %rbx
	xorq	8(%rsp), %r11
	movq	232(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r11, %rcx
	movq	%rcx, 232(%rsp)
	orq	%rdx, %r11
	movq	128(%rsp), %r10
	xorq	104(%rsp), %r10
	movq	240(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%r10, %rdi
	movq	%rdi, 128(%rsp)
	orq	%rdx, %r10
	xorq	96(%rsp), %r9
	movq	248(%rsp), %rsi
	movq	%rsi, %rcx
	xorq	%r9, %rcx
	movq	%rcx, 240(%rsp)
	orq	%rsi, %r9
	movq	144(%rsp), %r8
	xorq	88(%rsp), %r8
	movq	256(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%r8, %rdi
	movq	%rdi, 144(%rsp)
	orq	%rdx, %r8
	movq	152(%rsp), %r15
	xorq	80(%rsp), %r15
	movq	%r15, %rdi
	movq	264(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r15, %rcx
	movq	%rcx, 152(%rsp)
	orq	%rdx, %rdi
	movq	160(%rsp), %r15
	xorq	72(%rsp), %r15
	movq	%r15, %rsi
	movq	272(%rsp), %rdx
	movq	%rdx, %rcx
	xorq	%r15, %rcx
	movq	%rcx, 160(%rsp)
	orq	%rdx, %rsi
	movq	%rsi, 248(%rsp)
	xorq	64(%rsp), %rax
	movq	%rax, %rcx
	movq	280(%rsp), %rdx
	movq	%rdx, %rax
	xorq	%rcx, %rax
	movq	%rax, 256(%rsp)
	orq	%rdx, %rcx
	movq	168(%rsp), %r15
	xorq	56(%rsp), %r15
	movq	%r15, %rdx
	movq	288(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r15, %rsi
	movq	%rsi, 168(%rsp)
	orq	%rax, %rdx
	movq	176(%rsp), %r15
	xorq	48(%rsp), %r15
	movq	%r15, %rax
	movq	296(%rsp), %rsi
	movq	%rsi, %r15
	xorq	%rax, %r15
	movq	%r15, 176(%rsp)
	orq	%rsi, %rax
	movq	%rax, 264(%rsp)
	movq	184(%rsp), %rax
	xorq	112(%rsp), %rax
	movq	%rax, %rsi
	movq	304(%rsp), %r15
	xorq	%r15, %rax
	movq	%rax, 184(%rsp)
	movq	%rsi, %rax
	orq	%r15, %rax
	movq	%rax, 272(%rsp)
	movq	136(%rsp), %r15
	xorq	120(%rsp), %r15
	movq	312(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r15, %rsi
	movq	%rsi, 136(%rsp)
	orq	%rax, %r15
	xorq	(%rsp), %r14
	movq	192(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r14, %rsi
	movq	%rsi, 192(%rsp)
	orq	%rax, %r14
	xorq	40(%rsp), %r13
	movq	200(%rsp), %rsi
	movq	%rsi, %rax
	xorq	%r13, %rax
	movq	%rax, 200(%rsp)
	orq	%rsi, %r13
	xorq	32(%rsp), %r12
	movq	208(%rsp), %rsi
	movq	%rsi, %rax
	xorq	%r12, %rax
	movq	%rax, 208(%rsp)
	orq	%rsi, %r12
	xorq	24(%rsp), %rbp
	movq	216(%rsp), %rsi
	movq	%rsi, %rax
	xorq	%rbp, %rax
	movq	%rax, 216(%rsp)
	orq	%rsi, %rbp
	xorq	16(%rsp), %rbx
	movq	224(%rsp), %rsi
	movq	%rsi, %rax
	xorq	%rbx, %rax
	movq	%rax, 224(%rsp)
	orq	%rsi, %rbx
	xorq	8(%rsp), %r11
	movq	232(%rsp), %rsi
	movq	%rsi, %rax
	xorq	%r11, %rax
	movq	%rax, 232(%rsp)
	orq	%rsi, %r11
	xorq	104(%rsp), %r10
	movq	128(%rsp), %rsi
	movq	%rsi, %rax
	xorq	%r10, %rax
	movq	%rax, 128(%rsp)
	orq	%rsi, %r10
	xorq	96(%rsp), %r9
	movq	240(%rsp), %rax
	movq	%rax, %rsi
	xorq	%r9, %rsi
	movq	%rsi, 240(%rsp)
	orq	%rax, %r9
	xorq	88(%rsp), %r8
	movq	144(%rsp), %rsi
	movq	%rsi, %rax
	xorq	%r8, %rax
	movq	%rax, 144(%rsp)
	orq	%rsi, %r8
	movq	%r8, 280(%rsp)
	xorq	80(%rsp), %rdi
	movq	152(%rsp), %rsi
	movq	%rsi, %r8
	xorq	%rdi, %r8
	movq	%r8, 152(%rsp)
	orq	%rsi, %rdi
	movq	248(%rsp), %rsi
	xorq	72(%rsp), %rsi
	movq	160(%rsp), %r8
	movq	%r8, %rax
	xorq	%rsi, %rax
	movq	%rax, 160(%rsp)
	orq	%r8, %rsi
	xorq	64(%rsp), %rcx
	movq	256(%rsp), %r8
	movq	%r8, %rax
	xorq	%rcx, %rax
	movq	%rax, 248(%rsp)
	orq	%r8, %rcx
	xorq	56(%rsp), %rdx
	movq	168(%rsp), %r8
	movq	%r8, %rax
	xorq	%rdx, %rax
	movq	%rax, 168(%rsp)
	orq	%r8, %rdx
	movq	%rdx, 256(%rsp)
	movq	264(%rsp), %rax
	xorq	48(%rsp), %rax
	movq	176(%rsp), %rdx
	movq	%rdx, %r8
	xorq	%rax, %r8
	movq	%r8, 176(%rsp)
	movq	%rax, %r8
	orq	%rdx, %r8
	movq	%r8, 264(%rsp)
	movq	272(%rsp), %rax
	xorq	112(%rsp), %rax
	movq	184(%rsp), %r8
	movq	%r8, %rdx
	xorq	%rax, %rdx
	movq	%rdx, 184(%rsp)
	orq	%r8, %rax
	xorq	120(%rsp), %r15
	movq	136(%rsp), %rdx
	movq	%rdx, %r8
	xorq	%r15, %r8
	movq	%r8, 136(%rsp)
	orq	%rdx, %r15
	xorq	(%rsp), %r14
	movq	192(%rsp), %rdx
	movq	%rdx, %r8
	xorq	%r14, %r8
	movq	%r8, 192(%rsp)
	orq	%rdx, %r14
	xorq	40(%rsp), %r13
	movq	200(%rsp), %r8
	movq	%r8, %rdx
	xorq	%r13, %rdx
	movq	%rdx, 200(%rsp)
	orq	%r8, %r13
	xorq	32(%rsp), %r12
	movq	208(%rsp), %r8
	movq	%r8, %rdx
	xorq	%r12, %rdx
	movq	%rdx, 208(%rsp)
	orq	%r8, %r12
	xorq	24(%rsp), %rbp
	movq	216(%rsp), %r8
	movq	%r8, %rdx
	xorq	%rbp, %rdx
	movq	%rdx, 216(%rsp)
	orq	%r8, %rbp
	xorq	16(%rsp), %rbx
	movq	224(%rsp), %r8
	movq	%r8, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 224(%rsp)
	orq	%r8, %rbx
	xorq	8(%rsp), %r11
	movq	232(%rsp), %r8
	movq	%r8, %rdx
	xorq	%r11, %rdx
	movq	%rdx, 232(%rsp)
	orq	%r8, %r11
	xorq	104(%rsp), %r10
	movq	128(%rsp), %r8
	movq	%r8, %rdx
	xorq	%r10, %rdx
	movq	%rdx, 128(%rsp)
	orq	%r8, %r10
	movq	%r10, 272(%rsp)
	xorq	96(%rsp), %r9
	movq	240(%rsp), %rdx
	movq	%rdx, %r8
	xorq	%r9, %r8
	movq	%r8, 240(%rsp)
	orq	%rdx, %r9
	movq	280(%rsp), %r8
	xorq	88(%rsp), %r8
	movq	144(%rsp), %r10
	movq	%r10, %rdx
	xorq	%r8, %rdx
	movq	%rdx, 144(%rsp)
	orq	%r10, %r8
	xorq	80(%rsp), %rdi
	movq	152(%rsp), %r10
	movq	%r10, %rdx
	xorq	%rdi, %rdx
	movq	%rdx, 152(%rsp)
	orq	%r10, %rdi
	xorq	72(%rsp), %rsi
	movq	160(%rsp), %r10
	movq	%r10, %rdx
	xorq	%rsi, %rdx
	movq	%rdx, 160(%rsp)
	orq	%r10, %rsi
	xorq	64(%rsp), %rcx
	movq	248(%rsp), %r10
	movq	%r10, %rdx
	xorq	%rcx, %rdx
	movq	%rdx, 248(%rsp)
	orq	%r10, %rcx
	movq	%rcx, 280(%rsp)
	movq	256(%rsp), %rdx
	xorq	56(%rsp), %rdx
	movq	168(%rsp), %r10
	movq	%r10, %rcx
	xorq	%rdx, %rcx
	movq	%rcx, 168(%rsp)
	movq	%rdx, %rcx
	orq	%r10, %rcx
	movq	%rcx, 256(%rsp)
	movq	264(%rsp), %r10
	xorq	48(%rsp), %r10
	movq	%r10, %rdx
	movq	176(%rsp), %r10
	movq	%r10, %rcx
	xorq	%rdx, %rcx
	movq	%rcx, 176(%rsp)
	orq	%r10, %rdx
	xorq	112(%rsp), %rax
	movq	184(%rsp), %r10
	movq	%r10, %rcx
	xorq	%rax, %rcx
	movq	%rcx, 184(%rsp)
	orq	%r10, %rax
	movq	%rax, 264(%rsp)
	xorq	120(%rsp), %r15
	movq	136(%rsp), %r10
	movq	%r10, %rax
	xorq	%r15, %rax
	movq	%rax, 136(%rsp)
	orq	%r10, %r15
	xorq	(%rsp), %r14
	movq	192(%rsp), %r10
	movq	%r10, %rax
	xorq	%r14, %rax
	movq	%rax, 192(%rsp)
	orq	%r10, %r14
	xorq	40(%rsp), %r13
	movq	200(%rsp), %rcx
	movq	%rcx, %r10
	xorq	%r13, %r10
	movq	%r10, 200(%rsp)
	orq	%rcx, %r13
	xorq	32(%rsp), %r12
	movq	208(%rsp), %rcx
	movq	%rcx, %r10
	xorq	%r12, %r10
	movq	%r10, 208(%rsp)
	orq	%rcx, %r12
	xorq	24(%rsp), %rbp
	movq	216(%rsp), %rcx
	movq	%rcx, %r10
	xorq	%rbp, %r10
	movq	%r10, 216(%rsp)
	orq	%rcx, %rbp
	xorq	16(%rsp), %rbx
	movq	224(%rsp), %rcx
	movq	%rcx, %r10
	xorq	%rbx, %r10
	movq	%r10, 224(%rsp)
	orq	%rcx, %rbx
	xorq	8(%rsp), %r11
	movq	232(%rsp), %rcx
	movq	%rcx, %r10
	xorq	%r11, %r10
	movq	%r10, 232(%rsp)
	orq	%rcx, %r11
	movq	272(%rsp), %r10
	xorq	104(%rsp), %r10
	movq	128(%rsp), %rcx
	movq	%rcx, %rax
	xorq	%r10, %rax
	movq	%rax, 128(%rsp)
	orq	%rcx, %r10
	movq	%r10, 272(%rsp)
	xorq	96(%rsp), %r9
	movq	240(%rsp), %rcx
	movq	%rcx, %r10
	xorq	%r9, %r10
	movq	%r10, 240(%rsp)
	orq	%rcx, %r9
	xorq	88(%rsp), %r8
	movq	144(%rsp), %rax
	movq	%rax, %rcx
	xorq	%r8, %rcx
	movq	%rcx, 144(%rsp)
	orq	%rax, %r8
	xorq	80(%rsp), %rdi
	movq	152(%rsp), %rax
	movq	%rax, %rcx
	xorq	%rdi, %rcx
	movq	%rcx, 152(%rsp)
	orq	%rax, %rdi
	xorq	72(%rsp), %rsi
	movq	160(%rsp), %rax
	movq	%rax, %rcx
	xorq	%rsi, %rcx
	movq	%rcx, 160(%rsp)
	orq	%rax, %rsi
	movq	280(%rsp), %rcx
	xorq	64(%rsp), %rcx
	movq	248(%rsp), %rax
	movq	%rax, %r10
	xorq	%rcx, %r10
	movq	%r10, 248(%rsp)
	orq	%rax, %rcx
	movq	%rcx, 280(%rsp)
	movq	256(%rsp), %rax
	xorq	56(%rsp), %rax
	movq	%rax, %rcx
	movq	168(%rsp), %rax
	movq	%rax, %r10
	xorq	%rcx, %r10
	movq	%r10, 168(%rsp)
	orq	%rax, %rcx
	xorq	48(%rsp), %rdx
	movq	176(%rsp), %rax
	movq	%rax, %r10
	xorq	%rdx, %r10
	movq	%r10, 176(%rsp)
	orq	%rax, %rdx
	movq	%rdx, 256(%rsp)
	movq	264(%rsp), %rax
	xorq	112(%rsp), %rax
	movq	184(%rsp), %r10
	movq	%r10, %rdx
	xorq	%rax, %rdx
	movq	%rdx, 184(%rsp)
	orq	%r10, %rax
	movq	%rax, 264(%rsp)
	xorq	120(%rsp), %r15
	movq	136(%rsp), %rdx
	movq	%rdx, %rax
	xorq	%r15, %rax
	movq	%rax, 136(%rsp)
	orq	%rdx, %r15
	xorq	(%rsp), %r14
	movq	192(%rsp), %rax
	movq	%rax, %rdx
	xorq	%r14, %rdx
	movq	%rdx, 192(%rsp)
	orq	%rax, %r14
	xorq	40(%rsp), %r13
	movq	200(%rsp), %r10
	movq	%r10, %rdx
	xorq	%r13, %rdx
	movq	%rdx, 200(%rsp)
	orq	%r10, %r13
	xorq	32(%rsp), %r12
	movq	208(%rsp), %r10
	movq	%r10, %rdx
	xorq	%r12, %rdx
	movq	%rdx, 208(%rsp)
	orq	%r10, %r12
	xorq	24(%rsp), %rbp
	movq	216(%rsp), %r10
	movq	%r10, %rdx
	xorq	%rbp, %rdx
	movq	%rdx, 216(%rsp)
	orq	%r10, %rbp
	xorq	16(%rsp), %rbx
	movq	224(%rsp), %r10
	movq	%r10, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 224(%rsp)
	orq	%r10, %rbx
	xorq	8(%rsp), %r11
	movq	232(%rsp), %r10
	movq	%r10, %rdx
	xorq	%r11, %rdx
	movq	%rdx, 232(%rsp)
	orq	%r10, %r11
	movq	272(%rsp), %r10
	xorq	104(%rsp), %r10
	movq	128(%rsp), %rdx
	movq	%rdx, %rax
	xorq	%r10, %rax
	movq	%rax, 128(%rsp)
	orq	%rdx, %r10
	movq	%r10, 272(%rsp)
	xorq	96(%rsp), %r9
	movq	240(%rsp), %rdx
	movq	%rdx, %r10
	xorq	%r9, %r10
	movq	%r10, 240(%rsp)
	orq	%rdx, %r9
	xorq	88(%rsp), %r8
	movq	144(%rsp), %r10
	movq	%r10, %rax
	xorq	%r8, %rax
	movq	%rax, 144(%rsp)
	orq	%r10, %r8
	xorq	80(%rsp), %rdi
	movq	152(%rsp), %r10
	movq	%r10, %rax
	xorq	%rdi, %rax
	movq	%rax, 152(%rsp)
	orq	%r10, %rdi
	xorq	72(%rsp), %rsi
	movq	160(%rsp), %r10
	movq	%r10, %rax
	xorq	%rsi, %rax
	movq	%rax, 160(%rsp)
	orq	%r10, %rsi
	movq	%rsi, 288(%rsp)
	movq	280(%rsp), %rsi
	xorq	64(%rsp), %rsi
	movq	248(%rsp), %r10
	movq	%r10, %rax
	xorq	%rsi, %rax
	movq	%rax, 248(%rsp)
	orq	%r10, %rsi
	xorq	56(%rsp), %rcx
	movq	168(%rsp), %r10
	movq	%r10, %rax
	xorq	%rcx, %rax
	movq	%rax, 168(%rsp)
	orq	%r10, %rcx
	movq	%rcx, 280(%rsp)
	movq	256(%rsp), %rdx
	xorq	48(%rsp), %rdx
	movq	176(%rsp), %r10
	movq	%r10, %rcx
	xorq	%rdx, %rcx
	movq	%rcx, 176(%rsp)
	orq	%r10, %rdx
	movq	264(%rsp), %rax
	xorq	112(%rsp), %rax
	movq	184(%rsp), %r10
	movq	%r10, %rcx
	xorq	%rax, %rcx
	movq	%rcx, 184(%rsp)
	orq	%r10, %rax
	movq	%rax, 256(%rsp)
	xorq	120(%rsp), %r15
	movq	136(%rsp), %r10
	movq	%r10, %rcx
	xorq	%r15, %rcx
	movq	%rcx, 136(%rsp)
	orq	%r10, %r15
	xorq	(%rsp), %r14
	movq	192(%rsp), %r10
	movq	%r10, %rax
	xorq	%r14, %rax
	movq	%rax, 192(%rsp)
	orq	%r10, %r14
	xorq	40(%rsp), %r13
	movq	200(%rsp), %rcx
	movq	%rcx, %r10
	xorq	%r13, %r10
	movq	%r10, 200(%rsp)
	orq	%rcx, %r13
	xorq	32(%rsp), %r12
	movq	208(%rsp), %rcx
	movq	%rcx, %r10
	xorq	%r12, %r10
	movq	%r10, 208(%rsp)
	orq	%rcx, %r12
	xorq	24(%rsp), %rbp
	movq	216(%rsp), %rcx
	movq	%rcx, %r10
	xorq	%rbp, %r10
	movq	%r10, 216(%rsp)
	orq	%rcx, %rbp
	xorq	16(%rsp), %rbx
	movq	224(%rsp), %rcx
	movq	%rcx, %r10
	xorq	%rbx, %r10
	movq	%r10, 224(%rsp)
	orq	%rcx, %rbx
	xorq	8(%rsp), %r11
	movq	232(%rsp), %rcx
	movq	%rcx, %r10
	xorq	%r11, %r10
	movq	%r10, 232(%rsp)
	orq	%rcx, %r11
	movq	272(%rsp), %r10
	xorq	104(%rsp), %r10
	movq	128(%rsp), %rcx
	movq	%rcx, %rax
	xorq	%r10, %rax
	movq	%rax, 128(%rsp)
	orq	%rcx, %r10
	movq	%r10, 264(%rsp)
	xorq	96(%rsp), %r9
	movq	240(%rsp), %rcx
	movq	%rcx, %r10
	xorq	%r9, %r10
	movq	%r10, 240(%rsp)
	orq	%rcx, %r9
	xorq	88(%rsp), %r8
	movq	144(%rsp), %rax
	movq	%rax, %rcx
	xorq	%r8, %rcx
	movq	%rcx, 144(%rsp)
	orq	%rax, %r8
	xorq	80(%rsp), %rdi
	movq	152(%rsp), %rax
	movq	%rax, %rcx
	xorq	%rdi, %rcx
	movq	%rcx, 152(%rsp)
	orq	%rax, %rdi
	movq	%rdi, 272(%rsp)
	movq	288(%rsp), %rdi
	xorq	72(%rsp), %rdi
	movq	160(%rsp), %rax
	movq	%rax, %rcx
	xorq	%rdi, %rcx
	movq	%rcx, 160(%rsp)
	orq	%rax, %rdi
	movq	%rdi, 288(%rsp)
	xorq	64(%rsp), %rsi
	movq	248(%rsp), %rax
	movq	%rax, %rdi
	xorq	%rsi, %rdi
	movq	%rdi, 248(%rsp)
	orq	%rax, %rsi
	movq	280(%rsp), %rcx
	xorq	56(%rsp), %rcx
	movq	168(%rsp), %rdi
	movq	%rdi, %rax
	xorq	%rcx, %rax
	movq	%rax, 168(%rsp)
	orq	%rdi, %rcx
	xorq	48(%rsp), %rdx
	movq	176(%rsp), %rax
	movq	%rax, %rdi
	xorq	%rdx, %rdi
	movq	%rdi, 176(%rsp)
	orq	%rax, %rdx
	movq	%rdx, 280(%rsp)
	movq	256(%rsp), %rax
	xorq	112(%rsp), %rax
	movq	184(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%rax, %rdx
	movq	%rdx, 184(%rsp)
	orq	%rdi, %rax
	xorq	120(%rsp), %r15
	movq	136(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%r15, %rdi
	movq	%rdi, 136(%rsp)
	orq	%rdx, %r15
	xorq	(%rsp), %r14
	movq	192(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r14, %rdx
	movq	%rdx, 192(%rsp)
	orq	%rdi, %r14
	xorq	40(%rsp), %r13
	movq	200(%rsp), %r10
	movq	%r10, %rdx
	xorq	%r13, %rdx
	movq	%rdx, 200(%rsp)
	orq	%r10, %r13
	xorq	32(%rsp), %r12
	movq	208(%rsp), %r10
	movq	%r10, %rdx
	xorq	%r12, %rdx
	movq	%rdx, 208(%rsp)
	orq	%r10, %r12
	xorq	24(%rsp), %rbp
	movq	216(%rsp), %r10
	movq	%r10, %rdx
	xorq	%rbp, %rdx
	movq	%rdx, 216(%rsp)
	orq	%r10, %rbp
	xorq	16(%rsp), %rbx
	movq	224(%rsp), %r10
	movq	%r10, %rdx
	xorq	%rbx, %rdx
	movq	%rdx, 224(%rsp)
	orq	%r10, %rbx
	xorq	8(%rsp), %r11
	movq	232(%rsp), %r10
	movq	%r10, %rdx
	xorq	%r11, %rdx
	movq	%rdx, 232(%rsp)
	orq	%r10, %r11
	movq	264(%rsp), %r10
	xorq	104(%rsp), %r10
	movq	128(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%r10, %rdi
	movq	%rdi, 128(%rsp)
	orq	%rdx, %r10
	movq	%r10, 256(%rsp)
	xorq	96(%rsp), %r9
	movq	240(%rsp), %rdx
	movq	%rdx, %r10
	xorq	%r9, %r10
	movq	%r10, 240(%rsp)
	orq	%rdx, %r9
	xorq	88(%rsp), %r8
	movq	144(%rsp), %rdi
	movq	%rdi, %r10
	xorq	%r8, %r10
	movq	%r10, 144(%rsp)
	orq	%rdi, %r8
	movq	%r8, 264(%rsp)
	movq	272(%rsp), %r8
	xorq	80(%rsp), %r8
	movq	152(%rsp), %rdi
	movq	%rdi, %r10
	xorq	%r8, %r10
	movq	%r10, 152(%rsp)
	orq	%rdi, %r8
	movq	288(%rsp), %rdi
	xorq	72(%rsp), %rdi
	movq	160(%rsp), %r10
	movq	%r10, %rdx
	xorq	%rdi, %rdx
	movq	%rdx, 160(%rsp)
	orq	%r10, %rdi
	xorq	64(%rsp), %rsi
	movq	248(%rsp), %r10
	movq	%r10, %rdx
	xorq	%rsi, %rdx
	movq	%rdx, 248(%rsp)
	orq	%r10, %rsi
	xorq	56(%rsp), %rcx
	movq	168(%rsp), %r10
	movq	%r10, %rdx
	xorq	%rcx, %rdx
	movq	%rdx, 168(%rsp)
	orq	%r10, %rcx
	movq	%rcx, 272(%rsp)
	movq	280(%rsp), %rdx
	xorq	48(%rsp), %rdx
	movq	176(%rsp), %rcx
	movq	%rcx, %r10
	xorq	%rdx, %r10
	movq	%r10, 176(%rsp)
	orq	%rcx, %rdx
	xorq	112(%rsp), %rax
	movq	184(%rsp), %r10
	movq	%r10, %rcx
	xorq	%rax, %rcx
	movq	%rcx, 184(%rsp)
	orq	%r10, %rax
	movq	%rax, 280(%rsp)
	xorq	120(%rsp), %r15
	movq	136(%rsp), %rcx
	movq	%rcx, %r10
	xorq	%r15, %r10
	movq	%r10, 136(%rsp)
	orq	%rcx, %r15
	xorq	(%rsp), %r14
	movq	192(%rsp), %r10
	movq	%r10, %rcx
	xorq	%r14, %rcx
	movq	%rcx, 192(%rsp)
	orq	%r10, %r14
	xorq	40(%rsp), %r13
	movq	200(%rsp), %rax
	movq	%rax, %r10
	xorq	%r13, %r10
	movq	%r10, 200(%rsp)
	orq	%rax, %r13
	xorq	32(%rsp), %r12
	movq	208(%rsp), %rax
	movq	%rax, %r10
	xorq	%r12, %r10
	movq	%r10, 208(%rsp)
	orq	%rax, %r12
	xorq	24(%rsp), %rbp
	movq	216(%rsp), %rax
	movq	%rax, %rcx
	xorq	%rbp, %rcx
	movq	%rcx, 216(%rsp)
	orq	%rax, %rbp
	xorq	16(%rsp), %rbx
	movq	224(%rsp), %rax
	movq	%rax, %r10
	xorq	%rbx, %r10
	movq	%r10, 224(%rsp)
	orq	%rax, %rbx
	xorq	8(%rsp), %r11
	movq	232(%rsp), %rax
	movq	%rax, %rcx
	xorq	%r11, %rcx
	movq	%rcx, 232(%rsp)
	orq	%rax, %r11
	movq	256(%rsp), %r10
	xorq	104(%rsp), %r10
	movq	128(%rsp), %rax
	movq	%rax, %rcx
	xorq	%r10, %rcx
	movq	%rcx, 128(%rsp)
	orq	%rax, %r10
	xorq	96(%rsp), %r9
	movq	240(%rsp), %rcx
	movq	%rcx, %rax
	xorq	%r9, %rax
	movq	%rax, 240(%rsp)
	orq	%rcx, %r9
	movq	%r9, 256(%rsp)
	movq	264(%rsp), %r9
	xorq	88(%rsp), %r9
	movq	144(%rsp), %rax
	movq	%rax, %rcx
	xorq	%r9, %rcx
	movq	%rcx, 144(%rsp)
	orq	%rax, %r9
	xorq	80(%rsp), %r8
	movq	152(%rsp), %rax
	movq	%rax, %rcx
	xorq	%r8, %rcx
	movq	%rcx, 152(%rsp)
	orq	%rax, %r8
	xorq	72(%rsp), %rdi
	movq	160(%rsp), %rax
	movq	%rax, %rcx
	xorq	%rdi, %rcx
	movq	%rcx, 160(%rsp)
	orq	%rax, %rdi
	movq	%rdi, 264(%rsp)
	xorq	64(%rsp), %rsi
	movq	248(%rsp), %rax
	movq	%rax, %rdi
	xorq	%rsi, %rdi
	movq	%rdi, 248(%rsp)
	orq	%rax, %rsi
	movq	272(%rsp), %rcx
	xorq	56(%rsp), %rcx
	movq	168(%rsp), %rax
	movq	%rax, %rdi
	xorq	%rcx, %rdi
	movq	%rdi, 168(%rsp)
	orq	%rax, %rcx
	xorq	48(%rsp), %rdx
	movq	176(%rsp), %rax
	movq	%rax, %rdi
	xorq	%rdx, %rdi
	movq	%rdi, 176(%rsp)
	orq	%rax, %rdx
	movq	%rdx, 272(%rsp)
	movq	280(%rsp), %rax
	xorq	112(%rsp), %rax
	movq	184(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%rax, %rdi
	movq	%rdi, 184(%rsp)
	orq	%rdx, %rax
	xorq	120(%rsp), %r15
	movq	136(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r15, %rdx
	movq	%rdx, 136(%rsp)
	orq	%rdi, %r15
	xorq	(%rsp), %r14
	movq	192(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	%r14, %rdx
	movq	%rdx, 192(%rsp)
	orq	%rdi, %r14
	xorq	40(%rsp), %r13
	movq	200(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%r13, %rdi
	movq	%rdi, 200(%rsp)
	orq	%rdx, %r13
	xorq	32(%rsp), %r12
	movq	208(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%r12, %rdi
	movq	%rdi, 208(%rsp)
	orq	%rdx, %r12
	xorq	24(%rsp), %rbp
	movq	216(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%rbp, %rdi
	movq	%rdi, 216(%rsp)
	orq	%rdx, %rbp
	xorq	16(%rsp), %rbx
	movq	224(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%rbx, %rdi
	movq	%rdi, 224(%rsp)
	orq	%rdx, %rbx
	xorq	8(%rsp), %r11
	movq	232(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%r11, %rdi
	movq	%rdi, 232(%rsp)
	orq	%rdx, %r11
	xorq	104(%rsp), %r10
	movq	128(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%r10, %rdi
	movq	%rdi, 280(%rsp)
	orq	%rdx, %r10
	movq	256(%rsp), %rdx
	xorq	96(%rsp), %rdx
	movq	%rdx, 128(%rsp)
	movq	240(%rsp), %rdi
	movq	%rdi, %rdx
	xorq	128(%rsp), %rdx
	movq	%rdx, 256(%rsp)
	orq	128(%rsp), %rdi
	movq	%rdi, 288(%rsp)
	xorq	88(%rsp), %r9
	movq	144(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%r9, %rdi
	movq	%rdi, 144(%rsp)
	orq	%rdx, %r9
	movq	%r9, 296(%rsp)
	xorq	80(%rsp), %r8
	movq	152(%rsp), %r9
	movq	%r9, %rdx
	xorq	%r8, %rdx
	movq	%rdx, 152(%rsp)
	orq	%r9, %r8
	movq	%r8, 304(%rsp)
	movq	264(%rsp), %rdi
	xorq	72(%rsp), %rdi
	movq	160(%rsp), %r9
	movq	%r9, %r8
	xorq	%rdi, %r8
	movq	%r8, 160(%rsp)
	movq	%rdi, %rdx
	orq	%r9, %rdx
	movq	%rdx, 312(%rsp)
	xorq	64(%rsp), %rsi
	movq	248(%rsp), %r9
	movq	%r9, %rdx
	xorq	%rsi, %rdx
	movq	%rdx, 320(%rsp)
	orq	%r9, %rsi
	xorq	56(%rsp), %rcx
	movq	168(%rsp), %r9
	movq	%r9, %rdx
	xorq	%rcx, %rdx
	movq	%rdx, 168(%rsp)
	orq	%rcx, %r9
	movq	%r9, %rdi
	movq	272(%rsp), %rdx
	xorq	48(%rsp), %rdx
	movq	176(%rsp), %r9
	movq	%r9, %rcx
	xorq	%rdx, %rcx
	movq	%rcx, 176(%rsp)
	orq	%r9, %rdx
	xorq	112(%rsp), %rax
	movq	184(%rsp), %rcx
	movq	%rcx, %r9
	xorq	%rax, %r9
	movq	%r9, 184(%rsp)
	orq	%rax, %rcx
	xorq	120(%rsp), %r15
	movq	136(%rsp), %r9
	movq	%r9, %rax
	xorq	%r15, %rax
	movq	%rax, 136(%rsp)
	orq	%r9, %r15
	xorq	(%rsp), %r14
	movq	192(%rsp), %rax
	movq	%rax, %r9
	xorq	%r14, %r9
	movq	%r9, 192(%rsp)
	orq	%rax, %r14
	movq	40(%rsp), %rax
	xorq	%r13, %rax
	movq	200(%rsp), %r13
	movq	%r13, %r9
	xorq	%rax, %r9
	movq	%r9, 200(%rsp)
	orq	%rax, %r13
	movq	32(%rsp), %rax
	xorq	%r12, %rax
	movq	208(%rsp), %r12
	movq	%r12, %r9
	xorq	%rax, %r9
	movq	%r9, 208(%rsp)
	orq	%rax, %r12
	movq	24(%rsp), %rax
	xorq	%rbp, %rax
	movq	216(%rsp), %rbp
	movq	%rbp, %r9
	xorq	%rax, %r9
	movq	%r9, 216(%rsp)
	orq	%rax, %rbp
	movq	16(%rsp), %rax
	xorq	%rbx, %rax
	movq	224(%rsp), %rbx
	movq	%rbx, %r9
	xorq	%rax, %r9
	movq	%r9, 224(%rsp)
	orq	%rax, %rbx
	movq	8(%rsp), %rax
	xorq	%r11, %rax
	movq	232(%rsp), %r11
	movq	%r11, %r9
	xorq	%rax, %r9
	movq	%r9, 232(%rsp)
	orq	%rax, %r11
	movq	104(%rsp), %rax
	xorq	%r10, %rax
	movq	280(%rsp), %r10
	movq	%r10, %r9
	xorq	%rax, %r9
	movq	%r9, 240(%rsp)
	orq	%rax, %r10
	movq	%r10, 128(%rsp)
	movq	288(%rsp), %rax
	xorq	96(%rsp), %rax
	movq	256(%rsp), %r8
	movq	%r8, %r9
	xorq	%rax, %r9
	movq	%r9, 248(%rsp)
	orq	%rax, %r8
	movq	%r8, %r9
	movq	296(%rsp), %rax
	xorq	88(%rsp), %rax
	movq	144(%rsp), %r8
	movq	%r8, %r10
	xorq	%rax, %r10
	movq	%r10, 256(%rsp)
	orq	%rax, %r8
	movq	%r8, 144(%rsp)
	movq	304(%rsp), %rax
	xorq	80(%rsp), %rax
	movq	152(%rsp), %r10
	movq	%r10, %r8
	xorq	%rax, %r8
	movq	%r8, 264(%rsp)
	orq	%r10, %rax
	movq	%rax, 152(%rsp)
	movq	312(%rsp), %rax
	xorq	72(%rsp), %rax
	movq	160(%rsp), %r8
	movq	%r8, %r10
	xorq	%rax, %r10
	movq	%r10, 272(%rsp)
	orq	%r8, %rax
	movq	%rax, 160(%rsp)
	movq	64(%rsp), %rax
	xorq	%rsi, %rax
	movq	320(%rsp), %r8
	movq	%r8, %rsi
	xorq	%rax, %rsi
	movq	%rsi, 280(%rsp)
	orq	%r8, %rax
	xorq	56(%rsp), %rdi
	movq	168(%rsp), %r10
	movq	%r10, %rsi
	xorq	%rdi, %rsi
	movq	%rsi, 288(%rsp)
	movq	%rdi, %r8
	orq	%r10, %r8
	movq	%r8, 168(%rsp)
	movq	48(%rsp), %rsi
	xorq	%rdx, %rsi
	movq	176(%rsp), %rdi
	movq	%rdi, %r8
	xorq	%rsi, %r8
	movq	%r8, 296(%rsp)
	orq	%rdi, %rsi
	movq	%rsi, 176(%rsp)
	xorq	112(%rsp), %rcx
	movq	184(%rsp), %rdx
	movq	%rdx, %rdi
	xorq	%rcx, %rdi
	movq	%rdi, 304(%rsp)
	orq	%rdx, %rcx
	movq	%rcx, 184(%rsp)
	xorq	120(%rsp), %r15
	movq	136(%rsp), %rdi
	movq	%rdi, %rcx
	xorq	%r15, %rcx
	movq	%rcx, 312(%rsp)
	orq	%rdi, %r15
	movq	%r15, 136(%rsp)
	movq	(%rsp), %rsi
	cmpq	456(%rsp), %rsi
	jne	.L492
	movq	%rbx, (%rsp)
	movq	%r11, %r15
	movq	%r9, 16(%rsp)
	movq	%rax, 8(%rsp)
.L491:
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	(%rsp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	128(%rsp), %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	call	use_int@PLT
	movl	144(%rsp), %edi
	call	use_int@PLT
	movl	152(%rsp), %edi
	call	use_int@PLT
	movl	160(%rsp), %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	168(%rsp), %edi
	call	use_int@PLT
	movl	176(%rsp), %edi
	call	use_int@PLT
	movl	184(%rsp), %edi
	call	use_int@PLT
	movl	136(%rsp), %edi
	call	use_int@PLT
	addq	$472, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE168:
	.size	int64_bit_15, .-int64_bit_15
	.globl	int64_add_0
	.type	int64_add_0, @function
int64_add_0:
.LFB169:
	.cfi_startproc
	endbr64
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movq	%rdi, %rcx
	movl	12(%rsi), %eax
	leal	37421(%rax), %edx
	movslq	%edx, %rdx
	leal	255(%rax), %edi
	movslq	%edi, %rdi
	salq	$30, %rdi
	addq	%rdx, %rdi
	leal	21698325(%rax), %edx
	movslq	%edx, %rdx
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rdx, %rax
	leaq	-1(%rcx), %rdx
	testq	%rcx, %rcx
	je	.L496
.L497:
	movq	%rax, %rcx
	movq	%rdi, %rax
	negq	%rdi
	subq	%rcx, %rdi
	subq	$1, %rdx
	cmpq	$-1, %rdx
	jne	.L497
.L496:
	addl	%eax, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE169:
	.size	int64_add_0, .-int64_add_0
	.globl	int64_add_1
	.type	int64_add_1, @function
int64_add_1:
.LFB170:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	pushq	%rbx
	.cfi_def_cfa_offset 24
	.cfi_offset 3, -24
	subq	$8, %rsp
	.cfi_def_cfa_offset 32
	movl	12(%rsi), %edx
	leal	37421(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	255(%rdx), %eax
	cltq
	salq	$30, %rax
	addq	%rcx, %rax
	leal	21698325(%rdx), %ecx
	movslq	%ecx, %rcx
	addl	$65535, %edx
	movslq	%edx, %rdx
	salq	$29, %rdx
	addq	%rcx, %rdx
	movl	16(%rsi), %ecx
	leal	37420(%rcx), %esi
	movslq	%esi, %rsi
	leal	254(%rcx), %ebx
	movslq	%ebx, %rbx
	salq	$30, %rbx
	addq	%rsi, %rbx
	leal	21698324(%rcx), %esi
	movslq	%esi, %rsi
	leal	65534(%rcx), %ebp
	movslq	%ebp, %rbp
	salq	$29, %rbp
	addq	%rsi, %rbp
	leaq	-1(%rdi), %rcx
	testq	%rdi, %rdi
	je	.L501
.L502:
	movq	%rdx, %r8
	movq	%rax, %rdx
	movq	%rbp, %rsi
	movq	%rbx, %rbp
	negq	%rax
	subq	%r8, %rax
	negq	%rbx
	subq	%rsi, %rbx
	subq	$1, %rcx
	cmpq	$-1, %rcx
	jne	.L502
.L501:
	leal	(%rax,%rdx), %edi
	call	use_int@PLT
	leal	(%rbx,%rbp), %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 24
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE170:
	.size	int64_add_1, .-int64_add_1
	.globl	int64_add_2
	.type	int64_add_2, @function
int64_add_2:
.LFB171:
	.cfi_startproc
	endbr64
	pushq	%r13
	.cfi_def_cfa_offset 16
	.cfi_offset 13, -16
	pushq	%r12
	.cfi_def_cfa_offset 24
	.cfi_offset 12, -24
	pushq	%rbp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset 3, -40
	subq	$8, %rsp
	.cfi_def_cfa_offset 48
	movq	%rdi, %rcx
	movl	12(%rsi), %edx
	leal	37421(%rdx), %edi
	movslq	%edi, %rdi
	leal	255(%rdx), %eax
	cltq
	salq	$30, %rax
	addq	%rdi, %rax
	leal	21698325(%rdx), %edi
	movslq	%edi, %rdi
	leal	65535(%rdx), %r9d
	movslq	%r9d, %r9
	salq	$29, %r9
	addq	%rdi, %r9
	movl	16(%rsi), %edx
	leal	37420(%rdx), %edi
	movslq	%edi, %rdi
	leal	254(%rdx), %ebp
	movslq	%ebp, %rbp
	salq	$30, %rbp
	addq	%rdi, %rbp
	leal	21698324(%rdx), %edi
	movslq	%edi, %rdi
	leal	65534(%rdx), %r13d
	movslq	%r13d, %r13
	salq	$29, %r13
	addq	%rdi, %r13
	movl	20(%rsi), %edx
	leal	37419(%rdx), %esi
	movslq	%esi, %rsi
	leal	253(%rdx), %ebx
	movslq	%ebx, %rbx
	salq	$30, %rbx
	addq	%rsi, %rbx
	leal	21698323(%rdx), %esi
	movslq	%esi, %rsi
	leal	65533(%rdx), %r12d
	movslq	%r12d, %r12
	salq	$29, %r12
	addq	%rsi, %r12
	leaq	-1(%rcx), %rdx
	testq	%rcx, %rcx
	je	.L506
.L507:
	movq	%r9, %r8
	movq	%rax, %r9
	movq	%r13, %rsi
	movq	%rbp, %r13
	movq	%r12, %rcx
	movq	%rbx, %r12
	negq	%rax
	subq	%r8, %rax
	negq	%rbp
	subq	%rsi, %rbp
	negq	%rbx
	subq	%rcx, %rbx
	subq	$1, %rdx
	cmpq	$-1, %rdx
	jne	.L507
.L506:
	leal	(%rax,%r9), %edi
	call	use_int@PLT
	leal	0(%rbp,%r13), %edi
	call	use_int@PLT
	leal	(%rbx,%r12), %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%rbp
	.cfi_def_cfa_offset 24
	popq	%r12
	.cfi_def_cfa_offset 16
	popq	%r13
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE171:
	.size	int64_add_2, .-int64_add_2
	.globl	int64_add_3
	.type	int64_add_3, @function
int64_add_3:
.LFB172:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$8, %rsp
	.cfi_def_cfa_offset 64
	movq	%rdi, %r8
	movl	12(%rsi), %edx
	leal	37421(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	255(%rdx), %eax
	cltq
	salq	$30, %rax
	addq	%rcx, %rax
	leal	21698325(%rdx), %ecx
	movslq	%ecx, %rcx
	addl	$65535, %edx
	movslq	%edx, %rdx
	salq	$29, %rdx
	addq	%rcx, %rdx
	movl	16(%rsi), %ecx
	leal	37420(%rcx), %edi
	movslq	%edi, %rdi
	leal	254(%rcx), %r12d
	movslq	%r12d, %r12
	salq	$30, %r12
	addq	%rdi, %r12
	leal	21698324(%rcx), %edi
	movslq	%edi, %rdi
	leal	65534(%rcx), %r15d
	movslq	%r15d, %r15
	salq	$29, %r15
	addq	%rdi, %r15
	movl	20(%rsi), %ecx
	leal	37419(%rcx), %edi
	movslq	%edi, %rdi
	leal	253(%rcx), %ebp
	movslq	%ebp, %rbp
	salq	$30, %rbp
	addq	%rdi, %rbp
	leal	21698323(%rcx), %edi
	movslq	%edi, %rdi
	leal	65533(%rcx), %r14d
	movslq	%r14d, %r14
	salq	$29, %r14
	addq	%rdi, %r14
	movl	24(%rsi), %ecx
	leal	37418(%rcx), %esi
	movslq	%esi, %rsi
	leal	252(%rcx), %ebx
	movslq	%ebx, %rbx
	salq	$30, %rbx
	addq	%rsi, %rbx
	leal	21698322(%rcx), %esi
	movslq	%esi, %rsi
	leal	65532(%rcx), %r13d
	movslq	%r13d, %r13
	salq	$29, %r13
	addq	%rsi, %r13
	leaq	-1(%r8), %rcx
	testq	%r8, %r8
	je	.L511
.L512:
	movq	%rdx, %r10
	movq	%rax, %rdx
	movq	%r15, %r9
	movq	%r12, %r15
	movq	%r14, %r8
	movq	%rbp, %r14
	movq	%r13, %rsi
	movq	%rbx, %r13
	negq	%rax
	subq	%r10, %rax
	negq	%r12
	subq	%r9, %r12
	negq	%rbp
	subq	%r8, %rbp
	negq	%rbx
	subq	%rsi, %rbx
	subq	$1, %rcx
	cmpq	$-1, %rcx
	jne	.L512
.L511:
	leal	(%rax,%rdx), %edi
	call	use_int@PLT
	leal	(%r12,%r15), %edi
	call	use_int@PLT
	leal	0(%rbp,%r14), %edi
	call	use_int@PLT
	leal	(%rbx,%r13), %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE172:
	.size	int64_add_3, .-int64_add_3
	.globl	int64_add_4
	.type	int64_add_4, @function
int64_add_4:
.LFB173:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$24, %rsp
	.cfi_def_cfa_offset 80
	movq	%rdi, %r8
	movl	12(%rsi), %edx
	leal	37421(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	255(%rdx), %eax
	cltq
	salq	$30, %rax
	addq	%rcx, %rax
	leal	21698325(%rdx), %ecx
	movslq	%ecx, %rcx
	addl	$65535, %edx
	movslq	%edx, %rdx
	salq	$29, %rdx
	addq	%rcx, %rdx
	movl	16(%rsi), %ecx
	leal	37420(%rcx), %edi
	movslq	%edi, %rdi
	leal	254(%rcx), %ebp
	movslq	%ebp, %rbp
	salq	$30, %rbp
	addq	%rdi, %rbp
	leal	21698324(%rcx), %edi
	movslq	%edi, %rdi
	leal	65534(%rcx), %r15d
	movslq	%r15d, %r15
	salq	$29, %r15
	addq	%rdi, %r15
	movl	20(%rsi), %ecx
	leal	37419(%rcx), %edi
	movslq	%edi, %rdi
	leal	253(%rcx), %ebx
	movslq	%ebx, %rbx
	salq	$30, %rbx
	addq	%rdi, %rbx
	leal	21698323(%rcx), %edi
	movslq	%edi, %rdi
	leal	65533(%rcx), %r14d
	movslq	%r14d, %r14
	salq	$29, %r14
	addq	%rdi, %r14
	movl	24(%rsi), %ecx
	leal	37418(%rcx), %r9d
	movslq	%r9d, %r9
	leal	252(%rcx), %edi
	movslq	%edi, %rdi
	salq	$30, %rdi
	addq	%r9, %rdi
	movq	%rdi, (%rsp)
	leal	21698322(%rcx), %edi
	movslq	%edi, %rdi
	leal	65532(%rcx), %r13d
	movslq	%r13d, %r13
	salq	$29, %r13
	addq	%rdi, %r13
	movl	28(%rsi), %ecx
	leal	37417(%rcx), %edi
	movslq	%edi, %rdi
	leal	251(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 8(%rsp)
	leal	21698321(%rcx), %esi
	movslq	%esi, %rsi
	leal	65531(%rcx), %r12d
	movslq	%r12d, %r12
	salq	$29, %r12
	addq	%rsi, %r12
	leaq	-1(%r8), %rcx
	testq	%r8, %r8
	je	.L516
.L517:
	movq	%rdx, %r11
	movq	%rax, %rdx
	movq	%r15, %r10
	movq	%rbp, %r15
	movq	%r14, %r9
	movq	%rbx, %r14
	movq	%r13, %r8
	movq	(%rsp), %r13
	movq	%r12, %rsi
	movq	8(%rsp), %r12
	negq	%rax
	subq	%r11, %rax
	negq	%rbp
	subq	%r10, %rbp
	negq	%rbx
	subq	%r9, %rbx
	movq	%r13, %rdi
	negq	%rdi
	subq	%r8, %rdi
	movq	%rdi, (%rsp)
	movq	%r12, %rdi
	negq	%rdi
	subq	%rsi, %rdi
	movq	%rdi, 8(%rsp)
	subq	$1, %rcx
	cmpq	$-1, %rcx
	jne	.L517
.L516:
	leal	(%rax,%rdx), %edi
	call	use_int@PLT
	leal	0(%rbp,%r15), %edi
	call	use_int@PLT
	leal	(%rbx,%r14), %edi
	call	use_int@PLT
	movl	(%rsp), %edi
	addl	%r13d, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	addl	%r12d, %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE173:
	.size	int64_add_4, .-int64_add_4
	.globl	int64_add_5
	.type	int64_add_5, @function
int64_add_5:
.LFB174:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$40, %rsp
	.cfi_def_cfa_offset 96
	movq	%rdi, %r8
	movl	12(%rsi), %edx
	leal	37421(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	255(%rdx), %eax
	cltq
	salq	$30, %rax
	addq	%rcx, %rax
	leal	21698325(%rdx), %ecx
	movslq	%ecx, %rcx
	addl	$65535, %edx
	movslq	%edx, %rdx
	salq	$29, %rdx
	addq	%rcx, %rdx
	movl	16(%rsi), %ecx
	leal	37420(%rcx), %edi
	movslq	%edi, %rdi
	leal	254(%rcx), %ebx
	movslq	%ebx, %rbx
	salq	$30, %rbx
	addq	%rdi, %rbx
	leal	21698324(%rcx), %edi
	movslq	%edi, %rdi
	leal	65534(%rcx), %r15d
	movslq	%r15d, %r15
	salq	$29, %r15
	addq	%rdi, %r15
	movl	20(%rsi), %ecx
	leal	37419(%rcx), %r9d
	movslq	%r9d, %r9
	leal	253(%rcx), %edi
	movslq	%edi, %rdi
	salq	$30, %rdi
	addq	%r9, %rdi
	movq	%rdi, (%rsp)
	leal	21698323(%rcx), %edi
	movslq	%edi, %rdi
	leal	65533(%rcx), %r14d
	movslq	%r14d, %r14
	salq	$29, %r14
	addq	%rdi, %r14
	movl	24(%rsi), %ecx
	leal	37418(%rcx), %r9d
	movslq	%r9d, %r9
	leal	252(%rcx), %edi
	movslq	%edi, %rdi
	salq	$30, %rdi
	addq	%r9, %rdi
	movq	%rdi, 8(%rsp)
	leal	21698322(%rcx), %edi
	movslq	%edi, %rdi
	leal	65532(%rcx), %r13d
	movslq	%r13d, %r13
	salq	$29, %r13
	addq	%rdi, %r13
	movl	28(%rsi), %ecx
	leal	37417(%rcx), %r9d
	movslq	%r9d, %r9
	leal	251(%rcx), %edi
	movslq	%edi, %rdi
	salq	$30, %rdi
	addq	%r9, %rdi
	movq	%rdi, 16(%rsp)
	leal	21698321(%rcx), %edi
	movslq	%edi, %rdi
	leal	65531(%rcx), %r12d
	movslq	%r12d, %r12
	salq	$29, %r12
	addq	%rdi, %r12
	movl	32(%rsi), %ecx
	leal	37416(%rcx), %edi
	movslq	%edi, %rdi
	leal	250(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 24(%rsp)
	leal	21698320(%rcx), %esi
	movslq	%esi, %rsi
	leal	65530(%rcx), %ebp
	movslq	%ebp, %rbp
	salq	$29, %rbp
	addq	%rsi, %rbp
	leaq	-1(%r8), %rcx
	testq	%r8, %r8
	je	.L521
.L522:
	movq	%rdx, %rdi
	movq	%rax, %rdx
	movq	%r15, %r11
	movq	%rbx, %r15
	movq	%r14, %r10
	movq	(%rsp), %r14
	movq	%r13, %r9
	movq	8(%rsp), %r13
	movq	%r12, %r8
	movq	16(%rsp), %r12
	movq	%rbp, %rsi
	movq	24(%rsp), %rbp
	negq	%rax
	subq	%rdi, %rax
	negq	%rbx
	subq	%r11, %rbx
	movq	%r14, %rdi
	negq	%rdi
	subq	%r10, %rdi
	movq	%rdi, (%rsp)
	movq	%r13, %rdi
	negq	%rdi
	subq	%r9, %rdi
	movq	%rdi, 8(%rsp)
	movq	%r12, %rdi
	negq	%rdi
	subq	%r8, %rdi
	movq	%rdi, 16(%rsp)
	movq	%rbp, %rdi
	negq	%rdi
	subq	%rsi, %rdi
	movq	%rdi, 24(%rsp)
	subq	$1, %rcx
	cmpq	$-1, %rcx
	jne	.L522
.L521:
	leal	(%rax,%rdx), %edi
	call	use_int@PLT
	leal	(%rbx,%r15), %edi
	call	use_int@PLT
	movl	(%rsp), %edi
	addl	%r14d, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	addl	%r13d, %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	addl	%r12d, %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	addl	%ebp, %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE174:
	.size	int64_add_5, .-int64_add_5
	.globl	int64_add_6
	.type	int64_add_6, @function
int64_add_6:
.LFB175:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$72, %rsp
	.cfi_def_cfa_offset 128
	movq	%rdi, %rcx
	movl	12(%rsi), %eax
	leal	37421(%rax), %edi
	movslq	%edi, %rdi
	leal	255(%rax), %edx
	movslq	%edx, %rdx
	salq	$30, %rdx
	leaq	(%rdx,%rdi), %r9
	leal	21698325(%rax), %edx
	movslq	%edx, %rdx
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rdx, %rax
	movl	16(%rsi), %edx
	leal	37420(%rdx), %r8d
	movslq	%r8d, %r8
	leal	254(%rdx), %edi
	movslq	%edi, %rdi
	salq	$30, %rdi
	leaq	(%rdi,%r8), %rbx
	movq	%rbx, 24(%rsp)
	leal	21698324(%rdx), %edi
	movslq	%edi, %rdi
	addl	$65534, %edx
	movslq	%edx, %rdx
	salq	$29, %rdx
	leaq	(%rdx,%rdi), %r11
	movq	%r11, 32(%rsp)
	movl	20(%rsi), %edx
	leal	37419(%rdx), %edi
	movslq	%edi, %rdi
	leal	253(%rdx), %r14d
	movslq	%r14d, %r14
	salq	$30, %r14
	addq	%rdi, %r14
	leal	21698323(%rdx), %edi
	movslq	%edi, %rdi
	addl	$65533, %edx
	movslq	%edx, %rdx
	salq	$29, %rdx
	leaq	(%rdx,%rdi), %r10
	movq	%r10, 40(%rsp)
	movl	24(%rsi), %edx
	leal	37418(%rdx), %edi
	movslq	%edi, %rdi
	leal	252(%rdx), %r13d
	movslq	%r13d, %r13
	salq	$30, %r13
	addq	%rdi, %r13
	leal	21698322(%rdx), %edi
	movslq	%edi, %rdi
	addl	$65532, %edx
	movslq	%edx, %rdx
	salq	$29, %rdx
	leaq	(%rdx,%rdi), %r8
	movq	%r8, 48(%rsp)
	movl	28(%rsi), %edx
	leal	37417(%rdx), %edi
	movslq	%edi, %rdi
	leal	251(%rdx), %r12d
	movslq	%r12d, %r12
	salq	$30, %r12
	addq	%rdi, %r12
	leal	21698321(%rdx), %edi
	movslq	%edi, %rdi
	addl	$65531, %edx
	movslq	%edx, %rdx
	salq	$29, %rdx
	addq	%rdi, %rdx
	movq	%rdx, 8(%rsp)
	movl	32(%rsi), %edx
	leal	37416(%rdx), %edi
	movslq	%edi, %rdi
	leal	250(%rdx), %ebp
	movslq	%ebp, %rbp
	salq	$30, %rbp
	addq	%rdi, %rbp
	leal	21698320(%rdx), %edi
	movslq	%edi, %rdi
	addl	$65530, %edx
	movslq	%edx, %rdx
	salq	$29, %rdx
	addq	%rdx, %rdi
	movq	%rdi, 16(%rsp)
	movl	36(%rsi), %edx
	leal	37415(%rdx), %esi
	movslq	%esi, %rsi
	leal	249(%rdx), %ebx
	movslq	%ebx, %rbx
	salq	$30, %rbx
	addq	%rsi, %rbx
	leal	21698319(%rdx), %esi
	movslq	%esi, %rsi
	leal	65529(%rdx), %r15d
	movslq	%r15d, %r15
	salq	$29, %r15
	addq	%rsi, %r15
	testq	%rcx, %rcx
	je	.L526
	leaq	-1(%rcx), %rdx
	movq	%r10, %rsi
	movq	%r8, %rdi
	movq	8(%rsp), %r8
	movq	16(%rsp), %r10
	movq	24(%rsp), %rcx
.L527:
	movq	%rax, 8(%rsp)
	movq	%r9, %rax
	movq	%r11, 16(%rsp)
	movq	%rcx, %r11
	movq	%rsi, 24(%rsp)
	movq	%r14, %rsi
	movq	%rdi, 32(%rsp)
	movq	%r13, %rdi
	movq	%r8, 40(%rsp)
	movq	%r12, %r8
	movq	%r10, 48(%rsp)
	movq	%rbp, %r10
	movq	%r15, 56(%rsp)
	movq	%rbx, %r15
	negq	%r9
	subq	8(%rsp), %r9
	negq	%rcx
	subq	16(%rsp), %rcx
	negq	%r14
	subq	24(%rsp), %r14
	negq	%r13
	subq	32(%rsp), %r13
	negq	%r12
	subq	40(%rsp), %r12
	negq	%rbp
	subq	48(%rsp), %rbp
	negq	%rbx
	subq	56(%rsp), %rbx
	subq	$1, %rdx
	cmpq	$-1, %rdx
	jne	.L527
	movq	%r11, 32(%rsp)
	movq	%rsi, 40(%rsp)
	movq	%rdi, 48(%rsp)
	movq	%r8, 8(%rsp)
	movq	%r10, 16(%rsp)
	movq	%rcx, 24(%rsp)
.L526:
	leal	(%r9,%rax), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	addl	32(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	addl	%r14d, %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	addl	%r13d, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	addl	%r12d, %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	addl	%ebp, %edi
	call	use_int@PLT
	leal	(%rbx,%r15), %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE175:
	.size	int64_add_6, .-int64_add_6
	.globl	int64_add_7
	.type	int64_add_7, @function
int64_add_7:
.LFB176:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$88, %rsp
	.cfi_def_cfa_offset 144
	movq	%rdi, %r9
	movq	%rsi, %rdx
	movl	12(%rsi), %eax
	leal	37421(%rax), %edi
	movslq	%edi, %rdi
	leal	255(%rax), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rcx
	leal	21698325(%rax), %esi
	movslq	%esi, %rsi
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rsi, %rax
	movl	16(%rdx), %esi
	leal	37420(%rsi), %r8d
	movslq	%r8d, %r8
	leal	254(%rsi), %edi
	movslq	%edi, %rdi
	salq	$30, %rdi
	leaq	(%rdi,%r8), %rbx
	movq	%rbx, (%rsp)
	leal	21698324(%rsi), %edi
	movslq	%edi, %rdi
	addl	$65534, %esi
	movslq	%esi, %rsi
	salq	$29, %rsi
	leaq	(%rsi,%rdi), %r10
	movq	%r10, 48(%rsp)
	movl	20(%rdx), %esi
	leal	37419(%rsi), %r8d
	movslq	%r8d, %r8
	leal	253(%rsi), %edi
	movslq	%edi, %rdi
	salq	$30, %rdi
	leaq	(%rdi,%r8), %rbx
	movq	%rbx, 8(%rsp)
	leal	21698323(%rsi), %edi
	movslq	%edi, %rdi
	addl	$65533, %esi
	movslq	%esi, %rsi
	salq	$29, %rsi
	leaq	(%rsi,%rdi), %r11
	movq	%r11, 56(%rsp)
	movl	24(%rdx), %esi
	leal	37418(%rsi), %r8d
	movslq	%r8d, %r8
	leal	252(%rsi), %edi
	movslq	%edi, %rdi
	salq	$30, %rdi
	leaq	(%rdi,%r8), %rbx
	movq	%rbx, 32(%rsp)
	leal	21698322(%rsi), %edi
	movslq	%edi, %rdi
	addl	$65532, %esi
	movslq	%esi, %rsi
	salq	$29, %rsi
	addq	%rsi, %rdi
	movq	%rdi, 16(%rsp)
	movl	28(%rdx), %esi
	leal	37417(%rsi), %r8d
	movslq	%r8d, %r8
	leal	251(%rsi), %edi
	movslq	%edi, %rdi
	salq	$30, %rdi
	leaq	(%rdi,%r8), %r15
	movq	%r15, 40(%rsp)
	leal	21698321(%rsi), %edi
	movslq	%edi, %rdi
	addl	$65531, %esi
	movslq	%esi, %rsi
	salq	$29, %rsi
	leaq	(%rsi,%rdi), %r14
	movq	%r14, 24(%rsp)
	movl	32(%rdx), %esi
	leal	37416(%rsi), %edi
	movslq	%edi, %rdi
	leal	250(%rsi), %r12d
	movslq	%r12d, %r12
	salq	$30, %r12
	addq	%rdi, %r12
	leal	21698320(%rsi), %edi
	movslq	%edi, %rdi
	addl	$65530, %esi
	movslq	%esi, %rsi
	salq	$29, %rsi
	leaq	(%rsi,%rdi), %r15
	movl	36(%rdx), %esi
	leal	37415(%rsi), %edi
	movslq	%edi, %rdi
	leal	249(%rsi), %ebp
	movslq	%ebp, %rbp
	salq	$30, %rbp
	addq	%rdi, %rbp
	leal	21698319(%rsi), %edi
	movslq	%edi, %rdi
	leal	65529(%rsi), %r14d
	movslq	%r14d, %r14
	salq	$29, %r14
	addq	%rdi, %r14
	movl	40(%rdx), %edx
	leal	37414(%rdx), %esi
	movslq	%esi, %rsi
	leal	248(%rdx), %ebx
	movslq	%ebx, %rbx
	salq	$30, %rbx
	addq	%rsi, %rbx
	leal	21698318(%rdx), %esi
	movslq	%esi, %rsi
	leal	65528(%rdx), %r13d
	movslq	%r13d, %r13
	salq	$29, %r13
	addq	%rsi, %r13
	testq	%r9, %r9
	je	.L531
	leaq	-1(%r9), %rdx
	movq	%r10, %r8
	movq	%r11, %r9
	movq	16(%rsp), %r10
	movq	24(%rsp), %r11
	movq	32(%rsp), %rsi
	movq	40(%rsp), %rdi
.L532:
	movq	%rax, 16(%rsp)
	movq	%rcx, %rax
	movq	%r8, 24(%rsp)
	movq	(%rsp), %r8
	movq	%r9, 32(%rsp)
	movq	8(%rsp), %r9
	movq	%r10, 40(%rsp)
	movq	%rsi, %r10
	movq	%r11, 48(%rsp)
	movq	%rdi, %r11
	movq	%r15, 56(%rsp)
	movq	%r12, %r15
	movq	%r14, 64(%rsp)
	movq	%rbp, %r14
	movq	%r13, 72(%rsp)
	movq	%rbx, %r13
	negq	%rcx
	subq	16(%rsp), %rcx
	movq	%r8, %rsi
	negq	%rsi
	subq	24(%rsp), %rsi
	movq	%rsi, (%rsp)
	movq	%r9, %rsi
	negq	%rsi
	subq	32(%rsp), %rsi
	movq	%rsi, 8(%rsp)
	movq	%r10, %rsi
	negq	%rsi
	subq	40(%rsp), %rsi
	negq	%rdi
	subq	48(%rsp), %rdi
	negq	%r12
	subq	56(%rsp), %r12
	negq	%rbp
	subq	64(%rsp), %rbp
	negq	%rbx
	subq	72(%rsp), %rbx
	subq	$1, %rdx
	cmpq	$-1, %rdx
	jne	.L532
	movq	%r8, 48(%rsp)
	movq	%r9, 56(%rsp)
	movq	%r10, 16(%rsp)
	movq	%r11, 24(%rsp)
	movq	%rsi, 32(%rsp)
	movq	%rdi, 40(%rsp)
.L531:
	leal	(%rcx,%rax), %edi
	call	use_int@PLT
	movl	(%rsp), %edi
	addl	48(%rsp), %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	addl	56(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	addl	16(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	addl	24(%rsp), %edi
	call	use_int@PLT
	leal	(%r12,%r15), %edi
	call	use_int@PLT
	leal	0(%rbp,%r14), %edi
	call	use_int@PLT
	leal	(%rbx,%r13), %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE176:
	.size	int64_add_7, .-int64_add_7
	.globl	int64_add_8
	.type	int64_add_8, @function
int64_add_8:
.LFB177:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$120, %rsp
	.cfi_def_cfa_offset 176
	movq	%rdi, %r8
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	37421(%rcx), %edi
	movslq	%edi, %rdi
	leal	255(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rdx
	movq	%rdx, 8(%rsp)
	leal	21698325(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65535, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rdx
	movl	16(%rax), %ecx
	leal	37420(%rcx), %edi
	movslq	%edi, %rdi
	leal	254(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	movq	%rbx, 16(%rsp)
	leal	21698324(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65534, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r9
	movq	%r9, 64(%rsp)
	movl	20(%rax), %ecx
	leal	37419(%rcx), %edi
	movslq	%edi, %rdi
	leal	253(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	movq	%rbx, 24(%rsp)
	leal	21698323(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65533, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r10
	movq	%r10, 72(%rsp)
	movl	24(%rax), %ecx
	leal	37418(%rcx), %edi
	movslq	%edi, %rdi
	leal	252(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	movq	%rbx, 32(%rsp)
	leal	21698322(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65532, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r11
	movq	%r11, 80(%rsp)
	movl	28(%rax), %ecx
	leal	37417(%rcx), %edi
	movslq	%edi, %rdi
	leal	251(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	movq	%rbx, 48(%rsp)
	leal	21698321(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65531, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rdi
	movq	%rdi, 40(%rsp)
	movl	32(%rax), %ecx
	leal	37416(%rcx), %edi
	movslq	%edi, %rdi
	leal	250(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 56(%rsp)
	leal	21698320(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65530, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r15
	movl	36(%rax), %ecx
	leal	37415(%rcx), %edi
	movslq	%edi, %rdi
	leal	249(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rsi, %rdi
	movq	%rdi, 88(%rsp)
	leal	21698319(%rcx), %esi
	movslq	%esi, %rsi
	leal	65529(%rcx), %r14d
	movslq	%r14d, %r14
	salq	$29, %r14
	addq	%rsi, %r14
	movl	40(%rax), %ecx
	leal	37414(%rcx), %esi
	movslq	%esi, %rsi
	leal	248(%rcx), %ebp
	movslq	%ebp, %rbp
	salq	$30, %rbp
	addq	%rsi, %rbp
	leal	21698318(%rcx), %esi
	movslq	%esi, %rsi
	leal	65528(%rcx), %r13d
	movslq	%r13d, %r13
	salq	$29, %r13
	addq	%rsi, %r13
	movl	44(%rax), %eax
	leal	37413(%rax), %ecx
	movslq	%ecx, %rcx
	leal	247(%rax), %ebx
	movslq	%ebx, %rbx
	salq	$30, %rbx
	addq	%rcx, %rbx
	leal	21698317(%rax), %ecx
	movslq	%ecx, %rcx
	leal	65527(%rax), %r12d
	movslq	%r12d, %r12
	salq	$29, %r12
	addq	%rcx, %r12
	testq	%r8, %r8
	je	.L536
	leaq	-1(%r8), %rax
	movq	%r9, %r8
	movq	%r10, %r9
	movq	%r11, %r10
	movq	40(%rsp), %r11
	movq	48(%rsp), %rcx
	movq	56(%rsp), %rsi
.L537:
	movq	%rdx, 40(%rsp)
	movq	8(%rsp), %rdx
	movq	%r8, 48(%rsp)
	movq	16(%rsp), %r8
	movq	%r9, 56(%rsp)
	movq	24(%rsp), %r9
	movq	%r10, 64(%rsp)
	movq	32(%rsp), %r10
	movq	%r11, 72(%rsp)
	movq	%rcx, %r11
	movq	%r15, 80(%rsp)
	movq	%rsi, %r15
	movq	%r14, 88(%rsp)
	movq	%rdi, %r14
	movq	%r13, 96(%rsp)
	movq	%rbp, %r13
	movq	%r12, 104(%rsp)
	movq	%rbx, %r12
	movq	%rdx, %rcx
	negq	%rcx
	subq	40(%rsp), %rcx
	movq	%rcx, 8(%rsp)
	movq	%r8, %rcx
	negq	%rcx
	subq	48(%rsp), %rcx
	movq	%rcx, 16(%rsp)
	movq	%r9, %rcx
	negq	%rcx
	subq	56(%rsp), %rcx
	movq	%rcx, 24(%rsp)
	movq	%r10, %rcx
	negq	%rcx
	subq	64(%rsp), %rcx
	movq	%rcx, 32(%rsp)
	movq	%r11, %rcx
	negq	%rcx
	subq	72(%rsp), %rcx
	negq	%rsi
	subq	80(%rsp), %rsi
	negq	%rdi
	subq	88(%rsp), %rdi
	negq	%rbp
	subq	96(%rsp), %rbp
	negq	%rbx
	subq	104(%rsp), %rbx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L537
	movq	%r8, 64(%rsp)
	movq	%r9, 72(%rsp)
	movq	%r10, 80(%rsp)
	movq	%r11, 40(%rsp)
	movq	%rcx, 48(%rsp)
	movq	%rsi, 56(%rsp)
	movq	%rdi, 88(%rsp)
.L536:
	movl	8(%rsp), %edi
	addl	%edx, %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	addl	64(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	addl	72(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	addl	80(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	addl	40(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	addl	%r15d, %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	addl	%r14d, %edi
	call	use_int@PLT
	leal	0(%rbp,%r13), %edi
	call	use_int@PLT
	leal	(%rbx,%r12), %edi
	call	use_int@PLT
	addq	$120, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE177:
	.size	int64_add_8, .-int64_add_8
	.globl	int64_add_9
	.type	int64_add_9, @function
int64_add_9:
.LFB178:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$136, %rsp
	.cfi_def_cfa_offset 192
	movq	%rdi, %r8
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	37421(%rcx), %edi
	movslq	%edi, %rdi
	leal	255(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rdx
	movq	%rdx, 16(%rsp)
	leal	21698325(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65535, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rdx
	movl	16(%rax), %ecx
	leal	37420(%rcx), %edi
	movslq	%edi, %rdi
	leal	254(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	movq	%rbx, 24(%rsp)
	leal	21698324(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65534, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, (%rsp)
	movl	20(%rax), %ecx
	leal	37419(%rcx), %edi
	movslq	%edi, %rdi
	leal	253(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	movq	%rbx, 32(%rsp)
	leal	21698323(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65533, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r9
	movq	%r9, 8(%rsp)
	movl	24(%rax), %ecx
	leal	37418(%rcx), %edi
	movslq	%edi, %rdi
	leal	252(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	movq	%rbx, 40(%rsp)
	leal	21698322(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65532, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r10
	movq	%r10, 80(%rsp)
	movl	28(%rax), %ecx
	leal	37417(%rcx), %edi
	movslq	%edi, %rdi
	leal	251(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	movq	%rbx, 48(%rsp)
	leal	21698321(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65531, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r11
	movq	%r11, 88(%rsp)
	movl	32(%rax), %ecx
	leal	37416(%rcx), %edi
	movslq	%edi, %rdi
	leal	250(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	movq	%rbx, 64(%rsp)
	leal	21698320(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65530, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rdi
	movq	%rdi, 56(%rsp)
	movl	36(%rax), %ecx
	leal	37415(%rcx), %edi
	movslq	%edi, %rdi
	leal	249(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 72(%rsp)
	leal	21698319(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65529, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r15
	movl	40(%rax), %ecx
	leal	37414(%rcx), %edi
	movslq	%edi, %rdi
	leal	248(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rsi, %rdi
	movq	%rdi, 96(%rsp)
	leal	21698318(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65528, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r14
	movl	44(%rax), %ecx
	leal	37413(%rcx), %esi
	movslq	%esi, %rsi
	leal	247(%rcx), %ebp
	movslq	%ebp, %rbp
	salq	$30, %rbp
	addq	%rsi, %rbp
	leal	21698317(%rcx), %esi
	movslq	%esi, %rsi
	leal	65527(%rcx), %r13d
	movslq	%r13d, %r13
	salq	$29, %r13
	addq	%rsi, %r13
	movl	48(%rax), %eax
	leal	37412(%rax), %ecx
	movslq	%ecx, %rcx
	leal	246(%rax), %ebx
	movslq	%ebx, %rbx
	salq	$30, %rbx
	addq	%rcx, %rbx
	leal	21698316(%rax), %ecx
	movslq	%ecx, %rcx
	leal	65526(%rax), %r12d
	movslq	%r12d, %r12
	salq	$29, %r12
	addq	%rcx, %r12
	testq	%r8, %r8
	je	.L541
	leaq	-1(%r8), %rax
	movq	%rax, 8(%rsp)
	movq	%r9, %r8
	movq	%r10, %r9
	movq	%r11, %r10
	movq	56(%rsp), %r11
	movq	64(%rsp), %rcx
	movq	72(%rsp), %rsi
.L542:
	movq	%rdx, 56(%rsp)
	movq	16(%rsp), %rdx
	movq	(%rsp), %rax
	movq	%rax, 64(%rsp)
	movq	24(%rsp), %rax
	movq	%rax, (%rsp)
	movq	%r8, 72(%rsp)
	movq	32(%rsp), %r8
	movq	%r9, 80(%rsp)
	movq	40(%rsp), %r9
	movq	%r10, 88(%rsp)
	movq	48(%rsp), %r10
	movq	%r11, 96(%rsp)
	movq	%rcx, %r11
	movq	%r15, 104(%rsp)
	movq	%rsi, %r15
	movq	%r14, 112(%rsp)
	movq	%rdi, %r14
	movq	%r13, 120(%rsp)
	movq	%rbp, %r13
	movq	%r12, %rax
	movq	%rbx, %r12
	movq	%rdx, %rbx
	negq	%rbx
	subq	56(%rsp), %rbx
	movq	%rbx, 16(%rsp)
	movq	(%rsp), %rcx
	negq	%rcx
	subq	64(%rsp), %rcx
	movq	%rcx, 24(%rsp)
	movq	%r8, %rcx
	negq	%rcx
	subq	72(%rsp), %rcx
	movq	%rcx, 32(%rsp)
	movq	%r9, %rcx
	negq	%rcx
	subq	80(%rsp), %rcx
	movq	%rcx, 40(%rsp)
	movq	%r10, %rcx
	negq	%rcx
	subq	88(%rsp), %rcx
	movq	%rcx, 48(%rsp)
	movq	%r11, %rcx
	negq	%rcx
	subq	96(%rsp), %rcx
	negq	%rsi
	subq	104(%rsp), %rsi
	negq	%rdi
	subq	112(%rsp), %rdi
	negq	%rbp
	subq	120(%rsp), %rbp
	movq	%r12, %rbx
	negq	%rbx
	subq	%rax, %rbx
	subq	$1, 8(%rsp)
	movq	8(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L542
	movq	%r8, 8(%rsp)
	movq	%r9, 80(%rsp)
	movq	%r10, 88(%rsp)
	movq	%r11, 56(%rsp)
	movq	%rcx, 64(%rsp)
	movq	%rsi, 72(%rsp)
	movq	%rdi, 96(%rsp)
.L541:
	movl	16(%rsp), %edi
	addl	%edx, %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	addl	(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	addl	8(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	addl	80(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	addl	88(%rsp), %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	addl	56(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	addl	%r15d, %edi
	call	use_int@PLT
	movl	96(%rsp), %edi
	addl	%r14d, %edi
	call	use_int@PLT
	leal	0(%rbp,%r13), %edi
	call	use_int@PLT
	leal	(%rbx,%r12), %edi
	call	use_int@PLT
	addq	$136, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE178:
	.size	int64_add_9, .-int64_add_9
	.globl	int64_add_10
	.type	int64_add_10, @function
int64_add_10:
.LFB179:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$168, %rsp
	.cfi_def_cfa_offset 224
	movq	%rdi, %r8
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	37421(%rcx), %edi
	movslq	%edi, %rdi
	leal	255(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rdx
	movq	%rdx, 32(%rsp)
	leal	21698325(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65535, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rdx
	movl	16(%rax), %ecx
	leal	37420(%rcx), %edi
	movslq	%edi, %rdi
	leal	254(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 40(%rsp)
	leal	21698324(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65534, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rcx, %rsi
	movq	%rsi, (%rsp)
	movl	20(%rax), %ecx
	leal	37419(%rcx), %edi
	movslq	%edi, %rdi
	leal	253(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 48(%rsp)
	leal	21698323(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65533, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 24(%rsp)
	movl	24(%rax), %ecx
	leal	37418(%rcx), %edi
	movslq	%edi, %rdi
	leal	252(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 56(%rsp)
	leal	21698322(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65532, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 8(%rsp)
	movl	28(%rax), %ecx
	leal	37417(%rcx), %edi
	movslq	%edi, %rdi
	leal	251(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 64(%rsp)
	leal	21698321(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65531, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r9
	movq	%r9, 16(%rsp)
	movl	32(%rax), %ecx
	leal	37416(%rcx), %edi
	movslq	%edi, %rdi
	leal	250(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 80(%rsp)
	leal	21698320(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65530, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r10
	movq	%r10, 96(%rsp)
	movl	36(%rax), %ecx
	leal	37415(%rcx), %edi
	movslq	%edi, %rdi
	leal	249(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	movq	%rbx, 88(%rsp)
	leal	21698319(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65529, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r11
	movq	%r11, 104(%rsp)
	movl	40(%rax), %ecx
	leal	37414(%rcx), %edi
	movslq	%edi, %rdi
	leal	248(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rsi, %rdi
	movq	%rdi, 72(%rsp)
	leal	21698318(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65528, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r15
	movl	44(%rax), %ecx
	leal	37413(%rcx), %edi
	movslq	%edi, %rdi
	leal	247(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r14
	movq	%r14, %rdi
	movq	%r14, 112(%rsp)
	leal	21698317(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65527, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r14
	movl	48(%rax), %ecx
	leal	37412(%rcx), %esi
	movslq	%esi, %rsi
	leal	246(%rcx), %ebp
	movslq	%ebp, %rbp
	salq	$30, %rbp
	addq	%rsi, %rbp
	leal	21698316(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65526, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r13
	movl	52(%rax), %eax
	leal	37411(%rax), %ecx
	movslq	%ecx, %rcx
	leal	245(%rax), %ebx
	movslq	%ebx, %rbx
	salq	$30, %rbx
	addq	%rcx, %rbx
	leal	21698315(%rax), %ecx
	movslq	%ecx, %rcx
	leal	65525(%rax), %r12d
	movslq	%r12d, %r12
	salq	$29, %r12
	addq	%rcx, %r12
	testq	%r8, %r8
	je	.L546
	leaq	-1(%r8), %rax
	movq	%rax, 16(%rsp)
	movq	80(%rsp), %rcx
	movq	88(%rsp), %rsi
	movq	%rdi, %r8
.L547:
	movq	%rdx, 80(%rsp)
	movq	32(%rsp), %rdx
	movq	(%rsp), %rax
	movq	%rax, 88(%rsp)
	movq	40(%rsp), %rdi
	movq	%rdi, (%rsp)
	movq	24(%rsp), %rax
	movq	%rax, 96(%rsp)
	movq	48(%rsp), %rdi
	movq	%rdi, 24(%rsp)
	movq	8(%rsp), %rax
	movq	%rax, 104(%rsp)
	movq	56(%rsp), %rax
	movq	%rax, 8(%rsp)
	movq	%r9, 112(%rsp)
	movq	64(%rsp), %r9
	movq	%r10, 120(%rsp)
	movq	%rcx, %r10
	movq	%r11, 128(%rsp)
	movq	%rsi, %r11
	movq	%r15, 136(%rsp)
	movq	72(%rsp), %r15
	movq	%r14, 144(%rsp)
	movq	%r8, %r14
	movq	%r13, 152(%rsp)
	movq	%rbp, %r13
	movq	%r12, %rax
	movq	%rbx, %r12
	movq	%rdx, %rbx
	negq	%rbx
	subq	80(%rsp), %rbx
	movq	%rbx, 32(%rsp)
	movq	(%rsp), %rcx
	negq	%rcx
	movq	%rcx, %rbx
	subq	88(%rsp), %rbx
	movq	%rbx, 40(%rsp)
	negq	%rdi
	movq	%rdi, %rbx
	subq	96(%rsp), %rbx
	movq	%rbx, 48(%rsp)
	movq	8(%rsp), %rcx
	negq	%rcx
	subq	104(%rsp), %rcx
	movq	%rcx, 56(%rsp)
	movq	%r9, %rcx
	negq	%rcx
	subq	112(%rsp), %rcx
	movq	%rcx, 64(%rsp)
	movq	%r10, %rcx
	negq	%rcx
	subq	120(%rsp), %rcx
	negq	%rsi
	subq	128(%rsp), %rsi
	movq	%r15, %rdi
	negq	%rdi
	subq	136(%rsp), %rdi
	movq	%rdi, 72(%rsp)
	negq	%r8
	subq	144(%rsp), %r8
	negq	%rbp
	subq	152(%rsp), %rbp
	movq	%r12, %rbx
	negq	%rbx
	subq	%rax, %rbx
	subq	$1, 16(%rsp)
	movq	16(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L547
	movq	%r9, 16(%rsp)
	movq	%r10, 96(%rsp)
	movq	%r11, 104(%rsp)
	movq	%rcx, 80(%rsp)
	movq	%rsi, 88(%rsp)
	movq	%r8, 112(%rsp)
.L546:
	movl	32(%rsp), %edi
	addl	%edx, %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	addl	(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	addl	24(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	addl	8(%rsp), %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	addl	16(%rsp), %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	addl	96(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	addl	104(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	addl	%r15d, %edi
	call	use_int@PLT
	movl	112(%rsp), %edi
	addl	%r14d, %edi
	call	use_int@PLT
	leal	0(%rbp,%r13), %edi
	call	use_int@PLT
	leal	(%rbx,%r12), %edi
	call	use_int@PLT
	addq	$168, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE179:
	.size	int64_add_10, .-int64_add_10
	.globl	int64_add_11
	.type	int64_add_11, @function
int64_add_11:
.LFB180:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$200, %rsp
	.cfi_def_cfa_offset 256
	movq	%rdi, %r8
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	37421(%rcx), %edi
	movslq	%edi, %rdi
	leal	255(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rdx
	movq	%rdx, 48(%rsp)
	leal	21698325(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65535, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rdx
	movl	16(%rax), %ecx
	leal	37420(%rcx), %edi
	movslq	%edi, %rdi
	leal	254(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	movq	%rbx, 56(%rsp)
	leal	21698324(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65534, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 8(%rsp)
	movl	20(%rax), %ecx
	leal	37419(%rcx), %edi
	movslq	%edi, %rdi
	leal	253(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	movq	%rbx, 64(%rsp)
	leal	21698323(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65533, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rcx, %rsi
	movq	%rsi, 16(%rsp)
	movl	24(%rax), %ecx
	leal	37418(%rcx), %edi
	movslq	%edi, %rdi
	leal	252(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	movq	%rbx, 72(%rsp)
	leal	21698322(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65532, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rcx, %rsi
	movq	%rsi, 24(%rsp)
	movl	28(%rax), %ecx
	leal	37417(%rcx), %edi
	movslq	%edi, %rdi
	leal	251(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	movq	%rbx, 80(%rsp)
	leal	21698321(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65531, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rcx, %rsi
	movq	%rsi, 32(%rsp)
	movl	32(%rax), %ecx
	leal	37416(%rcx), %edi
	movslq	%edi, %rdi
	leal	250(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	movq	%rbx, 88(%rsp)
	leal	21698320(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65530, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r10
	movq	%r10, 128(%rsp)
	movl	36(%rax), %ecx
	leal	37415(%rcx), %edi
	movslq	%edi, %rdi
	leal	249(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r9
	movq	%r9, 40(%rsp)
	leal	21698319(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65529, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r11
	movq	%r11, 136(%rsp)
	movl	40(%rax), %ecx
	leal	37414(%rcx), %edi
	movslq	%edi, %rdi
	leal	248(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	movq	%rbx, 96(%rsp)
	leal	21698318(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65528, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r15
	movl	44(%rax), %ecx
	leal	37413(%rcx), %edi
	movslq	%edi, %rdi
	leal	247(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rsi, %rdi
	movq	%rdi, 104(%rsp)
	leal	21698317(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65527, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r13
	movl	48(%rax), %ecx
	leal	37412(%rcx), %edi
	movslq	%edi, %rdi
	leal	246(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r14
	movq	%r14, 112(%rsp)
	leal	21698316(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65526, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r14
	movl	52(%rax), %ecx
	leal	37411(%rcx), %edi
	movslq	%edi, %rdi
	leal	245(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbp
	movq	%rbp, 120(%rsp)
	leal	21698315(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65525, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r12
	movl	56(%rax), %eax
	leal	37410(%rax), %ecx
	movslq	%ecx, %rcx
	leal	244(%rax), %ebx
	movslq	%ebx, %rbx
	salq	$30, %rbx
	addq	%rcx, %rbx
	leal	21698314(%rax), %ecx
	movslq	%ecx, %rcx
	leal	65524(%rax), %ebp
	movslq	%ebp, %rbp
	salq	$29, %rbp
	addq	%rcx, %rbp
	testq	%r8, %r8
	je	.L551
	leaq	-1(%r8), %rax
	movq	96(%rsp), %rsi
	movq	104(%rsp), %rdi
	movq	112(%rsp), %r8
	movq	120(%rsp), %r9
.L552:
	movq	%rdx, 96(%rsp)
	movq	48(%rsp), %rdx
	movq	8(%rsp), %rcx
	movq	%rcx, 104(%rsp)
	movq	56(%rsp), %rcx
	movq	%rcx, 8(%rsp)
	movq	16(%rsp), %rcx
	movq	%rcx, 112(%rsp)
	movq	64(%rsp), %rcx
	movq	%rcx, 16(%rsp)
	movq	24(%rsp), %rcx
	movq	%rcx, 120(%rsp)
	movq	72(%rsp), %rcx
	movq	%rcx, 24(%rsp)
	movq	32(%rsp), %rcx
	movq	%rcx, 128(%rsp)
	movq	80(%rsp), %rcx
	movq	%rcx, 32(%rsp)
	movq	%r10, 136(%rsp)
	movq	88(%rsp), %r10
	movq	%r11, 144(%rsp)
	movq	40(%rsp), %r11
	movq	%r15, 152(%rsp)
	movq	%rsi, %r15
	movq	%r13, 160(%rsp)
	movq	%rdi, %r13
	movq	%r14, 168(%rsp)
	movq	%r8, %r14
	movq	%r12, 176(%rsp)
	movq	%r9, %r12
	movq	%rbp, 184(%rsp)
	movq	%rbx, %rbp
	movq	%rdx, %rbx
	negq	%rbx
	subq	96(%rsp), %rbx
	movq	%rbx, 48(%rsp)
	movq	8(%rsp), %rcx
	negq	%rcx
	movq	%rcx, %rbx
	subq	104(%rsp), %rbx
	movq	%rbx, 56(%rsp)
	movq	16(%rsp), %rcx
	negq	%rcx
	movq	%rcx, %rbx
	subq	112(%rsp), %rbx
	movq	%rbx, 64(%rsp)
	movq	24(%rsp), %rcx
	negq	%rcx
	movq	%rcx, %rdi
	subq	120(%rsp), %rdi
	movq	%rdi, 72(%rsp)
	movq	32(%rsp), %rcx
	negq	%rcx
	subq	128(%rsp), %rcx
	movq	%rcx, 80(%rsp)
	movq	%r10, %rcx
	negq	%rcx
	subq	136(%rsp), %rcx
	movq	%rcx, 88(%rsp)
	movq	%r11, %rcx
	negq	%rcx
	subq	144(%rsp), %rcx
	movq	%rcx, 40(%rsp)
	negq	%rsi
	subq	152(%rsp), %rsi
	movq	%r13, %rdi
	negq	%rdi
	subq	160(%rsp), %rdi
	negq	%r8
	subq	168(%rsp), %r8
	negq	%r9
	subq	176(%rsp), %r9
	movq	%rbp, %rbx
	negq	%rbx
	subq	184(%rsp), %rbx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L552
	movq	%r10, 128(%rsp)
	movq	%r11, 136(%rsp)
	movq	%rsi, 96(%rsp)
	movq	%rdi, 104(%rsp)
	movq	%r8, 112(%rsp)
	movq	%r9, 120(%rsp)
.L551:
	movl	48(%rsp), %edi
	addl	%edx, %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	addl	8(%rsp), %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	addl	16(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	addl	24(%rsp), %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	addl	32(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	addl	128(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	addl	136(%rsp), %edi
	call	use_int@PLT
	movl	96(%rsp), %edi
	addl	%r15d, %edi
	call	use_int@PLT
	movl	104(%rsp), %edi
	addl	%r13d, %edi
	call	use_int@PLT
	movl	112(%rsp), %edi
	addl	%r14d, %edi
	call	use_int@PLT
	movl	120(%rsp), %edi
	addl	%r12d, %edi
	call	use_int@PLT
	leal	(%rbx,%rbp), %edi
	call	use_int@PLT
	addq	$200, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE180:
	.size	int64_add_11, .-int64_add_11
	.globl	int64_add_12
	.type	int64_add_12, @function
int64_add_12:
.LFB181:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$216, %rsp
	.cfi_def_cfa_offset 272
	movq	%rdi, %r8
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	37421(%rcx), %edi
	movslq	%edi, %rdi
	leal	255(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rdx
	movq	%rdx, 56(%rsp)
	leal	21698325(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65535, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rdx
	movl	16(%rax), %ecx
	leal	37420(%rcx), %edi
	movslq	%edi, %rdi
	leal	254(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 64(%rsp)
	leal	21698324(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65534, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rcx, %rsi
	movq	%rsi, (%rsp)
	movl	20(%rax), %ecx
	leal	37419(%rcx), %edi
	movslq	%edi, %rdi
	leal	253(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 72(%rsp)
	leal	21698323(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65533, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rcx, %rsi
	movq	%rsi, 8(%rsp)
	movl	24(%rax), %ecx
	leal	37418(%rcx), %edi
	movslq	%edi, %rdi
	leal	252(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 80(%rsp)
	leal	21698322(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65532, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 16(%rsp)
	movl	28(%rax), %ecx
	leal	37417(%rcx), %edi
	movslq	%edi, %rdi
	leal	251(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 88(%rsp)
	leal	21698321(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65531, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 24(%rsp)
	movl	32(%rax), %ecx
	leal	37416(%rcx), %edi
	movslq	%edi, %rdi
	leal	250(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 96(%rsp)
	leal	21698320(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65530, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 32(%rsp)
	movl	36(%rax), %ecx
	leal	37415(%rcx), %edi
	movslq	%edi, %rdi
	leal	249(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r9
	movq	%r9, 48(%rsp)
	leal	21698319(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65529, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rcx, %rsi
	movq	%rsi, 40(%rsp)
	movl	40(%rax), %ecx
	leal	37414(%rcx), %edi
	movslq	%edi, %rdi
	leal	248(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r10
	movq	%r10, 144(%rsp)
	leal	21698318(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65528, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r11
	movq	%r11, 136(%rsp)
	movl	44(%rax), %ecx
	leal	37413(%rcx), %edi
	movslq	%edi, %rdi
	leal	247(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 104(%rsp)
	leal	21698317(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65527, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r15
	movl	48(%rax), %ecx
	leal	37412(%rcx), %edi
	movslq	%edi, %rdi
	leal	246(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	movq	%rbx, 112(%rsp)
	leal	21698316(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65526, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r13
	movl	52(%rax), %ecx
	leal	37411(%rcx), %edi
	movslq	%edi, %rdi
	leal	245(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r14
	movq	%r14, 120(%rsp)
	leal	21698315(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65525, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r14
	movl	56(%rax), %ecx
	leal	37410(%rcx), %edi
	movslq	%edi, %rdi
	leal	244(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbp
	movq	%rbp, 128(%rsp)
	leal	21698314(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65524, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %r12
	movl	60(%rax), %eax
	leal	37409(%rax), %ecx
	movslq	%ecx, %rcx
	leal	243(%rax), %ebx
	movslq	%ebx, %rbx
	salq	$30, %rbx
	addq	%rcx, %rbx
	leal	21698313(%rax), %ecx
	movslq	%ecx, %rcx
	addl	$65523, %eax
	cltq
	salq	$29, %rax
	leaq	(%rax,%rcx), %rbp
	testq	%r8, %r8
	je	.L556
	leaq	-1(%r8), %rax
	movq	%r10, %rsi
	movq	104(%rsp), %rdi
	movq	112(%rsp), %r8
	movq	120(%rsp), %r9
	movq	128(%rsp), %r10
.L557:
	movq	%rdx, 104(%rsp)
	movq	56(%rsp), %rdx
	movq	(%rsp), %rcx
	movq	%rcx, 112(%rsp)
	movq	64(%rsp), %rcx
	movq	%rcx, (%rsp)
	movq	8(%rsp), %rcx
	movq	%rcx, 120(%rsp)
	movq	72(%rsp), %rcx
	movq	%rcx, 8(%rsp)
	movq	16(%rsp), %rcx
	movq	%rcx, 128(%rsp)
	movq	80(%rsp), %rcx
	movq	%rcx, 16(%rsp)
	movq	24(%rsp), %rcx
	movq	%rcx, 136(%rsp)
	movq	88(%rsp), %rcx
	movq	%rcx, 24(%rsp)
	movq	32(%rsp), %rcx
	movq	%rcx, 144(%rsp)
	movq	96(%rsp), %rcx
	movq	%rcx, 32(%rsp)
	movq	40(%rsp), %rcx
	movq	%rcx, 152(%rsp)
	movq	48(%rsp), %rcx
	movq	%rcx, 40(%rsp)
	movq	%r11, 160(%rsp)
	movq	%rsi, %r11
	movq	%r15, 168(%rsp)
	movq	%rdi, %r15
	movq	%r13, 176(%rsp)
	movq	%r8, %r13
	movq	%r14, 184(%rsp)
	movq	%r9, %r14
	movq	%r12, 192(%rsp)
	movq	%r10, %r12
	movq	%rbp, 200(%rsp)
	movq	%rbx, %rbp
	movq	%rdx, %rbx
	negq	%rbx
	subq	104(%rsp), %rbx
	movq	%rbx, 56(%rsp)
	movq	(%rsp), %rcx
	negq	%rcx
	movq	%rcx, %rbx
	subq	112(%rsp), %rbx
	movq	%rbx, 64(%rsp)
	movq	8(%rsp), %rcx
	negq	%rcx
	movq	%rcx, %rbx
	subq	120(%rsp), %rbx
	movq	%rbx, 72(%rsp)
	movq	16(%rsp), %rcx
	negq	%rcx
	movq	%rcx, %rdi
	subq	128(%rsp), %rdi
	movq	%rdi, 80(%rsp)
	movq	24(%rsp), %rcx
	negq	%rcx
	movq	%rcx, %rbx
	subq	136(%rsp), %rbx
	movq	%rbx, 88(%rsp)
	movq	32(%rsp), %rcx
	negq	%rcx
	movq	%rcx, %rdi
	subq	144(%rsp), %rdi
	movq	%rdi, 96(%rsp)
	movq	40(%rsp), %rcx
	negq	%rcx
	subq	152(%rsp), %rcx
	movq	%rcx, 48(%rsp)
	negq	%rsi
	subq	160(%rsp), %rsi
	movq	%r15, %rdi
	negq	%rdi
	subq	168(%rsp), %rdi
	negq	%r8
	subq	176(%rsp), %r8
	negq	%r9
	subq	184(%rsp), %r9
	negq	%r10
	subq	192(%rsp), %r10
	movq	%rbp, %rbx
	negq	%rbx
	subq	200(%rsp), %rbx
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L557
	movq	%r11, 136(%rsp)
	movq	%rsi, 144(%rsp)
	movq	%rdi, 104(%rsp)
	movq	%r8, 112(%rsp)
	movq	%r9, 120(%rsp)
	movq	%r10, 128(%rsp)
.L556:
	movl	56(%rsp), %edi
	addl	%edx, %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	addl	(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	addl	8(%rsp), %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	addl	16(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	addl	24(%rsp), %edi
	call	use_int@PLT
	movl	96(%rsp), %edi
	addl	32(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	addl	40(%rsp), %edi
	call	use_int@PLT
	movl	144(%rsp), %edi
	addl	136(%rsp), %edi
	call	use_int@PLT
	movl	104(%rsp), %edi
	addl	%r15d, %edi
	call	use_int@PLT
	movl	112(%rsp), %edi
	addl	%r13d, %edi
	call	use_int@PLT
	movl	120(%rsp), %edi
	addl	%r14d, %edi
	call	use_int@PLT
	movl	128(%rsp), %edi
	addl	%r12d, %edi
	call	use_int@PLT
	leal	(%rbx,%rbp), %edi
	call	use_int@PLT
	addq	$216, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE181:
	.size	int64_add_12, .-int64_add_12
	.globl	int64_add_13
	.type	int64_add_13, @function
int64_add_13:
.LFB182:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$248, %rsp
	.cfi_def_cfa_offset 304
	movq	%rdi, %rdx
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	37421(%rcx), %edi
	movslq	%edi, %rdi
	leal	255(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r15
	leal	21698325(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65535, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 8(%rsp)
	movl	16(%rax), %ecx
	leal	37420(%rcx), %edi
	movslq	%edi, %rdi
	leal	254(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r14
	leal	21698324(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65534, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 16(%rsp)
	movl	20(%rax), %ecx
	leal	37419(%rcx), %edi
	movslq	%edi, %rdi
	leal	253(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r13
	leal	21698323(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65533, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 24(%rsp)
	movl	24(%rax), %ecx
	leal	37418(%rcx), %edi
	movslq	%edi, %rdi
	leal	252(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r12
	leal	21698322(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65532, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 32(%rsp)
	movl	28(%rax), %ecx
	leal	37417(%rcx), %edi
	movslq	%edi, %rdi
	leal	251(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbp
	leal	21698321(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65531, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 40(%rsp)
	movl	32(%rax), %ecx
	leal	37416(%rcx), %edi
	movslq	%edi, %rdi
	leal	250(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	leal	21698320(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65530, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rcx, %rsi
	movq	%rsi, 48(%rsp)
	movl	36(%rax), %ecx
	leal	37415(%rcx), %edi
	movslq	%edi, %rdi
	leal	249(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r8
	movq	%r8, 152(%rsp)
	leal	21698319(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65529, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rdi
	movq	%rdi, 56(%rsp)
	movl	40(%rax), %ecx
	leal	37414(%rcx), %edi
	movslq	%edi, %rdi
	leal	248(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r9
	movq	%r9, 160(%rsp)
	leal	21698318(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65528, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rcx, %rsi
	movq	%rsi, 64(%rsp)
	movl	44(%rax), %ecx
	leal	37413(%rcx), %edi
	movslq	%edi, %rdi
	leal	247(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r10
	movq	%r10, 168(%rsp)
	leal	21698317(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65527, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rdi
	movq	%rdi, 72(%rsp)
	movl	48(%rax), %ecx
	leal	37412(%rcx), %edi
	movslq	%edi, %rdi
	leal	246(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r11
	movq	%r11, 176(%rsp)
	leal	21698316(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65526, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rcx, %rsi
	movq	%rsi, 80(%rsp)
	movl	52(%rax), %ecx
	leal	37411(%rcx), %edi
	movslq	%edi, %rdi
	leal	245(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 128(%rsp)
	leal	21698315(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65525, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rdi
	movq	%rdi, 88(%rsp)
	movl	56(%rax), %ecx
	leal	37410(%rcx), %edi
	movslq	%edi, %rdi
	leal	244(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rsi, %rdi
	movq	%rdi, 136(%rsp)
	leal	21698314(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65524, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rsi, %rcx
	movq	%rcx, 96(%rsp)
	movl	60(%rax), %ecx
	leal	37409(%rcx), %edi
	movslq	%edi, %rdi
	leal	243(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 144(%rsp)
	leal	21698313(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65523, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rsi, %rcx
	movq	%rcx, 104(%rsp)
	movl	64(%rax), %eax
	leal	37408(%rax), %esi
	movslq	%esi, %rsi
	leal	242(%rax), %ecx
	movslq	%ecx, %rcx
	salq	$30, %rcx
	addq	%rsi, %rcx
	movq	%rcx, 120(%rsp)
	leal	21698312(%rax), %ecx
	movslq	%ecx, %rcx
	addl	$65522, %eax
	cltq
	salq	$29, %rax
	addq	%rcx, %rax
	movq	%rax, 112(%rsp)
	testq	%rdx, %rdx
	je	.L561
	leaq	-1(%rdx), %rax
	movq	%r8, %rdx
	movq	%r9, %rcx
	movq	%r10, %rsi
	movq	%r11, %rdi
	movq	128(%rsp), %r8
	movq	136(%rsp), %r9
	movq	144(%rsp), %r10
.L562:
	movq	8(%rsp), %r11
	movq	%r11, 128(%rsp)
	movq	%r15, 8(%rsp)
	movq	16(%rsp), %r11
	movq	%r11, 136(%rsp)
	movq	%r14, 16(%rsp)
	movq	24(%rsp), %r11
	movq	%r11, 144(%rsp)
	movq	%r13, 24(%rsp)
	movq	32(%rsp), %r11
	movq	%r11, 152(%rsp)
	movq	%r12, 32(%rsp)
	movq	40(%rsp), %r11
	movq	%r11, 160(%rsp)
	movq	%rbp, 40(%rsp)
	movq	48(%rsp), %r11
	movq	%r11, 168(%rsp)
	movq	%rbx, 48(%rsp)
	movq	56(%rsp), %r11
	movq	%r11, 176(%rsp)
	movq	%rdx, 56(%rsp)
	movq	64(%rsp), %r11
	movq	%r11, 184(%rsp)
	movq	%rcx, 64(%rsp)
	movq	72(%rsp), %r11
	movq	%r11, 192(%rsp)
	movq	%rsi, 72(%rsp)
	movq	80(%rsp), %r11
	movq	%r11, 200(%rsp)
	movq	%rdi, 80(%rsp)
	movq	88(%rsp), %r11
	movq	%r11, 208(%rsp)
	movq	%r8, 88(%rsp)
	movq	96(%rsp), %r11
	movq	%r11, 216(%rsp)
	movq	%r9, 96(%rsp)
	movq	104(%rsp), %r11
	movq	%r11, 224(%rsp)
	movq	%r10, 104(%rsp)
	movq	112(%rsp), %r11
	movq	%r11, 232(%rsp)
	movq	120(%rsp), %r11
	movq	%r11, 112(%rsp)
	negq	%r15
	subq	128(%rsp), %r15
	negq	%r14
	subq	136(%rsp), %r14
	negq	%r13
	subq	144(%rsp), %r13
	negq	%r12
	subq	152(%rsp), %r12
	negq	%rbp
	subq	160(%rsp), %rbp
	negq	%rbx
	subq	168(%rsp), %rbx
	negq	%rdx
	subq	176(%rsp), %rdx
	negq	%rcx
	subq	184(%rsp), %rcx
	negq	%rsi
	subq	192(%rsp), %rsi
	negq	%rdi
	subq	200(%rsp), %rdi
	negq	%r8
	subq	208(%rsp), %r8
	negq	%r9
	subq	216(%rsp), %r9
	negq	%r10
	subq	224(%rsp), %r10
	negq	%r11
	subq	232(%rsp), %r11
	movq	%r11, 120(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L562
	movq	%rdx, 152(%rsp)
	movq	%rcx, 160(%rsp)
	movq	%rsi, 168(%rsp)
	movq	%rdi, 176(%rsp)
	movq	%r8, 128(%rsp)
	movq	%r9, 136(%rsp)
	movq	%r10, 144(%rsp)
.L561:
	movl	8(%rsp), %edi
	addl	%r15d, %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	addl	%r14d, %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	addl	%r13d, %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	addl	%r12d, %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	addl	%ebp, %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	addl	%ebx, %edi
	call	use_int@PLT
	movl	152(%rsp), %edi
	addl	56(%rsp), %edi
	call	use_int@PLT
	movl	160(%rsp), %edi
	addl	64(%rsp), %edi
	call	use_int@PLT
	movl	168(%rsp), %edi
	addl	72(%rsp), %edi
	call	use_int@PLT
	movl	176(%rsp), %edi
	addl	80(%rsp), %edi
	call	use_int@PLT
	movl	128(%rsp), %edi
	addl	88(%rsp), %edi
	call	use_int@PLT
	movl	136(%rsp), %edi
	addl	96(%rsp), %edi
	call	use_int@PLT
	movl	144(%rsp), %edi
	addl	104(%rsp), %edi
	call	use_int@PLT
	movl	120(%rsp), %edi
	addl	112(%rsp), %edi
	call	use_int@PLT
	addq	$248, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE182:
	.size	int64_add_13, .-int64_add_13
	.globl	int64_add_14
	.type	int64_add_14, @function
int64_add_14:
.LFB183:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$264, %rsp
	.cfi_def_cfa_offset 320
	movq	%rdi, %rdx
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	37421(%rcx), %edi
	movslq	%edi, %rdi
	leal	255(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r15
	leal	21698325(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65535, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 48(%rsp)
	movl	16(%rax), %ecx
	leal	37420(%rcx), %edi
	movslq	%edi, %rdi
	leal	254(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r14
	leal	21698324(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65534, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 56(%rsp)
	movl	20(%rax), %ecx
	leal	37419(%rcx), %edi
	movslq	%edi, %rdi
	leal	253(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r13
	leal	21698323(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65533, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 64(%rsp)
	movl	24(%rax), %ecx
	leal	37418(%rcx), %edi
	movslq	%edi, %rdi
	leal	252(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r12
	leal	21698322(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65532, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 72(%rsp)
	movl	28(%rax), %ecx
	leal	37417(%rcx), %edi
	movslq	%edi, %rdi
	leal	251(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbp
	leal	21698321(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65531, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 80(%rsp)
	movl	32(%rax), %ecx
	leal	37416(%rcx), %edi
	movslq	%edi, %rdi
	leal	250(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	leal	21698320(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65530, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rcx, %rsi
	movq	%rsi, (%rsp)
	movl	36(%rax), %ecx
	leal	37415(%rcx), %edi
	movslq	%edi, %rdi
	leal	249(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rsi, %rdi
	movq	%rdi, 144(%rsp)
	leal	21698319(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65529, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rcx, %rsi
	movq	%rsi, 8(%rsp)
	movl	40(%rax), %ecx
	leal	37414(%rcx), %edi
	movslq	%edi, %rdi
	leal	248(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r8
	movq	%r8, 32(%rsp)
	leal	21698318(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65528, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rdi
	movq	%rdi, 16(%rsp)
	movl	44(%rax), %ecx
	leal	37413(%rcx), %edi
	movslq	%edi, %rdi
	leal	247(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r9
	movq	%r9, 40(%rsp)
	leal	21698317(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65527, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rcx, %rsi
	movq	%rsi, 88(%rsp)
	movl	48(%rax), %ecx
	leal	37412(%rcx), %edi
	movslq	%edi, %rdi
	leal	246(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r10
	movq	%r10, 24(%rsp)
	leal	21698316(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65526, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rdi
	movq	%rdi, 96(%rsp)
	movl	52(%rax), %ecx
	leal	37411(%rcx), %edi
	movslq	%edi, %rdi
	leal	245(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r11
	movq	%r11, 184(%rsp)
	leal	21698315(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65525, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rcx, %rsi
	movq	%rsi, 104(%rsp)
	movl	56(%rax), %ecx
	leal	37410(%rcx), %edi
	movslq	%edi, %rdi
	leal	244(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 152(%rsp)
	leal	21698314(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65524, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rdi
	movq	%rdi, 112(%rsp)
	movl	60(%rax), %ecx
	leal	37409(%rcx), %edi
	movslq	%edi, %rdi
	leal	243(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rsi, %rdi
	movq	%rdi, 160(%rsp)
	leal	21698313(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65523, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rsi, %rcx
	movq	%rcx, 120(%rsp)
	movl	64(%rax), %ecx
	leal	37408(%rcx), %edi
	movslq	%edi, %rdi
	leal	242(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 168(%rsp)
	leal	21698312(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65522, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rsi, %rcx
	movq	%rcx, 128(%rsp)
	movl	68(%rax), %eax
	leal	37407(%rax), %esi
	movslq	%esi, %rsi
	leal	241(%rax), %ecx
	movslq	%ecx, %rcx
	salq	$30, %rcx
	addq	%rsi, %rcx
	movq	%rcx, 176(%rsp)
	leal	21698311(%rax), %ecx
	movslq	%ecx, %rcx
	addl	$65521, %eax
	cltq
	salq	$29, %rax
	addq	%rcx, %rax
	movq	%rax, 136(%rsp)
	testq	%rdx, %rdx
	je	.L566
	leaq	-1(%rdx), %rax
	movq	%rax, 24(%rsp)
	movq	%r10, %rsi
	movq	%r11, %rdi
	movq	152(%rsp), %r8
	movq	160(%rsp), %r9
	movq	168(%rsp), %r10
	movq	176(%rsp), %r11
.L567:
	movq	48(%rsp), %rax
	movq	%rax, 152(%rsp)
	movq	%r15, 48(%rsp)
	movq	56(%rsp), %rax
	movq	%r14, 56(%rsp)
	movq	64(%rsp), %rdx
	movq	%rdx, 160(%rsp)
	movq	%r13, 64(%rsp)
	movq	72(%rsp), %rdx
	movq	%r12, 72(%rsp)
	movq	80(%rsp), %rcx
	movq	%rcx, 168(%rsp)
	movq	%rbp, 80(%rsp)
	movq	(%rsp), %rcx
	movq	%rcx, 176(%rsp)
	movq	%rbx, (%rsp)
	movq	8(%rsp), %rbx
	movq	%rbx, 184(%rsp)
	movq	144(%rsp), %rbx
	movq	%rbx, 8(%rsp)
	movq	16(%rsp), %rcx
	movq	%rcx, 192(%rsp)
	movq	32(%rsp), %rcx
	movq	%rcx, 16(%rsp)
	movq	88(%rsp), %rcx
	movq	%rcx, 200(%rsp)
	movq	40(%rsp), %rcx
	movq	%rcx, 88(%rsp)
	movq	96(%rsp), %rbx
	movq	%rbx, 208(%rsp)
	movq	%rsi, 96(%rsp)
	movq	104(%rsp), %rbx
	movq	%rbx, 216(%rsp)
	movq	%rdi, 104(%rsp)
	movq	112(%rsp), %rbx
	movq	%rbx, 224(%rsp)
	movq	%r8, 112(%rsp)
	movq	120(%rsp), %rbx
	movq	%rbx, 232(%rsp)
	movq	%r9, 120(%rsp)
	movq	128(%rsp), %rbx
	movq	%rbx, 240(%rsp)
	movq	%r10, 128(%rsp)
	movq	136(%rsp), %rbx
	movq	%rbx, 248(%rsp)
	movq	%r11, 136(%rsp)
	negq	%r15
	subq	152(%rsp), %r15
	negq	%r14
	subq	%rax, %r14
	negq	%r13
	subq	160(%rsp), %r13
	negq	%r12
	subq	%rdx, %r12
	negq	%rbp
	subq	168(%rsp), %rbp
	movq	(%rsp), %rbx
	negq	%rbx
	subq	176(%rsp), %rbx
	movq	8(%rsp), %rdx
	negq	%rdx
	subq	184(%rsp), %rdx
	movq	%rdx, 144(%rsp)
	movq	16(%rsp), %rdx
	negq	%rdx
	subq	192(%rsp), %rdx
	movq	%rdx, 32(%rsp)
	negq	%rcx
	subq	200(%rsp), %rcx
	movq	%rcx, 40(%rsp)
	negq	%rsi
	subq	208(%rsp), %rsi
	negq	%rdi
	subq	216(%rsp), %rdi
	negq	%r8
	subq	224(%rsp), %r8
	negq	%r9
	subq	232(%rsp), %r9
	negq	%r10
	subq	240(%rsp), %r10
	negq	%r11
	subq	248(%rsp), %r11
	subq	$1, 24(%rsp)
	movq	24(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L567
	movq	%rsi, 24(%rsp)
	movq	%rdi, 184(%rsp)
	movq	%r8, 152(%rsp)
	movq	%r9, 160(%rsp)
	movq	%r10, 168(%rsp)
	movq	%r11, 176(%rsp)
.L566:
	movl	48(%rsp), %edi
	addl	%r15d, %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	addl	%r14d, %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	addl	%r13d, %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	addl	%r12d, %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	addl	%ebp, %edi
	call	use_int@PLT
	movl	(%rsp), %edi
	addl	%ebx, %edi
	call	use_int@PLT
	movl	144(%rsp), %edi
	addl	8(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	addl	16(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	addl	88(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	addl	96(%rsp), %edi
	call	use_int@PLT
	movl	184(%rsp), %edi
	addl	104(%rsp), %edi
	call	use_int@PLT
	movl	152(%rsp), %edi
	addl	112(%rsp), %edi
	call	use_int@PLT
	movl	160(%rsp), %edi
	addl	120(%rsp), %edi
	call	use_int@PLT
	movl	168(%rsp), %edi
	addl	128(%rsp), %edi
	call	use_int@PLT
	movl	176(%rsp), %edi
	addl	136(%rsp), %edi
	call	use_int@PLT
	addq	$264, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE183:
	.size	int64_add_14, .-int64_add_14
	.globl	int64_add_15
	.type	int64_add_15, @function
int64_add_15:
.LFB184:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$280, %rsp
	.cfi_def_cfa_offset 336
	movq	%rdi, %rdx
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	37421(%rcx), %edi
	movslq	%edi, %rdi
	leal	255(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r15
	leal	21698325(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65535, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 48(%rsp)
	movl	16(%rax), %ecx
	leal	37420(%rcx), %edi
	movslq	%edi, %rdi
	leal	254(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r14
	leal	21698324(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65534, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, (%rsp)
	movl	20(%rax), %ecx
	leal	37419(%rcx), %edi
	movslq	%edi, %rdi
	leal	253(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r13
	leal	21698323(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65533, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 56(%rsp)
	movl	24(%rax), %ecx
	leal	37418(%rcx), %edi
	movslq	%edi, %rdi
	leal	252(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r12
	leal	21698322(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65532, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 64(%rsp)
	movl	28(%rax), %ecx
	leal	37417(%rcx), %edi
	movslq	%edi, %rdi
	leal	251(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbp
	leal	21698321(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65531, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	leaq	(%rcx,%rsi), %rbx
	movq	%rbx, 72(%rsp)
	movl	32(%rax), %ecx
	leal	37416(%rcx), %edi
	movslq	%edi, %rdi
	leal	250(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %rbx
	leal	21698320(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65530, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rsi, %rcx
	movq	%rcx, 8(%rsp)
	movl	36(%rax), %ecx
	leal	37415(%rcx), %edi
	movslq	%edi, %rdi
	leal	249(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 136(%rsp)
	leal	21698319(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65529, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rsi, %rcx
	movq	%rcx, 16(%rsp)
	movl	40(%rax), %ecx
	leal	37414(%rcx), %edi
	movslq	%edi, %rdi
	leal	248(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rsi, %rdi
	movq	%rdi, 144(%rsp)
	leal	21698318(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65528, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rsi, %rcx
	movq	%rcx, 24(%rsp)
	movl	44(%rax), %ecx
	leal	37413(%rcx), %edi
	movslq	%edi, %rdi
	leal	247(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r8
	movq	%r8, 40(%rsp)
	leal	21698317(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65527, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rsi, %rcx
	movq	%rcx, 32(%rsp)
	movl	48(%rax), %ecx
	leal	37412(%rcx), %edi
	movslq	%edi, %rdi
	leal	246(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r9
	movq	%r9, 184(%rsp)
	leal	21698316(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65526, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rsi, %rcx
	movq	%rcx, 80(%rsp)
	movl	52(%rax), %ecx
	leal	37411(%rcx), %edi
	movslq	%edi, %rdi
	leal	245(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r10
	movq	%r10, 192(%rsp)
	leal	21698315(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65525, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rsi, %rcx
	movq	%rcx, 88(%rsp)
	movl	56(%rax), %ecx
	leal	37410(%rcx), %edi
	movslq	%edi, %rdi
	leal	244(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	leaq	(%rsi,%rdi), %r11
	movq	%r11, 200(%rsp)
	leal	21698314(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65524, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rsi, %rcx
	movq	%rcx, 96(%rsp)
	movl	60(%rax), %ecx
	leal	37409(%rcx), %edi
	movslq	%edi, %rdi
	leal	243(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 152(%rsp)
	leal	21698313(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65523, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rsi, %rcx
	movq	%rcx, 104(%rsp)
	movl	64(%rax), %ecx
	leal	37408(%rcx), %edi
	movslq	%edi, %rdi
	leal	242(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rsi, %rdi
	movq	%rdi, 160(%rsp)
	leal	21698312(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65522, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rsi, %rcx
	movq	%rcx, 112(%rsp)
	movl	68(%rax), %ecx
	leal	37407(%rcx), %edi
	movslq	%edi, %rdi
	leal	241(%rcx), %esi
	movslq	%esi, %rsi
	salq	$30, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 168(%rsp)
	leal	21698311(%rcx), %esi
	movslq	%esi, %rsi
	addl	$65521, %ecx
	movslq	%ecx, %rcx
	salq	$29, %rcx
	addq	%rsi, %rcx
	movq	%rcx, 120(%rsp)
	movl	72(%rax), %eax
	leal	37406(%rax), %esi
	movslq	%esi, %rsi
	leal	240(%rax), %ecx
	movslq	%ecx, %rcx
	salq	$30, %rcx
	addq	%rsi, %rcx
	movq	%rcx, 176(%rsp)
	leal	21698310(%rax), %ecx
	movslq	%ecx, %rcx
	addl	$65520, %eax
	cltq
	salq	$29, %rax
	addq	%rcx, %rax
	movq	%rax, 128(%rsp)
	testq	%rdx, %rdx
	je	.L571
	leaq	-1(%rdx), %rcx
	movq	%rcx, 40(%rsp)
	movq	%r8, %rax
	movq	%r9, %rdx
	movq	%r10, %rsi
	movq	%r11, %rdi
	movq	152(%rsp), %r8
	movq	160(%rsp), %r9
	movq	168(%rsp), %r10
	movq	176(%rsp), %r11
.L572:
	movq	48(%rsp), %rcx
	movq	%rcx, 152(%rsp)
	movq	%r15, 48(%rsp)
	movq	(%rsp), %rcx
	movq	%r14, (%rsp)
	movq	56(%rsp), %r14
	movq	%r14, 160(%rsp)
	movq	%r13, 56(%rsp)
	movq	64(%rsp), %r14
	movq	%r14, 168(%rsp)
	movq	%r12, 64(%rsp)
	movq	72(%rsp), %r14
	movq	%r14, 176(%rsp)
	movq	%rbp, 72(%rsp)
	movq	8(%rsp), %r14
	movq	%r14, 184(%rsp)
	movq	%rbx, 8(%rsp)
	movq	16(%rsp), %rbx
	movq	%rbx, 192(%rsp)
	movq	136(%rsp), %rbx
	movq	%rbx, 16(%rsp)
	movq	24(%rsp), %rbx
	movq	%rbx, 200(%rsp)
	movq	144(%rsp), %r14
	movq	%r14, 24(%rsp)
	movq	32(%rsp), %rbx
	movq	%rbx, 208(%rsp)
	movq	%rax, 32(%rsp)
	movq	80(%rsp), %rax
	movq	%rax, 216(%rsp)
	movq	%rdx, 80(%rsp)
	movq	88(%rsp), %rax
	movq	%rax, 224(%rsp)
	movq	%rsi, 88(%rsp)
	movq	96(%rsp), %rax
	movq	%rax, 232(%rsp)
	movq	%rdi, 96(%rsp)
	movq	104(%rsp), %rbx
	movq	%rbx, 240(%rsp)
	movq	%r8, 104(%rsp)
	movq	112(%rsp), %rax
	movq	%rax, 248(%rsp)
	movq	%r9, 112(%rsp)
	movq	120(%rsp), %rbx
	movq	%rbx, 256(%rsp)
	movq	%r10, 120(%rsp)
	movq	128(%rsp), %r14
	movq	%r14, 264(%rsp)
	movq	%r11, 128(%rsp)
	negq	%r15
	subq	152(%rsp), %r15
	movq	(%rsp), %r14
	negq	%r14
	subq	%rcx, %r14
	negq	%r13
	subq	160(%rsp), %r13
	negq	%r12
	subq	168(%rsp), %r12
	negq	%rbp
	subq	176(%rsp), %rbp
	movq	8(%rsp), %rbx
	negq	%rbx
	subq	184(%rsp), %rbx
	movq	16(%rsp), %rax
	negq	%rax
	subq	192(%rsp), %rax
	movq	%rax, 136(%rsp)
	movq	24(%rsp), %rax
	negq	%rax
	subq	200(%rsp), %rax
	movq	%rax, 144(%rsp)
	movq	32(%rsp), %rax
	negq	%rax
	subq	208(%rsp), %rax
	negq	%rdx
	subq	216(%rsp), %rdx
	negq	%rsi
	subq	224(%rsp), %rsi
	negq	%rdi
	subq	232(%rsp), %rdi
	negq	%r8
	subq	240(%rsp), %r8
	negq	%r9
	subq	248(%rsp), %r9
	negq	%r10
	subq	256(%rsp), %r10
	negq	%r11
	subq	264(%rsp), %r11
	subq	$1, 40(%rsp)
	movq	40(%rsp), %rcx
	cmpq	$-1, %rcx
	jne	.L572
	movq	%rax, 40(%rsp)
	movq	%rdx, 184(%rsp)
	movq	%rsi, 192(%rsp)
	movq	%rdi, 200(%rsp)
	movq	%r8, 152(%rsp)
	movq	%r9, 160(%rsp)
	movq	%r10, 168(%rsp)
	movq	%r11, 176(%rsp)
.L571:
	movl	48(%rsp), %edi
	addl	%r15d, %edi
	call	use_int@PLT
	movl	(%rsp), %edi
	addl	%r14d, %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	addl	%r13d, %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	addl	%r12d, %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	addl	%ebp, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	addl	%ebx, %edi
	call	use_int@PLT
	movl	136(%rsp), %edi
	addl	16(%rsp), %edi
	call	use_int@PLT
	movl	144(%rsp), %edi
	addl	24(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	addl	32(%rsp), %edi
	call	use_int@PLT
	movl	184(%rsp), %edi
	addl	80(%rsp), %edi
	call	use_int@PLT
	movl	192(%rsp), %edi
	addl	88(%rsp), %edi
	call	use_int@PLT
	movl	200(%rsp), %edi
	addl	96(%rsp), %edi
	call	use_int@PLT
	movl	152(%rsp), %edi
	addl	104(%rsp), %edi
	call	use_int@PLT
	movl	160(%rsp), %edi
	addl	112(%rsp), %edi
	call	use_int@PLT
	movl	168(%rsp), %edi
	addl	120(%rsp), %edi
	call	use_int@PLT
	movl	176(%rsp), %edi
	addl	128(%rsp), %edi
	call	use_int@PLT
	addq	$280, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE184:
	.size	int64_add_15, .-int64_add_15
	.globl	int64_mul_0
	.type	int64_mul_0, @function
int64_mul_0:
.LFB185:
	.cfi_startproc
	endbr64
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movq	%rdi, %r8
	movl	12(%rsi), %eax
	leal	37421(%rax), %ecx
	movslq	%ecx, %rcx
	leal	7(%rax), %edx
	salq	$32, %rdx
	addq	%rcx, %rdx
	addl	$5, %eax
	cltq
	movq	%rdx, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	movq	%rdi, %rcx
	subq	%rdx, %rcx
	leaq	-1(%r8), %rdx
	testq	%r8, %r8
	je	.L576
.L577:
	subq	%rcx, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	imulq	%rax, %rdi
	subq	$1, %rdx
	cmpq	$-1, %rdx
	jne	.L577
.L576:
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE185:
	.size	int64_mul_0, .-int64_mul_0
	.globl	int64_mul_1
	.type	int64_mul_1, @function
int64_mul_1:
.LFB186:
	.cfi_startproc
	endbr64
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movq	%rdi, %r10
	movl	12(%rsi), %ecx
	leal	37421(%rcx), %eax
	cltq
	leal	7(%rcx), %edx
	salq	$32, %rdx
	addq	%rax, %rdx
	addl	$5, %ecx
	movslq	%ecx, %rcx
	movq	%rdx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	movq	%rax, %r9
	subq	%rdx, %r9
	movl	16(%rsi), %edx
	leal	37420(%rdx), %edi
	movslq	%edi, %rdi
	leal	6(%rdx), %esi
	salq	$32, %rsi
	addq	%rdi, %rsi
	addl	$4, %edx
	movslq	%edx, %rdx
	movq	%rsi, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	movq	%rbx, %r8
	subq	%rsi, %r8
	leaq	-1(%r10), %rsi
	testq	%r10, %r10
	je	.L581
.L582:
	subq	%r9, %rax
	subq	%r8, %rbx
	imulq	%rcx, %rax
	imulq	%rdx, %rbx
	imulq	%rcx, %rax
	imulq	%rdx, %rbx
	imulq	%rcx, %rax
	imulq	%rdx, %rbx
	imulq	%rcx, %rax
	imulq	%rdx, %rbx
	imulq	%rcx, %rax
	imulq	%rdx, %rbx
	imulq	%rcx, %rax
	imulq	%rdx, %rbx
	imulq	%rcx, %rax
	imulq	%rdx, %rbx
	imulq	%rcx, %rax
	imulq	%rdx, %rbx
	imulq	%rcx, %rax
	imulq	%rdx, %rbx
	imulq	%rcx, %rax
	imulq	%rdx, %rbx
	subq	$1, %rsi
	cmpq	$-1, %rsi
	jne	.L582
.L581:
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE186:
	.size	int64_mul_1, .-int64_mul_1
	.globl	int64_mul_2
	.type	int64_mul_2, @function
int64_mul_2:
.LFB187:
	.cfi_startproc
	endbr64
	pushq	%r12
	.cfi_def_cfa_offset 16
	.cfi_offset 12, -16
	pushq	%rbp
	.cfi_def_cfa_offset 24
	.cfi_offset 6, -24
	pushq	%rbx
	.cfi_def_cfa_offset 32
	.cfi_offset 3, -32
	movq	%rdi, %r12
	movq	%rsi, %r8
	movl	12(%rsi), %esi
	leal	37421(%rsi), %eax
	cltq
	leal	7(%rsi), %edx
	salq	$32, %rdx
	addq	%rax, %rdx
	addl	$5, %esi
	movslq	%esi, %rsi
	movq	%rdx, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	movq	%rax, %r11
	subq	%rdx, %r11
	movl	16(%r8), %ecx
	leal	37420(%rcx), %edi
	movslq	%edi, %rdi
	leal	6(%rcx), %edx
	salq	$32, %rdx
	addq	%rdi, %rdx
	addl	$4, %ecx
	movslq	%ecx, %rcx
	movq	%rdx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	movq	%rbp, %r10
	subq	%rdx, %r10
	movl	20(%r8), %edx
	leal	37419(%rdx), %r8d
	movslq	%r8d, %r8
	leal	5(%rdx), %edi
	salq	$32, %rdi
	addq	%r8, %rdi
	addl	$3, %edx
	movslq	%edx, %rdx
	movq	%rdi, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	movq	%rbx, %r9
	subq	%rdi, %r9
	leaq	-1(%r12), %r8
	testq	%r12, %r12
	je	.L586
.L587:
	subq	%r11, %rax
	subq	%r10, %rbp
	subq	%r9, %rbx
	imulq	%rsi, %rax
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%rsi, %rax
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%rsi, %rax
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%rsi, %rax
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%rsi, %rax
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%rsi, %rax
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%rsi, %rax
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%rsi, %rax
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%rsi, %rax
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%rsi, %rax
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	subq	$1, %r8
	cmpq	$-1, %r8
	jne	.L587
.L586:
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%rbp
	.cfi_def_cfa_offset 16
	popq	%r12
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE187:
	.size	int64_mul_2, .-int64_mul_2
	.globl	int64_mul_3
	.type	int64_mul_3, @function
int64_mul_3:
.LFB188:
	.cfi_startproc
	endbr64
	pushq	%r14
	.cfi_def_cfa_offset 16
	.cfi_offset 14, -16
	pushq	%r13
	.cfi_def_cfa_offset 24
	.cfi_offset 13, -24
	pushq	%r12
	.cfi_def_cfa_offset 32
	.cfi_offset 12, -32
	pushq	%rbp
	.cfi_def_cfa_offset 40
	.cfi_offset 6, -40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	.cfi_offset 3, -48
	movq	%rdi, %r14
	movq	%rsi, %rdx
	movl	12(%rsi), %r8d
	leal	37421(%r8), %eax
	cltq
	leal	7(%r8), %ecx
	salq	$32, %rcx
	addq	%rax, %rcx
	addl	$5, %r8d
	movslq	%r8d, %r8
	movq	%rcx, %rax
	imulq	%r8, %rax
	imulq	%r8, %rax
	imulq	%r8, %rax
	imulq	%r8, %rax
	imulq	%r8, %rax
	imulq	%r8, %rax
	imulq	%r8, %rax
	imulq	%r8, %rax
	imulq	%r8, %rax
	imulq	%r8, %rax
	movq	%rax, %r13
	subq	%rcx, %r13
	movl	16(%rsi), %esi
	leal	37420(%rsi), %edi
	movslq	%edi, %rdi
	leal	6(%rsi), %ecx
	salq	$32, %rcx
	addq	%rdi, %rcx
	addl	$4, %esi
	movslq	%esi, %rsi
	movq	%rcx, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	movq	%r12, %r11
	subq	%rcx, %r11
	movl	20(%rdx), %ecx
	leal	37419(%rcx), %r9d
	movslq	%r9d, %r9
	leal	5(%rcx), %edi
	salq	$32, %rdi
	addq	%r9, %rdi
	addl	$3, %ecx
	movslq	%ecx, %rcx
	movq	%rdi, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	movq	%rbp, %r10
	subq	%rdi, %r10
	movl	24(%rdx), %edx
	leal	37418(%rdx), %edi
	movslq	%edi, %rdi
	leal	4(%rdx), %r9d
	salq	$32, %r9
	addq	%rdi, %r9
	addl	$2, %edx
	movslq	%edx, %rdx
	movq	%r9, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	movq	%rbx, %rdi
	subq	%r9, %rdi
	leaq	-1(%r14), %r9
	testq	%r14, %r14
	je	.L591
.L592:
	subq	%r13, %rax
	subq	%r11, %r12
	subq	%r10, %rbp
	subq	%rdi, %rbx
	imulq	%r8, %rax
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r8, %rax
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r8, %rax
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r8, %rax
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r8, %rax
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r8, %rax
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r8, %rax
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r8, %rax
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r8, %rax
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r8, %rax
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	subq	$1, %r9
	cmpq	$-1, %r9
	jne	.L592
.L591:
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%rbp
	.cfi_def_cfa_offset 32
	popq	%r12
	.cfi_def_cfa_offset 24
	popq	%r13
	.cfi_def_cfa_offset 16
	popq	%r14
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE188:
	.size	int64_mul_3, .-int64_mul_3
	.globl	int64_mul_4
	.type	int64_mul_4, @function
int64_mul_4:
.LFB189:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$24, %rsp
	.cfi_def_cfa_offset 80
	movq	%rdi, 8(%rsp)
	movq	%rsi, %rdx
	movl	12(%rsi), %r9d
	leal	37421(%r9), %eax
	cltq
	leal	7(%r9), %ecx
	salq	$32, %rcx
	addq	%rax, %rcx
	addl	$5, %r9d
	movslq	%r9d, %r9
	movq	%rcx, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	movq	%rax, %r11
	subq	%rcx, %r11
	movl	16(%rsi), %r8d
	leal	37420(%r8), %esi
	movslq	%esi, %rsi
	leal	6(%r8), %ecx
	salq	$32, %rcx
	addq	%rsi, %rcx
	addl	$4, %r8d
	movslq	%r8d, %r8
	movq	%rcx, %r13
	imulq	%r8, %r13
	imulq	%r8, %r13
	imulq	%r8, %r13
	imulq	%r8, %r13
	imulq	%r8, %r13
	imulq	%r8, %r13
	imulq	%r8, %r13
	imulq	%r8, %r13
	imulq	%r8, %r13
	imulq	%r8, %r13
	movq	%r13, %r15
	subq	%rcx, %r15
	movl	20(%rdx), %esi
	leal	37419(%rsi), %edi
	movslq	%edi, %rdi
	leal	5(%rsi), %ecx
	salq	$32, %rcx
	addq	%rdi, %rcx
	addl	$3, %esi
	movslq	%esi, %rsi
	movq	%rcx, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	imulq	%rsi, %r12
	movq	%r12, %rbx
	subq	%rcx, %rbx
	movq	%rbx, (%rsp)
	movl	24(%rdx), %ecx
	leal	37418(%rcx), %edi
	movslq	%edi, %rdi
	leal	4(%rcx), %r10d
	salq	$32, %r10
	addq	%rdi, %r10
	addl	$2, %ecx
	movslq	%ecx, %rcx
	movq	%r10, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	imulq	%rcx, %rbp
	movq	%rbp, %r14
	subq	%r10, %r14
	movl	28(%rdx), %edx
	leal	37417(%rdx), %edi
	movslq	%edi, %rdi
	leal	3(%rdx), %r10d
	salq	$32, %r10
	addq	%rdi, %r10
	addl	$1, %edx
	movslq	%edx, %rdx
	movq	%r10, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	imulq	%rdx, %rbx
	movq	%rbx, %rdi
	subq	%r10, %rdi
	movq	8(%rsp), %r10
	subq	$1, %r10
	cmpq	$0, 8(%rsp)
	je	.L596
.L597:
	subq	%r11, %rax
	subq	%r15, %r13
	subq	(%rsp), %r12
	subq	%r14, %rbp
	subq	%rdi, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r13
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r13
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r13
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r13
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r13
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r13
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r13
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r13
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r13
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r13
	imulq	%rsi, %r12
	imulq	%rcx, %rbp
	imulq	%rdx, %rbx
	subq	$1, %r10
	cmpq	$-1, %r10
	jne	.L597
.L596:
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE189:
	.size	int64_mul_4, .-int64_mul_4
	.globl	int64_mul_5
	.type	int64_mul_5, @function
int64_mul_5:
.LFB190:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$56, %rsp
	.cfi_def_cfa_offset 112
	movq	%rdi, %r11
	movq	%rsi, %r10
	movl	12(%rsi), %r9d
	leal	37421(%r9), %eax
	cltq
	leal	7(%r9), %edx
	salq	$32, %rdx
	addq	%rax, %rdx
	addl	$5, %r9d
	movslq	%r9d, %r9
	movq	%rdx, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	imulq	%r9, %rax
	movq	%rax, %rsi
	subq	%rdx, %rsi
	movq	%rsi, (%rsp)
	movl	16(%r10), %r8d
	leal	37420(%r8), %ecx
	movslq	%ecx, %rcx
	leal	6(%r8), %edx
	salq	$32, %rdx
	addq	%rcx, %rdx
	addl	$4, %r8d
	movslq	%r8d, %r8
	movq	%rdx, %r12
	imulq	%r8, %r12
	imulq	%r8, %r12
	imulq	%r8, %r12
	imulq	%r8, %r12
	imulq	%r8, %r12
	imulq	%r8, %r12
	imulq	%r8, %r12
	imulq	%r8, %r12
	imulq	%r8, %r12
	imulq	%r8, %r12
	movq	%r12, %rsi
	subq	%rdx, %rsi
	movq	%rsi, 8(%rsp)
	movl	20(%r10), %esi
	leal	37419(%rsi), %edx
	movslq	%edx, %rdx
	leal	5(%rsi), %ecx
	salq	$32, %rcx
	addq	%rdx, %rcx
	addl	$3, %esi
	movslq	%esi, %rsi
	movq	%rcx, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	movq	%rdx, %r13
	movq	%rdx, %rdi
	subq	%rcx, %rdi
	movq	%rdi, 16(%rsp)
	movl	24(%r10), %ecx
	leal	37418(%rcx), %edi
	movslq	%edi, %rdi
	leal	4(%rcx), %edx
	salq	$32, %rdx
	addq	%rdi, %rdx
	addl	$2, %ecx
	movslq	%ecx, %rcx
	movq	%rdx, %rdi
	imulq	%rcx, %rdi
	imulq	%rcx, %rdi
	imulq	%rcx, %rdi
	imulq	%rcx, %rdi
	imulq	%rcx, %rdi
	imulq	%rcx, %rdi
	imulq	%rcx, %rdi
	imulq	%rcx, %rdi
	imulq	%rcx, %rdi
	imulq	%rcx, %rdi
	movq	%rdi, %r14
	movq	%rdi, %r15
	subq	%rdx, %r15
	movq	%r15, 40(%rsp)
	movl	28(%r10), %edx
	leal	37417(%rdx), %edi
	movslq	%edi, %rdi
	leal	3(%rdx), %ebx
	salq	$32, %rbx
	addq	%rdi, %rbx
	addl	$1, %edx
	movslq	%edx, %rdx
	movq	%rbx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	movq	%rbp, %rdi
	subq	%rbx, %rdi
	movq	%rdi, 24(%rsp)
	movl	32(%r10), %r10d
	leal	37416(%r10), %ebx
	movslq	%ebx, %rbx
	leal	2(%r10), %edi
	salq	$32, %rdi
	addq	%rbx, %rdi
	movslq	%r10d, %r10
	movq	%rdi, %rbx
	imulq	%r10, %rbx
	imulq	%r10, %rbx
	imulq	%r10, %rbx
	imulq	%r10, %rbx
	imulq	%r10, %rbx
	imulq	%r10, %rbx
	imulq	%r10, %rbx
	imulq	%r10, %rbx
	imulq	%r10, %rbx
	imulq	%r10, %rbx
	movq	%rbx, %r15
	subq	%rdi, %r15
	movq	%r15, 32(%rsp)
	testq	%r11, %r11
	je	.L601
	leaq	-1(%r11), %rdi
	movq	40(%rsp), %r15
	movq	%rdi, %r11
.L602:
	subq	(%rsp), %rax
	movq	%r12, %rdi
	subq	8(%rsp), %rdi
	subq	16(%rsp), %r13
	subq	%r15, %r14
	subq	24(%rsp), %rbp
	subq	32(%rsp), %rbx
	imulq	%r9, %rax
	imulq	%r8, %rdi
	movq	%rdi, %r12
	imulq	%rsi, %r13
	imulq	%rcx, %r14
	imulq	%rdx, %rbp
	imulq	%r10, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r12
	imulq	%rsi, %r13
	imulq	%rcx, %r14
	imulq	%rdx, %rbp
	imulq	%r10, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r12
	imulq	%rsi, %r13
	imulq	%rcx, %r14
	imulq	%rdx, %rbp
	imulq	%r10, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r12
	imulq	%rsi, %r13
	imulq	%rcx, %r14
	imulq	%rdx, %rbp
	imulq	%r10, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r12
	imulq	%rsi, %r13
	imulq	%rcx, %r14
	imulq	%rdx, %rbp
	imulq	%r10, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r12
	imulq	%rsi, %r13
	imulq	%rcx, %r14
	imulq	%rdx, %rbp
	imulq	%r10, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r12
	imulq	%rsi, %r13
	imulq	%rcx, %r14
	imulq	%rdx, %rbp
	imulq	%r10, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r12
	imulq	%rsi, %r13
	imulq	%rcx, %r14
	imulq	%rdx, %rbp
	imulq	%r10, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r12
	imulq	%rsi, %r13
	imulq	%rcx, %r14
	imulq	%rdx, %rbp
	imulq	%r10, %rbx
	imulq	%r9, %rax
	imulq	%r8, %r12
	imulq	%rsi, %r13
	imulq	%rcx, %r14
	imulq	%rdx, %rbp
	imulq	%r10, %rbx
	subq	$1, %r11
	cmpq	$-1, %r11
	jne	.L602
.L601:
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE190:
	.size	int64_mul_5, .-int64_mul_5
	.globl	int64_mul_6
	.type	int64_mul_6, @function
int64_mul_6:
.LFB191:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$88, %rsp
	.cfi_def_cfa_offset 144
	movq	%rdi, 72(%rsp)
	movq	%rsi, %rax
	movl	12(%rsi), %r9d
	leal	37421(%r9), %edx
	movslq	%edx, %rdx
	leal	7(%r9), %ecx
	salq	$32, %rcx
	addq	%rdx, %rcx
	addl	$5, %r9d
	movslq	%r9d, %r9
	movq	%rcx, %rdx
	imulq	%r9, %rdx
	imulq	%r9, %rdx
	imulq	%r9, %rdx
	imulq	%r9, %rdx
	imulq	%r9, %rdx
	imulq	%r9, %rdx
	imulq	%r9, %rdx
	imulq	%r9, %rdx
	imulq	%r9, %rdx
	imulq	%r9, %rdx
	movq	%rdx, %r15
	movq	%rdx, %rsi
	subq	%rcx, %rsi
	movq	%rsi, 8(%rsp)
	movl	16(%rax), %r8d
	leal	37420(%r8), %edx
	movslq	%edx, %rdx
	leal	6(%r8), %ecx
	salq	$32, %rcx
	addq	%rdx, %rcx
	addl	$4, %r8d
	movslq	%r8d, %r8
	movq	%rcx, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	movq	%rdx, %r14
	movq	%rdx, %rsi
	subq	%rcx, %rsi
	movq	%rsi, 16(%rsp)
	movl	20(%rax), %esi
	leal	37419(%rsi), %edx
	movslq	%edx, %rdx
	leal	5(%rsi), %ecx
	salq	$32, %rcx
	addq	%rdx, %rcx
	addl	$3, %esi
	movslq	%esi, %rsi
	movq	%rcx, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	movq	%rdx, %r13
	subq	%rcx, %rdx
	movq	%rdx, 24(%rsp)
	movl	24(%rax), %ecx
	leal	37418(%rcx), %edi
	movslq	%edi, %rdi
	leal	4(%rcx), %edx
	salq	$32, %rdx
	addq	%rdi, %rdx
	addl	$2, %ecx
	movslq	%ecx, %rcx
	movq	%rdx, %rbx
	imulq	%rcx, %rbx
	movq	%rbx, %rdi
	imulq	%rcx, %rdi
	imulq	%rcx, %rdi
	imulq	%rcx, %rdi
	imulq	%rcx, %rdi
	imulq	%rcx, %rdi
	imulq	%rcx, %rdi
	imulq	%rcx, %rdi
	imulq	%rcx, %rdi
	movq	%rdi, %r12
	imulq	%rcx, %r12
	movq	%r12, %rbx
	subq	%rdx, %rbx
	movq	%rbx, 32(%rsp)
	movl	28(%rax), %edx
	leal	37417(%rdx), %r10d
	movslq	%r10d, %r10
	leal	3(%rdx), %edi
	salq	$32, %rdi
	addq	%r10, %rdi
	addl	$1, %edx
	movslq	%edx, %rdx
	movq	%rdi, %r10
	imulq	%rdx, %r10
	imulq	%rdx, %r10
	imulq	%rdx, %r10
	imulq	%rdx, %r10
	imulq	%rdx, %r10
	imulq	%rdx, %r10
	imulq	%rdx, %r10
	imulq	%rdx, %r10
	imulq	%rdx, %r10
	imulq	%rdx, %r10
	movq	%r10, 64(%rsp)
	movq	%r10, %rbx
	subq	%rdi, %rbx
	movq	%rbx, 40(%rsp)
	movl	32(%rax), %r10d
	leal	37416(%r10), %edi
	movslq	%edi, %rdi
	leal	2(%r10), %r11d
	salq	$32, %r11
	addq	%rdi, %r11
	movslq	%r10d, %r10
	movq	%r11, %rbp
	imulq	%r10, %rbp
	imulq	%r10, %rbp
	imulq	%r10, %rbp
	imulq	%r10, %rbp
	imulq	%r10, %rbp
	imulq	%r10, %rbp
	imulq	%r10, %rbp
	imulq	%r10, %rbp
	imulq	%r10, %rbp
	imulq	%r10, %rbp
	movq	%rbp, %rbx
	subq	%r11, %rbx
	movq	%rbx, 48(%rsp)
	movl	36(%rax), %eax
	leal	37415(%rax), %edi
	movslq	%edi, %rdi
	leal	1(%rax), %r11d
	salq	$32, %r11
	addq	%rdi, %r11
	subl	$1, %eax
	cltq
	movq	%r11, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	movq	%rbx, %rdi
	subq	%r11, %rdi
	movq	%rdi, 56(%rsp)
	movq	72(%rsp), %r11
	testq	%r11, %r11
	je	.L606
	leaq	-1(%r11), %rdi
	movq	%rdi, %r11
	movq	64(%rsp), %rdi
.L607:
	subq	8(%rsp), %r15
	subq	16(%rsp), %r14
	subq	24(%rsp), %r13
	subq	32(%rsp), %r12
	subq	40(%rsp), %rdi
	subq	48(%rsp), %rbp
	subq	56(%rsp), %rbx
	imulq	%r9, %r15
	imulq	%r8, %r14
	imulq	%rsi, %r13
	imulq	%rcx, %r12
	imulq	%rdx, %rdi
	imulq	%r10, %rbp
	imulq	%rax, %rbx
	imulq	%r9, %r15
	imulq	%r8, %r14
	imulq	%rsi, %r13
	imulq	%rcx, %r12
	imulq	%rdx, %rdi
	imulq	%r10, %rbp
	imulq	%rax, %rbx
	imulq	%r9, %r15
	imulq	%r8, %r14
	imulq	%rsi, %r13
	imulq	%rcx, %r12
	imulq	%rdx, %rdi
	imulq	%r10, %rbp
	imulq	%rax, %rbx
	imulq	%r9, %r15
	imulq	%r8, %r14
	imulq	%rsi, %r13
	imulq	%rcx, %r12
	imulq	%rdx, %rdi
	imulq	%r10, %rbp
	imulq	%rax, %rbx
	imulq	%r9, %r15
	imulq	%r8, %r14
	imulq	%rsi, %r13
	imulq	%rcx, %r12
	imulq	%rdx, %rdi
	imulq	%r10, %rbp
	imulq	%rax, %rbx
	imulq	%r9, %r15
	imulq	%r8, %r14
	imulq	%rsi, %r13
	imulq	%rcx, %r12
	imulq	%rdx, %rdi
	imulq	%r10, %rbp
	imulq	%rax, %rbx
	imulq	%r9, %r15
	imulq	%r8, %r14
	imulq	%rsi, %r13
	imulq	%rcx, %r12
	imulq	%rdx, %rdi
	imulq	%r10, %rbp
	imulq	%rax, %rbx
	imulq	%r9, %r15
	imulq	%r8, %r14
	imulq	%rsi, %r13
	imulq	%rcx, %r12
	imulq	%rdx, %rdi
	imulq	%r10, %rbp
	imulq	%rax, %rbx
	imulq	%r9, %r15
	imulq	%r8, %r14
	imulq	%rsi, %r13
	imulq	%rcx, %r12
	imulq	%rdx, %rdi
	imulq	%r10, %rbp
	imulq	%rax, %rbx
	imulq	%r9, %r15
	imulq	%r8, %r14
	imulq	%rsi, %r13
	imulq	%rcx, %r12
	imulq	%rdx, %rdi
	imulq	%r10, %rbp
	imulq	%rax, %rbx
	subq	$1, %r11
	cmpq	$-1, %r11
	jne	.L607
	movq	%rdi, 64(%rsp)
.L606:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE191:
	.size	int64_mul_6, .-int64_mul_6
	.globl	int64_mul_7
	.type	int64_mul_7, @function
int64_mul_7:
.LFB192:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$104, %rsp
	.cfi_def_cfa_offset 160
	movq	%rdi, 8(%rsp)
	movq	%rsi, %rax
	movl	12(%rsi), %r8d
	leal	37421(%r8), %edx
	movslq	%edx, %rdx
	leal	7(%r8), %ecx
	salq	$32, %rcx
	addq	%rdx, %rcx
	addl	$5, %r8d
	movslq	%r8d, %r8
	movq	%rcx, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	imulq	%r8, %rdx
	movq	%rdx, %r15
	subq	%rcx, %rdx
	movq	%rdx, 16(%rsp)
	movl	16(%rsi), %esi
	leal	37420(%rsi), %edx
	movslq	%edx, %rdx
	leal	6(%rsi), %ecx
	salq	$32, %rcx
	addq	%rdx, %rcx
	addl	$4, %esi
	movslq	%esi, %rsi
	movq	%rcx, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	movq	%rdx, %r14
	subq	%rcx, %rdx
	movq	%rdx, 24(%rsp)
	movl	20(%rax), %edx
	leal	37419(%rdx), %edi
	movslq	%edi, %rdi
	leal	5(%rdx), %ecx
	salq	$32, %rcx
	addq	%rdi, %rcx
	addl	$3, %edx
	movslq	%edx, %rdi
	movq	%rcx, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	movq	%rdx, %r13
	subq	%rcx, %rdx
	movq	%rdx, 32(%rsp)
	movl	24(%rax), %ecx
	leal	37418(%rcx), %r9d
	movslq	%r9d, %r9
	leal	4(%rcx), %edx
	salq	$32, %rdx
	addq	%r9, %rdx
	addl	$2, %ecx
	movslq	%ecx, %rbx
	movq	%rbx, (%rsp)
	movq	%rbx, %rcx
	imulq	%rdx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	movq	%rcx, %r12
	subq	%rdx, %rcx
	movq	%rcx, 40(%rsp)
	movl	28(%rax), %ecx
	leal	37417(%rcx), %r9d
	movslq	%r9d, %r9
	leal	3(%rcx), %edx
	salq	$32, %rdx
	addq	%r9, %rdx
	addl	$1, %ecx
	movslq	%ecx, %rcx
	movq	%rdx, %r9
	imulq	%rcx, %r9
	imulq	%rcx, %r9
	imulq	%rcx, %r9
	imulq	%rcx, %r9
	imulq	%rcx, %r9
	imulq	%rcx, %r9
	imulq	%rcx, %r9
	imulq	%rcx, %r9
	imulq	%rcx, %r9
	imulq	%rcx, %r9
	movq	%r9, 80(%rsp)
	movq	%r9, %rbx
	subq	%rdx, %rbx
	movq	%rbx, 48(%rsp)
	movl	32(%rax), %r9d
	leal	37416(%r9), %r10d
	movslq	%r10d, %r10
	leal	2(%r9), %edx
	salq	$32, %rdx
	addq	%r10, %rdx
	movslq	%r9d, %r9
	movq	%rdx, %r10
	imulq	%r9, %r10
	imulq	%r9, %r10
	imulq	%r9, %r10
	imulq	%r9, %r10
	imulq	%r9, %r10
	imulq	%r9, %r10
	imulq	%r9, %r10
	imulq	%r9, %r10
	imulq	%r9, %r10
	imulq	%r9, %r10
	movq	%r10, 88(%rsp)
	movq	%r10, %r11
	subq	%rdx, %r11
	movq	%r11, 56(%rsp)
	movl	36(%rax), %edx
	leal	37415(%rdx), %r10d
	movslq	%r10d, %r10
	leal	1(%rdx), %r11d
	salq	$32, %r11
	addq	%r11, %r10
	subl	$1, %edx
	movslq	%edx, %rdx
	movq	%r10, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	movq	%rbp, %rbx
	subq	%r10, %rbx
	movq	%rbx, 64(%rsp)
	movl	40(%rax), %eax
	leal	37414(%rax), %r10d
	movslq	%r10d, %r10
	movq	%rax, %r11
	salq	$32, %r11
	addq	%r11, %r10
	subl	$2, %eax
	cltq
	movq	%r10, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	movq	%rbx, %r11
	subq	%r10, %r11
	movq	%r11, 72(%rsp)
	movq	8(%rsp), %r11
	leaq	-1(%r11), %r10
	movq	%r10, 8(%rsp)
	testq	%r11, %r11
	je	.L611
	movq	%r8, %r11
	movq	%rsi, %r10
	movq	%rdi, %r8
	movq	80(%rsp), %rsi
	movq	88(%rsp), %rdi
.L612:
	subq	16(%rsp), %r15
	subq	24(%rsp), %r14
	subq	32(%rsp), %r13
	subq	40(%rsp), %r12
	subq	48(%rsp), %rsi
	subq	56(%rsp), %rdi
	subq	64(%rsp), %rbp
	subq	72(%rsp), %rbx
	imulq	%r11, %r15
	imulq	%r10, %r14
	imulq	%r8, %r13
	imulq	(%rsp), %r12
	imulq	%rcx, %rsi
	imulq	%r9, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	%r10, %r14
	imulq	%r8, %r13
	imulq	(%rsp), %r12
	imulq	%rcx, %rsi
	imulq	%r9, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	%r10, %r14
	imulq	%r8, %r13
	imulq	(%rsp), %r12
	imulq	%rcx, %rsi
	imulq	%r9, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	%r10, %r14
	imulq	%r8, %r13
	imulq	(%rsp), %r12
	imulq	%rcx, %rsi
	imulq	%r9, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	%r10, %r14
	imulq	%r8, %r13
	imulq	(%rsp), %r12
	imulq	%rcx, %rsi
	imulq	%r9, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	%r10, %r14
	imulq	%r8, %r13
	imulq	(%rsp), %r12
	imulq	%rcx, %rsi
	imulq	%r9, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	%r10, %r14
	imulq	%r8, %r13
	imulq	(%rsp), %r12
	imulq	%rcx, %rsi
	imulq	%r9, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	%r10, %r14
	imulq	%r8, %r13
	imulq	(%rsp), %r12
	imulq	%rcx, %rsi
	imulq	%r9, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	%r10, %r14
	imulq	%r8, %r13
	imulq	(%rsp), %r12
	imulq	%rcx, %rsi
	imulq	%r9, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	%r10, %r14
	imulq	%r8, %r13
	imulq	(%rsp), %r12
	imulq	%rcx, %rsi
	imulq	%r9, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	subq	$1, 8(%rsp)
	cmpq	$-1, 8(%rsp)
	jne	.L612
	movq	%rsi, 80(%rsp)
	movq	%rdi, 88(%rsp)
.L611:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE192:
	.size	int64_mul_7, .-int64_mul_7
	.globl	int64_mul_8
	.type	int64_mul_8, @function
int64_mul_8:
.LFB193:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$136, %rsp
	.cfi_def_cfa_offset 192
	movq	%rdi, %r11
	movq	%rsi, %rax
	movl	12(%rsi), %edx
	leal	37421(%rdx), %esi
	movslq	%esi, %rsi
	leal	7(%rdx), %ecx
	salq	$32, %rcx
	addq	%rsi, %rcx
	addl	$5, %edx
	movslq	%edx, %rdi
	movq	%rcx, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	movq	%rdx, %r15
	subq	%rcx, %rdx
	movq	%rdx, 32(%rsp)
	movl	16(%rax), %edx
	leal	37420(%rdx), %esi
	movslq	%esi, %rsi
	leal	6(%rdx), %ecx
	salq	$32, %rcx
	addq	%rsi, %rcx
	addl	$4, %edx
	movslq	%edx, %rbx
	movq	%rbx, (%rsp)
	movq	%rbx, %rdx
	imulq	%rcx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %r14
	subq	%rcx, %rdx
	movq	%rdx, 40(%rsp)
	movl	20(%rax), %edx
	leal	37419(%rdx), %esi
	movslq	%esi, %rsi
	leal	5(%rdx), %ecx
	salq	$32, %rcx
	addq	%rsi, %rcx
	addl	$3, %edx
	movslq	%edx, %rsi
	movq	%rsi, 8(%rsp)
	movq	%rsi, %rdx
	imulq	%rcx, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	movq	%rdx, %r13
	subq	%rcx, %rdx
	movq	%rdx, 48(%rsp)
	movl	24(%rax), %ecx
	leal	37418(%rcx), %esi
	movslq	%esi, %rsi
	leal	4(%rcx), %edx
	salq	$32, %rdx
	addq	%rsi, %rdx
	addl	$2, %ecx
	movslq	%ecx, %rbx
	movq	%rbx, 16(%rsp)
	movq	%rbx, %rcx
	imulq	%rdx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	movq	%rcx, %r12
	subq	%rdx, %rcx
	movq	%rcx, 56(%rsp)
	movl	28(%rax), %esi
	leal	37417(%rsi), %ecx
	movslq	%ecx, %rcx
	leal	3(%rsi), %edx
	salq	$32, %rdx
	addq	%rcx, %rdx
	addl	$1, %esi
	movslq	%esi, %rsi
	movq	%rdx, %rcx
	imulq	%rsi, %rcx
	imulq	%rsi, %rcx
	imulq	%rsi, %rcx
	imulq	%rsi, %rcx
	imulq	%rsi, %rcx
	imulq	%rsi, %rcx
	imulq	%rsi, %rcx
	imulq	%rsi, %rcx
	imulq	%rsi, %rcx
	imulq	%rsi, %rcx
	movq	%rcx, 104(%rsp)
	movq	%rcx, %rbx
	subq	%rdx, %rbx
	movq	%rbx, 64(%rsp)
	movl	32(%rax), %r8d
	leal	37416(%r8), %ecx
	movslq	%ecx, %rcx
	leal	2(%r8), %edx
	salq	$32, %rdx
	addq	%rcx, %rdx
	movslq	%r8d, %r8
	movq	%rdx, %rcx
	imulq	%r8, %rcx
	imulq	%r8, %rcx
	imulq	%r8, %rcx
	imulq	%r8, %rcx
	imulq	%r8, %rcx
	imulq	%r8, %rcx
	imulq	%r8, %rcx
	imulq	%r8, %rcx
	imulq	%r8, %rcx
	imulq	%r8, %rcx
	movq	%rcx, %r10
	movq	%rcx, 112(%rsp)
	subq	%rdx, %r10
	movq	%r10, 72(%rsp)
	movl	36(%rax), %ecx
	leal	37415(%rcx), %edx
	movslq	%edx, %rdx
	leal	1(%rcx), %r9d
	salq	$32, %r9
	addq	%rdx, %r9
	subl	$1, %ecx
	movslq	%ecx, %rcx
	movq	%r9, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	movq	%rdx, 120(%rsp)
	movq	%rdx, %rbp
	subq	%r9, %rbp
	movq	%rbp, 80(%rsp)
	movl	40(%rax), %edx
	leal	37414(%rdx), %r9d
	movslq	%r9d, %r9
	movq	%rdx, %rbx
	salq	$32, %rbx
	addq	%rbx, %r9
	subl	$2, %edx
	movslq	%edx, %rdx
	movq	%r9, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	movq	%rbp, %rbx
	subq	%r9, %rbx
	movq	%rbx, 88(%rsp)
	movl	44(%rax), %eax
	leal	37413(%rax), %r9d
	movslq	%r9d, %r9
	leal	-1(%rax), %r10d
	salq	$32, %r10
	addq	%r10, %r9
	subl	$3, %eax
	cltq
	movq	%r9, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	movq	%rbx, %r10
	subq	%r9, %r10
	movq	%r10, 96(%rsp)
	leaq	-1(%r11), %r9
	movq	%r9, 24(%rsp)
	testq	%r11, %r11
	je	.L616
	movq	%rdi, %r11
	movq	%rsi, %r10
	movq	%r8, %r9
	movq	104(%rsp), %r8
	movq	112(%rsp), %rsi
	movq	120(%rsp), %rdi
.L617:
	subq	32(%rsp), %r15
	subq	40(%rsp), %r14
	subq	48(%rsp), %r13
	subq	56(%rsp), %r12
	subq	64(%rsp), %r8
	subq	72(%rsp), %rsi
	subq	80(%rsp), %rdi
	subq	88(%rsp), %rbp
	subq	96(%rsp), %rbx
	imulq	%r11, %r15
	imulq	(%rsp), %r14
	imulq	8(%rsp), %r13
	imulq	16(%rsp), %r12
	imulq	%r10, %r8
	imulq	%r9, %rsi
	imulq	%rcx, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	(%rsp), %r14
	imulq	8(%rsp), %r13
	imulq	16(%rsp), %r12
	imulq	%r10, %r8
	imulq	%r9, %rsi
	imulq	%rcx, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	(%rsp), %r14
	imulq	8(%rsp), %r13
	imulq	16(%rsp), %r12
	imulq	%r10, %r8
	imulq	%r9, %rsi
	imulq	%rcx, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	(%rsp), %r14
	imulq	8(%rsp), %r13
	imulq	16(%rsp), %r12
	imulq	%r10, %r8
	imulq	%r9, %rsi
	imulq	%rcx, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	(%rsp), %r14
	imulq	8(%rsp), %r13
	imulq	16(%rsp), %r12
	imulq	%r10, %r8
	imulq	%r9, %rsi
	imulq	%rcx, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	(%rsp), %r14
	imulq	8(%rsp), %r13
	imulq	16(%rsp), %r12
	imulq	%r10, %r8
	imulq	%r9, %rsi
	imulq	%rcx, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	(%rsp), %r14
	imulq	8(%rsp), %r13
	imulq	16(%rsp), %r12
	imulq	%r10, %r8
	imulq	%r9, %rsi
	imulq	%rcx, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	(%rsp), %r14
	imulq	8(%rsp), %r13
	imulq	16(%rsp), %r12
	imulq	%r10, %r8
	imulq	%r9, %rsi
	imulq	%rcx, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	(%rsp), %r14
	imulq	8(%rsp), %r13
	imulq	16(%rsp), %r12
	imulq	%r10, %r8
	imulq	%r9, %rsi
	imulq	%rcx, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	imulq	%r11, %r15
	imulq	(%rsp), %r14
	imulq	8(%rsp), %r13
	imulq	16(%rsp), %r12
	imulq	%r10, %r8
	imulq	%r9, %rsi
	imulq	%rcx, %rdi
	imulq	%rdx, %rbp
	imulq	%rax, %rbx
	subq	$1, 24(%rsp)
	cmpq	$-1, 24(%rsp)
	jne	.L617
	movq	%r8, 104(%rsp)
	movq	%rsi, 112(%rsp)
	movq	%rdi, 120(%rsp)
.L616:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	104(%rsp), %edi
	call	use_int@PLT
	movl	112(%rsp), %edi
	call	use_int@PLT
	movl	120(%rsp), %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$136, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE193:
	.size	int64_mul_8, .-int64_mul_8
	.globl	int64_mul_9
	.type	int64_mul_9, @function
int64_mul_9:
.LFB194:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$168, %rsp
	.cfi_def_cfa_offset 224
	movq	%rdi, %r10
	movq	%rsi, %rax
	movl	12(%rsi), %edx
	leal	37421(%rdx), %esi
	movslq	%esi, %rsi
	leal	7(%rdx), %ecx
	salq	$32, %rcx
	addq	%rsi, %rcx
	addl	$5, %edx
	movslq	%edx, %rbx
	movq	%rbx, (%rsp)
	movq	%rbx, %rdx
	imulq	%rcx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %r15
	movq	%rdx, %rdi
	subq	%rcx, %rdi
	movq	%rdi, 48(%rsp)
	movl	16(%rax), %edx
	leal	37420(%rdx), %esi
	movslq	%esi, %rsi
	leal	6(%rdx), %ecx
	salq	$32, %rcx
	addq	%rsi, %rcx
	addl	$4, %edx
	movslq	%edx, %rsi
	movq	%rsi, 8(%rsp)
	movq	%rsi, %rdx
	imulq	%rcx, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	movq	%rdx, %r14
	movq	%rdx, %rdi
	subq	%rcx, %rdi
	movq	%rdi, 56(%rsp)
	movl	20(%rax), %edx
	leal	37419(%rdx), %esi
	movslq	%esi, %rsi
	leal	5(%rdx), %ecx
	salq	$32, %rcx
	addq	%rsi, %rcx
	addl	$3, %edx
	movslq	%edx, %rdi
	movq	%rdi, 16(%rsp)
	movq	%rdi, %rdx
	imulq	%rcx, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	movq	%rdx, %r13
	movq	%rdx, %rdi
	subq	%rcx, %rdi
	movq	%rdi, 64(%rsp)
	movl	24(%rax), %ecx
	leal	37418(%rcx), %esi
	movslq	%esi, %rsi
	leal	4(%rcx), %edx
	salq	$32, %rdx
	addq	%rsi, %rdx
	addl	$2, %ecx
	movslq	%ecx, %rbx
	movq	%rbx, 24(%rsp)
	movq	%rbx, %rcx
	imulq	%rdx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	movq	%rcx, %r12
	movq	%rcx, %rdi
	subq	%rdx, %rdi
	movq	%rdi, 72(%rsp)
	movl	28(%rax), %ecx
	leal	37417(%rcx), %esi
	movslq	%esi, %rsi
	leal	3(%rcx), %edx
	salq	$32, %rdx
	addq	%rsi, %rdx
	addl	$1, %ecx
	movslq	%ecx, %rsi
	movq	%rsi, 32(%rsp)
	movq	%rsi, %rcx
	imulq	%rdx, %rcx
	imulq	%rsi, %rcx
	imulq	%rsi, %rcx
	imulq	%rsi, %rcx
	imulq	%rsi, %rcx
	imulq	%rsi, %rcx
	imulq	%rsi, %rcx
	imulq	%rsi, %rcx
	imulq	%rsi, %rcx
	imulq	%rsi, %rcx
	movq	%rcx, 128(%rsp)
	movq	%rcx, %rbx
	subq	%rdx, %rbx
	movq	%rbx, 80(%rsp)
	movl	32(%rax), %ecx
	leal	37416(%rcx), %esi
	movslq	%esi, %rsi
	leal	2(%rcx), %edx
	salq	$32, %rdx
	addq	%rsi, %rdx
	movslq	%ecx, %rdi
	movq	%rdx, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	movq	%rcx, %rsi
	movq	%rcx, 136(%rsp)
	subq	%rdx, %rsi
	movq	%rsi, 88(%rsp)
	movl	36(%rax), %esi
	leal	37415(%rsi), %edx
	movslq	%edx, %rdx
	leal	1(%rsi), %ecx
	salq	$32, %rcx
	addq	%rdx, %rcx
	subl	$1, %esi
	movslq	%esi, %rsi
	movq	%rcx, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	movq	%rdx, %r11
	movq	%rdx, 144(%rsp)
	subq	%rcx, %r11
	movq	%r11, 96(%rsp)
	movl	40(%rax), %ecx
	leal	37414(%rcx), %edx
	movslq	%edx, %rdx
	movq	%rcx, %r8
	salq	$32, %r8
	addq	%rdx, %r8
	subl	$2, %ecx
	movslq	%ecx, %rcx
	movq	%r8, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	movq	%rdx, %r9
	movq	%rdx, 152(%rsp)
	subq	%r8, %r9
	movq	%r9, 104(%rsp)
	movl	44(%rax), %edx
	leal	37413(%rdx), %r8d
	movslq	%r8d, %r8
	leal	-1(%rdx), %r9d
	salq	$32, %r9
	addq	%r9, %r8
	subl	$3, %edx
	movslq	%edx, %rdx
	movq	%r8, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	movq	%rbp, %rbx
	subq	%r8, %rbx
	movq	%rbx, 112(%rsp)
	movl	48(%rax), %eax
	leal	37412(%rax), %r8d
	movslq	%r8d, %r8
	leal	-2(%rax), %r9d
	salq	$32, %r9
	addq	%r9, %r8
	subl	$4, %eax
	cltq
	movq	%r8, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	movq	%rbx, %r11
	subq	%r8, %r11
	movq	%r11, 120(%rsp)
	leaq	-1(%r10), %r8
	movq	%r8, 40(%rsp)
	testq	%r10, %r10
	je	.L621
	movq	%rdi, %r11
	movq	%rsi, %r10
	movq	%rcx, %r9
	movq	%rdx, %r8
	movq	128(%rsp), %rdi
	movq	136(%rsp), %rdx
	movq	144(%rsp), %rcx
	movq	152(%rsp), %rsi
.L622:
	subq	48(%rsp), %r15
	subq	56(%rsp), %r14
	subq	64(%rsp), %r13
	subq	72(%rsp), %r12
	subq	80(%rsp), %rdi
	subq	88(%rsp), %rdx
	subq	96(%rsp), %rcx
	subq	104(%rsp), %rsi
	subq	112(%rsp), %rbp
	subq	120(%rsp), %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rdi
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rsi
	imulq	%r8, %rbp
	imulq	%rax, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rdi
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rsi
	imulq	%r8, %rbp
	imulq	%rax, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rdi
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rsi
	imulq	%r8, %rbp
	imulq	%rax, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rdi
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rsi
	imulq	%r8, %rbp
	imulq	%rax, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rdi
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rsi
	imulq	%r8, %rbp
	imulq	%rax, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rdi
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rsi
	imulq	%r8, %rbp
	imulq	%rax, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rdi
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rsi
	imulq	%r8, %rbp
	imulq	%rax, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rdi
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rsi
	imulq	%r8, %rbp
	imulq	%rax, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rdi
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rsi
	imulq	%r8, %rbp
	imulq	%rax, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rdi
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rsi
	imulq	%r8, %rbp
	imulq	%rax, %rbx
	subq	$1, 40(%rsp)
	cmpq	$-1, 40(%rsp)
	jne	.L622
	movq	%rdi, 128(%rsp)
	movq	%rdx, 136(%rsp)
	movq	%rcx, 144(%rsp)
	movq	%rsi, 152(%rsp)
.L621:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	128(%rsp), %edi
	call	use_int@PLT
	movl	136(%rsp), %edi
	call	use_int@PLT
	movl	144(%rsp), %edi
	call	use_int@PLT
	movl	152(%rsp), %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$168, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE194:
	.size	int64_mul_9, .-int64_mul_9
	.globl	int64_mul_10
	.type	int64_mul_10, @function
int64_mul_10:
.LFB195:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$200, %rsp
	.cfi_def_cfa_offset 256
	movq	%rdi, %r9
	movq	%rsi, %rax
	movl	12(%rsi), %edx
	leal	37421(%rdx), %esi
	movslq	%esi, %rsi
	leal	7(%rdx), %ecx
	salq	$32, %rcx
	addq	%rsi, %rcx
	addl	$5, %edx
	movslq	%edx, %rbx
	movq	%rbx, (%rsp)
	movq	%rbx, %rdx
	imulq	%rcx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %r15
	movq	%rdx, %rsi
	subq	%rcx, %rsi
	movq	%rsi, 64(%rsp)
	movl	16(%rax), %edx
	leal	37420(%rdx), %esi
	movslq	%esi, %rsi
	leal	6(%rdx), %ecx
	salq	$32, %rcx
	addq	%rsi, %rcx
	addl	$4, %edx
	movslq	%edx, %rdi
	movq	%rdi, 8(%rsp)
	movq	%rdi, %rdx
	imulq	%rcx, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	movq	%rdx, %r14
	movq	%rdx, %rsi
	subq	%rcx, %rsi
	movq	%rsi, 72(%rsp)
	movl	20(%rax), %edx
	leal	37419(%rdx), %esi
	movslq	%esi, %rsi
	leal	5(%rdx), %ecx
	salq	$32, %rcx
	addq	%rsi, %rcx
	addl	$3, %edx
	movslq	%edx, %rsi
	movq	%rsi, 16(%rsp)
	movq	%rsi, %rdx
	imulq	%rcx, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	imulq	%rsi, %rdx
	movq	%rdx, %r13
	movq	%rdx, %rsi
	subq	%rcx, %rsi
	movq	%rsi, 80(%rsp)
	movl	24(%rax), %ecx
	leal	37418(%rcx), %esi
	movslq	%esi, %rsi
	leal	4(%rcx), %edx
	salq	$32, %rdx
	addq	%rsi, %rdx
	addl	$2, %ecx
	movslq	%ecx, %rbx
	movq	%rbx, 24(%rsp)
	movq	%rbx, %rcx
	imulq	%rdx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	movq	%rcx, %r12
	movq	%rcx, %rsi
	subq	%rdx, %rsi
	movq	%rsi, 88(%rsp)
	movl	28(%rax), %ecx
	leal	37417(%rcx), %esi
	movslq	%esi, %rsi
	leal	3(%rcx), %edx
	salq	$32, %rdx
	addq	%rsi, %rdx
	addl	$1, %ecx
	movslq	%ecx, %rdi
	movq	%rdi, 32(%rsp)
	movq	%rdi, %rcx
	imulq	%rdx, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	movq	%rcx, 152(%rsp)
	movq	%rcx, %rdi
	subq	%rdx, %rdi
	movq	%rdi, 96(%rsp)
	movl	32(%rax), %ecx
	leal	37416(%rcx), %esi
	movslq	%esi, %rsi
	leal	2(%rcx), %edx
	salq	$32, %rdx
	addq	%rsi, %rdx
	movslq	%ecx, %rbx
	movq	%rbx, 40(%rsp)
	movq	%rbx, %rcx
	imulq	%rdx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	movq	%rcx, %rsi
	imulq	%rbx, %rsi
	movq	%rsi, 160(%rsp)
	movq	%rsi, %rbx
	subq	%rdx, %rbx
	movq	%rbx, 104(%rsp)
	movl	36(%rax), %ecx
	leal	37415(%rcx), %esi
	movslq	%esi, %rsi
	leal	1(%rcx), %edx
	salq	$32, %rdx
	addq	%rsi, %rdx
	subl	$1, %ecx
	movslq	%ecx, %rbx
	movq	%rbx, 48(%rsp)
	movq	%rbx, %rcx
	imulq	%rdx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	imulq	%rbx, %rcx
	movq	%rcx, %r10
	movq	%rcx, 168(%rsp)
	subq	%rdx, %r10
	movq	%r10, 112(%rsp)
	movl	40(%rax), %ecx
	leal	37414(%rcx), %esi
	movslq	%esi, %rsi
	movq	%rcx, %rdx
	salq	$32, %rdx
	addq	%rsi, %rdx
	subl	$2, %ecx
	movslq	%ecx, %rdi
	movq	%rdx, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	imulq	%rdi, %rcx
	movq	%rcx, %r11
	movq	%rcx, 176(%rsp)
	subq	%rdx, %r11
	movq	%r11, 120(%rsp)
	movl	44(%rax), %ecx
	leal	37413(%rcx), %edx
	movslq	%edx, %rdx
	leal	-1(%rcx), %esi
	salq	$32, %rsi
	addq	%rdx, %rsi
	subl	$3, %ecx
	movslq	%ecx, %rcx
	movq	%rsi, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	movq	%rdx, %r8
	movq	%rdx, 184(%rsp)
	subq	%rsi, %r8
	movq	%r8, 128(%rsp)
	movl	48(%rax), %edx
	leal	37412(%rdx), %esi
	movslq	%esi, %rsi
	leal	-2(%rdx), %r8d
	salq	$32, %r8
	addq	%r8, %rsi
	subl	$4, %edx
	movslq	%edx, %rdx
	movq	%rsi, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	imulq	%rdx, %rbp
	movq	%rbp, %rbx
	subq	%rsi, %rbx
	movq	%rbx, 136(%rsp)
	movl	52(%rax), %eax
	leal	37411(%rax), %esi
	movslq	%esi, %rsi
	leal	-3(%rax), %r8d
	salq	$32, %r8
	addq	%r8, %rsi
	subl	$5, %eax
	cltq
	movq	%rsi, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	movq	%rbx, %r10
	subq	%rsi, %r10
	movq	%r10, 144(%rsp)
	leaq	-1(%r9), %rsi
	movq	%rsi, 56(%rsp)
	testq	%r9, %r9
	je	.L626
	movq	%rdi, %r11
	movq	%rcx, %r10
	movq	%rdx, %r9
	movq	%rax, %r8
	movq	152(%rsp), %rsi
	movq	160(%rsp), %rdi
	movq	168(%rsp), %rax
	movq	176(%rsp), %rdx
	movq	184(%rsp), %rcx
.L627:
	subq	64(%rsp), %r15
	subq	72(%rsp), %r14
	subq	80(%rsp), %r13
	subq	88(%rsp), %r12
	subq	96(%rsp), %rsi
	subq	104(%rsp), %rdi
	subq	112(%rsp), %rax
	subq	120(%rsp), %rdx
	subq	128(%rsp), %rcx
	subq	136(%rsp), %rbp
	subq	144(%rsp), %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rsi
	imulq	40(%rsp), %rdi
	imulq	48(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rbp
	imulq	%r8, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rsi
	imulq	40(%rsp), %rdi
	imulq	48(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rbp
	imulq	%r8, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rsi
	imulq	40(%rsp), %rdi
	imulq	48(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rbp
	imulq	%r8, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rsi
	imulq	40(%rsp), %rdi
	imulq	48(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rbp
	imulq	%r8, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rsi
	imulq	40(%rsp), %rdi
	imulq	48(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rbp
	imulq	%r8, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rsi
	imulq	40(%rsp), %rdi
	imulq	48(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rbp
	imulq	%r8, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rsi
	imulq	40(%rsp), %rdi
	imulq	48(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rbp
	imulq	%r8, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rsi
	imulq	40(%rsp), %rdi
	imulq	48(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rbp
	imulq	%r8, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rsi
	imulq	40(%rsp), %rdi
	imulq	48(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rbp
	imulq	%r8, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rsi
	imulq	40(%rsp), %rdi
	imulq	48(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r10, %rcx
	imulq	%r9, %rbp
	imulq	%r8, %rbx
	subq	$1, 56(%rsp)
	cmpq	$-1, 56(%rsp)
	jne	.L627
	movq	%rsi, 152(%rsp)
	movq	%rdi, 160(%rsp)
	movq	%rax, 168(%rsp)
	movq	%rdx, 176(%rsp)
	movq	%rcx, 184(%rsp)
.L626:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	152(%rsp), %edi
	call	use_int@PLT
	movl	160(%rsp), %edi
	call	use_int@PLT
	movl	168(%rsp), %edi
	call	use_int@PLT
	movl	176(%rsp), %edi
	call	use_int@PLT
	movl	184(%rsp), %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$200, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE195:
	.size	int64_mul_10, .-int64_mul_10
	.globl	int64_mul_11
	.type	int64_mul_11, @function
int64_mul_11:
.LFB196:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$248, %rsp
	.cfi_def_cfa_offset 304
	movq	%rdi, %r8
	movq	%rsi, %rcx
	movl	12(%rsi), %eax
	leal	37421(%rax), %esi
	movslq	%esi, %rsi
	leal	7(%rax), %edx
	salq	$32, %rdx
	addq	%rsi, %rdx
	addl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, (%rsp)
	movq	%rbx, %rax
	imulq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	movq	%rax, %r13
	subq	%rdx, %rax
	movq	%rax, 80(%rsp)
	movl	16(%rcx), %eax
	leal	37420(%rax), %esi
	movslq	%esi, %rsi
	leal	6(%rax), %edx
	salq	$32, %rdx
	addq	%rsi, %rdx
	addl	$4, %eax
	movslq	%eax, %rsi
	movq	%rsi, 8(%rsp)
	movq	%rsi, %rax
	imulq	%rdx, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	movq	%rax, %r12
	subq	%rdx, %rax
	movq	%rax, 88(%rsp)
	movl	20(%rcx), %eax
	leal	37419(%rax), %esi
	movslq	%esi, %rsi
	leal	5(%rax), %edx
	salq	$32, %rdx
	addq	%rsi, %rdx
	addl	$3, %eax
	movslq	%eax, %rdi
	movq	%rdi, 16(%rsp)
	movq	%rdi, %rax
	imulq	%rdx, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	movq	%rax, %rbp
	subq	%rdx, %rax
	movq	%rax, 96(%rsp)
	movl	24(%rcx), %edx
	leal	37418(%rdx), %esi
	movslq	%esi, %rsi
	leal	4(%rdx), %eax
	salq	$32, %rax
	addq	%rsi, %rax
	addl	$2, %edx
	movslq	%edx, %rbx
	movq	%rbx, 24(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %r9
	movq	%rdx, 224(%rsp)
	movq	%rdx, %rsi
	subq	%rax, %rsi
	movq	%rsi, 104(%rsp)
	movl	28(%rcx), %edx
	leal	37417(%rdx), %esi
	movslq	%esi, %rsi
	leal	3(%rdx), %eax
	salq	$32, %rax
	addq	%rsi, %rax
	addl	$1, %edx
	movslq	%edx, %rdi
	movq	%rdi, 32(%rsp)
	movq	%rdi, %rdx
	imulq	%rax, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	movq	%rdx, %r10
	movq	%rdx, 232(%rsp)
	movq	%rdx, %rsi
	subq	%rax, %rsi
	movq	%rsi, 112(%rsp)
	movl	32(%rcx), %edx
	leal	37416(%rdx), %esi
	movslq	%esi, %rsi
	leal	2(%rdx), %eax
	salq	$32, %rax
	addq	%rsi, %rax
	movslq	%edx, %rbx
	movq	%rbx, 40(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, 176(%rsp)
	movq	%rdx, %rsi
	subq	%rax, %rsi
	movq	%rsi, 120(%rsp)
	movl	36(%rcx), %edx
	leal	37415(%rdx), %esi
	movslq	%esi, %rsi
	leal	1(%rdx), %eax
	salq	$32, %rax
	addq	%rsi, %rax
	subl	$1, %edx
	movslq	%edx, %rbx
	movq	%rbx, 48(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %rdi
	imulq	%rbx, %rdi
	movq	%rdi, 184(%rsp)
	movq	%rdi, %rbx
	subq	%rax, %rbx
	movq	%rbx, 128(%rsp)
	movl	40(%rcx), %edx
	leal	37414(%rdx), %esi
	movslq	%esi, %rsi
	movq	%rdx, %rax
	salq	$32, %rax
	addq	%rsi, %rax
	subl	$2, %edx
	movslq	%edx, %rbx
	movq	%rbx, 56(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %r11
	movq	%rdx, 192(%rsp)
	subq	%rax, %r11
	movq	%r11, 136(%rsp)
	movl	44(%rcx), %edx
	leal	37413(%rdx), %esi
	movslq	%esi, %rsi
	leal	-1(%rdx), %eax
	salq	$32, %rax
	addq	%rsi, %rax
	subl	$3, %edx
	movslq	%edx, %rbx
	movq	%rbx, 64(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %r14
	movq	%rdx, 200(%rsp)
	subq	%rax, %r14
	movq	%r14, 144(%rsp)
	movl	48(%rcx), %edi
	leal	37412(%rdi), %eax
	cltq
	leal	-2(%rdi), %edx
	salq	$32, %rdx
	addq	%rax, %rdx
	subl	$4, %edi
	movslq	%edi, %rdi
	movq	%rdx, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	movq	%rax, %r15
	movq	%rax, 208(%rsp)
	subq	%rdx, %r15
	movq	%r15, 152(%rsp)
	movl	52(%rcx), %r15d
	leal	37411(%r15), %eax
	cltq
	leal	-3(%r15), %edx
	salq	$32, %rdx
	addq	%rax, %rdx
	subl	$5, %r15d
	movslq	%r15d, %r15
	movq	%rdx, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	movq	%rax, 216(%rsp)
	movq	%rax, %rsi
	subq	%rdx, %rsi
	movq	%rsi, 160(%rsp)
	movl	56(%rcx), %r14d
	leal	37410(%r14), %eax
	cltq
	leal	-4(%r14), %edx
	salq	$32, %rdx
	addq	%rdx, %rax
	subl	$6, %r14d
	movslq	%r14d, %r14
	movq	%rax, %rcx
	imulq	%r14, %rcx
	movq	%rcx, %rbx
	imulq	%r14, %rbx
	imulq	%r14, %rbx
	imulq	%r14, %rbx
	imulq	%r14, %rbx
	imulq	%r14, %rbx
	imulq	%r14, %rbx
	imulq	%r14, %rbx
	imulq	%r14, %rbx
	imulq	%r14, %rbx
	movq	%rbx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 168(%rsp)
	leaq	-1(%r8), %rcx
	movq	%rcx, 72(%rsp)
	testq	%r8, %r8
	je	.L631
	movq	%rdi, %r11
	movq	%r9, %rsi
	movq	%r10, %rdi
	movq	176(%rsp), %r8
	movq	184(%rsp), %r9
	movq	192(%rsp), %r10
	movq	200(%rsp), %rax
	movq	208(%rsp), %rdx
	movq	216(%rsp), %rcx
.L632:
	subq	80(%rsp), %r13
	subq	88(%rsp), %r12
	subq	96(%rsp), %rbp
	subq	104(%rsp), %rsi
	subq	112(%rsp), %rdi
	subq	120(%rsp), %r8
	subq	128(%rsp), %r9
	subq	136(%rsp), %r10
	subq	144(%rsp), %rax
	subq	152(%rsp), %rdx
	subq	160(%rsp), %rcx
	subq	168(%rsp), %rbx
	imulq	(%rsp), %r13
	imulq	8(%rsp), %r12
	imulq	16(%rsp), %rbp
	imulq	24(%rsp), %rsi
	imulq	32(%rsp), %rdi
	imulq	40(%rsp), %r8
	imulq	48(%rsp), %r9
	imulq	56(%rsp), %r10
	imulq	64(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rcx
	imulq	%r14, %rbx
	imulq	(%rsp), %r13
	imulq	8(%rsp), %r12
	imulq	16(%rsp), %rbp
	imulq	24(%rsp), %rsi
	imulq	32(%rsp), %rdi
	imulq	40(%rsp), %r8
	imulq	48(%rsp), %r9
	imulq	56(%rsp), %r10
	imulq	64(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rcx
	imulq	%r14, %rbx
	imulq	(%rsp), %r13
	imulq	8(%rsp), %r12
	imulq	16(%rsp), %rbp
	imulq	24(%rsp), %rsi
	imulq	32(%rsp), %rdi
	imulq	40(%rsp), %r8
	imulq	48(%rsp), %r9
	imulq	56(%rsp), %r10
	imulq	64(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rcx
	imulq	%r14, %rbx
	imulq	(%rsp), %r13
	imulq	8(%rsp), %r12
	imulq	16(%rsp), %rbp
	imulq	24(%rsp), %rsi
	imulq	32(%rsp), %rdi
	imulq	40(%rsp), %r8
	imulq	48(%rsp), %r9
	imulq	56(%rsp), %r10
	imulq	64(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rcx
	imulq	%r14, %rbx
	imulq	(%rsp), %r13
	imulq	8(%rsp), %r12
	imulq	16(%rsp), %rbp
	imulq	24(%rsp), %rsi
	imulq	32(%rsp), %rdi
	imulq	40(%rsp), %r8
	imulq	48(%rsp), %r9
	imulq	56(%rsp), %r10
	imulq	64(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rcx
	imulq	%r14, %rbx
	imulq	(%rsp), %r13
	imulq	8(%rsp), %r12
	imulq	16(%rsp), %rbp
	imulq	24(%rsp), %rsi
	imulq	32(%rsp), %rdi
	imulq	40(%rsp), %r8
	imulq	48(%rsp), %r9
	imulq	56(%rsp), %r10
	imulq	64(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rcx
	imulq	%r14, %rbx
	imulq	(%rsp), %r13
	imulq	8(%rsp), %r12
	imulq	16(%rsp), %rbp
	imulq	24(%rsp), %rsi
	imulq	32(%rsp), %rdi
	imulq	40(%rsp), %r8
	imulq	48(%rsp), %r9
	imulq	56(%rsp), %r10
	imulq	64(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rcx
	imulq	%r14, %rbx
	imulq	(%rsp), %r13
	imulq	8(%rsp), %r12
	imulq	16(%rsp), %rbp
	imulq	24(%rsp), %rsi
	imulq	32(%rsp), %rdi
	imulq	40(%rsp), %r8
	imulq	48(%rsp), %r9
	imulq	56(%rsp), %r10
	imulq	64(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rcx
	imulq	%r14, %rbx
	imulq	(%rsp), %r13
	imulq	8(%rsp), %r12
	imulq	16(%rsp), %rbp
	imulq	24(%rsp), %rsi
	imulq	32(%rsp), %rdi
	imulq	40(%rsp), %r8
	imulq	48(%rsp), %r9
	imulq	56(%rsp), %r10
	imulq	64(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rcx
	imulq	%r14, %rbx
	imulq	(%rsp), %r13
	imulq	8(%rsp), %r12
	imulq	16(%rsp), %rbp
	imulq	24(%rsp), %rsi
	imulq	32(%rsp), %rdi
	imulq	40(%rsp), %r8
	imulq	48(%rsp), %r9
	imulq	56(%rsp), %r10
	imulq	64(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rcx
	imulq	%r14, %rbx
	subq	$1, 72(%rsp)
	cmpq	$-1, 72(%rsp)
	jne	.L632
	movq	%rsi, 224(%rsp)
	movq	%rdi, 232(%rsp)
	movq	%r8, 176(%rsp)
	movq	%r9, 184(%rsp)
	movq	%r10, 192(%rsp)
	movq	%rax, 200(%rsp)
	movq	%rdx, 208(%rsp)
	movq	%rcx, 216(%rsp)
.L631:
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	224(%rsp), %edi
	call	use_int@PLT
	movl	232(%rsp), %edi
	call	use_int@PLT
	movl	176(%rsp), %edi
	call	use_int@PLT
	movl	184(%rsp), %edi
	call	use_int@PLT
	movl	192(%rsp), %edi
	call	use_int@PLT
	movl	200(%rsp), %edi
	call	use_int@PLT
	movl	208(%rsp), %edi
	call	use_int@PLT
	movl	216(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$248, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE196:
	.size	int64_mul_11, .-int64_mul_11
	.globl	int64_mul_12
	.type	int64_mul_12, @function
int64_mul_12:
.LFB197:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$280, %rsp
	.cfi_def_cfa_offset 336
	movq	%rdi, %r8
	movl	12(%rsi), %eax
	leal	37421(%rax), %ecx
	movslq	%ecx, %rcx
	leal	7(%rax), %edx
	salq	$32, %rdx
	addq	%rcx, %rdx
	addl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, 8(%rsp)
	movq	%rbx, %rax
	imulq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	movq	%rax, %r14
	subq	%rdx, %rax
	movq	%rax, 104(%rsp)
	movl	16(%rsi), %eax
	leal	37420(%rax), %ecx
	movslq	%ecx, %rcx
	leal	6(%rax), %edx
	salq	$32, %rdx
	addq	%rcx, %rdx
	addl	$4, %eax
	movslq	%eax, %rcx
	movq	%rcx, 16(%rsp)
	movq	%rcx, %rax
	imulq	%rdx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	movq	%rax, %r13
	subq	%rdx, %rax
	movq	%rax, 112(%rsp)
	movl	20(%rsi), %eax
	leal	37419(%rax), %ecx
	movslq	%ecx, %rcx
	leal	5(%rax), %edx
	salq	$32, %rdx
	addq	%rcx, %rdx
	addl	$3, %eax
	movslq	%eax, %rdi
	movq	%rdi, 24(%rsp)
	movq	%rdi, %rax
	imulq	%rdx, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	movq	%rax, %r12
	subq	%rdx, %rax
	movq	%rax, 120(%rsp)
	movl	24(%rsi), %edx
	leal	37418(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	4(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	addl	$2, %edx
	movslq	%edx, %rbx
	movq	%rbx, 32(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %rbp
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 128(%rsp)
	movl	28(%rsi), %edx
	leal	37417(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	3(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	addl	$1, %edx
	movslq	%edx, %rcx
	movq	%rcx, 40(%rsp)
	movq	%rcx, %rdx
	imulq	%rax, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	movq	%rdx, %r9
	movq	%rdx, 256(%rsp)
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 136(%rsp)
	movl	32(%rsi), %edx
	leal	37416(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	2(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	movslq	%edx, %rdi
	movq	%rdi, 48(%rsp)
	movq	%rdi, %rdx
	imulq	%rax, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	movq	%rdx, %r10
	movq	%rdx, 264(%rsp)
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 144(%rsp)
	movl	36(%rsi), %edx
	leal	37415(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	1(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$1, %edx
	movslq	%edx, %rbx
	movq	%rbx, 56(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, 208(%rsp)
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 152(%rsp)
	movl	40(%rsi), %edx
	leal	37414(%rdx), %ecx
	movslq	%ecx, %rcx
	movq	%rdx, %rax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$2, %edx
	movslq	%edx, %rbx
	movq	%rbx, 64(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %rdi
	imulq	%rbx, %rdi
	movq	%rdi, 216(%rsp)
	movq	%rdi, %rbx
	subq	%rax, %rbx
	movq	%rbx, 160(%rsp)
	movl	44(%rsi), %edx
	leal	37413(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	-1(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$3, %edx
	movslq	%edx, %rbx
	movq	%rbx, 72(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %r11
	movq	%rdx, 224(%rsp)
	subq	%rax, %r11
	movq	%r11, 168(%rsp)
	movl	48(%rsi), %edx
	leal	37412(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	-2(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$4, %edx
	movslq	%edx, %rbx
	movq	%rbx, 80(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %r15
	movq	%rdx, 232(%rsp)
	subq	%rax, %r15
	movq	%r15, 176(%rsp)
	movl	52(%rsi), %eax
	leal	37411(%rax), %edx
	movslq	%edx, %rdx
	leal	-3(%rax), %ecx
	salq	$32, %rcx
	addq	%rcx, %rdx
	subl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, 88(%rsp)
	movq	%rbx, %rax
	imulq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	movq	%rax, 240(%rsp)
	movq	%rax, %rcx
	subq	%rdx, %rcx
	movq	%rcx, 184(%rsp)
	movl	56(%rsi), %edi
	leal	37410(%rdi), %eax
	cltq
	leal	-4(%rdi), %edx
	salq	$32, %rdx
	addq	%rax, %rdx
	subl	$6, %edi
	movslq	%edi, %rdi
	movq	%rdx, %rbx
	imulq	%rdi, %rbx
	movq	%rbx, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	movq	%rax, %rbx
	imulq	%rdi, %rbx
	movq	%rbx, 248(%rsp)
	subq	%rdx, %rbx
	movq	%rbx, 192(%rsp)
	movl	60(%rsi), %r15d
	leal	37409(%r15), %eax
	cltq
	leal	-5(%r15), %edx
	salq	$32, %rdx
	addq	%rdx, %rax
	subl	$7, %r15d
	movslq	%r15d, %r15
	movq	%rax, %rsi
	imulq	%r15, %rsi
	movq	%rsi, %rbx
	imulq	%r15, %rbx
	imulq	%r15, %rbx
	imulq	%r15, %rbx
	imulq	%r15, %rbx
	imulq	%r15, %rbx
	imulq	%r15, %rbx
	imulq	%r15, %rbx
	imulq	%r15, %rbx
	imulq	%r15, %rbx
	movq	%rbx, %rsi
	subq	%rax, %rsi
	movq	%rsi, 200(%rsp)
	leaq	-1(%r8), %rsi
	movq	%rsi, 96(%rsp)
	testq	%r8, %r8
	je	.L636
	movq	%rdi, %r11
	movq	%r9, %rcx
	movq	%r10, %rsi
	movq	208(%rsp), %rdi
	movq	216(%rsp), %r8
	movq	224(%rsp), %r9
	movq	232(%rsp), %r10
	movq	240(%rsp), %rax
	movq	248(%rsp), %rdx
.L637:
	subq	104(%rsp), %r14
	subq	112(%rsp), %r13
	subq	120(%rsp), %r12
	subq	128(%rsp), %rbp
	subq	136(%rsp), %rcx
	subq	144(%rsp), %rsi
	subq	152(%rsp), %rdi
	subq	160(%rsp), %r8
	subq	168(%rsp), %r9
	subq	176(%rsp), %r10
	subq	184(%rsp), %rax
	subq	192(%rsp), %rdx
	subq	200(%rsp), %rbx
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rcx
	imulq	48(%rsp), %rsi
	imulq	56(%rsp), %rdi
	imulq	64(%rsp), %r8
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r10
	imulq	88(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rbx
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rcx
	imulq	48(%rsp), %rsi
	imulq	56(%rsp), %rdi
	imulq	64(%rsp), %r8
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r10
	imulq	88(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rbx
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rcx
	imulq	48(%rsp), %rsi
	imulq	56(%rsp), %rdi
	imulq	64(%rsp), %r8
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r10
	imulq	88(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rbx
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rcx
	imulq	48(%rsp), %rsi
	imulq	56(%rsp), %rdi
	imulq	64(%rsp), %r8
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r10
	imulq	88(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rbx
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rcx
	imulq	48(%rsp), %rsi
	imulq	56(%rsp), %rdi
	imulq	64(%rsp), %r8
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r10
	imulq	88(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rbx
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rcx
	imulq	48(%rsp), %rsi
	imulq	56(%rsp), %rdi
	imulq	64(%rsp), %r8
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r10
	imulq	88(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rbx
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rcx
	imulq	48(%rsp), %rsi
	imulq	56(%rsp), %rdi
	imulq	64(%rsp), %r8
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r10
	imulq	88(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rbx
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rcx
	imulq	48(%rsp), %rsi
	imulq	56(%rsp), %rdi
	imulq	64(%rsp), %r8
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r10
	imulq	88(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rbx
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rcx
	imulq	48(%rsp), %rsi
	imulq	56(%rsp), %rdi
	imulq	64(%rsp), %r8
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r10
	imulq	88(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rbx
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rcx
	imulq	48(%rsp), %rsi
	imulq	56(%rsp), %rdi
	imulq	64(%rsp), %r8
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r10
	imulq	88(%rsp), %rax
	imulq	%r11, %rdx
	imulq	%r15, %rbx
	subq	$1, 96(%rsp)
	cmpq	$-1, 96(%rsp)
	jne	.L637
	movq	%rcx, 256(%rsp)
	movq	%rsi, 264(%rsp)
	movq	%rdi, 208(%rsp)
	movq	%r8, 216(%rsp)
	movq	%r9, 224(%rsp)
	movq	%r10, 232(%rsp)
	movq	%rax, 240(%rsp)
	movq	%rdx, 248(%rsp)
.L636:
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	256(%rsp), %edi
	call	use_int@PLT
	movl	264(%rsp), %edi
	call	use_int@PLT
	movl	208(%rsp), %edi
	call	use_int@PLT
	movl	216(%rsp), %edi
	call	use_int@PLT
	movl	224(%rsp), %edi
	call	use_int@PLT
	movl	232(%rsp), %edi
	call	use_int@PLT
	movl	240(%rsp), %edi
	call	use_int@PLT
	movl	248(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$280, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE197:
	.size	int64_mul_12, .-int64_mul_12
	.globl	int64_mul_13
	.type	int64_mul_13, @function
int64_mul_13:
.LFB198:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$296, %rsp
	.cfi_def_cfa_offset 352
	movq	%rdi, %r8
	movl	12(%rsi), %eax
	leal	37421(%rax), %ecx
	movslq	%ecx, %rcx
	leal	7(%rax), %edx
	salq	$32, %rdx
	addq	%rcx, %rdx
	addl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, (%rsp)
	movq	%rbx, %rax
	imulq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	movq	%rax, %r15
	subq	%rdx, %rax
	movq	%rax, 112(%rsp)
	movl	16(%rsi), %eax
	leal	37420(%rax), %ecx
	movslq	%ecx, %rcx
	leal	6(%rax), %edx
	salq	$32, %rdx
	addq	%rcx, %rdx
	addl	$4, %eax
	movslq	%eax, %rcx
	movq	%rcx, 8(%rsp)
	movq	%rcx, %rax
	imulq	%rdx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	movq	%rax, %r14
	subq	%rdx, %rax
	movq	%rax, 120(%rsp)
	movl	20(%rsi), %eax
	leal	37419(%rax), %ecx
	movslq	%ecx, %rcx
	leal	5(%rax), %edx
	salq	$32, %rdx
	addq	%rcx, %rdx
	addl	$3, %eax
	movslq	%eax, %rdi
	movq	%rdi, 16(%rsp)
	movq	%rdi, %rax
	imulq	%rdx, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	movq	%rax, %r13
	subq	%rdx, %rax
	movq	%rax, 128(%rsp)
	movl	24(%rsi), %edx
	leal	37418(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	4(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	addl	$2, %edx
	movslq	%edx, %rbx
	movq	%rbx, 24(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %r12
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 136(%rsp)
	movl	28(%rsi), %edx
	leal	37417(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	3(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	addl	$1, %edx
	movslq	%edx, %rcx
	movq	%rcx, 32(%rsp)
	movq	%rcx, %rdx
	imulq	%rax, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	movq	%rdx, %rbp
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 144(%rsp)
	movl	32(%rsi), %edx
	leal	37416(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	2(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	movslq	%edx, %rdi
	movq	%rdi, 40(%rsp)
	movq	%rdi, %rdx
	imulq	%rax, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	movq	%rdx, %r9
	movq	%rdx, 272(%rsp)
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 152(%rsp)
	movl	36(%rsi), %edx
	leal	37415(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	1(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$1, %edx
	movslq	%edx, %rbx
	movq	%rbx, 48(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %r10
	movq	%rdx, 280(%rsp)
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 160(%rsp)
	movl	40(%rsi), %edx
	leal	37414(%rdx), %ecx
	movslq	%ecx, %rcx
	movq	%rdx, %rax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$2, %edx
	movslq	%edx, %rdi
	movq	%rdi, 56(%rsp)
	movq	%rdi, %rdx
	imulq	%rax, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	imulq	%rdi, %rdx
	movq	%rdx, 224(%rsp)
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 168(%rsp)
	movl	44(%rsi), %edx
	leal	37413(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	-1(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$3, %edx
	movslq	%edx, %rbx
	movq	%rbx, 64(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %rdi
	imulq	%rbx, %rdi
	movq	%rdi, 232(%rsp)
	movq	%rdi, %rbx
	subq	%rax, %rbx
	movq	%rbx, 176(%rsp)
	movl	48(%rsi), %edx
	leal	37412(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	-2(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$4, %edx
	movslq	%edx, %rbx
	movq	%rbx, 72(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %r11
	movq	%rdx, 240(%rsp)
	subq	%rax, %r11
	movq	%r11, 184(%rsp)
	movl	52(%rsi), %eax
	leal	37411(%rax), %edx
	movslq	%edx, %rdx
	leal	-3(%rax), %ecx
	salq	$32, %rcx
	addq	%rcx, %rdx
	subl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, 80(%rsp)
	movq	%rbx, %rax
	imulq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	movq	%rax, 248(%rsp)
	subq	%rdx, %rax
	movq	%rax, 192(%rsp)
	movl	56(%rsi), %eax
	leal	37410(%rax), %edx
	movslq	%edx, %rdx
	leal	-4(%rax), %ecx
	salq	$32, %rcx
	addq	%rcx, %rdx
	subl	$6, %eax
	movslq	%eax, %rbx
	movq	%rbx, 88(%rsp)
	movq	%rbx, %rax
	imulq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	movq	%rax, %rcx
	imulq	%rbx, %rcx
	movq	%rcx, 256(%rsp)
	movq	%rcx, %rbx
	subq	%rdx, %rbx
	movq	%rbx, 200(%rsp)
	movl	60(%rsi), %eax
	leal	37409(%rax), %edx
	movslq	%edx, %rdx
	leal	-5(%rax), %ecx
	salq	$32, %rcx
	addq	%rcx, %rdx
	subl	$7, %eax
	movslq	%eax, %rbx
	movq	%rbx, 96(%rsp)
	movq	%rbx, %rax
	imulq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	movq	%rax, 264(%rsp)
	movq	%rax, %rcx
	subq	%rdx, %rcx
	movq	%rcx, 208(%rsp)
	movl	64(%rsi), %edi
	leal	37408(%rdi), %eax
	cltq
	leal	-6(%rdi), %edx
	salq	$32, %rdx
	addq	%rdx, %rax
	subl	$8, %edi
	movslq	%edi, %rdi
	movq	%rax, %rsi
	imulq	%rdi, %rsi
	movq	%rsi, %rbx
	imulq	%rdi, %rbx
	imulq	%rdi, %rbx
	imulq	%rdi, %rbx
	imulq	%rdi, %rbx
	imulq	%rdi, %rbx
	imulq	%rdi, %rbx
	imulq	%rdi, %rbx
	imulq	%rdi, %rbx
	imulq	%rdi, %rbx
	movq	%rbx, %rsi
	subq	%rax, %rsi
	movq	%rsi, 216(%rsp)
	leaq	-1(%r8), %rsi
	movq	%rsi, 104(%rsp)
	testq	%r8, %r8
	je	.L641
	movq	%rdi, %r11
	movq	%r9, %rdx
	movq	%r10, %rcx
	movq	224(%rsp), %rsi
	movq	232(%rsp), %rdi
	movq	240(%rsp), %r8
	movq	248(%rsp), %r9
	movq	256(%rsp), %rax
	movq	264(%rsp), %r10
.L642:
	subq	112(%rsp), %r15
	subq	120(%rsp), %r14
	subq	128(%rsp), %r13
	subq	136(%rsp), %r12
	subq	144(%rsp), %rbp
	subq	152(%rsp), %rdx
	subq	160(%rsp), %rcx
	subq	168(%rsp), %rsi
	subq	176(%rsp), %rdi
	subq	184(%rsp), %r8
	subq	192(%rsp), %r9
	subq	200(%rsp), %rax
	subq	208(%rsp), %r10
	subq	216(%rsp), %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rdx
	imulq	48(%rsp), %rcx
	imulq	56(%rsp), %rsi
	imulq	64(%rsp), %rdi
	imulq	72(%rsp), %r8
	imulq	80(%rsp), %r9
	imulq	88(%rsp), %rax
	imulq	96(%rsp), %r10
	imulq	%r11, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rdx
	imulq	48(%rsp), %rcx
	imulq	56(%rsp), %rsi
	imulq	64(%rsp), %rdi
	imulq	72(%rsp), %r8
	imulq	80(%rsp), %r9
	imulq	88(%rsp), %rax
	imulq	96(%rsp), %r10
	imulq	%r11, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rdx
	imulq	48(%rsp), %rcx
	imulq	56(%rsp), %rsi
	imulq	64(%rsp), %rdi
	imulq	72(%rsp), %r8
	imulq	80(%rsp), %r9
	imulq	88(%rsp), %rax
	imulq	96(%rsp), %r10
	imulq	%r11, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rdx
	imulq	48(%rsp), %rcx
	imulq	56(%rsp), %rsi
	imulq	64(%rsp), %rdi
	imulq	72(%rsp), %r8
	imulq	80(%rsp), %r9
	imulq	88(%rsp), %rax
	imulq	96(%rsp), %r10
	imulq	%r11, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rdx
	imulq	48(%rsp), %rcx
	imulq	56(%rsp), %rsi
	imulq	64(%rsp), %rdi
	imulq	72(%rsp), %r8
	imulq	80(%rsp), %r9
	imulq	88(%rsp), %rax
	imulq	96(%rsp), %r10
	imulq	%r11, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rdx
	imulq	48(%rsp), %rcx
	imulq	56(%rsp), %rsi
	imulq	64(%rsp), %rdi
	imulq	72(%rsp), %r8
	imulq	80(%rsp), %r9
	imulq	88(%rsp), %rax
	imulq	96(%rsp), %r10
	imulq	%r11, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rdx
	imulq	48(%rsp), %rcx
	imulq	56(%rsp), %rsi
	imulq	64(%rsp), %rdi
	imulq	72(%rsp), %r8
	imulq	80(%rsp), %r9
	imulq	88(%rsp), %rax
	imulq	96(%rsp), %r10
	imulq	%r11, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rdx
	imulq	48(%rsp), %rcx
	imulq	56(%rsp), %rsi
	imulq	64(%rsp), %rdi
	imulq	72(%rsp), %r8
	imulq	80(%rsp), %r9
	imulq	88(%rsp), %rax
	imulq	96(%rsp), %r10
	imulq	%r11, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rdx
	imulq	48(%rsp), %rcx
	imulq	56(%rsp), %rsi
	imulq	64(%rsp), %rdi
	imulq	72(%rsp), %r8
	imulq	80(%rsp), %r9
	imulq	88(%rsp), %rax
	imulq	96(%rsp), %r10
	imulq	%r11, %rbx
	imulq	(%rsp), %r15
	imulq	8(%rsp), %r14
	imulq	16(%rsp), %r13
	imulq	24(%rsp), %r12
	imulq	32(%rsp), %rbp
	imulq	40(%rsp), %rdx
	imulq	48(%rsp), %rcx
	imulq	56(%rsp), %rsi
	imulq	64(%rsp), %rdi
	imulq	72(%rsp), %r8
	imulq	80(%rsp), %r9
	imulq	88(%rsp), %rax
	imulq	96(%rsp), %r10
	imulq	%r11, %rbx
	subq	$1, 104(%rsp)
	cmpq	$-1, 104(%rsp)
	jne	.L642
	movq	%rdx, 272(%rsp)
	movq	%rcx, 280(%rsp)
	movq	%rsi, 224(%rsp)
	movq	%rdi, 232(%rsp)
	movq	%r8, 240(%rsp)
	movq	%r9, 248(%rsp)
	movq	%rax, 256(%rsp)
	movq	%r10, 264(%rsp)
.L641:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	272(%rsp), %edi
	call	use_int@PLT
	movl	280(%rsp), %edi
	call	use_int@PLT
	movl	224(%rsp), %edi
	call	use_int@PLT
	movl	232(%rsp), %edi
	call	use_int@PLT
	movl	240(%rsp), %edi
	call	use_int@PLT
	movl	248(%rsp), %edi
	call	use_int@PLT
	movl	256(%rsp), %edi
	call	use_int@PLT
	movl	264(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$296, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE198:
	.size	int64_mul_13, .-int64_mul_13
	.globl	int64_mul_14
	.type	int64_mul_14, @function
int64_mul_14:
.LFB199:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$328, %rsp
	.cfi_def_cfa_offset 384
	movq	%rdi, %r8
	movl	12(%rsi), %eax
	leal	37421(%rax), %ecx
	movslq	%ecx, %rcx
	leal	7(%rax), %edx
	salq	$32, %rdx
	addq	%rcx, %rdx
	addl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, 8(%rsp)
	movq	%rbx, %rax
	imulq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	movq	%rax, %rdi
	subq	%rdx, %rax
	movq	%rax, 136(%rsp)
	movl	16(%rsi), %eax
	leal	37420(%rax), %ecx
	movslq	%ecx, %rcx
	leal	6(%rax), %edx
	salq	$32, %rdx
	addq	%rcx, %rdx
	addl	$4, %eax
	movslq	%eax, %rcx
	movq	%rcx, 16(%rsp)
	movq	%rcx, %rax
	imulq	%rdx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	movq	%rax, %r15
	subq	%rdx, %rax
	movq	%rax, 144(%rsp)
	movl	20(%rsi), %eax
	leal	37419(%rax), %ecx
	movslq	%ecx, %rcx
	leal	5(%rax), %edx
	salq	$32, %rdx
	addq	%rcx, %rdx
	addl	$3, %eax
	movslq	%eax, %rbx
	movq	%rbx, 24(%rsp)
	movq	%rbx, %rax
	imulq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	movq	%rax, %r14
	subq	%rdx, %rax
	movq	%rax, 152(%rsp)
	movl	24(%rsi), %edx
	leal	37418(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	4(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	addl	$2, %edx
	movslq	%edx, %rcx
	movq	%rcx, 32(%rsp)
	movq	%rcx, %rdx
	imulq	%rax, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	movq	%rdx, %r13
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 160(%rsp)
	movl	28(%rsi), %edx
	leal	37417(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	3(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	addl	$1, %edx
	movslq	%edx, %rbx
	movq	%rbx, 40(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %r12
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 168(%rsp)
	movl	32(%rsi), %edx
	leal	37416(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	2(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	movslq	%edx, %rcx
	movq	%rcx, 48(%rsp)
	movq	%rcx, %rdx
	imulq	%rax, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	movq	%rdx, %rbp
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 176(%rsp)
	movl	36(%rsi), %edx
	leal	37415(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	1(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$1, %edx
	movslq	%edx, %rbx
	movq	%rbx, 56(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %r9
	movq	%rdx, 296(%rsp)
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 184(%rsp)
	movl	40(%rsi), %edx
	leal	37414(%rdx), %ecx
	movslq	%ecx, %rcx
	movq	%rdx, %rax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$2, %edx
	movslq	%edx, %rcx
	movq	%rcx, 64(%rsp)
	movq	%rcx, %rdx
	imulq	%rax, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	movq	%rdx, %r10
	movq	%rdx, 304(%rsp)
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 192(%rsp)
	movl	44(%rsi), %edx
	leal	37413(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	-1(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$3, %edx
	movslq	%edx, %rbx
	movq	%rbx, 72(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %r11
	movq	%rdx, 312(%rsp)
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 200(%rsp)
	movl	48(%rsi), %edx
	leal	37412(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	-2(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$4, %edx
	movslq	%edx, %rbx
	movq	%rbx, 80(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, 256(%rsp)
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 208(%rsp)
	movl	52(%rsi), %edx
	leal	37411(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	-3(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$5, %edx
	movslq	%edx, %rbx
	movq	%rbx, 88(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %rbx
	movq	%rdx, 264(%rsp)
	subq	%rax, %rbx
	movq	%rbx, 216(%rsp)
	movl	56(%rsi), %eax
	leal	37410(%rax), %edx
	movslq	%edx, %rdx
	leal	-4(%rax), %ecx
	salq	$32, %rcx
	addq	%rcx, %rdx
	subl	$6, %eax
	movslq	%eax, %rbx
	movq	%rbx, 96(%rsp)
	movq	%rbx, %rax
	imulq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	movq	%rax, 272(%rsp)
	subq	%rdx, %rax
	movq	%rax, 224(%rsp)
	movl	60(%rsi), %eax
	leal	37409(%rax), %edx
	movslq	%edx, %rdx
	leal	-5(%rax), %ecx
	salq	$32, %rcx
	addq	%rcx, %rdx
	subl	$7, %eax
	movslq	%eax, %rbx
	movq	%rbx, 104(%rsp)
	movq	%rbx, %rax
	imulq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	movq	%rax, %rcx
	imulq	%rbx, %rcx
	movq	%rcx, 280(%rsp)
	subq	%rdx, %rcx
	movq	%rcx, 232(%rsp)
	movl	64(%rsi), %eax
	leal	37408(%rax), %edx
	movslq	%edx, %rdx
	leal	-6(%rax), %ecx
	salq	$32, %rcx
	addq	%rcx, %rdx
	subl	$8, %eax
	movslq	%eax, %rbx
	movq	%rbx, 112(%rsp)
	movq	%rbx, %rax
	imulq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	movq	%rax, 288(%rsp)
	movq	%rax, %rbx
	subq	%rdx, %rbx
	movq	%rbx, 240(%rsp)
	movl	68(%rsi), %eax
	leal	37407(%rax), %edx
	movslq	%edx, %rdx
	leal	-7(%rax), %ecx
	salq	$32, %rcx
	addq	%rcx, %rdx
	subl	$9, %eax
	cltq
	movq	%rax, 120(%rsp)
	movq	%rax, %rbx
	imulq	%rdx, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	imulq	%rax, %rbx
	movq	%rbx, %rsi
	subq	%rdx, %rsi
	movq	%rsi, 248(%rsp)
	leaq	-1(%r8), %rsi
	movq	%rsi, 128(%rsp)
	testq	%r8, %r8
	je	.L646
	movq	%r9, %rax
	movq	%r10, %rdx
	movq	%r11, %rcx
	movq	256(%rsp), %rsi
	movq	264(%rsp), %r8
	movq	272(%rsp), %r9
	movq	280(%rsp), %r10
	movq	288(%rsp), %r11
.L647:
	subq	136(%rsp), %rdi
	subq	144(%rsp), %r15
	subq	152(%rsp), %r14
	subq	160(%rsp), %r13
	subq	168(%rsp), %r12
	subq	176(%rsp), %rbp
	subq	184(%rsp), %rax
	subq	192(%rsp), %rdx
	subq	200(%rsp), %rcx
	subq	208(%rsp), %rsi
	subq	216(%rsp), %r8
	subq	224(%rsp), %r9
	subq	232(%rsp), %r10
	subq	240(%rsp), %r11
	subq	248(%rsp), %rbx
	imulq	8(%rsp), %rdi
	imulq	16(%rsp), %r15
	imulq	24(%rsp), %r14
	imulq	32(%rsp), %r13
	imulq	40(%rsp), %r12
	imulq	48(%rsp), %rbp
	imulq	56(%rsp), %rax
	imulq	64(%rsp), %rdx
	imulq	72(%rsp), %rcx
	imulq	80(%rsp), %rsi
	imulq	88(%rsp), %r8
	imulq	96(%rsp), %r9
	imulq	104(%rsp), %r10
	imulq	112(%rsp), %r11
	imulq	120(%rsp), %rbx
	imulq	8(%rsp), %rdi
	imulq	16(%rsp), %r15
	imulq	24(%rsp), %r14
	imulq	32(%rsp), %r13
	imulq	40(%rsp), %r12
	imulq	48(%rsp), %rbp
	imulq	56(%rsp), %rax
	imulq	64(%rsp), %rdx
	imulq	72(%rsp), %rcx
	imulq	80(%rsp), %rsi
	imulq	88(%rsp), %r8
	imulq	96(%rsp), %r9
	imulq	104(%rsp), %r10
	imulq	112(%rsp), %r11
	imulq	120(%rsp), %rbx
	imulq	8(%rsp), %rdi
	imulq	16(%rsp), %r15
	imulq	24(%rsp), %r14
	imulq	32(%rsp), %r13
	imulq	40(%rsp), %r12
	imulq	48(%rsp), %rbp
	imulq	56(%rsp), %rax
	imulq	64(%rsp), %rdx
	imulq	72(%rsp), %rcx
	imulq	80(%rsp), %rsi
	imulq	88(%rsp), %r8
	imulq	96(%rsp), %r9
	imulq	104(%rsp), %r10
	imulq	112(%rsp), %r11
	imulq	120(%rsp), %rbx
	imulq	8(%rsp), %rdi
	imulq	16(%rsp), %r15
	imulq	24(%rsp), %r14
	imulq	32(%rsp), %r13
	imulq	40(%rsp), %r12
	imulq	48(%rsp), %rbp
	imulq	56(%rsp), %rax
	imulq	64(%rsp), %rdx
	imulq	72(%rsp), %rcx
	imulq	80(%rsp), %rsi
	imulq	88(%rsp), %r8
	imulq	96(%rsp), %r9
	imulq	104(%rsp), %r10
	imulq	112(%rsp), %r11
	imulq	120(%rsp), %rbx
	imulq	8(%rsp), %rdi
	imulq	16(%rsp), %r15
	imulq	24(%rsp), %r14
	imulq	32(%rsp), %r13
	imulq	40(%rsp), %r12
	imulq	48(%rsp), %rbp
	imulq	56(%rsp), %rax
	imulq	64(%rsp), %rdx
	imulq	72(%rsp), %rcx
	imulq	80(%rsp), %rsi
	imulq	88(%rsp), %r8
	imulq	96(%rsp), %r9
	imulq	104(%rsp), %r10
	imulq	112(%rsp), %r11
	imulq	120(%rsp), %rbx
	imulq	8(%rsp), %rdi
	imulq	16(%rsp), %r15
	imulq	24(%rsp), %r14
	imulq	32(%rsp), %r13
	imulq	40(%rsp), %r12
	imulq	48(%rsp), %rbp
	imulq	56(%rsp), %rax
	imulq	64(%rsp), %rdx
	imulq	72(%rsp), %rcx
	imulq	80(%rsp), %rsi
	imulq	88(%rsp), %r8
	imulq	96(%rsp), %r9
	imulq	104(%rsp), %r10
	imulq	112(%rsp), %r11
	imulq	120(%rsp), %rbx
	imulq	8(%rsp), %rdi
	imulq	16(%rsp), %r15
	imulq	24(%rsp), %r14
	imulq	32(%rsp), %r13
	imulq	40(%rsp), %r12
	imulq	48(%rsp), %rbp
	imulq	56(%rsp), %rax
	imulq	64(%rsp), %rdx
	imulq	72(%rsp), %rcx
	imulq	80(%rsp), %rsi
	imulq	88(%rsp), %r8
	imulq	96(%rsp), %r9
	imulq	104(%rsp), %r10
	imulq	112(%rsp), %r11
	imulq	120(%rsp), %rbx
	imulq	8(%rsp), %rdi
	imulq	16(%rsp), %r15
	imulq	24(%rsp), %r14
	imulq	32(%rsp), %r13
	imulq	40(%rsp), %r12
	imulq	48(%rsp), %rbp
	imulq	56(%rsp), %rax
	imulq	64(%rsp), %rdx
	imulq	72(%rsp), %rcx
	imulq	80(%rsp), %rsi
	imulq	88(%rsp), %r8
	imulq	96(%rsp), %r9
	imulq	104(%rsp), %r10
	imulq	112(%rsp), %r11
	imulq	120(%rsp), %rbx
	imulq	8(%rsp), %rdi
	imulq	16(%rsp), %r15
	imulq	24(%rsp), %r14
	imulq	32(%rsp), %r13
	imulq	40(%rsp), %r12
	imulq	48(%rsp), %rbp
	imulq	56(%rsp), %rax
	imulq	64(%rsp), %rdx
	imulq	72(%rsp), %rcx
	imulq	80(%rsp), %rsi
	imulq	88(%rsp), %r8
	imulq	96(%rsp), %r9
	imulq	104(%rsp), %r10
	imulq	112(%rsp), %r11
	imulq	120(%rsp), %rbx
	imulq	8(%rsp), %rdi
	imulq	16(%rsp), %r15
	imulq	24(%rsp), %r14
	imulq	32(%rsp), %r13
	imulq	40(%rsp), %r12
	imulq	48(%rsp), %rbp
	imulq	56(%rsp), %rax
	imulq	64(%rsp), %rdx
	imulq	72(%rsp), %rcx
	imulq	80(%rsp), %rsi
	imulq	88(%rsp), %r8
	imulq	96(%rsp), %r9
	imulq	104(%rsp), %r10
	imulq	112(%rsp), %r11
	imulq	120(%rsp), %rbx
	subq	$1, 128(%rsp)
	cmpq	$-1, 128(%rsp)
	jne	.L647
	movq	%rax, 296(%rsp)
	movq	%rdx, 304(%rsp)
	movq	%rcx, 312(%rsp)
	movq	%rsi, 256(%rsp)
	movq	%r8, 264(%rsp)
	movq	%r9, 272(%rsp)
	movq	%r10, 280(%rsp)
	movq	%r11, 288(%rsp)
.L646:
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	296(%rsp), %edi
	call	use_int@PLT
	movl	304(%rsp), %edi
	call	use_int@PLT
	movl	312(%rsp), %edi
	call	use_int@PLT
	movl	256(%rsp), %edi
	call	use_int@PLT
	movl	264(%rsp), %edi
	call	use_int@PLT
	movl	272(%rsp), %edi
	call	use_int@PLT
	movl	280(%rsp), %edi
	call	use_int@PLT
	movl	288(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$328, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE199:
	.size	int64_mul_14, .-int64_mul_14
	.globl	int64_mul_15
	.type	int64_mul_15, @function
int64_mul_15:
.LFB200:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$360, %rsp
	.cfi_def_cfa_offset 416
	movl	12(%rsi), %eax
	leal	37421(%rax), %ecx
	movslq	%ecx, %rcx
	leal	7(%rax), %edx
	salq	$32, %rdx
	addq	%rcx, %rdx
	addl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, 8(%rsp)
	movq	%rbx, %rax
	imulq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	movq	%rax, %r15
	subq	%rdx, %rax
	movq	%rax, 168(%rsp)
	movl	16(%rsi), %eax
	leal	37420(%rax), %ecx
	movslq	%ecx, %rcx
	leal	6(%rax), %edx
	salq	$32, %rdx
	addq	%rcx, %rdx
	addl	$4, %eax
	movslq	%eax, %rbx
	movq	%rbx, 16(%rsp)
	movq	%rbx, %rax
	imulq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	movq	%rax, %r14
	subq	%rdx, %rax
	movq	%rax, 176(%rsp)
	movl	20(%rsi), %eax
	leal	37419(%rax), %ecx
	movslq	%ecx, %rcx
	leal	5(%rax), %edx
	salq	$32, %rdx
	addq	%rcx, %rdx
	addl	$3, %eax
	movslq	%eax, %rcx
	movq	%rcx, 24(%rsp)
	movq	%rcx, %rax
	imulq	%rdx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	movq	%rax, %r13
	subq	%rdx, %rax
	movq	%rax, 184(%rsp)
	movl	24(%rsi), %edx
	leal	37418(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	4(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	addl	$2, %edx
	movslq	%edx, %rbx
	movq	%rbx, 32(%rsp)
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	imulq	%rbx, %rdx
	movq	%rdx, %r12
	movq	%rdx, %rbx
	subq	%rax, %rbx
	movq	%rbx, 192(%rsp)
	movl	28(%rsi), %edx
	leal	37417(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	3(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	addl	$1, %edx
	movslq	%edx, %rcx
	movq	%rcx, 40(%rsp)
	movq	%rcx, %rdx
	imulq	%rax, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	movq	%rdx, %rbp
	movq	%rdx, %rbx
	subq	%rax, %rbx
	movq	%rbx, 200(%rsp)
	movl	32(%rsi), %edx
	leal	37416(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	2(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	movslq	%edx, %rcx
	movq	%rcx, 48(%rsp)
	movq	%rcx, %rdx
	imulq	%rax, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	movq	%rdx, %rbx
	imulq	%rcx, %rbx
	movq	%rbx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 208(%rsp)
	movl	36(%rsi), %edx
	leal	37415(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	1(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$1, %edx
	movslq	%edx, %rcx
	movq	%rcx, 56(%rsp)
	movq	%rcx, %rdx
	imulq	%rax, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	movq	%rdx, %r11
	movq	%rdx, 320(%rsp)
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 216(%rsp)
	movl	40(%rsi), %edx
	leal	37414(%rdx), %ecx
	movslq	%ecx, %rcx
	movq	%rdx, %rax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$2, %edx
	movslq	%edx, %rcx
	movq	%rcx, 64(%rsp)
	movq	%rcx, %rdx
	imulq	%rax, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	movq	%rdx, %r10
	movq	%rdx, 328(%rsp)
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 224(%rsp)
	movl	44(%rsi), %edx
	leal	37413(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	-1(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$3, %edx
	movslq	%edx, %rcx
	movq	%rcx, 72(%rsp)
	movq	%rcx, %rdx
	imulq	%rax, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	movq	%rdx, %r9
	movq	%rdx, 336(%rsp)
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 232(%rsp)
	movl	48(%rsi), %edx
	leal	37412(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	-2(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$4, %edx
	movslq	%edx, %rcx
	movq	%rcx, 80(%rsp)
	movq	%rcx, %rdx
	imulq	%rax, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	movq	%rdx, %r8
	movq	%rdx, 344(%rsp)
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 240(%rsp)
	movl	52(%rsi), %edx
	leal	37411(%rdx), %ecx
	movslq	%ecx, %rcx
	leal	-3(%rdx), %eax
	salq	$32, %rax
	addq	%rcx, %rax
	subl	$5, %edx
	movslq	%edx, %rcx
	movq	%rcx, 88(%rsp)
	movq	%rcx, %rdx
	imulq	%rax, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	imulq	%rcx, %rdx
	movq	%rdx, 160(%rsp)
	movq	%rdx, %rcx
	subq	%rax, %rcx
	movq	%rcx, 248(%rsp)
	movl	56(%rsi), %eax
	leal	37410(%rax), %edx
	movslq	%edx, %rdx
	leal	-4(%rax), %ecx
	salq	$32, %rcx
	addq	%rcx, %rdx
	subl	$6, %eax
	movslq	%eax, %rcx
	movq	%rcx, 96(%rsp)
	movq	%rcx, %rax
	imulq	%rdx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	movq	%rax, 296(%rsp)
	subq	%rdx, %rax
	movq	%rax, 256(%rsp)
	movl	60(%rsi), %eax
	leal	37409(%rax), %edx
	movslq	%edx, %rdx
	leal	-5(%rax), %ecx
	salq	$32, %rcx
	addq	%rcx, %rdx
	subl	$7, %eax
	movslq	%eax, %rcx
	movq	%rcx, 104(%rsp)
	movq	%rcx, %rax
	imulq	%rdx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	movq	%rax, %rcx
	movq	%rax, 304(%rsp)
	subq	%rdx, %rcx
	movq	%rcx, 264(%rsp)
	movl	64(%rsi), %eax
	leal	37408(%rax), %edx
	movslq	%edx, %rdx
	leal	-6(%rax), %ecx
	salq	$32, %rcx
	addq	%rcx, %rdx
	subl	$8, %eax
	movslq	%eax, %rcx
	movq	%rcx, 112(%rsp)
	movq	%rcx, %rax
	imulq	%rdx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	movq	%rax, 312(%rsp)
	subq	%rdx, %rax
	movq	%rax, 272(%rsp)
	movl	68(%rsi), %eax
	leal	37407(%rax), %edx
	movslq	%edx, %rdx
	leal	-7(%rax), %ecx
	salq	$32, %rcx
	addq	%rcx, %rdx
	subl	$9, %eax
	movslq	%eax, %rcx
	movq	%rcx, 120(%rsp)
	movq	%rcx, %rax
	imulq	%rdx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	movq	%rax, %rcx
	movq	%rax, 144(%rsp)
	subq	%rdx, %rcx
	movq	%rcx, 280(%rsp)
	movl	72(%rsi), %eax
	leal	37406(%rax), %edx
	movslq	%edx, %rdx
	leal	-8(%rax), %ecx
	salq	$32, %rcx
	addq	%rcx, %rdx
	subl	$10, %eax
	movslq	%eax, %rsi
	movq	%rsi, 128(%rsp)
	movq	%rsi, %rax
	imulq	%rdx, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	movq	%rax, %rsi
	movq	%rax, 152(%rsp)
	subq	%rdx, %rsi
	movq	%rsi, 288(%rsp)
	leaq	-1(%rdi), %rsi
	movq	%rsi, 136(%rsp)
	testq	%rdi, %rdi
	je	.L651
	movq	160(%rsp), %rdi
	movq	296(%rsp), %rsi
	movq	304(%rsp), %rcx
	movq	312(%rsp), %rdx
.L652:
	subq	168(%rsp), %r15
	subq	176(%rsp), %r14
	subq	184(%rsp), %r13
	subq	192(%rsp), %r12
	subq	200(%rsp), %rbp
	subq	208(%rsp), %rbx
	subq	216(%rsp), %r11
	subq	224(%rsp), %r10
	subq	232(%rsp), %r9
	subq	240(%rsp), %r8
	subq	248(%rsp), %rdi
	subq	256(%rsp), %rsi
	subq	264(%rsp), %rcx
	subq	272(%rsp), %rdx
	movq	%rdx, 160(%rsp)
	movq	144(%rsp), %rax
	subq	280(%rsp), %rax
	movq	152(%rsp), %rdx
	subq	288(%rsp), %rdx
	imulq	8(%rsp), %r15
	imulq	16(%rsp), %r14
	imulq	24(%rsp), %r13
	imulq	32(%rsp), %r12
	imulq	40(%rsp), %rbp
	imulq	48(%rsp), %rbx
	imulq	56(%rsp), %r11
	imulq	64(%rsp), %r10
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r8
	imulq	88(%rsp), %rdi
	imulq	96(%rsp), %rsi
	movq	%rsi, 144(%rsp)
	imulq	104(%rsp), %rcx
	movq	160(%rsp), %rsi
	imulq	112(%rsp), %rsi
	movq	%rsi, 152(%rsp)
	imulq	120(%rsp), %rax
	imulq	128(%rsp), %rdx
	movq	%rdx, 160(%rsp)
	imulq	8(%rsp), %r15
	imulq	16(%rsp), %r14
	imulq	24(%rsp), %r13
	imulq	32(%rsp), %r12
	imulq	40(%rsp), %rbp
	imulq	48(%rsp), %rbx
	imulq	56(%rsp), %r11
	imulq	64(%rsp), %r10
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r8
	imulq	88(%rsp), %rdi
	movq	144(%rsp), %rsi
	imulq	96(%rsp), %rsi
	imulq	104(%rsp), %rcx
	movq	%rcx, 144(%rsp)
	movq	152(%rsp), %rcx
	imulq	112(%rsp), %rcx
	movq	%rcx, %rdx
	imulq	120(%rsp), %rax
	movq	%rax, 152(%rsp)
	movq	160(%rsp), %rax
	imulq	128(%rsp), %rax
	movq	%rax, 160(%rsp)
	imulq	8(%rsp), %r15
	imulq	16(%rsp), %r14
	imulq	24(%rsp), %r13
	imulq	32(%rsp), %r12
	imulq	40(%rsp), %rbp
	imulq	48(%rsp), %rbx
	imulq	56(%rsp), %r11
	imulq	64(%rsp), %r10
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r8
	imulq	88(%rsp), %rdi
	imulq	96(%rsp), %rsi
	movq	144(%rsp), %rcx
	imulq	104(%rsp), %rcx
	imulq	112(%rsp), %rdx
	movq	152(%rsp), %rax
	imulq	120(%rsp), %rax
	movq	%rax, 144(%rsp)
	movq	160(%rsp), %rax
	imulq	128(%rsp), %rax
	movq	%rax, 152(%rsp)
	imulq	8(%rsp), %r15
	imulq	16(%rsp), %r14
	imulq	24(%rsp), %r13
	imulq	32(%rsp), %r12
	imulq	40(%rsp), %rbp
	imulq	48(%rsp), %rbx
	imulq	56(%rsp), %r11
	imulq	64(%rsp), %r10
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r8
	imulq	88(%rsp), %rdi
	imulq	96(%rsp), %rsi
	imulq	104(%rsp), %rcx
	imulq	112(%rsp), %rdx
	movq	144(%rsp), %rax
	imulq	120(%rsp), %rax
	movq	%rax, 144(%rsp)
	movq	152(%rsp), %rax
	imulq	128(%rsp), %rax
	movq	%rax, 152(%rsp)
	imulq	8(%rsp), %r15
	imulq	16(%rsp), %r14
	imulq	24(%rsp), %r13
	imulq	32(%rsp), %r12
	imulq	40(%rsp), %rbp
	imulq	48(%rsp), %rbx
	imulq	56(%rsp), %r11
	imulq	64(%rsp), %r10
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r8
	imulq	88(%rsp), %rdi
	imulq	96(%rsp), %rsi
	imulq	104(%rsp), %rcx
	imulq	112(%rsp), %rdx
	movq	144(%rsp), %rax
	imulq	120(%rsp), %rax
	movq	%rax, 144(%rsp)
	movq	152(%rsp), %rax
	imulq	128(%rsp), %rax
	movq	%rax, 152(%rsp)
	imulq	8(%rsp), %r15
	imulq	16(%rsp), %r14
	imulq	24(%rsp), %r13
	imulq	32(%rsp), %r12
	imulq	40(%rsp), %rbp
	imulq	48(%rsp), %rbx
	imulq	56(%rsp), %r11
	imulq	64(%rsp), %r10
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r8
	imulq	88(%rsp), %rdi
	imulq	96(%rsp), %rsi
	imulq	104(%rsp), %rcx
	imulq	112(%rsp), %rdx
	movq	144(%rsp), %rax
	imulq	120(%rsp), %rax
	movq	%rax, 144(%rsp)
	movq	152(%rsp), %rax
	imulq	128(%rsp), %rax
	movq	%rax, 152(%rsp)
	imulq	8(%rsp), %r15
	imulq	16(%rsp), %r14
	imulq	24(%rsp), %r13
	imulq	32(%rsp), %r12
	imulq	40(%rsp), %rbp
	imulq	48(%rsp), %rbx
	imulq	56(%rsp), %r11
	imulq	64(%rsp), %r10
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r8
	imulq	88(%rsp), %rdi
	imulq	96(%rsp), %rsi
	imulq	104(%rsp), %rcx
	imulq	112(%rsp), %rdx
	movq	144(%rsp), %rax
	imulq	120(%rsp), %rax
	movq	%rax, 144(%rsp)
	movq	152(%rsp), %rax
	imulq	128(%rsp), %rax
	movq	%rax, 152(%rsp)
	imulq	8(%rsp), %r15
	imulq	16(%rsp), %r14
	imulq	24(%rsp), %r13
	imulq	32(%rsp), %r12
	imulq	40(%rsp), %rbp
	imulq	48(%rsp), %rbx
	imulq	56(%rsp), %r11
	imulq	64(%rsp), %r10
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r8
	imulq	88(%rsp), %rdi
	imulq	96(%rsp), %rsi
	imulq	104(%rsp), %rcx
	imulq	112(%rsp), %rdx
	movq	144(%rsp), %rax
	imulq	120(%rsp), %rax
	movq	%rax, 144(%rsp)
	movq	152(%rsp), %rax
	imulq	128(%rsp), %rax
	movq	%rax, 152(%rsp)
	imulq	8(%rsp), %r15
	imulq	16(%rsp), %r14
	imulq	24(%rsp), %r13
	imulq	32(%rsp), %r12
	imulq	40(%rsp), %rbp
	imulq	48(%rsp), %rbx
	imulq	56(%rsp), %r11
	imulq	64(%rsp), %r10
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r8
	imulq	88(%rsp), %rdi
	imulq	96(%rsp), %rsi
	imulq	104(%rsp), %rcx
	imulq	112(%rsp), %rdx
	movq	144(%rsp), %rax
	imulq	120(%rsp), %rax
	movq	%rax, 144(%rsp)
	movq	152(%rsp), %rax
	imulq	128(%rsp), %rax
	movq	%rax, 152(%rsp)
	imulq	8(%rsp), %r15
	imulq	16(%rsp), %r14
	imulq	24(%rsp), %r13
	imulq	32(%rsp), %r12
	imulq	40(%rsp), %rbp
	imulq	48(%rsp), %rbx
	imulq	56(%rsp), %r11
	imulq	64(%rsp), %r10
	imulq	72(%rsp), %r9
	imulq	80(%rsp), %r8
	imulq	88(%rsp), %rdi
	imulq	96(%rsp), %rsi
	imulq	104(%rsp), %rcx
	imulq	112(%rsp), %rdx
	movq	144(%rsp), %rax
	imulq	120(%rsp), %rax
	movq	%rax, 144(%rsp)
	movq	152(%rsp), %rax
	imulq	128(%rsp), %rax
	movq	%rax, 152(%rsp)
	subq	$1, 136(%rsp)
	movq	136(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L652
	movq	%r11, 320(%rsp)
	movq	%r10, 328(%rsp)
	movq	%r9, 336(%rsp)
	movq	%r8, 344(%rsp)
	movq	%rdi, 160(%rsp)
	movq	%rsi, 296(%rsp)
	movq	%rcx, 304(%rsp)
	movq	%rdx, 312(%rsp)
.L651:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	320(%rsp), %edi
	call	use_int@PLT
	movl	328(%rsp), %edi
	call	use_int@PLT
	movl	336(%rsp), %edi
	call	use_int@PLT
	movl	344(%rsp), %edi
	call	use_int@PLT
	movl	160(%rsp), %edi
	call	use_int@PLT
	movl	296(%rsp), %edi
	call	use_int@PLT
	movl	304(%rsp), %edi
	call	use_int@PLT
	movl	312(%rsp), %edi
	call	use_int@PLT
	movl	144(%rsp), %edi
	call	use_int@PLT
	movl	152(%rsp), %edi
	call	use_int@PLT
	addq	$360, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE200:
	.size	int64_mul_15, .-int64_mul_15
	.globl	int64_div_0
	.type	int64_div_0, @function
int64_div_0:
.LFB201:
	.cfi_startproc
	endbr64
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movl	12(%rsi), %ecx
	addl	$37, %ecx
	movslq	%ecx, %rax
	salq	$33, %rcx
	addq	%rax, %rcx
	movq	%rcx, %rsi
	addq	$17, %rcx
	salq	$13, %rcx
	leaq	-1(%rdi), %r8
	testq	%rdi, %rdi
	je	.L656
.L657:
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	subq	$1, %r8
	cmpq	$-1, %r8
	jne	.L657
.L656:
	movl	%esi, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE201:
	.size	int64_div_0, .-int64_div_0
	.globl	int64_div_1
	.type	int64_div_1, @function
int64_div_1:
.LFB202:
	.cfi_startproc
	endbr64
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movq	%rdi, %rax
	movl	12(%rsi), %ebx
	leal	37(%rbx), %r8d
	movslq	%r8d, %rdx
	salq	$33, %r8
	addq	%rdx, %r8
	movq	%r8, %rdi
	addq	$17, %r8
	salq	$13, %r8
	movl	16(%rsi), %ecx
	addl	$36, %ecx
	movslq	%ecx, %rdx
	salq	$33, %rcx
	addq	%rdx, %rcx
	movq	%rcx, %rbx
	addq	$17, %rcx
	salq	$13, %rcx
	leaq	-1(%rax), %rsi
	testq	%rax, %rax
	je	.L661
.L662:
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rax, %r9
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rax, %rbx
	subq	$1, %rsi
	cmpq	$-1, %rsi
	jne	.L662
.L661:
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE202:
	.size	int64_div_1, .-int64_div_1
	.globl	int64_div_2
	.type	int64_div_2, @function
int64_div_2:
.LFB203:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	pushq	%rbx
	.cfi_def_cfa_offset 24
	.cfi_offset 3, -24
	subq	$8, %rsp
	.cfi_def_cfa_offset 32
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ebx
	leal	37(%rbx), %r8d
	movslq	%r8d, %rcx
	salq	$33, %r8
	addq	%rcx, %r8
	movq	%r8, %rdi
	addq	$17, %r8
	salq	$13, %r8
	movl	16(%rsi), %ebx
	leal	36(%rbx), %esi
	movslq	%esi, %rcx
	salq	$33, %rsi
	addq	%rcx, %rsi
	movq	%rsi, %rbp
	addq	$17, %rsi
	salq	$13, %rsi
	movl	20(%rdx), %ecx
	addl	$35, %ecx
	movslq	%ecx, %rdx
	salq	$33, %rcx
	addq	%rdx, %rcx
	movq	%rcx, %rbx
	addq	$17, %rcx
	salq	$13, %rcx
	leaq	-1(%rax), %r9
	testq	%rax, %rax
	je	.L666
.L667:
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rsi, %rax
	cqto
	idivq	%rbp
	movq	%rax, %r11
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rax, %r10
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rsi, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rsi, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rsi, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rsi, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rsi, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rsi, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rsi, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rsi, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%r8, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rsi, %rax
	cqto
	idivq	%r11
	movq	%rax, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rax, %rbx
	subq	$1, %r9
	cmpq	$-1, %r9
	jne	.L667
.L666:
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 24
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE203:
	.size	int64_div_2, .-int64_div_2
	.globl	int64_div_3
	.type	int64_div_3, @function
int64_div_3:
.LFB204:
	.cfi_startproc
	endbr64
	pushq	%r13
	.cfi_def_cfa_offset 16
	.cfi_offset 13, -16
	pushq	%r12
	.cfi_def_cfa_offset 24
	.cfi_offset 12, -24
	pushq	%rbp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset 3, -40
	subq	$8, %rsp
	.cfi_def_cfa_offset 48
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %esi
	leal	37(%rsi), %r9d
	movslq	%r9d, %rcx
	salq	$33, %r9
	addq	%rcx, %r9
	movq	%r9, %rdi
	addq	$17, %r9
	salq	$13, %r9
	movl	16(%rdx), %esi
	leal	36(%rsi), %r8d
	movslq	%r8d, %rcx
	salq	$33, %r8
	addq	%rcx, %r8
	movq	%r8, %r12
	addq	$17, %r8
	salq	$13, %r8
	movl	20(%rdx), %esi
	addl	$35, %esi
	movslq	%esi, %rcx
	salq	$33, %rsi
	addq	%rcx, %rsi
	movq	%rsi, %rbp
	addq	$17, %rsi
	salq	$13, %rsi
	movl	24(%rdx), %ecx
	addl	$34, %ecx
	movslq	%ecx, %rdx
	salq	$33, %rcx
	addq	%rdx, %rcx
	movq	%rcx, %rbx
	addq	$17, %rcx
	salq	$13, %rcx
	leaq	-1(%rax), %r10
	testq	%rax, %rax
	je	.L671
.L672:
	movq	%r9, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r8, %rax
	cqto
	idivq	%r12
	movq	%rax, %r13
	movq	%rsi, %rax
	cqto
	idivq	%rbp
	movq	%rax, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rax, %r11
	movq	%r9, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r8, %rax
	cqto
	idivq	%r13
	movq	%rax, %rbp
	movq	%rsi, %rax
	cqto
	idivq	%r12
	movq	%rax, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r9, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r8, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%rsi, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r9, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r8, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%rsi, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r9, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r8, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%rsi, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r9, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r8, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%rsi, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r9, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r8, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%rsi, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r9, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r8, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%rsi, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r9, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r8, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%rsi, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r9, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r8, %rax
	cqto
	idivq	%rbp
	movq	%rax, %r12
	movq	%rsi, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rax, %rbx
	subq	$1, %r10
	cmpq	$-1, %r10
	jne	.L672
.L671:
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%rbp
	.cfi_def_cfa_offset 24
	popq	%r12
	.cfi_def_cfa_offset 16
	popq	%r13
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE204:
	.size	int64_div_3, .-int64_div_3
	.globl	int64_div_4
	.type	int64_div_4, @function
int64_div_4:
.LFB205:
	.cfi_startproc
	endbr64
	pushq	%r13
	.cfi_def_cfa_offset 16
	.cfi_offset 13, -16
	pushq	%r12
	.cfi_def_cfa_offset 24
	.cfi_offset 12, -24
	pushq	%rbp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset 3, -40
	subq	$8, %rsp
	.cfi_def_cfa_offset 48
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %esi
	leal	37(%rsi), %r10d
	movslq	%r10d, %rcx
	salq	$33, %r10
	addq	%rcx, %r10
	movq	%r10, %rdi
	addq	$17, %r10
	salq	$13, %r10
	movl	16(%rdx), %esi
	leal	36(%rsi), %r9d
	movslq	%r9d, %rcx
	salq	$33, %r9
	addq	%rcx, %r9
	movq	%r9, %rbp
	addq	$17, %r9
	salq	$13, %r9
	movl	20(%rdx), %esi
	leal	35(%rsi), %r8d
	movslq	%r8d, %rcx
	salq	$33, %r8
	addq	%rcx, %r8
	movq	%r8, %rbx
	addq	$17, %r8
	salq	$13, %r8
	movl	24(%rdx), %esi
	addl	$34, %esi
	movslq	%esi, %rcx
	salq	$33, %rsi
	addq	%rcx, %rsi
	movq	%rsi, %r13
	addq	$17, %rsi
	salq	$13, %rsi
	movl	28(%rdx), %ecx
	addl	$33, %ecx
	movslq	%ecx, %rdx
	salq	$33, %rcx
	addq	%rdx, %rcx
	movq	%rcx, %r12
	addq	$17, %rcx
	salq	$13, %rcx
	leaq	-1(%rax), %r11
	testq	%rax, %rax
	je	.L676
.L677:
	movq	%r10, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r9, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r8, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%rsi, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r10, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r9, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r8, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%rsi, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r10, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r9, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r8, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%rsi, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r10, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r9, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r8, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%rsi, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r10, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r9, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r8, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%rsi, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r10, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r9, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r8, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%rsi, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r10, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r9, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r8, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%rsi, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r10, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r9, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r8, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%rsi, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r10, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r9, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r8, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%rsi, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r10, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r9, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r8, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%rsi, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	subq	$1, %r11
	cmpq	$-1, %r11
	jne	.L677
.L676:
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%rbp
	.cfi_def_cfa_offset 24
	popq	%r12
	.cfi_def_cfa_offset 16
	popq	%r13
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE205:
	.size	int64_div_4, .-int64_div_4
	.globl	int64_div_5
	.type	int64_div_5, @function
int64_div_5:
.LFB206:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$8, %rsp
	.cfi_def_cfa_offset 64
	movq	%rdi, %rax
	movq	%rsi, %rdx
	movl	12(%rsi), %ebx
	leal	37(%rbx), %r11d
	movslq	%r11d, %rcx
	salq	$33, %r11
	addq	%rcx, %r11
	movq	%r11, %rdi
	addq	$17, %r11
	salq	$13, %r11
	movl	16(%rsi), %ebx
	leal	36(%rbx), %r10d
	movslq	%r10d, %rcx
	salq	$33, %r10
	addq	%rcx, %r10
	movq	%r10, %r15
	addq	$17, %r10
	salq	$13, %r10
	movl	20(%rsi), %ebx
	leal	35(%rbx), %r9d
	movslq	%r9d, %rcx
	salq	$33, %r9
	addq	%rcx, %r9
	movq	%r9, %r14
	addq	$17, %r9
	salq	$13, %r9
	movl	24(%rsi), %ebx
	leal	34(%rbx), %r8d
	movslq	%r8d, %rcx
	salq	$33, %r8
	addq	%rcx, %r8
	movq	%r8, %r13
	addq	$17, %r8
	salq	$13, %r8
	movl	28(%rsi), %ebx
	leal	33(%rbx), %esi
	movslq	%esi, %rcx
	salq	$33, %rsi
	addq	%rcx, %rsi
	movq	%rsi, %r12
	addq	$17, %rsi
	salq	$13, %rsi
	movl	32(%rdx), %ecx
	addl	$32, %ecx
	movslq	%ecx, %rdx
	salq	$33, %rcx
	addq	%rdx, %rcx
	movq	%rcx, %rbp
	addq	$17, %rcx
	salq	$13, %rcx
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	je	.L681
.L682:
	movq	%r11, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r10, %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	%r9, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	%r8, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rsi, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r11, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r10, %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	%r9, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	%r8, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rsi, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r11, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r10, %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	%r9, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	%r8, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rsi, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r11, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r10, %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	%r9, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	%r8, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rsi, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r11, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r10, %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	%r9, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	%r8, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rsi, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r11, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r10, %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	%r9, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	%r8, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rsi, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r11, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r10, %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	%r9, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	%r8, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rsi, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r11, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r10, %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	%r9, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	%r8, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rsi, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r11, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r10, %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	%r9, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	%r8, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rsi, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r11, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r10, %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	%r9, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	%r8, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%rsi, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	subq	$1, %rbx
	cmpq	$-1, %rbx
	jne	.L682
.L681:
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE206:
	.size	int64_div_5, .-int64_div_5
	.globl	int64_div_6
	.type	int64_div_6, @function
int64_div_6:
.LFB207:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$72, %rsp
	.cfi_def_cfa_offset 128
	movq	%rdi, %rdx
	movq	%rsi, %rcx
	movl	12(%rsi), %eax
	addl	$37, %eax
	movslq	%eax, %rsi
	salq	$33, %rax
	addq	%rsi, %rax
	movq	%rax, %rdi
	addq	$17, %rax
	salq	$13, %rax
	movq	%rax, (%rsp)
	movl	16(%rcx), %eax
	addl	$36, %eax
	movslq	%eax, %rsi
	salq	$33, %rax
	addq	%rsi, %rax
	movq	%rax, 16(%rsp)
	addq	$17, %rax
	movq	%rax, %rbx
	salq	$13, %rbx
	movl	20(%rcx), %esi
	leal	35(%rsi), %eax
	movslq	%eax, %rsi
	salq	$33, %rax
	addq	%rsi, %rax
	movq	%rax, 24(%rsp)
	addq	$17, %rax
	movq	%rax, %r15
	salq	$13, %r15
	movl	24(%rcx), %r14d
	leal	34(%r14), %eax
	movslq	%eax, %rsi
	salq	$33, %rax
	addq	%rsi, %rax
	movq	%rax, %r8
	movq	%rax, 32(%rsp)
	addq	$17, %rax
	movq	%rax, %r14
	salq	$13, %r14
	movl	28(%rcx), %r9d
	leal	33(%r9), %eax
	movslq	%eax, %rsi
	salq	$33, %rax
	addq	%rsi, %rax
	movq	%rax, %r9
	movq	%rax, 40(%rsp)
	addq	$17, %rax
	movq	%rax, %r13
	salq	$13, %r13
	movl	32(%rcx), %r10d
	leal	32(%r10), %eax
	movslq	%eax, %rsi
	salq	$33, %rax
	addq	%rsi, %rax
	movq	%rax, %r10
	movq	%rax, 48(%rsp)
	addq	$17, %rax
	movq	%rax, %r12
	salq	$13, %r12
	movl	36(%rcx), %eax
	addl	$31, %eax
	movslq	%eax, %rcx
	salq	$33, %rax
	addq	%rcx, %rax
	movq	%rax, %r11
	movq	%rax, 56(%rsp)
	addq	$17, %rax
	movq	%rax, %rbp
	salq	$13, %rbp
	leaq	-1(%rdx), %rcx
	movq	%rcx, 8(%rsp)
	testq	%rdx, %rdx
	je	.L686
	movq	16(%rsp), %rcx
	movq	24(%rsp), %rsi
.L687:
	movq	(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rbx, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%r15, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r14, %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	%r13, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r12, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%rbp, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rbx, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%r15, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r14, %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	%r13, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r12, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%rbp, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rbx, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%r15, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r14, %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	%r13, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r12, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%rbp, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rbx, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%r15, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r14, %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	%r13, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r12, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%rbp, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rbx, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%r15, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r14, %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	%r13, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r12, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%rbp, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rbx, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%r15, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r14, %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	%r13, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r12, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%rbp, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rbx, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%r15, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r14, %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	%r13, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r12, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%rbp, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rbx, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%r15, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r14, %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	%r13, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r12, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%rbp, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rbx, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%r15, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r14, %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	%r13, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r12, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%rbp, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%rbx, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%r15, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r14, %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	%r13, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r12, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%rbp, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	subq	$1, 8(%rsp)
	movq	8(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L687
	movq	%rcx, 16(%rsp)
	movq	%rsi, 24(%rsp)
	movq	%r8, 32(%rsp)
	movq	%r9, 40(%rsp)
	movq	%r10, 48(%rsp)
	movq	%r11, 56(%rsp)
.L686:
	call	use_int@PLT
	movl	16(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE207:
	.size	int64_div_6, .-int64_div_6
	.globl	int64_div_7
	.type	int64_div_7, @function
int64_div_7:
.LFB208:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$56, %rsp
	.cfi_def_cfa_offset 112
	movq	%rdi, %r10
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	37(%rcx), %r9d
	movslq	%r9d, %rdx
	salq	$33, %r9
	addq	%rdx, %r9
	movq	%r9, %r15
	addq	$17, %r9
	salq	$13, %r9
	movl	16(%rsi), %ecx
	leal	36(%rcx), %r8d
	movslq	%r8d, %rdx
	salq	$33, %r8
	addq	%rdx, %r8
	movq	%r8, %r14
	addq	$17, %r8
	salq	$13, %r8
	movl	20(%rsi), %ecx
	leal	35(%rcx), %edi
	movslq	%edi, %rdx
	salq	$33, %rdi
	addq	%rdx, %rdi
	movq	%rdi, %r13
	addq	$17, %rdi
	salq	$13, %rdi
	movl	24(%rsi), %ecx
	leal	34(%rcx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r12
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, (%rsp)
	movl	28(%rsi), %ecx
	leal	33(%rcx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %rbp
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 8(%rsp)
	movl	32(%rsi), %esi
	leal	32(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 32(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rcx
	salq	$13, %rcx
	movq	%rcx, 16(%rsp)
	movl	36(%rax), %ecx
	leal	31(%rcx), %esi
	movslq	%esi, %rdx
	salq	$33, %rsi
	addq	%rdx, %rsi
	movq	%rsi, %rdx
	movq	%rsi, 40(%rsp)
	addq	$17, %rsi
	salq	$13, %rsi
	movl	40(%rax), %ecx
	addl	$30, %ecx
	movslq	%ecx, %rax
	salq	$33, %rcx
	addq	%rax, %rcx
	movq	%rcx, %rbx
	addq	$17, %rcx
	salq	$13, %rcx
	leaq	-1(%r10), %rax
	movq	%rax, 24(%rsp)
	testq	%r10, %r10
	je	.L691
	movq	%r15, 40(%rsp)
	movq	%rcx, %r15
	movq	%r14, %r11
	movq	%rsi, %r14
	movq	%r13, %r10
	movq	%rdi, %r13
	movq	%rbp, %rax
	movq	%r9, %rbp
	movq	%r12, %r9
	movq	%r8, %r12
	movq	32(%rsp), %rdi
	movq	%rdx, %rsi
	movq	%rax, %r8
	movq	40(%rsp), %rcx
.L692:
	movq	%rbp, %rax
	cqto
	idivq	%rcx
	movq	%rax, 32(%rsp)
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	8(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	16(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r14, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r15, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rcx
	movq	%rbp, %rax
	cqto
	idivq	32(%rsp)
	movq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	8(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	16(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r14, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r15, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%rbp, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	8(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	16(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r14, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r15, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%rbp, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	8(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	16(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r14, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r15, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%rbp, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	8(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	16(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r14, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r15, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%rbp, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	8(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	16(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r14, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r15, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%rbp, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	8(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	16(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r14, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r15, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%rbp, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	8(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	16(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r14, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r15, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%rbp, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	8(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	16(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r14, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r15, %rax
	cqto
	idivq	%rcx
	movq	%rax, 32(%rsp)
	movq	%rbp, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rcx
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	8(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	16(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r14, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	%r15, %rax
	cqto
	idivq	32(%rsp)
	movq	%rax, %rbx
	subq	$1, 24(%rsp)
	movq	24(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L692
	movq	%rcx, %r15
	movq	%r11, %r14
	movq	%r10, %r13
	movq	%r9, %r12
	movq	%r8, %rbp
	movq	%rdi, 32(%rsp)
	movq	%rsi, 40(%rsp)
.L691:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE208:
	.size	int64_div_7, .-int64_div_7
	.globl	int64_div_8
	.type	int64_div_8, @function
int64_div_8:
.LFB209:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$88, %rsp
	.cfi_def_cfa_offset 144
	movq	%rdi, %r9
	movq	%rsi, %rax
	movl	12(%rsi), %ebx
	leal	37(%rbx), %r8d
	movslq	%r8d, %rdx
	salq	$33, %r8
	addq	%rdx, %r8
	movq	%r8, %r15
	addq	$17, %r8
	salq	$13, %r8
	movl	16(%rsi), %ebx
	leal	36(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r14
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, (%rsp)
	movl	20(%rsi), %ebx
	leal	35(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r13
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 8(%rsp)
	movl	24(%rsi), %ebx
	leal	34(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r12
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 16(%rsp)
	movl	28(%rsi), %ebx
	leal	33(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %rbp
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 24(%rsp)
	movl	32(%rsi), %ebx
	leal	32(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 56(%rsp)
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 32(%rsp)
	movl	36(%rsi), %ebx
	leal	31(%rbx), %edi
	movslq	%edi, %rdx
	salq	$33, %rdi
	addq	%rdx, %rdi
	movq	%rdi, 64(%rsp)
	addq	$17, %rdi
	salq	$13, %rdi
	movl	40(%rsi), %ebx
	leal	30(%rbx), %esi
	movslq	%esi, %rdx
	salq	$33, %rsi
	addq	%rdx, %rsi
	movq	%rsi, %rdx
	movq	%rsi, 40(%rsp)
	addq	$17, %rsi
	salq	$13, %rsi
	movl	44(%rax), %ecx
	addl	$29, %ecx
	movslq	%ecx, %rax
	salq	$33, %rcx
	addq	%rax, %rcx
	movq	%rcx, %rbx
	addq	$17, %rcx
	salq	$13, %rcx
	leaq	-1(%r9), %rax
	movq	%rax, 48(%rsp)
	testq	%r9, %r9
	je	.L696
	movq	%rcx, 40(%rsp)
	movq	%r15, %rcx
	movq	%rsi, %r15
	movq	%r14, %r11
	movq	%rdi, %r14
	movq	%r13, %rax
	movq	%r8, %r13
	movq	%rdx, %rsi
	movq	%rax, %r8
.L697:
	movq	%r13, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, 72(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r9
	movq	16(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r8
	movq	24(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rdi
	movq	32(%rsp), %rax
	cqto
	idivq	56(%rsp)
	movq	%rax, %rbp
	movq	%r14, %rax
	cqto
	idivq	64(%rsp)
	movq	%rax, %r11
	movq	%r15, %rax
	cqto
	idivq	%rsi
	movq	%rax, %r10
	movq	40(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rsi
	movq	%r13, %rax
	cqto
	idivq	%rcx
	movq	%rax, %rbx
	movq	(%rsp), %rax
	cqto
	idivq	72(%rsp)
	movq	%rax, %rcx
	movq	8(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	16(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	24(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	32(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r14, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r15, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	40(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %r12
	movq	%r13, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rsi
	movq	(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	8(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	16(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	24(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rbx
	movq	32(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r14, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r15, %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	40(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %rdi
	movq	%r13, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	8(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	16(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r12
	movq	24(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	32(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r14, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	%r15, %rax
	cqto
	idivq	%r10
	movq	%rax, %r8
	movq	40(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r13, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	8(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r10
	movq	16(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	24(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	32(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	%r14, %rax
	cqto
	idivq	%r11
	movq	%rax, %r9
	movq	%r15, %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	40(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r13, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %r11
	movq	8(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	16(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	24(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	32(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rcx
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r15, %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	40(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r13, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rbp
	movq	(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	8(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	16(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	24(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rsi
	movq	32(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r15, %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	40(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	8(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	16(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %rdi
	movq	24(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	32(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r15, %rax
	cqto
	idivq	%r8
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	8(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r8
	movq	16(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	24(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	32(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, 56(%rsp)
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rax, %r10
	movq	40(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rcx
	movq	(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	8(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	16(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %r12
	movq	24(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rbp
	movq	32(%rsp), %rax
	cqto
	idivq	56(%rsp)
	movq	%rax, 56(%rsp)
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rax, 64(%rsp)
	movq	%r15, %rax
	cqto
	idivq	%r10
	movq	%rax, %rsi
	movq	40(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	subq	$1, 48(%rsp)
	movq	48(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L697
	movq	%rcx, %r15
	movq	%r11, %r14
	movq	%r8, %r13
	movq	%rsi, 40(%rsp)
.L696:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE209:
	.size	int64_div_8, .-int64_div_8
	.globl	int64_div_9
	.type	int64_div_9, @function
int64_div_9:
.LFB210:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$120, %rsp
	.cfi_def_cfa_offset 176
	movq	%rdi, %r8
	movq	%rsi, %rax
	movl	12(%rsi), %ecx
	leal	37(%rcx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r15
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 8(%rsp)
	movl	16(%rsi), %esi
	leal	36(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r14
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 16(%rsp)
	movl	20(%rax), %ecx
	leal	35(%rcx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r13
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 24(%rsp)
	movl	24(%rax), %esi
	leal	34(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r12
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 32(%rsp)
	movl	28(%rax), %ecx
	leal	33(%rcx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %rbp
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 40(%rsp)
	movl	32(%rax), %esi
	leal	32(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 80(%rsp)
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 48(%rsp)
	movl	36(%rax), %ecx
	leal	31(%rcx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 88(%rsp)
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 56(%rsp)
	movl	40(%rax), %esi
	leal	30(%rsi), %edi
	movslq	%edi, %rdx
	salq	$33, %rdi
	addq	%rdx, %rdi
	movq	%rdi, 96(%rsp)
	addq	$17, %rdi
	salq	$13, %rdi
	movl	44(%rax), %ecx
	leal	29(%rcx), %esi
	movslq	%esi, %rdx
	salq	$33, %rsi
	addq	%rdx, %rsi
	movq	%rsi, %rdx
	movq	%rsi, 64(%rsp)
	addq	$17, %rsi
	salq	$13, %rsi
	movl	48(%rax), %ecx
	addl	$28, %ecx
	movslq	%ecx, %rax
	salq	$33, %rcx
	addq	%rax, %rcx
	movq	%rcx, %rbx
	addq	$17, %rcx
	salq	$13, %rcx
	leaq	-1(%r8), %rax
	movq	%rax, 72(%rsp)
	testq	%r8, %r8
	je	.L701
	movq	%rcx, 64(%rsp)
	movq	%r15, %rcx
	movq	%rsi, %r15
	movq	%r14, %r9
	movq	%rdi, %r14
	movq	%rdx, %r8
.L702:
	movq	8(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, 104(%rsp)
	movq	16(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %rdi
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %rsi
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %rcx
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	80(%rsp)
	movq	%rax, %r11
	movq	56(%rsp), %rax
	cqto
	idivq	88(%rsp)
	movq	%rax, %r10
	movq	%r14, %rax
	cqto
	idivq	96(%rsp)
	movq	%rax, %r9
	movq	%r15, %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	64(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	104(%rsp)
	movq	%rax, %r12
	movq	16(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	24(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	32(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	56(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r15, %rax
	cqto
	idivq	%r8
	movq	%rax, %r13
	movq	64(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r8
	movq	16(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	24(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	32(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	56(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r15, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	64(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rcx
	movq	8(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	16(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	24(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rbx
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	56(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	%r15, %rax
	cqto
	idivq	%r13
	movq	%rax, %rsi
	movq	64(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	8(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	16(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %r13
	movq	24(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	56(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rax, %rdi
	movq	%r15, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	64(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	8(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r9
	movq	16(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	24(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	56(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r8
	movq	%r14, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r15, %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	64(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %r10
	movq	8(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	16(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	24(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %rcx
	movq	56(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	%r14, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	%r15, %rax
	cqto
	idivq	%rsi
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	8(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	16(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	24(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rsi
	movq	48(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	56(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	%r14, %rax
	cqto
	idivq	%rdi
	movq	%rax, %rbp
	movq	%r15, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	8(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	16(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	24(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %rdi
	movq	40(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	48(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, 80(%rsp)
	movq	56(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	%r14, %rax
	cqto
	idivq	%rbp
	movq	%rax, 96(%rsp)
	movq	%r15, %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	8(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %rcx
	movq	16(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r9
	movq	24(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	80(%rsp)
	movq	%rax, 80(%rsp)
	movq	56(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, 88(%rsp)
	movq	%r14, %rax
	cqto
	idivq	96(%rsp)
	movq	%rax, 96(%rsp)
	movq	%r15, %rax
	cqto
	idivq	%r11
	movq	%rax, %r8
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %rbx
	subq	$1, 72(%rsp)
	movq	72(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L702
	movq	%rcx, %r15
	movq	%r9, %r14
	movq	%r8, 64(%rsp)
.L701:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	call	use_int@PLT
	movl	96(%rsp), %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$120, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE210:
	.size	int64_div_9, .-int64_div_9
	.globl	int64_div_10
	.type	int64_div_10, @function
int64_div_10:
.LFB211:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$136, %rsp
	.cfi_def_cfa_offset 192
	movq	%rsi, %rax
	movl	12(%rsi), %esi
	leal	37(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r15
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 8(%rsp)
	movl	16(%rax), %esi
	leal	36(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r14
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 16(%rsp)
	movl	20(%rax), %esi
	leal	35(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r13
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 24(%rsp)
	movl	24(%rax), %esi
	leal	34(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r12
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 32(%rsp)
	movl	28(%rax), %esi
	leal	33(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %rbp
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 40(%rsp)
	movl	32(%rax), %esi
	leal	32(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 104(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 48(%rsp)
	movl	36(%rax), %esi
	leal	31(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r10
	movq	%rdx, 112(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rcx
	salq	$13, %rcx
	movq	%rcx, 56(%rsp)
	movl	40(%rax), %esi
	leal	30(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r9
	movq	%rdx, 120(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rbx
	salq	$13, %rbx
	movq	%rbx, 64(%rsp)
	movl	44(%rax), %esi
	leal	29(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 88(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rbx
	salq	$13, %rbx
	movq	%rbx, 72(%rsp)
	movl	48(%rax), %ecx
	leal	28(%rcx), %esi
	movslq	%esi, %rdx
	salq	$33, %rsi
	addq	%rdx, %rsi
	movq	%rsi, 96(%rsp)
	addq	$17, %rsi
	salq	$13, %rsi
	movl	52(%rax), %ecx
	addl	$27, %ecx
	movslq	%ecx, %rax
	salq	$33, %rcx
	addq	%rax, %rcx
	movq	%rcx, %rbx
	addq	$17, %rcx
	salq	$13, %rcx
	leaq	-1(%rdi), %rax
	movq	%rax, 80(%rsp)
	testq	%rdi, %rdi
	je	.L706
	movq	%r15, %r8
	movq	%rcx, %r15
	movq	%r14, %r11
	movq	%rsi, %r14
	movq	104(%rsp), %rdi
	movq	%r10, %rsi
	movq	%r9, %rcx
.L707:
	movq	8(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, 104(%rsp)
	movq	16(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r10
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r9
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %r8
	movq	48(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	56(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	64(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	72(%rsp), %rax
	cqto
	idivq	88(%rsp)
	movq	%rax, %r13
	movq	%r14, %rax
	cqto
	idivq	96(%rsp)
	movq	%rax, %r12
	movq	%r15, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbp
	movq	8(%rsp), %rax
	cqto
	idivq	104(%rsp)
	movq	%rax, %rbx
	movq	16(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	24(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	32(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	40(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	48(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	56(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	64(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	72(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%r14, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r15, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	8(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	16(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	24(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	32(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	40(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	48(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	56(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	64(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	72(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%r14, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r15, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	8(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	16(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	24(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	32(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	40(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	48(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	56(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	64(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	72(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%r14, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r15, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	8(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	16(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	24(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	32(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	40(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	48(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	56(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	64(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	72(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%r14, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r15, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	8(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	16(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	24(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	32(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	40(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	48(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	56(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	64(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	72(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%r14, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r15, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	8(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	16(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	24(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	32(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	40(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	48(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	56(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	64(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	72(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%r14, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r15, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	8(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	16(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	24(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	32(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	40(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	48(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	56(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	64(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	72(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%r14, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r15, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	8(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	16(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	24(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	32(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	40(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, 88(%rsp)
	movq	48(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	56(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	64(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	72(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, 96(%rsp)
	movq	%r14, %rax
	cqto
	idivq	%r12
	movq	%rax, 104(%rsp)
	movq	%r15, %rax
	cqto
	idivq	%rbp
	movq	%rax, 112(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %r8
	movq	16(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	24(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	88(%rsp)
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	56(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	64(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	72(%rsp), %rax
	cqto
	idivq	96(%rsp)
	movq	%rax, 88(%rsp)
	movq	%r14, %rax
	cqto
	idivq	104(%rsp)
	movq	%rax, 96(%rsp)
	movq	%r15, %rax
	cqto
	idivq	112(%rsp)
	movq	%rax, %rbx
	subq	$1, 80(%rsp)
	movq	80(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L707
	movq	%r8, %r15
	movq	%r11, %r14
	movq	%rdi, 104(%rsp)
	movq	%rsi, 112(%rsp)
	movq	%rcx, 120(%rsp)
.L706:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	104(%rsp), %edi
	call	use_int@PLT
	movl	112(%rsp), %edi
	call	use_int@PLT
	movl	120(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	call	use_int@PLT
	movl	96(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$136, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE211:
	.size	int64_div_10, .-int64_div_10
	.globl	int64_div_11
	.type	int64_div_11, @function
int64_div_11:
.LFB212:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$152, %rsp
	.cfi_def_cfa_offset 208
	movq	%rsi, %rax
	movl	12(%rsi), %esi
	leal	37(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r15
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, (%rsp)
	movl	16(%rax), %esi
	leal	36(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r14
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 8(%rsp)
	movl	20(%rax), %esi
	leal	35(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r13
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 16(%rsp)
	movl	24(%rax), %esi
	leal	34(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r12
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 24(%rsp)
	movl	28(%rax), %esi
	leal	33(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %rbp
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 32(%rsp)
	movl	32(%rax), %esi
	leal	32(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r10
	movq	%rdx, 120(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 40(%rsp)
	movl	36(%rax), %esi
	leal	31(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %rsi
	movq	%rdx, 128(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rcx
	salq	$13, %rcx
	movq	%rcx, 48(%rsp)
	movl	40(%rax), %ecx
	leal	30(%rcx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r9
	movq	%rdx, 136(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rbx
	salq	$13, %rbx
	movq	%rbx, 56(%rsp)
	movl	44(%rax), %ecx
	leal	29(%rcx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 96(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rbx
	salq	$13, %rbx
	movq	%rbx, 64(%rsp)
	movl	48(%rax), %ecx
	leal	28(%rcx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 104(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rbx
	salq	$13, %rbx
	movq	%rbx, 72(%rsp)
	movl	52(%rax), %ecx
	leal	27(%rcx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 112(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rbx
	salq	$13, %rbx
	movq	%rbx, 80(%rsp)
	movl	56(%rax), %ecx
	addl	$26, %ecx
	movslq	%ecx, %rax
	salq	$33, %rcx
	addq	%rax, %rcx
	movq	%rcx, %rbx
	addq	$17, %rcx
	salq	$13, %rcx
	leaq	-1(%rdi), %rax
	movq	%rax, 88(%rsp)
	testq	%rdi, %rdi
	je	.L711
	movq	%r15, %r8
	movq	%rcx, %r15
	movq	%r10, %rdi
	movq	%r9, %rcx
.L712:
	movq	(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, 120(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r11
	movq	16(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r10
	movq	24(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r9
	movq	32(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %r8
	movq	40(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	48(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	56(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	64(%rsp), %rax
	cqto
	idivq	96(%rsp)
	movq	%rax, %r14
	movq	72(%rsp), %rax
	cqto
	idivq	104(%rsp)
	movq	%rax, %r13
	movq	80(%rsp), %rax
	cqto
	idivq	112(%rsp)
	movq	%rax, %r12
	movq	%r15, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbp
	movq	(%rsp), %rax
	cqto
	idivq	120(%rsp)
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	16(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	24(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	32(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	40(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	48(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	56(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	64(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	72(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	80(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r15, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	16(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	24(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	32(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	40(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	48(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	56(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	64(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	72(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	80(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r15, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	16(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	24(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	32(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	40(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	48(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	56(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	64(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	72(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	80(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r15, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	16(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	24(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	32(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	40(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	48(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	56(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	64(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	72(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	80(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r15, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	16(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	24(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	32(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	40(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	48(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	56(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	64(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	72(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	80(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r15, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	16(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	24(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	32(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	40(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	48(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	56(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	64(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	72(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	80(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r15, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	16(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	24(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	32(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	40(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	48(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	56(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	64(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	72(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	80(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r15, %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	16(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	24(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	32(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, 96(%rsp)
	movq	40(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	48(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	56(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	64(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, 104(%rsp)
	movq	72(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, 112(%rsp)
	movq	80(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, 120(%rsp)
	movq	%r15, %rax
	cqto
	idivq	%rbp
	movq	%rax, 128(%rsp)
	movq	(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %r8
	movq	8(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r14
	movq	16(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r13
	movq	24(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r12
	movq	32(%rsp), %rax
	cqto
	idivq	96(%rsp)
	movq	%rax, %rbp
	movq	40(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	48(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	56(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	64(%rsp), %rax
	cqto
	idivq	104(%rsp)
	movq	%rax, 96(%rsp)
	movq	72(%rsp), %rax
	cqto
	idivq	112(%rsp)
	movq	%rax, 104(%rsp)
	movq	80(%rsp), %rax
	cqto
	idivq	120(%rsp)
	movq	%rax, 112(%rsp)
	movq	%r15, %rax
	cqto
	idivq	128(%rsp)
	movq	%rax, %rbx
	subq	$1, 88(%rsp)
	movq	88(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L712
	movq	%r8, %r15
	movq	%rdi, 120(%rsp)
	movq	%rsi, 128(%rsp)
	movq	%rcx, 136(%rsp)
.L711:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	120(%rsp), %edi
	call	use_int@PLT
	movl	128(%rsp), %edi
	call	use_int@PLT
	movl	136(%rsp), %edi
	call	use_int@PLT
	movl	96(%rsp), %edi
	call	use_int@PLT
	movl	104(%rsp), %edi
	call	use_int@PLT
	movl	112(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$152, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE212:
	.size	int64_div_11, .-int64_div_11
	.globl	int64_div_12
	.type	int64_div_12, @function
int64_div_12:
.LFB213:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$184, %rsp
	.cfi_def_cfa_offset 240
	movq	%rsi, %rax
	movl	12(%rsi), %esi
	leal	37(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r15
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 8(%rsp)
	movl	16(%rax), %ebx
	leal	36(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r14
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 16(%rsp)
	movl	20(%rax), %esi
	leal	35(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r13
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 24(%rsp)
	movl	24(%rax), %ebx
	leal	34(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r12
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 32(%rsp)
	movl	28(%rax), %esi
	leal	33(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %rbp
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 40(%rsp)
	movl	32(%rax), %ebx
	leal	32(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r11
	movq	%rdx, 128(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 48(%rsp)
	movl	36(%rax), %esi
	leal	31(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r10
	movq	%rdx, 136(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rbx
	salq	$13, %rbx
	movq	%rbx, 56(%rsp)
	movl	40(%rax), %ebx
	leal	30(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r9
	movq	%rdx, 144(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 64(%rsp)
	movl	44(%rax), %esi
	leal	29(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r8
	movq	%rdx, 152(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rbx
	salq	$13, %rbx
	movq	%rbx, 72(%rsp)
	movl	48(%rax), %ebx
	leal	28(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 120(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 80(%rsp)
	movl	52(%rax), %esi
	leal	27(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %rsi
	movq	%rdx, 160(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rbx
	salq	$13, %rbx
	movq	%rbx, 88(%rsp)
	movl	56(%rax), %ebx
	leal	26(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %rcx
	movq	%rdx, 168(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rbx
	salq	$13, %rbx
	movq	%rbx, 96(%rsp)
	movl	60(%rax), %eax
	addl	$25, %eax
	movslq	%eax, %rdx
	salq	$33, %rax
	addq	%rdx, %rax
	movq	%rax, %rbx
	addq	$17, %rax
	salq	$13, %rax
	movq	%rax, 104(%rsp)
	leaq	-1(%rdi), %rax
	movq	%rax, 112(%rsp)
	testq	%rdi, %rdi
	je	.L716
	movq	120(%rsp), %rdi
.L717:
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	56(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	64(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	72(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	80(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	88(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	96(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	104(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	56(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	64(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	72(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	80(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	88(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	96(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	104(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	56(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	64(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	72(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	80(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	88(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	96(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	104(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	56(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	64(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	72(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	80(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	88(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	96(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	104(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	56(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	64(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	72(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	80(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	88(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	96(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	104(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	56(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	64(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	72(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	80(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	88(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	96(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	104(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	56(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	64(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	72(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	80(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	88(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	96(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	104(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	56(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	64(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	72(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	80(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	88(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	96(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	104(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	56(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	64(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	72(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	80(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	88(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	96(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	104(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	56(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	64(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	72(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	80(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	88(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	96(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	104(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	subq	$1, 112(%rsp)
	movq	112(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L717
	movq	%r11, 128(%rsp)
	movq	%r10, 136(%rsp)
	movq	%r9, 144(%rsp)
	movq	%r8, 152(%rsp)
	movq	%rdi, 120(%rsp)
	movq	%rsi, 160(%rsp)
	movq	%rcx, 168(%rsp)
.L716:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	128(%rsp), %edi
	call	use_int@PLT
	movl	136(%rsp), %edi
	call	use_int@PLT
	movl	144(%rsp), %edi
	call	use_int@PLT
	movl	152(%rsp), %edi
	call	use_int@PLT
	movl	120(%rsp), %edi
	call	use_int@PLT
	movl	160(%rsp), %edi
	call	use_int@PLT
	movl	168(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$184, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE213:
	.size	int64_div_12, .-int64_div_12
	.globl	int64_div_13
	.type	int64_div_13, @function
int64_div_13:
.LFB214:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$200, %rsp
	.cfi_def_cfa_offset 256
	movq	%rsi, %rax
	movl	12(%rsi), %ebx
	leal	37(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r15
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 8(%rsp)
	movl	16(%rsi), %ebx
	leal	36(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r14
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 16(%rsp)
	movl	20(%rsi), %ebx
	leal	35(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r13
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 24(%rsp)
	movl	24(%rsi), %ebx
	leal	34(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r12
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 32(%rsp)
	movl	28(%rsi), %ebx
	leal	33(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %rbp
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 40(%rsp)
	movl	32(%rsi), %ebx
	leal	32(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %rbx
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 48(%rsp)
	movl	36(%rsi), %esi
	leal	31(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r11
	movq	%rdx, 152(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 56(%rsp)
	movl	40(%rax), %esi
	leal	30(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r10
	movq	%rdx, 160(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 64(%rsp)
	movl	44(%rax), %esi
	leal	29(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r9
	movq	%rdx, 168(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 72(%rsp)
	movl	48(%rax), %esi
	leal	28(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r8
	movq	%rdx, 176(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 80(%rsp)
	movl	52(%rax), %esi
	leal	27(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 136(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 88(%rsp)
	movl	56(%rax), %esi
	leal	26(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 144(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rcx
	salq	$13, %rcx
	movq	%rcx, 96(%rsp)
	movl	60(%rax), %ecx
	leal	25(%rcx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %rcx
	movq	%rdx, 184(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 104(%rsp)
	movl	64(%rax), %eax
	addl	$24, %eax
	movslq	%eax, %rdx
	salq	$33, %rax
	addq	%rdx, %rax
	movq	%rax, 128(%rsp)
	addq	$17, %rax
	salq	$13, %rax
	movq	%rax, 112(%rsp)
	leaq	-1(%rdi), %rax
	movq	%rax, 120(%rsp)
	testq	%rdi, %rdi
	je	.L721
	movq	136(%rsp), %rdi
	movq	144(%rsp), %rsi
.L722:
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	128(%rsp)
	movq	%rax, 128(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	128(%rsp)
	movq	%rax, 128(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	128(%rsp)
	movq	%rax, 128(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	128(%rsp)
	movq	%rax, 128(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	128(%rsp)
	movq	%rax, 128(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	128(%rsp)
	movq	%rax, 128(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	128(%rsp)
	movq	%rax, 128(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	128(%rsp)
	movq	%rax, 128(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	128(%rsp)
	movq	%rax, 128(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	128(%rsp)
	movq	%rax, 128(%rsp)
	subq	$1, 120(%rsp)
	movq	120(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L722
	movq	%r11, 152(%rsp)
	movq	%r10, 160(%rsp)
	movq	%r9, 168(%rsp)
	movq	%r8, 176(%rsp)
	movq	%rdi, 136(%rsp)
	movq	%rsi, 144(%rsp)
	movq	%rcx, 184(%rsp)
.L721:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	152(%rsp), %edi
	call	use_int@PLT
	movl	160(%rsp), %edi
	call	use_int@PLT
	movl	168(%rsp), %edi
	call	use_int@PLT
	movl	176(%rsp), %edi
	call	use_int@PLT
	movl	136(%rsp), %edi
	call	use_int@PLT
	movl	144(%rsp), %edi
	call	use_int@PLT
	movl	184(%rsp), %edi
	call	use_int@PLT
	movl	128(%rsp), %edi
	call	use_int@PLT
	addq	$200, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE214:
	.size	int64_div_13, .-int64_div_13
	.globl	int64_div_14
	.type	int64_div_14, @function
int64_div_14:
.LFB215:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$216, %rsp
	.cfi_def_cfa_offset 272
	movq	%rsi, %rax
	movl	12(%rsi), %ebx
	leal	37(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r15
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 8(%rsp)
	movl	16(%rsi), %ebx
	leal	36(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r14
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 16(%rsp)
	movl	20(%rsi), %ebx
	leal	35(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r13
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 24(%rsp)
	movl	24(%rsi), %ebx
	leal	34(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r12
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 32(%rsp)
	movl	28(%rsi), %ebx
	leal	33(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %rbp
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 40(%rsp)
	movl	32(%rsi), %ebx
	leal	32(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %rbx
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 48(%rsp)
	movl	36(%rsi), %esi
	leal	31(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r11
	movq	%rdx, 176(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 56(%rsp)
	movl	40(%rax), %esi
	leal	30(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r10
	movq	%rdx, 184(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 64(%rsp)
	movl	44(%rax), %esi
	leal	29(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r9
	movq	%rdx, 192(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 72(%rsp)
	movl	48(%rax), %esi
	leal	28(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r8
	movq	%rdx, 200(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 80(%rsp)
	movl	52(%rax), %ecx
	leal	27(%rcx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 152(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 88(%rsp)
	movl	56(%rax), %esi
	leal	26(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 160(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rcx
	salq	$13, %rcx
	movq	%rcx, 96(%rsp)
	movl	60(%rax), %ecx
	leal	25(%rcx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 168(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 104(%rsp)
	movl	64(%rax), %esi
	leal	24(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 136(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 112(%rsp)
	movl	68(%rax), %eax
	addl	$23, %eax
	movslq	%eax, %rdx
	salq	$33, %rax
	addq	%rdx, %rax
	movq	%rax, 144(%rsp)
	addq	$17, %rax
	salq	$13, %rax
	movq	%rax, 120(%rsp)
	leaq	-1(%rdi), %rax
	movq	%rax, 128(%rsp)
	testq	%rdi, %rdi
	je	.L726
	movq	152(%rsp), %rdi
	movq	160(%rsp), %rsi
	movq	168(%rsp), %rcx
.L727:
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	136(%rsp)
	movq	%rax, 136(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	136(%rsp)
	movq	%rax, 136(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	136(%rsp)
	movq	%rax, 136(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	136(%rsp)
	movq	%rax, 136(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	136(%rsp)
	movq	%rax, 136(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	136(%rsp)
	movq	%rax, 136(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	136(%rsp)
	movq	%rax, 136(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	136(%rsp)
	movq	%rax, 136(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	136(%rsp)
	movq	%rax, 136(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	136(%rsp)
	movq	%rax, 136(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	subq	$1, 128(%rsp)
	movq	128(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L727
	movq	%r11, 176(%rsp)
	movq	%r10, 184(%rsp)
	movq	%r9, 192(%rsp)
	movq	%r8, 200(%rsp)
	movq	%rdi, 152(%rsp)
	movq	%rsi, 160(%rsp)
	movq	%rcx, 168(%rsp)
.L726:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	176(%rsp), %edi
	call	use_int@PLT
	movl	184(%rsp), %edi
	call	use_int@PLT
	movl	192(%rsp), %edi
	call	use_int@PLT
	movl	200(%rsp), %edi
	call	use_int@PLT
	movl	152(%rsp), %edi
	call	use_int@PLT
	movl	160(%rsp), %edi
	call	use_int@PLT
	movl	168(%rsp), %edi
	call	use_int@PLT
	movl	136(%rsp), %edi
	call	use_int@PLT
	movl	144(%rsp), %edi
	call	use_int@PLT
	addq	$216, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE215:
	.size	int64_div_14, .-int64_div_14
	.globl	int64_div_15
	.type	int64_div_15, @function
int64_div_15:
.LFB216:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$232, %rsp
	.cfi_def_cfa_offset 288
	movq	%rsi, %rax
	movl	12(%rsi), %ebx
	leal	37(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r15
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 8(%rsp)
	movl	16(%rsi), %ebx
	leal	36(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r14
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 16(%rsp)
	movl	20(%rsi), %ebx
	leal	35(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r13
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 24(%rsp)
	movl	24(%rsi), %ebx
	leal	34(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r12
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 32(%rsp)
	movl	28(%rsi), %ebx
	leal	33(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %rbp
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 40(%rsp)
	movl	32(%rsi), %ebx
	leal	32(%rbx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %rbx
	addq	$17, %rdx
	salq	$13, %rdx
	movq	%rdx, 48(%rsp)
	movl	36(%rsi), %esi
	leal	31(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r11
	movq	%rdx, 192(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 56(%rsp)
	movl	40(%rax), %esi
	leal	30(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r10
	movq	%rdx, 200(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 64(%rsp)
	movl	44(%rax), %esi
	leal	29(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r9
	movq	%rdx, 208(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 72(%rsp)
	movl	48(%rax), %ecx
	leal	28(%rcx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, %r8
	movq	%rdx, 216(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 80(%rsp)
	movl	52(%rax), %esi
	leal	27(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 168(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 88(%rsp)
	movl	56(%rax), %ecx
	leal	26(%rcx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 176(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rcx
	salq	$13, %rcx
	movq	%rcx, 96(%rsp)
	movl	60(%rax), %ecx
	leal	25(%rcx), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 184(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 104(%rsp)
	movl	64(%rax), %esi
	leal	24(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 144(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 112(%rsp)
	movl	68(%rax), %esi
	leal	23(%rsi), %edx
	movslq	%edx, %rcx
	salq	$33, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 152(%rsp)
	addq	$17, %rdx
	movq	%rdx, %rsi
	salq	$13, %rsi
	movq	%rsi, 120(%rsp)
	movl	72(%rax), %eax
	addl	$22, %eax
	movslq	%eax, %rdx
	salq	$33, %rax
	addq	%rdx, %rax
	movq	%rax, 160(%rsp)
	addq	$17, %rax
	salq	$13, %rax
	movq	%rax, 128(%rsp)
	leaq	-1(%rdi), %rax
	movq	%rax, 136(%rsp)
	testq	%rdi, %rdi
	je	.L731
	movq	168(%rsp), %rdi
	movq	176(%rsp), %rsi
	movq	184(%rsp), %rcx
.L732:
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	152(%rsp)
	movq	%rax, 152(%rsp)
	movq	128(%rsp), %rax
	cqto
	idivq	160(%rsp)
	movq	%rax, 160(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	152(%rsp)
	movq	%rax, 152(%rsp)
	movq	128(%rsp), %rax
	cqto
	idivq	160(%rsp)
	movq	%rax, 160(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	152(%rsp)
	movq	%rax, 152(%rsp)
	movq	128(%rsp), %rax
	cqto
	idivq	160(%rsp)
	movq	%rax, 160(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	152(%rsp)
	movq	%rax, 152(%rsp)
	movq	128(%rsp), %rax
	cqto
	idivq	160(%rsp)
	movq	%rax, 160(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	152(%rsp)
	movq	%rax, 152(%rsp)
	movq	128(%rsp), %rax
	cqto
	idivq	160(%rsp)
	movq	%rax, 160(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	152(%rsp)
	movq	%rax, 152(%rsp)
	movq	128(%rsp), %rax
	cqto
	idivq	160(%rsp)
	movq	%rax, 160(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	152(%rsp)
	movq	%rax, 152(%rsp)
	movq	128(%rsp), %rax
	cqto
	idivq	160(%rsp)
	movq	%rax, 160(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	152(%rsp)
	movq	%rax, 152(%rsp)
	movq	128(%rsp), %rax
	cqto
	idivq	160(%rsp)
	movq	%rax, 160(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	152(%rsp)
	movq	%rax, 152(%rsp)
	movq	128(%rsp), %rax
	cqto
	idivq	160(%rsp)
	movq	%rax, 160(%rsp)
	movq	8(%rsp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	16(%rsp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	24(%rsp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	32(%rsp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	40(%rsp), %rax
	cqto
	idivq	%rbp
	movq	%rax, %rbp
	movq	48(%rsp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	56(%rsp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	64(%rsp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	72(%rsp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	80(%rsp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	88(%rsp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	96(%rsp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	104(%rsp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	112(%rsp), %rax
	cqto
	idivq	144(%rsp)
	movq	%rax, 144(%rsp)
	movq	120(%rsp), %rax
	cqto
	idivq	152(%rsp)
	movq	%rax, 152(%rsp)
	movq	128(%rsp), %rax
	cqto
	idivq	160(%rsp)
	movq	%rax, 160(%rsp)
	subq	$1, 136(%rsp)
	movq	136(%rsp), %rax
	cmpq	$-1, %rax
	jne	.L732
	movq	%r11, 192(%rsp)
	movq	%r10, 200(%rsp)
	movq	%r9, 208(%rsp)
	movq	%r8, 216(%rsp)
	movq	%rdi, 168(%rsp)
	movq	%rsi, 176(%rsp)
	movq	%rcx, 184(%rsp)
.L731:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	192(%rsp), %edi
	call	use_int@PLT
	movl	200(%rsp), %edi
	call	use_int@PLT
	movl	208(%rsp), %edi
	call	use_int@PLT
	movl	216(%rsp), %edi
	call	use_int@PLT
	movl	168(%rsp), %edi
	call	use_int@PLT
	movl	176(%rsp), %edi
	call	use_int@PLT
	movl	184(%rsp), %edi
	call	use_int@PLT
	movl	144(%rsp), %edi
	call	use_int@PLT
	movl	152(%rsp), %edi
	call	use_int@PLT
	movl	160(%rsp), %edi
	call	use_int@PLT
	addq	$232, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE216:
	.size	int64_div_15, .-int64_div_15
	.globl	int64_mod_0
	.type	int64_mod_0, @function
int64_mod_0:
.LFB217:
	.cfi_startproc
	endbr64
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movq	%rdi, %r8
	movslq	12(%rsi), %rdi
	testq	%r8, %r8
	je	.L736
	movl	$0, %ecx
.L737:
	addq	$1, %rcx
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rdi
	xorq	%rsi, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rsi
	xorq	%rdi, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rdi
	xorq	%rsi, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rsi
	xorq	%rdi, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rdi
	xorq	%rsi, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rsi
	xorq	%rdi, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rdi
	xorq	%rsi, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rsi
	xorq	%rdi, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rdi
	xorq	%rsi, %rdi
	cmpq	%rcx, %r8
	jne	.L737
.L736:
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE217:
	.size	int64_mod_0, .-int64_mod_0
	.globl	int64_mod_1
	.type	int64_mod_1, @function
int64_mod_1:
.LFB218:
	.cfi_startproc
	endbr64
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movslq	12(%rsi), %r8
	movslq	16(%rsi), %rbx
	testq	%rdi, %rdi
	je	.L741
	movq	%rdi, %r9
	movl	$0, %ecx
.L742:
	addq	$1, %rcx
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rdi
	xorq	%rbx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %r8
	xorq	%rdi, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rdi
	xorq	%rsi, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %r8
	xorq	%rdi, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rdi
	xorq	%rsi, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %r8
	xorq	%rdi, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rdi
	xorq	%rsi, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %r8
	xorq	%rdi, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rdi
	xorq	%rsi, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %r8
	xorq	%rdi, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rdi
	xorq	%rsi, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %r8
	xorq	%rdi, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rbx
	xorq	%rsi, %rbx
	cmpq	%rcx, %r9
	jne	.L742
.L741:
	movl	%r8d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE218:
	.size	int64_mod_1, .-int64_mod_1
	.globl	int64_mod_2
	.type	int64_mod_2, @function
int64_mod_2:
.LFB219:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	pushq	%rbx
	.cfi_def_cfa_offset 24
	.cfi_offset 3, -24
	subq	$8, %rsp
	.cfi_def_cfa_offset 32
	movslq	12(%rsi), %r8
	movslq	16(%rsi), %rbp
	movslq	20(%rsi), %rbx
	testq	%rdi, %rdi
	je	.L746
	movq	%rdi, %r11
	movl	$0, %ecx
.L747:
	addq	$1, %rcx
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rdx, %r8
	xorq	%rbp, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rsi
	xorq	%rbx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rdi
	xorq	%r9, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %r9
	xorq	%r8, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %r8
	xorq	%rsi, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rsi
	xorq	%rdi, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rdi
	xorq	%r9, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %r9
	xorq	%r8, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %r8
	xorq	%rsi, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rsi
	xorq	%rdi, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rdi
	xorq	%r9, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %r9
	xorq	%r8, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %r8
	xorq	%rsi, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rsi
	xorq	%rdi, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rdi
	xorq	%r9, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %r9
	xorq	%r8, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %r8
	xorq	%rsi, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rsi
	xorq	%rdi, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rdi
	xorq	%r9, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %r9
	xorq	%r8, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %r8
	xorq	%rsi, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rsi
	xorq	%rdi, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rdi
	xorq	%r9, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %r10
	xorq	%r8, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %r9
	xorq	%rsi, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rsi
	xorq	%rdi, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rbx
	xorq	%rsi, %rbx
	cmpq	%rcx, %r11
	jne	.L747
.L746:
	movl	%r8d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 24
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE219:
	.size	int64_mod_2, .-int64_mod_2
	.globl	int64_mod_3
	.type	int64_mod_3, @function
int64_mod_3:
.LFB220:
	.cfi_startproc
	endbr64
	pushq	%r12
	.cfi_def_cfa_offset 16
	.cfi_offset 12, -16
	pushq	%rbp
	.cfi_def_cfa_offset 24
	.cfi_offset 6, -24
	pushq	%rbx
	.cfi_def_cfa_offset 32
	.cfi_offset 3, -32
	movslq	12(%rsi), %r8
	movslq	16(%rsi), %r12
	movslq	20(%rsi), %rbp
	movslq	24(%rsi), %rbx
	testq	%rdi, %rdi
	je	.L751
	movq	%rdi, %r11
	movl	$0, %ecx
.L752:
	addq	$1, %rcx
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rdi
	xorq	%r12, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rbp, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rbx, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rdi
	xorq	%r10, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rdi
	xorq	%r10, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rdi
	xorq	%r10, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rdi
	xorq	%r10, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rdi
	xorq	%r10, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rdi
	xorq	%r10, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rdx, %r8
	xorq	%r10, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rdx, %r12
	xorq	%r9, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rbp
	xorq	%rsi, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rbx
	xorq	%rdi, %rbx
	cmpq	%rcx, %r11
	jne	.L752
.L751:
	movl	%r8d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%rbp
	.cfi_def_cfa_offset 16
	popq	%r12
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE220:
	.size	int64_mod_3, .-int64_mod_3
	.globl	int64_mod_4
	.type	int64_mod_4, @function
int64_mod_4:
.LFB221:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	movslq	12(%rsi), %r8
	movslq	16(%rsi), %r13
	movslq	20(%rsi), %r12
	movslq	24(%rsi), %rbp
	movslq	28(%rsi), %r15
	testq	%rdi, %rdi
	je	.L756
	movq	%rdi, %r14
	movl	$0, %ecx
.L757:
	addq	$1, %rcx
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rsi
	xorq	%r13, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%r12, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rdx, %rdi
	xorq	%rbp, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%r15, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rdi
	xorq	%r11, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rdi
	xorq	%r11, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rdi
	xorq	%r11, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rdi
	xorq	%r11, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rdi
	xorq	%r11, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rdi
	xorq	%r11, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%r11, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	cmpq	%rcx, %r14
	jne	.L757
.L756:
	movl	%r8d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE221:
	.size	int64_mod_4, .-int64_mod_4
	.globl	int64_mod_5
	.type	int64_mod_5, @function
int64_mod_5:
.LFB222:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$8, %rsp
	.cfi_def_cfa_offset 64
	movslq	12(%rsi), %r8
	movslq	16(%rsi), %r14
	movslq	20(%rsi), %r13
	movslq	24(%rsi), %r12
	movslq	28(%rsi), %rbp
	movslq	32(%rsi), %rbx
	testq	%rdi, %rdi
	je	.L761
	movq	%rdi, %r15
	movl	$0, %ecx
.L762:
	addq	$1, %rcx
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%r14, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%r13, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rsi
	xorq	%r12, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rbp, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rdi
	xorq	%rbx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rbx
	xorq	%r11, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rdi
	xorq	%rbx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rbx
	xorq	%r11, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rdi
	xorq	%rbx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rbx
	xorq	%r11, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rdi
	xorq	%rbx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rbx
	xorq	%r11, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rdi
	xorq	%rbx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rbx
	xorq	%r11, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rdi
	xorq	%rbx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rbx
	xorq	%r11, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rdi
	xorq	%rbx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rbx
	xorq	%r11, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rbx, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%r11, %rdx
	movq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rbx
	xorq	%rdi, %rbx
	cmpq	%rcx, %r15
	jne	.L762
.L761:
	movl	%r8d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE222:
	.size	int64_mod_5, .-int64_mod_5
	.globl	int64_mod_6
	.type	int64_mod_6, @function
int64_mod_6:
.LFB223:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$24, %rsp
	.cfi_def_cfa_offset 80
	movq	%rdi, 8(%rsp)
	movslq	12(%rsi), %r8
	movslq	16(%rsi), %r15
	movslq	20(%rsi), %r14
	movslq	24(%rsi), %r13
	movslq	28(%rsi), %r12
	movslq	32(%rsi), %rbp
	movslq	36(%rsi), %rbx
	testq	%rdi, %rdi
	je	.L766
	movl	$0, %ecx
.L767:
	addq	$1, %rcx
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%r15, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rdx, %rsi
	xorq	%rbp, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rbx, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rdi
	xorq	%r8, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%r14, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%r13, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%r12, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rbx
	xorq	%rsi, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rsi
	xorq	%r11, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rdx, %rdi
	xorq	%rbp, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rbx, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rsi
	xorq	%r12, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rbx
	xorq	%rdi, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rdi
	xorq	%r11, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rbp, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rsi
	xorq	%rbx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rbx
	xorq	%rdi, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rdi
	xorq	%r11, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%r12, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rsi
	xorq	%rbx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rdx, %rdi
	xorq	%rbp, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rbx
	xorq	%r11, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rsi
	xorq	%r12, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rdi
	xorq	%rbx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rbx
	xorq	%r11, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rbp, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rdi
	xorq	%rbx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%r11, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rbx
	xorq	%r12, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rsi
	xorq	%r8, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rbp, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rbx, %rdx
	movq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%r11, %rdx
	movq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rbx
	xorq	%rdi, %rbx
	cmpq	%rcx, 8(%rsp)
	jne	.L767
.L766:
	movl	%r8d, %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE223:
	.size	int64_mod_6, .-int64_mod_6
	.globl	int64_mod_7
	.type	int64_mod_7, @function
int64_mod_7:
.LFB224:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$24, %rsp
	.cfi_def_cfa_offset 80
	movq	%rdi, 8(%rsp)
	movslq	12(%rsi), %r11
	movslq	16(%rsi), %r15
	movslq	20(%rsi), %r14
	movslq	24(%rsi), %r13
	movslq	28(%rsi), %r12
	movslq	32(%rsi), %rbp
	movslq	36(%rsi), %rbx
	movslq	40(%rsi), %rax
	movq	%rax, (%rsp)
	testq	%rdi, %rdi
	je	.L771
	movl	$0, %ecx
.L772:
	addq	$1, %rcx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rdi
	xorq	%r11, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%r15, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%r13, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%r12, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rsi
	xorq	%rbx, %rsi
	movq	%rcx, %rax
	movq	(%rsp), %rbx
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%r14, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rbp, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rdi
	xorq	%r12, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rsi
	xorq	%r11, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%r13, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rdi
	xorq	%rsi, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rsi
	xorq	%r11, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rbx, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rbx
	xorq	%rsi, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rdx, %rdi
	xorq	%rbp, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rsi
	xorq	%r11, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%r12, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rdi
	xorq	%rsi, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rsi
	xorq	%r11, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rbx, %rdx
	movq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rbx
	xorq	%r13, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rdi
	xorq	%rsi, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%r14, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rsi
	xorq	%rbx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rbx
	xorq	%r11, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rbp, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rsi
	xorq	%rbx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rbx
	xorq	%r11, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rdi
	xorq	%r13, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%r14, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%r12, %rdx
	movq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, (%rsp)
	cmpq	%rcx, 8(%rsp)
	jne	.L772
.L771:
	movl	%r11d, %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	(%rsp), %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE224:
	.size	int64_mod_7, .-int64_mod_7
	.globl	int64_mod_8
	.type	int64_mod_8, @function
int64_mod_8:
.LFB225:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$40, %rsp
	.cfi_def_cfa_offset 96
	movq	%rdi, 24(%rsp)
	movslq	12(%rsi), %r11
	movslq	16(%rsi), %r15
	movslq	20(%rsi), %r14
	movslq	24(%rsi), %r13
	movslq	28(%rsi), %rax
	movq	%rax, 8(%rsp)
	movslq	32(%rsi), %rax
	movq	%rax, 16(%rsp)
	movslq	36(%rsi), %rbx
	movslq	40(%rsi), %rbp
	movslq	44(%rsi), %r12
	testq	%rdi, %rdi
	je	.L776
	movl	$0, %ecx
.L777:
	addq	$1, %rcx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rdi
	xorq	%r11, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%r15, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%r13, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	movq	8(%rsp), %rsi
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rsi, %r9
	movq	%rcx, %rax
	movq	16(%rsp), %rsi
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%r14, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rdi
	xorq	%r11, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rsi
	xorq	%r13, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rdi
	xorq	%r12, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rsi
	xorq	%r9, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rbp, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rdi
	xorq	%r10, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rsi
	xorq	%rbx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rdx, %rbx
	xorq	%rbp, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%r12, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%r13, %rdx
	movq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%r11, %rdx
	movq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%r14, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rsi
	xorq	%r10, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%r13, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%r11, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rsi
	xorq	%r12, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rbp, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rsi
	xorq	%r11, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rdi
	xorq	%r12, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%r13, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%r14, %rdx
	movq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%r15, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rbx, %rdx
	movq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rbx
	xorq	%rdi, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	cmpq	%rcx, 24(%rsp)
	jne	.L777
.L776:
	movl	%r11d, %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE225:
	.size	int64_mod_8, .-int64_mod_8
	.globl	int64_mod_9
	.type	int64_mod_9, @function
int64_mod_9:
.LFB226:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$72, %rsp
	.cfi_def_cfa_offset 128
	movq	%rdi, 56(%rsp)
	movslq	12(%rsi), %r11
	movslq	16(%rsi), %r15
	movslq	20(%rsi), %rax
	movq	%rax, 8(%rsp)
	movslq	24(%rsi), %rax
	movq	%rax, 16(%rsp)
	movslq	28(%rsi), %rax
	movq	%rax, 24(%rsp)
	movslq	32(%rsi), %rbp
	movslq	36(%rsi), %rbx
	movslq	40(%rsi), %r12
	movslq	44(%rsi), %r13
	movslq	48(%rsi), %r14
	testq	%rdi, %rdi
	je	.L781
	movl	$0, %ecx
.L782:
	addq	$1, %rcx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rdi
	xorq	%r11, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	movq	8(%rsp), %rsi
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rsi, %r11
	movq	%rcx, %rax
	movq	16(%rsp), %rsi
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rsi, %r10
	movq	%rcx, %rax
	movq	24(%rsp), %rsi
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rsi, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r15
	movq	%rdx, %rsi
	xorq	%r15, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%r14, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rdi
	xorq	%r8, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%r13, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rsi
	xorq	%r9, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%r12, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rdi
	xorq	%r8, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rbx, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rsi
	xorq	%r9, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rbx
	xorq	%rdi, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rdi
	xorq	%r13, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%r14, %rdx
	movq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%r10, %rdx
	movq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rbp, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%r15, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rsi
	xorq	%r10, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%r14, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%r11, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rsi
	xorq	%r10, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%r14, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%r15, %rdx
	movq	%rdx, 32(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	movq	%rdx, %rax
	xorq	%rbp, %rax
	movq	%rax, 40(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rbx, %rdx
	movq	%rdx, 48(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rdx, %r14
	xorq	%r12, %r14
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %r11
	xorq	%rdi, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r13
	movq	%rdx, %r15
	xorq	%r13, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rbx
	xorq	%r8, %rbx
	movq	%rbx, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rbx
	xorq	%rsi, %rbx
	movq	%rbx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rbx
	xorq	%r9, %rbx
	movq	%rbx, 24(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rbp
	xorq	%r10, %rbp
	movq	%rcx, %rax
	movq	32(%rsp), %rsi
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rsi, %rbx
	movq	%rcx, %rax
	movq	40(%rsp), %rsi
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rsi, %r12
	movq	%rcx, %rax
	movq	48(%rsp), %rdi
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rdi, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	cmpq	%rcx, 56(%rsp)
	jne	.L782
.L781:
	movl	%r11d, %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE226:
	.size	int64_mod_9, .-int64_mod_9
	.globl	int64_mod_10
	.type	int64_mod_10, @function
int64_mod_10:
.LFB227:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$88, %rsp
	.cfi_def_cfa_offset 144
	movq	%rdi, %rax
	movq	%rdi, 72(%rsp)
	movslq	12(%rsi), %rdi
	movslq	16(%rsi), %r15
	movslq	20(%rsi), %rbx
	movq	%rbx, 8(%rsp)
	movslq	24(%rsi), %rbx
	movq	%rbx, 16(%rsp)
	movslq	28(%rsi), %rbx
	movq	%rbx, 24(%rsp)
	movslq	32(%rsi), %rbp
	movslq	36(%rsi), %rbx
	movslq	40(%rsi), %r12
	movslq	44(%rsi), %r13
	movslq	48(%rsi), %r14
	movslq	52(%rsi), %rsi
	movq	%rsi, 32(%rsp)
	testq	%rax, %rax
	je	.L786
	movl	$0, %ecx
.L787:
	addq	$1, %rcx
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%r15, %rdx
	movq	%rdx, %r11
	movq	%rcx, %rax
	movq	8(%rsp), %rsi
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rsi, %r10
	movq	%rcx, %rax
	movq	16(%rsp), %rsi
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rsi, %r9
	movq	%rcx, %rax
	movq	24(%rsp), %rsi
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rsi, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	movq	32(%rsp), %rsi
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r14
	movq	%rdx, %rdi
	xorq	%r14, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rsi
	xorq	%r13, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rdi
	xorq	%r12, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rsi
	xorq	%rbx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rbx
	xorq	%rdi, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r15
	movq	%rdx, %rdi
	xorq	%r15, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rbp, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r15
	movq	%rdx, %rsi
	xorq	%r15, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%r8, %rdx
	movq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%r9, %rdx
	movq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rsi
	xorq	%r10, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%r15, %rdx
	movq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rbp, %rdx
	movq	%rdx, 32(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rbx, %rax
	movq	%rax, 40(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rbx
	xorq	%r12, %rbx
	movq	%rbx, 48(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%r13, %rdx
	movq	%rdx, 56(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r14
	movq	%rdx, %r15
	xorq	%r14, %r15
	movq	%r15, 64(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%r11
	movq	%rdx, %r14
	xorq	%r11, %r14
	movq	%r14, %r15
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	movq	%rdx, %r11
	xorq	%rsi, %r11
	movq	%r11, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r8
	movq	%rdx, %r11
	xorq	%r8, %r11
	movq	%r11, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r9
	movq	%rdx, %r8
	xorq	%r9, %r8
	movq	%r8, 24(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rbp
	xorq	%r10, %rbp
	movq	%rcx, %rax
	movq	32(%rsp), %rsi
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rsi, %rbx
	movq	%rcx, %rax
	movq	40(%rsp), %rsi
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rsi, %r12
	movq	%rcx, %rax
	movq	48(%rsp), %rsi
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rsi, %r13
	movq	%rcx, %rax
	movq	56(%rsp), %rsi
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rsi, %r14
	movq	%rcx, %rax
	movq	64(%rsp), %rsi
	cqto
	idivq	%rsi
	xorq	%rsi, %rdx
	movq	%rdx, 32(%rsp)
	cmpq	%rcx, 72(%rsp)
	jne	.L787
.L786:
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE227:
	.size	int64_mod_10, .-int64_mod_10
	.globl	int64_mod_11
	.type	int64_mod_11, @function
int64_mod_11:
.LFB228:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$56, %rsp
	.cfi_def_cfa_offset 112
	movq	%rdi, %rax
	movq	%rdi, 16(%rsp)
	movslq	12(%rsi), %rdi
	movslq	16(%rsi), %r15
	movslq	20(%rsi), %r14
	movslq	24(%rsi), %r13
	movslq	28(%rsi), %r12
	movslq	32(%rsi), %rbp
	movslq	36(%rsi), %rbx
	movslq	40(%rsi), %rdx
	movq	%rdx, 40(%rsp)
	movslq	44(%rsi), %rcx
	movq	%rcx, 24(%rsp)
	movslq	48(%rsi), %r9
	movq	%r9, (%rsp)
	movslq	52(%rsi), %r8
	movq	%r8, 32(%rsp)
	movslq	56(%rsi), %rsi
	movq	%rsi, 8(%rsp)
	testq	%rax, %rax
	je	.L791
	movl	$0, %ecx
	movq	%r15, %r11
	movq	%r14, %r10
	movq	%r13, %r9
	movq	%r12, %r8
	movq	%rbp, %rsi
	movq	%rbx, %r15
	movq	%rdi, %rbx
	movq	%rdx, %r14
	movq	24(%rsp), %r13
	movq	32(%rsp), %rbp
.L792:
	addq	$1, %rcx
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	movq	(%rsp), %rdi
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rdi, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	movq	8(%rsp), %rdi
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%r12, %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdi, %rdx
	movq	%rdx, 8(%rsp)
	cmpq	%rcx, 16(%rsp)
	jne	.L792
	movq	%rbx, %rdi
	movq	%r8, %r12
	movq	%r15, %rbx
	movq	%r11, %r15
	movq	%r14, 40(%rsp)
	movq	%r10, %r14
	movq	%r13, 24(%rsp)
	movq	%r9, %r13
	movq	%rbp, 32(%rsp)
	movq	%rsi, %rbp
.L791:
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	call	use_int@PLT
	movl	(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE228:
	.size	int64_mod_11, .-int64_mod_11
	.globl	int64_mod_12
	.type	int64_mod_12, @function
int64_mod_12:
.LFB229:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$72, %rsp
	.cfi_def_cfa_offset 128
	movq	%rdi, 8(%rsp)
	movslq	12(%rsi), %r15
	movslq	16(%rsi), %r14
	movslq	20(%rsi), %r13
	movslq	24(%rsi), %r12
	movslq	28(%rsi), %rbp
	movslq	32(%rsi), %rbx
	movslq	36(%rsi), %r11
	movq	%r11, 16(%rsp)
	movslq	40(%rsi), %r10
	movq	%r10, 24(%rsp)
	movslq	44(%rsi), %r9
	movq	%r9, 32(%rsp)
	movslq	48(%rsi), %r8
	movq	%r8, 40(%rsp)
	movslq	52(%rsi), %rax
	movq	%rax, 48(%rsp)
	movslq	56(%rsi), %rdx
	movq	%rdx, 56(%rsp)
	movslq	60(%rsi), %rsi
	movq	%rsi, (%rsp)
	testq	%rdi, %rdi
	je	.L796
	movl	$0, %ecx
	movq	%rax, %rdi
	movq	%rdx, %rsi
.L797:
	addq	$1, %rcx
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	cmpq	%rcx, 8(%rsp)
	jne	.L797
	movq	%r11, 16(%rsp)
	movq	%r10, 24(%rsp)
	movq	%r9, 32(%rsp)
	movq	%r8, 40(%rsp)
	movq	%rdi, 48(%rsp)
	movq	%rsi, 56(%rsp)
.L796:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	movl	(%rsp), %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE229:
	.size	int64_mod_12, .-int64_mod_12
	.globl	int64_mod_13
	.type	int64_mod_13, @function
int64_mod_13:
.LFB230:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$88, %rsp
	.cfi_def_cfa_offset 144
	movq	%rdi, 24(%rsp)
	movslq	12(%rsi), %r15
	movslq	16(%rsi), %r14
	movslq	20(%rsi), %r13
	movslq	24(%rsi), %r12
	movslq	28(%rsi), %rbp
	movslq	32(%rsi), %rbx
	movslq	36(%rsi), %r11
	movq	%r11, 32(%rsp)
	movslq	40(%rsi), %r10
	movq	%r10, 40(%rsp)
	movslq	44(%rsi), %r9
	movq	%r9, 48(%rsp)
	movslq	48(%rsi), %r8
	movq	%r8, 56(%rsp)
	movslq	52(%rsi), %rax
	movq	%rax, 64(%rsp)
	movslq	56(%rsi), %rdx
	movq	%rdx, 72(%rsp)
	movslq	60(%rsi), %rcx
	movq	%rcx, 8(%rsp)
	movslq	64(%rsi), %rsi
	movq	%rsi, 16(%rsp)
	testq	%rdi, %rdi
	je	.L801
	movl	$0, %ecx
	movq	%rax, %rdi
	movq	%rdx, %rsi
.L802:
	addq	$1, %rcx
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	xorq	8(%rsp), %rdx
	movq	%rdx, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	movq	%rdx, %rax
	xorq	16(%rsp), %rax
	movq	%rax, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	xorq	8(%rsp), %rdx
	movq	%rdx, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	movq	%rdx, %rax
	xorq	8(%rsp), %rax
	movq	%rax, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	xorq	8(%rsp), %rdx
	movq	%rdx, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	movq	%rdx, %rax
	xorq	16(%rsp), %rax
	movq	%rax, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	xorq	8(%rsp), %rdx
	movq	%rdx, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	movq	%rdx, %rax
	xorq	8(%rsp), %rax
	movq	%rax, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	xorq	8(%rsp), %rdx
	movq	%rdx, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	movq	%rdx, %rax
	xorq	16(%rsp), %rax
	movq	%rax, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	xorq	8(%rsp), %rdx
	movq	%rdx, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	movq	%rdx, %rax
	xorq	8(%rsp), %rax
	movq	%rax, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	xorq	8(%rsp), %rdx
	movq	%rdx, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	cmpq	%rcx, 24(%rsp)
	jne	.L802
	movq	%r11, 32(%rsp)
	movq	%r10, 40(%rsp)
	movq	%r9, 48(%rsp)
	movq	%r8, 56(%rsp)
	movq	%rdi, 64(%rsp)
	movq	%rsi, 72(%rsp)
.L801:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE230:
	.size	int64_mod_13, .-int64_mod_13
	.globl	int64_mod_14
	.type	int64_mod_14, @function
int64_mod_14:
.LFB231:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$88, %rsp
	.cfi_def_cfa_offset 144
	movq	%rdi, 24(%rsp)
	movslq	12(%rsi), %r15
	movslq	16(%rsi), %r14
	movslq	20(%rsi), %r13
	movslq	24(%rsi), %r12
	movslq	28(%rsi), %rbp
	movslq	32(%rsi), %rbx
	movslq	36(%rsi), %r11
	movq	%r11, 32(%rsp)
	movslq	40(%rsi), %r10
	movq	%r10, 40(%rsp)
	movslq	44(%rsi), %r9
	movq	%r9, 48(%rsp)
	movslq	48(%rsi), %r8
	movq	%r8, 56(%rsp)
	movslq	52(%rsi), %rax
	movq	%rax, 64(%rsp)
	movslq	56(%rsi), %rdx
	movq	%rdx, 72(%rsp)
	movslq	60(%rsi), %rcx
	movq	%rcx, (%rsp)
	movslq	64(%rsi), %rcx
	movq	%rcx, 8(%rsp)
	movslq	68(%rsi), %rsi
	movq	%rsi, 16(%rsp)
	testq	%rdi, %rdi
	je	.L806
	movl	$0, %ecx
	movq	%rax, %rdi
	movq	%rdx, %rsi
.L807:
	addq	$1, %rcx
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	movq	%rdx, %rax
	xorq	8(%rsp), %rax
	movq	%rax, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	movq	%rdx, %rax
	xorq	8(%rsp), %rax
	movq	%rax, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	movq	%rdx, %rax
	xorq	8(%rsp), %rax
	movq	%rax, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	movq	%rdx, %rax
	xorq	8(%rsp), %rax
	movq	%rax, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	movq	%rdx, %rax
	xorq	8(%rsp), %rax
	movq	%rax, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	movq	%rdx, %rax
	xorq	8(%rsp), %rax
	movq	%rax, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	movq	%rdx, %rax
	xorq	8(%rsp), %rax
	movq	%rax, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	movq	%rdx, %rax
	xorq	8(%rsp), %rax
	movq	%rax, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	movq	%rdx, %rax
	xorq	8(%rsp), %rax
	movq	%rax, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	(%rsp)
	xorq	(%rsp), %rdx
	movq	%rdx, (%rsp)
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	xorq	8(%rsp), %rdx
	movq	%rdx, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	cmpq	%rcx, 24(%rsp)
	jne	.L807
	movq	%r11, 32(%rsp)
	movq	%r10, 40(%rsp)
	movq	%r9, 48(%rsp)
	movq	%r8, 56(%rsp)
	movq	%rdi, 64(%rsp)
	movq	%rsi, 72(%rsp)
.L806:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	movl	40(%rsp), %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	call	use_int@PLT
	movl	(%rsp), %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE231:
	.size	int64_mod_14, .-int64_mod_14
	.globl	int64_mod_15
	.type	int64_mod_15, @function
int64_mod_15:
.LFB232:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$104, %rsp
	.cfi_def_cfa_offset 160
	movq	%rdi, 40(%rsp)
	movslq	12(%rsi), %r15
	movslq	16(%rsi), %r14
	movslq	20(%rsi), %r13
	movslq	24(%rsi), %r12
	movslq	28(%rsi), %rbp
	movslq	32(%rsi), %rbx
	movslq	36(%rsi), %r11
	movq	%r11, 48(%rsp)
	movslq	40(%rsi), %r10
	movq	%r10, 56(%rsp)
	movslq	44(%rsi), %r9
	movq	%r9, 64(%rsp)
	movslq	48(%rsi), %r8
	movq	%r8, 72(%rsp)
	movslq	52(%rsi), %rax
	movq	%rax, 80(%rsp)
	movslq	56(%rsi), %rdx
	movq	%rdx, 88(%rsp)
	movslq	60(%rsi), %rcx
	movq	%rcx, 8(%rsp)
	movslq	64(%rsi), %rcx
	movq	%rcx, 16(%rsp)
	movslq	68(%rsi), %rcx
	movq	%rcx, 24(%rsp)
	movslq	72(%rsi), %rsi
	movq	%rsi, 32(%rsp)
	testq	%rdi, %rdi
	je	.L811
	movl	$0, %ecx
	movq	%rax, %rdi
	movq	%rdx, %rsi
.L812:
	addq	$1, %rcx
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	xorq	8(%rsp), %rdx
	movq	%rdx, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	movq	%rdx, %rax
	xorq	16(%rsp), %rax
	movq	%rax, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	24(%rsp)
	xorq	24(%rsp), %rdx
	movq	%rdx, 24(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	32(%rsp)
	movq	%rdx, %rax
	xorq	32(%rsp), %rax
	movq	%rax, 32(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	xorq	8(%rsp), %rdx
	movq	%rdx, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	24(%rsp)
	movq	%rdx, %rax
	xorq	24(%rsp), %rax
	movq	%rax, 24(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	32(%rsp)
	xorq	32(%rsp), %rdx
	movq	%rdx, 32(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	movq	%rdx, %rax
	xorq	8(%rsp), %rax
	movq	%rax, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	24(%rsp)
	xorq	24(%rsp), %rdx
	movq	%rdx, 24(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	32(%rsp)
	movq	%rdx, %rax
	xorq	32(%rsp), %rax
	movq	%rax, 32(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	xorq	8(%rsp), %rdx
	movq	%rdx, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	movq	%rdx, %rax
	xorq	16(%rsp), %rax
	movq	%rax, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	24(%rsp)
	xorq	24(%rsp), %rdx
	movq	%rdx, 24(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	32(%rsp)
	xorq	32(%rsp), %rdx
	movq	%rdx, 32(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	movq	%rdx, %rax
	xorq	8(%rsp), %rax
	movq	%rax, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	24(%rsp)
	movq	%rdx, %rax
	xorq	24(%rsp), %rax
	movq	%rax, 24(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	32(%rsp)
	xorq	32(%rsp), %rdx
	movq	%rdx, 32(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	xorq	8(%rsp), %rdx
	movq	%rdx, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	movq	%rdx, %rax
	xorq	16(%rsp), %rax
	movq	%rax, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	24(%rsp)
	xorq	24(%rsp), %rdx
	movq	%rdx, 24(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	32(%rsp)
	movq	%rdx, %rax
	xorq	32(%rsp), %rax
	movq	%rax, 32(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	xorq	8(%rsp), %rdx
	movq	%rdx, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	24(%rsp)
	movq	%rdx, %rax
	xorq	24(%rsp), %rax
	movq	%rax, 24(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	32(%rsp)
	xorq	32(%rsp), %rdx
	movq	%rdx, 32(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	movq	%rdx, %rax
	xorq	8(%rsp), %rax
	movq	%rax, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	xorq	16(%rsp), %rdx
	movq	%rdx, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	24(%rsp)
	xorq	24(%rsp), %rdx
	movq	%rdx, 24(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	32(%rsp)
	movq	%rdx, %rax
	xorq	32(%rsp), %rax
	movq	%rax, 32(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	xorq	8(%rsp), %rdx
	movq	%rdx, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	movq	%rdx, %rax
	xorq	16(%rsp), %rax
	movq	%rax, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	24(%rsp)
	xorq	24(%rsp), %rdx
	movq	%rdx, 24(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	32(%rsp)
	xorq	32(%rsp), %rdx
	movq	%rdx, 32(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	xorq	%rdx, %r15
	movq	%rcx, %rax
	cqto
	idivq	%r14
	xorq	%rdx, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	xorq	%rdx, %r13
	movq	%rcx, %rax
	cqto
	idivq	%r12
	xorq	%rdx, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbp
	xorq	%rdx, %rbp
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	xorq	%rdx, %rbx
	movq	%rcx, %rax
	cqto
	idivq	%r11
	xorq	%rdx, %r11
	movq	%rcx, %rax
	cqto
	idivq	%r10
	xorq	%rdx, %r10
	movq	%rcx, %rax
	cqto
	idivq	%r9
	xorq	%rdx, %r9
	movq	%rcx, %rax
	cqto
	idivq	%r8
	xorq	%rdx, %r8
	movq	%rcx, %rax
	cqto
	idivq	%rdi
	xorq	%rdx, %rdi
	movq	%rcx, %rax
	cqto
	idivq	%rsi
	xorq	%rdx, %rsi
	movq	%rcx, %rax
	cqto
	idivq	8(%rsp)
	movq	%rdx, %rax
	xorq	8(%rsp), %rax
	movq	%rax, 8(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	16(%rsp)
	movq	%rdx, %rax
	xorq	16(%rsp), %rax
	movq	%rax, 16(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	24(%rsp)
	movq	%rdx, %rax
	xorq	24(%rsp), %rax
	movq	%rax, 24(%rsp)
	movq	%rcx, %rax
	cqto
	idivq	32(%rsp)
	xorq	32(%rsp), %rdx
	movq	%rdx, 32(%rsp)
	cmpq	%rcx, 40(%rsp)
	jne	.L812
	movq	%r11, 48(%rsp)
	movq	%r10, 56(%rsp)
	movq	%r9, 64(%rsp)
	movq	%r8, 72(%rsp)
	movq	%rdi, 80(%rsp)
	movq	%rsi, 88(%rsp)
.L811:
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebp, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	movl	48(%rsp), %edi
	call	use_int@PLT
	movl	56(%rsp), %edi
	call	use_int@PLT
	movl	64(%rsp), %edi
	call	use_int@PLT
	movl	72(%rsp), %edi
	call	use_int@PLT
	movl	80(%rsp), %edi
	call	use_int@PLT
	movl	88(%rsp), %edi
	call	use_int@PLT
	movl	8(%rsp), %edi
	call	use_int@PLT
	movl	16(%rsp), %edi
	call	use_int@PLT
	movl	24(%rsp), %edi
	call	use_int@PLT
	movl	32(%rsp), %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE232:
	.size	int64_mod_15, .-int64_mod_15
	.globl	float_add_0
	.type	float_add_0, @function
float_add_0:
.LFB233:
	.cfi_startproc
	endbr64
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	pxor	%xmm0, %xmm0
	cvtsd2ss	80(%rsi), %xmm0
	addss	.LC1(%rip), %xmm0
	pxor	%xmm1, %xmm1
	cvtsi2ssl	8(%rsi), %xmm1
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L816
.L817:
	addss	%xmm1, %xmm0
	addss	%xmm0, %xmm0
	addss	%xmm0, %xmm0
	addss	%xmm0, %xmm0
	addss	%xmm0, %xmm0
	addss	%xmm0, %xmm0
	addss	%xmm0, %xmm0
	addss	%xmm0, %xmm0
	addss	%xmm0, %xmm0
	addss	%xmm0, %xmm0
	addss	%xmm0, %xmm0
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L817
.L816:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE233:
	.size	float_add_0, .-float_add_0
	.globl	float_add_1
	.type	float_add_1, @function
float_add_1:
.LFB234:
	.cfi_startproc
	endbr64
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	pxor	%xmm0, %xmm0
	cvtsd2ss	80(%rsi), %xmm0
	movss	.LC1(%rip), %xmm1
	addss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	8(%rsi), %xmm2
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 12(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L821
.L822:
	addss	%xmm2, %xmm0
	movss	12(%rsp), %xmm1
	addss	%xmm2, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	movss	%xmm1, 12(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L822
.L821:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE234:
	.size	float_add_1, .-float_add_1
	.globl	float_add_2
	.type	float_add_2, @function
float_add_2:
.LFB235:
	.cfi_startproc
	endbr64
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	pxor	%xmm0, %xmm0
	cvtsd2ss	80(%rsi), %xmm0
	movss	.LC1(%rip), %xmm1
	addss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	8(%rsi), %xmm2
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 8(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	96(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 12(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L826
.L827:
	addss	%xmm2, %xmm0
	movss	8(%rsp), %xmm3
	addss	%xmm2, %xmm3
	movss	12(%rsp), %xmm1
	addss	%xmm2, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm3, %xmm3
	movss	%xmm3, 8(%rsp)
	addss	%xmm1, %xmm1
	movss	%xmm1, 12(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L827
.L826:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE235:
	.size	float_add_2, .-float_add_2
	.globl	float_add_3
	.type	float_add_3, @function
float_add_3:
.LFB236:
	.cfi_startproc
	endbr64
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	pxor	%xmm0, %xmm0
	cvtsd2ss	80(%rsi), %xmm0
	movss	.LC1(%rip), %xmm1
	addss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	8(%rsi), %xmm2
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 4(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	96(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 8(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	104(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 12(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L831
.L832:
	addss	%xmm2, %xmm0
	movss	4(%rsp), %xmm4
	addss	%xmm2, %xmm4
	movss	8(%rsp), %xmm3
	addss	%xmm2, %xmm3
	movss	12(%rsp), %xmm1
	addss	%xmm2, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm4, %xmm4
	movss	%xmm4, 4(%rsp)
	addss	%xmm3, %xmm3
	movss	%xmm3, 8(%rsp)
	addss	%xmm1, %xmm1
	movss	%xmm1, 12(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L832
.L831:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE236:
	.size	float_add_3, .-float_add_3
	.globl	float_add_4
	.type	float_add_4, @function
float_add_4:
.LFB237:
	.cfi_startproc
	endbr64
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	pxor	%xmm0, %xmm0
	cvtsd2ss	80(%rsi), %xmm0
	movss	.LC1(%rip), %xmm1
	addss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	8(%rsi), %xmm2
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, (%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	96(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 4(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	104(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 8(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	112(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 12(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L836
.L837:
	addss	%xmm2, %xmm0
	movss	(%rsp), %xmm5
	addss	%xmm2, %xmm5
	movss	4(%rsp), %xmm4
	addss	%xmm2, %xmm4
	movss	8(%rsp), %xmm3
	addss	%xmm2, %xmm3
	movss	12(%rsp), %xmm1
	addss	%xmm2, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm5, %xmm5
	movss	%xmm5, (%rsp)
	addss	%xmm4, %xmm4
	movss	%xmm4, 4(%rsp)
	addss	%xmm3, %xmm3
	movss	%xmm3, 8(%rsp)
	addss	%xmm1, %xmm1
	movss	%xmm1, 12(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L837
.L836:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE237:
	.size	float_add_4, .-float_add_4
	.globl	float_add_5
	.type	float_add_5, @function
float_add_5:
.LFB238:
	.cfi_startproc
	endbr64
	subq	$40, %rsp
	.cfi_def_cfa_offset 48
	pxor	%xmm0, %xmm0
	cvtsd2ss	80(%rsi), %xmm0
	movss	.LC1(%rip), %xmm1
	addss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	8(%rsi), %xmm2
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 12(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	96(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 16(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	104(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 20(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	112(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 24(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	120(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 28(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L841
.L842:
	addss	%xmm2, %xmm0
	movss	12(%rsp), %xmm6
	addss	%xmm2, %xmm6
	movss	16(%rsp), %xmm5
	addss	%xmm2, %xmm5
	movss	20(%rsp), %xmm4
	addss	%xmm2, %xmm4
	movss	24(%rsp), %xmm3
	addss	%xmm2, %xmm3
	movss	28(%rsp), %xmm1
	addss	%xmm2, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm6, %xmm6
	movss	%xmm6, 12(%rsp)
	addss	%xmm5, %xmm5
	movss	%xmm5, 16(%rsp)
	addss	%xmm4, %xmm4
	movss	%xmm4, 20(%rsp)
	addss	%xmm3, %xmm3
	movss	%xmm3, 24(%rsp)
	addss	%xmm1, %xmm1
	movss	%xmm1, 28(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L842
.L841:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE238:
	.size	float_add_5, .-float_add_5
	.globl	float_add_6
	.type	float_add_6, @function
float_add_6:
.LFB239:
	.cfi_startproc
	endbr64
	subq	$40, %rsp
	.cfi_def_cfa_offset 48
	pxor	%xmm0, %xmm0
	cvtsd2ss	80(%rsi), %xmm0
	movss	.LC1(%rip), %xmm1
	addss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	8(%rsi), %xmm2
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 8(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	96(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 12(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	104(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 16(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	112(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 20(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	120(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 24(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	128(%rsi), %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, 28(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L846
.L847:
	addss	%xmm2, %xmm0
	movss	8(%rsp), %xmm7
	addss	%xmm2, %xmm7
	movss	12(%rsp), %xmm6
	addss	%xmm2, %xmm6
	movss	16(%rsp), %xmm5
	addss	%xmm2, %xmm5
	movss	20(%rsp), %xmm4
	addss	%xmm2, %xmm4
	movss	24(%rsp), %xmm3
	addss	%xmm2, %xmm3
	movss	28(%rsp), %xmm1
	addss	%xmm2, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm7, %xmm7
	movss	%xmm7, 8(%rsp)
	addss	%xmm6, %xmm6
	movss	%xmm6, 12(%rsp)
	addss	%xmm5, %xmm5
	movss	%xmm5, 16(%rsp)
	addss	%xmm4, %xmm4
	movss	%xmm4, 20(%rsp)
	addss	%xmm3, %xmm3
	movss	%xmm3, 24(%rsp)
	addss	%xmm1, %xmm1
	movss	%xmm1, 28(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L847
.L846:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE239:
	.size	float_add_6, .-float_add_6
	.globl	float_add_7
	.type	float_add_7, @function
float_add_7:
.LFB240:
	.cfi_startproc
	endbr64
	subq	$40, %rsp
	.cfi_def_cfa_offset 48
	pxor	%xmm8, %xmm8
	cvtsd2ss	80(%rsi), %xmm8
	movss	.LC1(%rip), %xmm0
	addss	%xmm0, %xmm8
	pxor	%xmm2, %xmm2
	cvtsi2ssl	8(%rsi), %xmm2
	pxor	%xmm1, %xmm1
	cvtsd2ss	88(%rsi), %xmm1
	addss	%xmm0, %xmm1
	movss	%xmm1, 4(%rsp)
	pxor	%xmm1, %xmm1
	cvtsd2ss	96(%rsi), %xmm1
	addss	%xmm0, %xmm1
	movss	%xmm1, 8(%rsp)
	pxor	%xmm1, %xmm1
	cvtsd2ss	104(%rsi), %xmm1
	addss	%xmm0, %xmm1
	movss	%xmm1, 12(%rsp)
	pxor	%xmm1, %xmm1
	cvtsd2ss	112(%rsi), %xmm1
	addss	%xmm0, %xmm1
	movss	%xmm1, 16(%rsp)
	pxor	%xmm1, %xmm1
	cvtsd2ss	120(%rsi), %xmm1
	addss	%xmm0, %xmm1
	movss	%xmm1, 20(%rsp)
	pxor	%xmm1, %xmm1
	cvtsd2ss	128(%rsi), %xmm1
	addss	%xmm0, %xmm1
	movss	%xmm1, 24(%rsp)
	pxor	%xmm1, %xmm1
	cvtsd2ss	136(%rsi), %xmm1
	addss	%xmm0, %xmm1
	movss	%xmm1, 28(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L851
.L852:
	addss	%xmm2, %xmm8
	movss	4(%rsp), %xmm7
	addss	%xmm2, %xmm7
	movss	8(%rsp), %xmm6
	addss	%xmm2, %xmm6
	movss	12(%rsp), %xmm5
	addss	%xmm2, %xmm5
	movss	16(%rsp), %xmm4
	addss	%xmm2, %xmm4
	movss	20(%rsp), %xmm3
	addss	%xmm2, %xmm3
	movss	24(%rsp), %xmm1
	addss	%xmm2, %xmm1
	movss	28(%rsp), %xmm0
	addss	%xmm2, %xmm0
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	movss	%xmm7, 4(%rsp)
	addss	%xmm6, %xmm6
	movss	%xmm6, 8(%rsp)
	addss	%xmm5, %xmm5
	movss	%xmm5, 12(%rsp)
	addss	%xmm4, %xmm4
	movss	%xmm4, 16(%rsp)
	addss	%xmm3, %xmm3
	movss	%xmm3, 20(%rsp)
	addss	%xmm1, %xmm1
	movss	%xmm1, 24(%rsp)
	addss	%xmm0, %xmm0
	movss	%xmm0, 28(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L852
.L851:
	cvttss2sil	%xmm8, %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE240:
	.size	float_add_7, .-float_add_7
	.globl	float_add_8
	.type	float_add_8, @function
float_add_8:
.LFB241:
	.cfi_startproc
	endbr64
	subq	$40, %rsp
	.cfi_def_cfa_offset 48
	pxor	%xmm0, %xmm0
	cvtsd2ss	80(%rsi), %xmm0
	movss	.LC1(%rip), %xmm1
	addss	%xmm1, %xmm0
	pxor	%xmm7, %xmm7
	cvtsi2ssl	8(%rsi), %xmm7
	pxor	%xmm2, %xmm2
	cvtsd2ss	88(%rsi), %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, (%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	96(%rsi), %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, 4(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	104(%rsi), %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, 8(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	112(%rsi), %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, 12(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	120(%rsi), %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, 16(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	128(%rsi), %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, 20(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	136(%rsi), %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, 24(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	144(%rsi), %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, 28(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L856
.L857:
	addss	%xmm7, %xmm0
	movss	(%rsp), %xmm6
	addss	%xmm7, %xmm6
	movss	4(%rsp), %xmm5
	addss	%xmm7, %xmm5
	movss	8(%rsp), %xmm4
	addss	%xmm7, %xmm4
	movss	12(%rsp), %xmm3
	addss	%xmm7, %xmm3
	movss	16(%rsp), %xmm2
	addss	%xmm7, %xmm2
	movss	20(%rsp), %xmm1
	addss	%xmm7, %xmm1
	movss	24(%rsp), %xmm11
	addss	%xmm7, %xmm11
	movss	28(%rsp), %xmm10
	addss	%xmm7, %xmm10
	addss	%xmm0, %xmm0
	addss	%xmm6, %xmm6
	movaps	%xmm6, %xmm9
	addss	%xmm5, %xmm5
	movaps	%xmm5, %xmm8
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	movaps	%xmm3, %xmm5
	addss	%xmm2, %xmm2
	movaps	%xmm2, %xmm6
	addss	%xmm1, %xmm1
	movaps	%xmm1, %xmm3
	movaps	%xmm11, %xmm2
	addss	%xmm11, %xmm2
	movaps	%xmm10, %xmm1
	addss	%xmm10, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm4, %xmm4
	addss	%xmm5, %xmm5
	addss	%xmm6, %xmm6
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm4, %xmm4
	addss	%xmm5, %xmm5
	addss	%xmm6, %xmm6
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm4, %xmm4
	addss	%xmm5, %xmm5
	addss	%xmm6, %xmm6
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm4, %xmm4
	addss	%xmm5, %xmm5
	addss	%xmm6, %xmm6
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm4, %xmm4
	addss	%xmm5, %xmm5
	addss	%xmm6, %xmm6
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm4, %xmm4
	addss	%xmm5, %xmm5
	addss	%xmm6, %xmm6
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm4, %xmm4
	movaps	%xmm4, %xmm10
	addss	%xmm5, %xmm5
	addss	%xmm6, %xmm6
	movaps	%xmm6, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm9, %xmm9
	movss	%xmm9, (%rsp)
	addss	%xmm8, %xmm8
	movss	%xmm8, 4(%rsp)
	addss	%xmm6, %xmm6
	movss	%xmm6, 8(%rsp)
	addss	%xmm5, %xmm5
	movss	%xmm5, 12(%rsp)
	addss	%xmm4, %xmm4
	movss	%xmm4, 16(%rsp)
	addss	%xmm3, %xmm3
	movss	%xmm3, 20(%rsp)
	addss	%xmm2, %xmm2
	movss	%xmm2, 24(%rsp)
	addss	%xmm1, %xmm1
	movss	%xmm1, 28(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L857
.L856:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE241:
	.size	float_add_8, .-float_add_8
	.globl	float_add_9
	.type	float_add_9, @function
float_add_9:
.LFB242:
	.cfi_startproc
	endbr64
	subq	$56, %rsp
	.cfi_def_cfa_offset 64
	pxor	%xmm0, %xmm0
	cvtsd2ss	80(%rsi), %xmm0
	movss	.LC1(%rip), %xmm1
	addss	%xmm1, %xmm0
	pxor	%xmm6, %xmm6
	cvtsi2ssl	8(%rsi), %xmm6
	pxor	%xmm2, %xmm2
	cvtsd2ss	88(%rsi), %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, 12(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	96(%rsi), %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, 16(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	104(%rsi), %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, 20(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	112(%rsi), %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, 24(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	120(%rsi), %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, 28(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	128(%rsi), %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, 32(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	136(%rsi), %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, 36(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	144(%rsi), %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, 40(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	152(%rsi), %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, 44(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L861
.L862:
	addss	%xmm6, %xmm0
	movss	12(%rsp), %xmm5
	addss	%xmm6, %xmm5
	movss	16(%rsp), %xmm4
	addss	%xmm6, %xmm4
	movss	20(%rsp), %xmm3
	addss	%xmm6, %xmm3
	movss	24(%rsp), %xmm2
	addss	%xmm6, %xmm2
	movss	28(%rsp), %xmm1
	addss	%xmm6, %xmm1
	movss	32(%rsp), %xmm10
	addss	%xmm6, %xmm10
	movss	36(%rsp), %xmm9
	addss	%xmm6, %xmm9
	movss	40(%rsp), %xmm8
	addss	%xmm6, %xmm8
	movss	44(%rsp), %xmm7
	addss	%xmm6, %xmm7
	addss	%xmm0, %xmm0
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm1
	movaps	%xmm10, %xmm11
	addss	%xmm10, %xmm11
	movaps	%xmm9, %xmm12
	addss	%xmm9, %xmm12
	movaps	%xmm8, %xmm13
	addss	%xmm8, %xmm13
	addss	%xmm7, %xmm7
	movaps	%xmm7, %xmm14
	addss	%xmm0, %xmm0
	addss	%xmm5, %xmm5
	movaps	%xmm5, %xmm10
	addss	%xmm4, %xmm4
	movaps	%xmm4, %xmm9
	addss	%xmm3, %xmm3
	movaps	%xmm3, %xmm8
	addss	%xmm2, %xmm2
	movaps	%xmm2, %xmm7
	addss	%xmm1, %xmm1
	addss	%xmm11, %xmm11
	addss	%xmm12, %xmm12
	addss	%xmm13, %xmm13
	addss	%xmm14, %xmm14
	addss	%xmm0, %xmm0
	addss	%xmm5, %xmm10
	addss	%xmm4, %xmm9
	addss	%xmm3, %xmm8
	addss	%xmm2, %xmm7
	addss	%xmm1, %xmm1
	addss	%xmm11, %xmm11
	addss	%xmm12, %xmm12
	addss	%xmm13, %xmm13
	addss	%xmm14, %xmm14
	addss	%xmm0, %xmm0
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm1, %xmm1
	addss	%xmm11, %xmm11
	addss	%xmm12, %xmm12
	addss	%xmm13, %xmm13
	addss	%xmm14, %xmm14
	addss	%xmm0, %xmm0
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm1, %xmm1
	addss	%xmm11, %xmm11
	addss	%xmm12, %xmm12
	addss	%xmm13, %xmm13
	addss	%xmm14, %xmm14
	addss	%xmm0, %xmm0
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm1, %xmm1
	addss	%xmm11, %xmm11
	addss	%xmm12, %xmm12
	addss	%xmm13, %xmm13
	addss	%xmm14, %xmm14
	addss	%xmm0, %xmm0
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm1, %xmm1
	addss	%xmm11, %xmm11
	addss	%xmm12, %xmm12
	addss	%xmm13, %xmm13
	addss	%xmm14, %xmm14
	addss	%xmm0, %xmm0
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	movaps	%xmm1, %xmm5
	addss	%xmm1, %xmm5
	addss	%xmm11, %xmm11
	addss	%xmm12, %xmm12
	addss	%xmm13, %xmm13
	movaps	%xmm14, %xmm1
	addss	%xmm14, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm5, %xmm5
	movaps	%xmm11, %xmm4
	addss	%xmm11, %xmm4
	movaps	%xmm12, %xmm3
	addss	%xmm12, %xmm3
	movaps	%xmm13, %xmm2
	addss	%xmm13, %xmm2
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	addss	%xmm10, %xmm10
	movss	%xmm10, 12(%rsp)
	addss	%xmm9, %xmm9
	movss	%xmm9, 16(%rsp)
	addss	%xmm8, %xmm8
	movss	%xmm8, 20(%rsp)
	addss	%xmm7, %xmm7
	movss	%xmm7, 24(%rsp)
	addss	%xmm5, %xmm5
	movss	%xmm5, 28(%rsp)
	addss	%xmm4, %xmm4
	movss	%xmm4, 32(%rsp)
	addss	%xmm3, %xmm3
	movss	%xmm3, 36(%rsp)
	addss	%xmm2, %xmm2
	movss	%xmm2, 40(%rsp)
	addss	%xmm1, %xmm1
	movss	%xmm1, 44(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L862
.L861:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE242:
	.size	float_add_9, .-float_add_9
	.globl	float_add_10
	.type	float_add_10, @function
float_add_10:
.LFB243:
	.cfi_startproc
	endbr64
	subq	$56, %rsp
	.cfi_def_cfa_offset 64
	pxor	%xmm1, %xmm1
	cvtsd2ss	80(%rsi), %xmm1
	movss	.LC1(%rip), %xmm2
	addss	%xmm2, %xmm1
	pxor	%xmm5, %xmm5
	cvtsi2ssl	8(%rsi), %xmm5
	pxor	%xmm0, %xmm0
	cvtsd2ss	88(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 8(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	96(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 12(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	104(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 16(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	112(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 20(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	120(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 24(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	128(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 28(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	136(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 32(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	144(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 36(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	152(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 40(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	160(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 44(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L866
.L867:
	addss	%xmm5, %xmm1
	movss	8(%rsp), %xmm12
	addss	%xmm5, %xmm12
	movss	12(%rsp), %xmm11
	addss	%xmm5, %xmm11
	movss	16(%rsp), %xmm10
	addss	%xmm5, %xmm10
	movss	20(%rsp), %xmm9
	addss	%xmm5, %xmm9
	movss	24(%rsp), %xmm8
	addss	%xmm5, %xmm8
	movss	28(%rsp), %xmm7
	addss	%xmm5, %xmm7
	movss	32(%rsp), %xmm6
	addss	%xmm5, %xmm6
	movss	36(%rsp), %xmm4
	addss	%xmm5, %xmm4
	movss	40(%rsp), %xmm3
	addss	%xmm5, %xmm3
	movss	44(%rsp), %xmm2
	addss	%xmm5, %xmm2
	addss	%xmm1, %xmm1
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	movaps	%xmm9, %xmm0
	addss	%xmm9, %xmm0
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm1
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm0, %xmm0
	movaps	%xmm0, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm1
	movaps	%xmm1, %xmm0
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm0
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	movaps	%xmm0, %xmm1
	addss	%xmm0, %xmm1
	addss	%xmm12, %xmm12
	movss	%xmm12, 8(%rsp)
	addss	%xmm11, %xmm11
	movss	%xmm11, 12(%rsp)
	addss	%xmm10, %xmm10
	movss	%xmm10, 16(%rsp)
	addss	%xmm9, %xmm9
	movss	%xmm9, 20(%rsp)
	addss	%xmm8, %xmm8
	movss	%xmm8, 24(%rsp)
	addss	%xmm7, %xmm7
	movss	%xmm7, 28(%rsp)
	addss	%xmm6, %xmm6
	movss	%xmm6, 32(%rsp)
	addss	%xmm4, %xmm4
	movss	%xmm4, 36(%rsp)
	addss	%xmm3, %xmm3
	movss	%xmm3, 40(%rsp)
	addss	%xmm2, %xmm2
	movss	%xmm2, 44(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L867
.L866:
	cvttss2sil	%xmm1, %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE243:
	.size	float_add_10, .-float_add_10
	.globl	float_add_11
	.type	float_add_11, @function
float_add_11:
.LFB244:
	.cfi_startproc
	endbr64
	subq	$56, %rsp
	.cfi_def_cfa_offset 64
	pxor	%xmm1, %xmm1
	cvtsd2ss	80(%rsi), %xmm1
	movss	.LC1(%rip), %xmm2
	addss	%xmm2, %xmm1
	pxor	%xmm5, %xmm5
	cvtsi2ssl	8(%rsi), %xmm5
	pxor	%xmm0, %xmm0
	cvtsd2ss	88(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 4(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	96(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 8(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	104(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 12(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	112(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 16(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	120(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 20(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	128(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 24(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	136(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 28(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	144(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 32(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	152(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 36(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	160(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 40(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	168(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 44(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L871
.L872:
	addss	%xmm5, %xmm1
	movss	4(%rsp), %xmm13
	addss	%xmm5, %xmm13
	movss	8(%rsp), %xmm12
	addss	%xmm5, %xmm12
	movss	12(%rsp), %xmm11
	addss	%xmm5, %xmm11
	movss	16(%rsp), %xmm10
	addss	%xmm5, %xmm10
	movss	20(%rsp), %xmm9
	addss	%xmm5, %xmm9
	movss	24(%rsp), %xmm8
	addss	%xmm5, %xmm8
	movss	28(%rsp), %xmm7
	addss	%xmm5, %xmm7
	movss	32(%rsp), %xmm6
	addss	%xmm5, %xmm6
	movss	36(%rsp), %xmm4
	addss	%xmm5, %xmm4
	movss	40(%rsp), %xmm3
	addss	%xmm5, %xmm3
	movss	44(%rsp), %xmm2
	addss	%xmm5, %xmm2
	addss	%xmm1, %xmm1
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	movaps	%xmm11, %xmm0
	addss	%xmm11, %xmm0
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm1
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm0, %xmm0
	movaps	%xmm0, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm1
	movaps	%xmm1, %xmm0
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm0
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	movaps	%xmm0, %xmm1
	addss	%xmm0, %xmm1
	addss	%xmm13, %xmm13
	movss	%xmm13, 4(%rsp)
	addss	%xmm12, %xmm12
	movss	%xmm12, 8(%rsp)
	addss	%xmm11, %xmm11
	movss	%xmm11, 12(%rsp)
	addss	%xmm10, %xmm10
	movss	%xmm10, 16(%rsp)
	addss	%xmm9, %xmm9
	movss	%xmm9, 20(%rsp)
	addss	%xmm8, %xmm8
	movss	%xmm8, 24(%rsp)
	addss	%xmm7, %xmm7
	movss	%xmm7, 28(%rsp)
	addss	%xmm6, %xmm6
	movss	%xmm6, 32(%rsp)
	addss	%xmm4, %xmm4
	movss	%xmm4, 36(%rsp)
	addss	%xmm3, %xmm3
	movss	%xmm3, 40(%rsp)
	addss	%xmm2, %xmm2
	movss	%xmm2, 44(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L872
.L871:
	cvttss2sil	%xmm1, %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE244:
	.size	float_add_11, .-float_add_11
	.globl	float_add_12
	.type	float_add_12, @function
float_add_12:
.LFB245:
	.cfi_startproc
	endbr64
	subq	$56, %rsp
	.cfi_def_cfa_offset 64
	pxor	%xmm1, %xmm1
	cvtsd2ss	80(%rsi), %xmm1
	movss	.LC1(%rip), %xmm2
	addss	%xmm2, %xmm1
	pxor	%xmm5, %xmm5
	cvtsi2ssl	8(%rsi), %xmm5
	pxor	%xmm0, %xmm0
	cvtsd2ss	88(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, (%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	96(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 4(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	104(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 8(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	112(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 12(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	120(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 16(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	128(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 20(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	136(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 24(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	144(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 28(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	152(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 32(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	160(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 36(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	168(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 40(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	176(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 44(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L876
.L877:
	addss	%xmm5, %xmm1
	movss	(%rsp), %xmm14
	addss	%xmm5, %xmm14
	movss	4(%rsp), %xmm13
	addss	%xmm5, %xmm13
	movss	8(%rsp), %xmm12
	addss	%xmm5, %xmm12
	movss	12(%rsp), %xmm11
	addss	%xmm5, %xmm11
	movss	16(%rsp), %xmm10
	addss	%xmm5, %xmm10
	movss	20(%rsp), %xmm9
	addss	%xmm5, %xmm9
	movss	24(%rsp), %xmm8
	addss	%xmm5, %xmm8
	movss	28(%rsp), %xmm7
	addss	%xmm5, %xmm7
	movss	32(%rsp), %xmm6
	addss	%xmm5, %xmm6
	movss	36(%rsp), %xmm4
	addss	%xmm5, %xmm4
	movss	40(%rsp), %xmm3
	addss	%xmm5, %xmm3
	movss	44(%rsp), %xmm2
	addss	%xmm5, %xmm2
	addss	%xmm1, %xmm1
	addss	%xmm14, %xmm14
	movaps	%xmm13, %xmm0
	addss	%xmm13, %xmm0
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm1
	addss	%xmm14, %xmm14
	addss	%xmm0, %xmm0
	movaps	%xmm0, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm1
	movaps	%xmm1, %xmm0
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm0
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	movaps	%xmm0, %xmm1
	addss	%xmm0, %xmm1
	addss	%xmm14, %xmm14
	movss	%xmm14, (%rsp)
	addss	%xmm13, %xmm13
	movss	%xmm13, 4(%rsp)
	addss	%xmm12, %xmm12
	movss	%xmm12, 8(%rsp)
	addss	%xmm11, %xmm11
	movss	%xmm11, 12(%rsp)
	addss	%xmm10, %xmm10
	movss	%xmm10, 16(%rsp)
	addss	%xmm9, %xmm9
	movss	%xmm9, 20(%rsp)
	addss	%xmm8, %xmm8
	movss	%xmm8, 24(%rsp)
	addss	%xmm7, %xmm7
	movss	%xmm7, 28(%rsp)
	addss	%xmm6, %xmm6
	movss	%xmm6, 32(%rsp)
	addss	%xmm4, %xmm4
	movss	%xmm4, 36(%rsp)
	addss	%xmm3, %xmm3
	movss	%xmm3, 40(%rsp)
	addss	%xmm2, %xmm2
	movss	%xmm2, 44(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L877
.L876:
	cvttss2sil	%xmm1, %edi
	call	use_int@PLT
	cvttss2sil	(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE245:
	.size	float_add_12, .-float_add_12
	.globl	float_add_13
	.type	float_add_13, @function
float_add_13:
.LFB246:
	.cfi_startproc
	endbr64
	subq	$72, %rsp
	.cfi_def_cfa_offset 80
	pxor	%xmm1, %xmm1
	cvtsd2ss	80(%rsi), %xmm1
	movss	.LC1(%rip), %xmm2
	addss	%xmm2, %xmm1
	pxor	%xmm5, %xmm5
	cvtsi2ssl	8(%rsi), %xmm5
	pxor	%xmm0, %xmm0
	cvtsd2ss	88(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 12(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	96(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 16(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	104(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 20(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	112(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 24(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	120(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 28(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	128(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 32(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	136(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 36(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	144(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 40(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	152(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 44(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	160(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 48(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	168(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 52(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	176(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 56(%rsp)
	pxor	%xmm0, %xmm0
	cvtsd2ss	184(%rsi), %xmm0
	addss	%xmm2, %xmm0
	movss	%xmm0, 60(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L881
.L882:
	addss	%xmm5, %xmm1
	movss	12(%rsp), %xmm15
	addss	%xmm5, %xmm15
	movss	16(%rsp), %xmm14
	addss	%xmm5, %xmm14
	movss	20(%rsp), %xmm13
	addss	%xmm5, %xmm13
	movss	24(%rsp), %xmm12
	addss	%xmm5, %xmm12
	movss	28(%rsp), %xmm11
	addss	%xmm5, %xmm11
	movss	32(%rsp), %xmm10
	addss	%xmm5, %xmm10
	movss	36(%rsp), %xmm9
	addss	%xmm5, %xmm9
	movss	40(%rsp), %xmm8
	addss	%xmm5, %xmm8
	movss	44(%rsp), %xmm7
	addss	%xmm5, %xmm7
	movss	48(%rsp), %xmm6
	addss	%xmm5, %xmm6
	movss	52(%rsp), %xmm4
	addss	%xmm5, %xmm4
	movss	56(%rsp), %xmm3
	addss	%xmm5, %xmm3
	movss	60(%rsp), %xmm2
	addss	%xmm5, %xmm2
	addss	%xmm1, %xmm1
	movaps	%xmm15, %xmm0
	addss	%xmm15, %xmm0
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm1
	addss	%xmm0, %xmm0
	movaps	%xmm0, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm1
	movaps	%xmm1, %xmm0
	addss	%xmm15, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm1, %xmm0
	addss	%xmm15, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm15, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm15, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm15, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm15, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm15, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	movaps	%xmm0, %xmm1
	addss	%xmm0, %xmm1
	addss	%xmm15, %xmm15
	movss	%xmm15, 12(%rsp)
	addss	%xmm14, %xmm14
	movss	%xmm14, 16(%rsp)
	addss	%xmm13, %xmm13
	movss	%xmm13, 20(%rsp)
	addss	%xmm12, %xmm12
	movss	%xmm12, 24(%rsp)
	addss	%xmm11, %xmm11
	movss	%xmm11, 28(%rsp)
	addss	%xmm10, %xmm10
	movss	%xmm10, 32(%rsp)
	addss	%xmm9, %xmm9
	movss	%xmm9, 36(%rsp)
	addss	%xmm8, %xmm8
	movss	%xmm8, 40(%rsp)
	addss	%xmm7, %xmm7
	movss	%xmm7, 44(%rsp)
	addss	%xmm6, %xmm6
	movss	%xmm6, 48(%rsp)
	addss	%xmm4, %xmm4
	movss	%xmm4, 52(%rsp)
	addss	%xmm3, %xmm3
	movss	%xmm3, 56(%rsp)
	addss	%xmm2, %xmm2
	movss	%xmm2, 60(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L882
.L881:
	cvttss2sil	%xmm1, %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE246:
	.size	float_add_13, .-float_add_13
	.globl	float_add_14
	.type	float_add_14, @function
float_add_14:
.LFB247:
	.cfi_startproc
	endbr64
	subq	$72, %rsp
	.cfi_def_cfa_offset 80
	pxor	%xmm1, %xmm1
	cvtsd2ss	80(%rsi), %xmm1
	movss	.LC1(%rip), %xmm0
	addss	%xmm0, %xmm1
	pxor	%xmm15, %xmm15
	cvtsi2ssl	8(%rsi), %xmm15
	pxor	%xmm2, %xmm2
	cvtsd2ss	88(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movaps	%xmm2, %xmm14
	movss	%xmm2, 12(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	96(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movaps	%xmm2, %xmm13
	movss	%xmm2, 16(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	104(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movaps	%xmm2, %xmm12
	movss	%xmm2, 20(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	112(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movaps	%xmm2, %xmm11
	movss	%xmm2, 24(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	120(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movaps	%xmm2, %xmm10
	movss	%xmm2, 28(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	128(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movaps	%xmm2, %xmm9
	movss	%xmm2, 32(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	136(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movaps	%xmm2, %xmm8
	movss	%xmm2, 36(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	144(%rsi), %xmm2
	movaps	%xmm2, %xmm7
	addss	%xmm0, %xmm7
	movss	%xmm7, 40(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	152(%rsi), %xmm2
	movaps	%xmm2, %xmm6
	addss	%xmm0, %xmm6
	movss	%xmm6, 44(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	160(%rsi), %xmm2
	movaps	%xmm2, %xmm5
	addss	%xmm0, %xmm5
	movss	%xmm5, 48(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	168(%rsi), %xmm2
	movaps	%xmm2, %xmm4
	addss	%xmm0, %xmm4
	movss	%xmm4, 52(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	176(%rsi), %xmm2
	movaps	%xmm2, %xmm3
	addss	%xmm0, %xmm3
	movss	%xmm3, 56(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	184(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movss	%xmm2, 8(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	192(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movss	%xmm2, 60(%rsp)
	testq	%rdi, %rdi
	je	.L886
	movaps	%xmm2, %xmm0
	leaq	-1(%rdi), %rax
	movss	8(%rsp), %xmm2
.L887:
	addss	%xmm15, %xmm1
	addss	%xmm15, %xmm14
	addss	%xmm15, %xmm13
	addss	%xmm15, %xmm12
	addss	%xmm15, %xmm11
	addss	%xmm15, %xmm10
	addss	%xmm15, %xmm9
	addss	%xmm15, %xmm8
	addss	%xmm15, %xmm7
	addss	%xmm15, %xmm6
	addss	%xmm15, %xmm5
	addss	%xmm15, %xmm4
	addss	%xmm15, %xmm3
	addss	%xmm15, %xmm2
	addss	%xmm15, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L887
	movss	%xmm14, 12(%rsp)
	movss	%xmm13, 16(%rsp)
	movss	%xmm12, 20(%rsp)
	movss	%xmm11, 24(%rsp)
	movss	%xmm10, 28(%rsp)
	movss	%xmm9, 32(%rsp)
	movss	%xmm8, 36(%rsp)
	movss	%xmm7, 40(%rsp)
	movss	%xmm6, 44(%rsp)
	movss	%xmm5, 48(%rsp)
	movss	%xmm4, 52(%rsp)
	movss	%xmm3, 56(%rsp)
	movss	%xmm2, 8(%rsp)
	movss	%xmm0, 60(%rsp)
.L886:
	cvttss2sil	%xmm1, %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE247:
	.size	float_add_14, .-float_add_14
	.globl	float_add_15
	.type	float_add_15, @function
float_add_15:
.LFB248:
	.cfi_startproc
	endbr64
	subq	$72, %rsp
	.cfi_def_cfa_offset 80
	pxor	%xmm1, %xmm1
	cvtsd2ss	80(%rsi), %xmm1
	movss	.LC1(%rip), %xmm0
	addss	%xmm0, %xmm1
	pxor	%xmm3, %xmm3
	cvtsi2ssl	8(%rsi), %xmm3
	movss	%xmm3, (%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	88(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movaps	%xmm2, %xmm15
	movss	%xmm2, 8(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	96(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movaps	%xmm2, %xmm14
	movss	%xmm2, 12(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	104(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movaps	%xmm2, %xmm13
	movss	%xmm2, 16(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	112(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movaps	%xmm2, %xmm12
	movss	%xmm2, 20(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	120(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movaps	%xmm2, %xmm11
	movss	%xmm2, 24(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	128(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movaps	%xmm2, %xmm10
	movss	%xmm2, 28(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	136(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movaps	%xmm2, %xmm9
	movss	%xmm2, 32(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	144(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movaps	%xmm2, %xmm8
	movss	%xmm2, 36(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	152(%rsi), %xmm2
	movaps	%xmm2, %xmm7
	addss	%xmm0, %xmm7
	movss	%xmm7, 40(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	160(%rsi), %xmm2
	movaps	%xmm2, %xmm6
	addss	%xmm0, %xmm6
	movss	%xmm6, 44(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	168(%rsi), %xmm2
	movaps	%xmm2, %xmm5
	addss	%xmm0, %xmm5
	movss	%xmm5, 48(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	176(%rsi), %xmm2
	movaps	%xmm2, %xmm4
	addss	%xmm0, %xmm4
	movss	%xmm4, 52(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	184(%rsi), %xmm2
	movaps	%xmm2, %xmm3
	addss	%xmm0, %xmm3
	movss	%xmm3, 56(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	192(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movss	%xmm2, 4(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	200(%rsi), %xmm2
	addss	%xmm0, %xmm2
	movss	%xmm2, 60(%rsp)
	testq	%rdi, %rdi
	je	.L891
	movaps	%xmm2, %xmm0
	leaq	-1(%rdi), %rax
	movss	4(%rsp), %xmm2
.L892:
	addss	(%rsp), %xmm1
	addss	(%rsp), %xmm15
	addss	(%rsp), %xmm14
	addss	(%rsp), %xmm13
	addss	(%rsp), %xmm12
	addss	(%rsp), %xmm11
	addss	(%rsp), %xmm10
	addss	(%rsp), %xmm9
	addss	(%rsp), %xmm8
	addss	(%rsp), %xmm7
	addss	(%rsp), %xmm6
	addss	(%rsp), %xmm5
	addss	(%rsp), %xmm4
	addss	(%rsp), %xmm3
	addss	(%rsp), %xmm2
	addss	(%rsp), %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm15, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm15, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm15, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm15, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm15, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm15, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm15, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm15, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm15, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	addss	%xmm1, %xmm1
	addss	%xmm15, %xmm15
	addss	%xmm14, %xmm14
	addss	%xmm13, %xmm13
	addss	%xmm12, %xmm12
	addss	%xmm11, %xmm11
	addss	%xmm10, %xmm10
	addss	%xmm9, %xmm9
	addss	%xmm8, %xmm8
	addss	%xmm7, %xmm7
	addss	%xmm6, %xmm6
	addss	%xmm5, %xmm5
	addss	%xmm4, %xmm4
	addss	%xmm3, %xmm3
	addss	%xmm2, %xmm2
	addss	%xmm0, %xmm0
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L892
	movss	%xmm15, 8(%rsp)
	movss	%xmm14, 12(%rsp)
	movss	%xmm13, 16(%rsp)
	movss	%xmm12, 20(%rsp)
	movss	%xmm11, 24(%rsp)
	movss	%xmm10, 28(%rsp)
	movss	%xmm9, 32(%rsp)
	movss	%xmm8, 36(%rsp)
	movss	%xmm7, 40(%rsp)
	movss	%xmm6, 44(%rsp)
	movss	%xmm5, 48(%rsp)
	movss	%xmm4, 52(%rsp)
	movss	%xmm3, 56(%rsp)
	movss	%xmm2, 4(%rsp)
	movss	%xmm0, 60(%rsp)
.L891:
	cvttss2sil	%xmm1, %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE248:
	.size	float_add_15, .-float_add_15
	.globl	float_mul_0
	.type	float_mul_0, @function
float_mul_0:
.LFB249:
	.cfi_startproc
	endbr64
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	movsd	80(%rsi), %xmm3
	pxor	%xmm0, %xmm0
	cvtsd2ss	%xmm3, %xmm0
	mulss	.LC2(%rip), %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	4(%rsi), %xmm2
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm2, %xmm1
	mulsd	.LC3(%rip), %xmm1
	mulsd	%xmm3, %xmm1
	divsd	.LC4(%rip), %xmm1
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm1, %xmm5
	movss	%xmm5, 12(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L896
.L897:
	mulss	%xmm0, %xmm0
	movss	12(%rsp), %xmm4
	mulss	%xmm4, %xmm0
	mulss	%xmm0, %xmm0
	mulss	%xmm4, %xmm0
	mulss	%xmm0, %xmm0
	mulss	%xmm4, %xmm0
	mulss	%xmm0, %xmm0
	mulss	%xmm4, %xmm0
	mulss	%xmm0, %xmm0
	mulss	%xmm4, %xmm0
	mulss	%xmm0, %xmm0
	mulss	%xmm4, %xmm0
	mulss	%xmm0, %xmm0
	mulss	%xmm4, %xmm0
	mulss	%xmm0, %xmm0
	mulss	%xmm4, %xmm0
	mulss	%xmm0, %xmm0
	mulss	%xmm4, %xmm0
	mulss	%xmm0, %xmm0
	mulss	%xmm4, %xmm0
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L897
.L896:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE249:
	.size	float_mul_0, .-float_mul_0
	.globl	float_mul_1
	.type	float_mul_1, @function
float_mul_1:
.LFB250:
	.cfi_startproc
	endbr64
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	movsd	80(%rsi), %xmm2
	pxor	%xmm0, %xmm0
	cvtsd2ss	%xmm2, %xmm0
	movss	.LC2(%rip), %xmm4
	mulss	%xmm4, %xmm0
	pxor	%xmm1, %xmm1
	cvtsi2ssl	4(%rsi), %xmm1
	cvtss2sd	%xmm1, %xmm1
	mulsd	.LC3(%rip), %xmm1
	mulsd	%xmm1, %xmm2
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm2
	cvtsd2ss	%xmm2, %xmm2
	movss	%xmm2, 8(%rsp)
	movsd	88(%rsi), %xmm2
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm2, %xmm5
	mulss	%xmm4, %xmm5
	movss	%xmm5, 4(%rsp)
	mulsd	%xmm2, %xmm1
	divsd	%xmm3, %xmm1
	pxor	%xmm3, %xmm3
	cvtsd2ss	%xmm1, %xmm3
	movss	%xmm3, 12(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L901
.L902:
	mulss	%xmm0, %xmm0
	movss	8(%rsp), %xmm7
	mulss	%xmm7, %xmm0
	movss	4(%rsp), %xmm1
	mulss	%xmm1, %xmm1
	movss	12(%rsp), %xmm5
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	movss	%xmm1, 4(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L902
.L901:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE250:
	.size	float_mul_1, .-float_mul_1
	.globl	float_mul_2
	.type	float_mul_2, @function
float_mul_2:
.LFB251:
	.cfi_startproc
	endbr64
	subq	$40, %rsp
	.cfi_def_cfa_offset 48
	movsd	80(%rsi), %xmm2
	pxor	%xmm0, %xmm0
	cvtsd2ss	%xmm2, %xmm0
	movss	.LC2(%rip), %xmm4
	mulss	%xmm4, %xmm0
	pxor	%xmm1, %xmm1
	cvtsi2ssl	4(%rsi), %xmm1
	cvtss2sd	%xmm1, %xmm1
	mulsd	.LC3(%rip), %xmm1
	mulsd	%xmm1, %xmm2
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm2
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm2, %xmm6
	movss	%xmm6, 20(%rsp)
	movsd	88(%rsi), %xmm2
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm2, %xmm5
	mulss	%xmm4, %xmm5
	movss	%xmm5, 12(%rsp)
	mulsd	%xmm1, %xmm2
	divsd	%xmm3, %xmm2
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm2, %xmm6
	movss	%xmm6, 24(%rsp)
	movsd	96(%rsi), %xmm2
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm2, %xmm5
	mulss	%xmm4, %xmm5
	movss	%xmm5, 16(%rsp)
	mulsd	%xmm2, %xmm1
	divsd	%xmm3, %xmm1
	pxor	%xmm4, %xmm4
	cvtsd2ss	%xmm1, %xmm4
	movss	%xmm4, 28(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L906
.L907:
	mulss	%xmm0, %xmm0
	movss	20(%rsp), %xmm7
	mulss	%xmm7, %xmm0
	movss	12(%rsp), %xmm2
	mulss	%xmm2, %xmm2
	movss	24(%rsp), %xmm3
	mulss	%xmm3, %xmm2
	movss	16(%rsp), %xmm1
	mulss	%xmm1, %xmm1
	movss	28(%rsp), %xmm5
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm2, %xmm2
	mulss	%xmm3, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm2, %xmm2
	mulss	%xmm3, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm2, %xmm2
	mulss	%xmm3, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm2, %xmm2
	mulss	%xmm3, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm2, %xmm2
	mulss	%xmm3, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm2, %xmm2
	mulss	%xmm3, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm2, %xmm2
	mulss	%xmm3, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm2, %xmm2
	mulss	%xmm3, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm2, %xmm2
	mulss	%xmm3, %xmm2
	movss	%xmm2, 12(%rsp)
	mulss	%xmm1, %xmm1
	mulss	%xmm5, %xmm1
	movss	%xmm1, 16(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L907
.L906:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE251:
	.size	float_mul_2, .-float_mul_2
	.globl	float_mul_3
	.type	float_mul_3, @function
float_mul_3:
.LFB252:
	.cfi_startproc
	endbr64
	subq	$40, %rsp
	.cfi_def_cfa_offset 48
	movsd	80(%rsi), %xmm3
	pxor	%xmm0, %xmm0
	cvtsd2ss	%xmm3, %xmm0
	movss	.LC2(%rip), %xmm1
	mulss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	4(%rsi), %xmm2
	cvtss2sd	%xmm2, %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movsd	.LC4(%rip), %xmm4
	divsd	%xmm4, %xmm3
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm3, %xmm7
	movss	%xmm7, 16(%rsp)
	movsd	88(%rsi), %xmm3
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm3, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 4(%rsp)
	mulsd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm3, %xmm7
	movss	%xmm7, 20(%rsp)
	movsd	96(%rsi), %xmm3
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm3, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 8(%rsp)
	mulsd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm3, %xmm7
	movss	%xmm7, 24(%rsp)
	movsd	104(%rsi), %xmm3
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm3, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 12(%rsp)
	mulsd	%xmm3, %xmm2
	divsd	%xmm4, %xmm2
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm2, %xmm7
	movss	%xmm7, 28(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L911
.L912:
	mulss	%xmm0, %xmm0
	movss	16(%rsp), %xmm7
	mulss	%xmm7, %xmm0
	movss	4(%rsp), %xmm3
	mulss	%xmm3, %xmm3
	movss	20(%rsp), %xmm5
	mulss	%xmm5, %xmm3
	movss	8(%rsp), %xmm2
	mulss	%xmm2, %xmm2
	movss	24(%rsp), %xmm8
	mulss	%xmm8, %xmm2
	movss	12(%rsp), %xmm1
	mulss	%xmm1, %xmm1
	movss	28(%rsp), %xmm9
	mulss	%xmm9, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm3, %xmm3
	mulss	%xmm5, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm8, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm9, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm3, %xmm3
	mulss	%xmm5, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm8, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm9, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm3, %xmm3
	mulss	%xmm5, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm8, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm9, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm3, %xmm3
	mulss	%xmm5, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm8, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm9, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm3, %xmm3
	mulss	%xmm5, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm8, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm9, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm3, %xmm3
	mulss	%xmm5, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm8, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm9, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm3, %xmm3
	mulss	%xmm5, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm8, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm9, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm3, %xmm3
	mulss	%xmm5, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm8, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm9, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm3, %xmm3
	mulss	%xmm5, %xmm3
	movss	%xmm3, 4(%rsp)
	mulss	%xmm2, %xmm2
	mulss	%xmm8, %xmm2
	movss	%xmm2, 8(%rsp)
	mulss	%xmm1, %xmm1
	mulss	%xmm9, %xmm1
	movss	%xmm1, 12(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L912
.L911:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE252:
	.size	float_mul_3, .-float_mul_3
	.globl	float_mul_4
	.type	float_mul_4, @function
float_mul_4:
.LFB253:
	.cfi_startproc
	endbr64
	subq	$56, %rsp
	.cfi_def_cfa_offset 64
	movsd	80(%rsi), %xmm3
	pxor	%xmm0, %xmm0
	cvtsd2ss	%xmm3, %xmm0
	movss	.LC2(%rip), %xmm1
	mulss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	4(%rsi), %xmm2
	cvtss2sd	%xmm2, %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movsd	.LC4(%rip), %xmm4
	divsd	%xmm4, %xmm3
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm3, %xmm7
	movss	%xmm7, 28(%rsp)
	movsd	88(%rsi), %xmm3
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm3, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 12(%rsp)
	mulsd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm3, %xmm7
	movss	%xmm7, 32(%rsp)
	movsd	96(%rsi), %xmm3
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm3, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 16(%rsp)
	mulsd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm3, %xmm7
	movss	%xmm7, 36(%rsp)
	movsd	104(%rsi), %xmm3
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm3, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 20(%rsp)
	mulsd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm3, %xmm7
	movss	%xmm7, 40(%rsp)
	movsd	112(%rsi), %xmm3
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm3, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 24(%rsp)
	mulsd	%xmm3, %xmm2
	divsd	%xmm4, %xmm2
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm2, %xmm7
	movss	%xmm7, 44(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L916
.L917:
	mulss	%xmm0, %xmm0
	movss	28(%rsp), %xmm7
	mulss	%xmm7, %xmm0
	movss	12(%rsp), %xmm4
	mulss	%xmm4, %xmm4
	movss	32(%rsp), %xmm5
	mulss	%xmm5, %xmm4
	movss	16(%rsp), %xmm3
	mulss	%xmm3, %xmm3
	movss	36(%rsp), %xmm8
	mulss	%xmm8, %xmm3
	movss	20(%rsp), %xmm2
	mulss	%xmm2, %xmm2
	movss	40(%rsp), %xmm9
	mulss	%xmm9, %xmm2
	movss	24(%rsp), %xmm1
	mulss	%xmm1, %xmm1
	movss	44(%rsp), %xmm11
	mulss	%xmm11, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm4, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm8, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm9, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm11, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm4, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm8, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm9, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm11, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm4, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm8, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm9, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm11, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm4, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm8, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm9, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm11, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm4, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm8, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm9, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm11, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm4, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm8, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm9, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm11, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm4, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm8, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm9, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm11, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm4, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm8, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm9, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm11, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm4, %xmm4
	mulss	%xmm5, %xmm4
	movss	%xmm4, 12(%rsp)
	mulss	%xmm3, %xmm3
	mulss	%xmm8, %xmm3
	movss	%xmm3, 16(%rsp)
	mulss	%xmm2, %xmm2
	mulss	%xmm9, %xmm2
	movss	%xmm2, 20(%rsp)
	mulss	%xmm1, %xmm1
	mulss	%xmm11, %xmm1
	movss	%xmm1, 24(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L917
.L916:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE253:
	.size	float_mul_4, .-float_mul_4
	.globl	float_mul_5
	.type	float_mul_5, @function
float_mul_5:
.LFB254:
	.cfi_startproc
	endbr64
	subq	$56, %rsp
	.cfi_def_cfa_offset 64
	movsd	80(%rsi), %xmm3
	pxor	%xmm0, %xmm0
	cvtsd2ss	%xmm3, %xmm0
	movss	.LC2(%rip), %xmm1
	mulss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	4(%rsi), %xmm2
	cvtss2sd	%xmm2, %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 24(%rsp)
	movsd	88(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 4(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 28(%rsp)
	movsd	96(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 8(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 32(%rsp)
	movsd	104(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 12(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 36(%rsp)
	movsd	112(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 16(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 40(%rsp)
	movsd	120(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 20(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm2, %xmm7
	movss	%xmm7, 44(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L921
.L922:
	mulss	%xmm0, %xmm0
	movss	24(%rsp), %xmm7
	mulss	%xmm7, %xmm0
	movss	4(%rsp), %xmm5
	mulss	%xmm5, %xmm5
	movss	28(%rsp), %xmm8
	mulss	%xmm8, %xmm5
	movss	8(%rsp), %xmm4
	mulss	%xmm4, %xmm4
	movss	32(%rsp), %xmm10
	mulss	%xmm10, %xmm4
	movss	12(%rsp), %xmm3
	mulss	%xmm3, %xmm3
	movss	36(%rsp), %xmm11
	mulss	%xmm11, %xmm3
	movss	16(%rsp), %xmm2
	mulss	%xmm2, %xmm2
	movss	40(%rsp), %xmm13
	mulss	%xmm13, %xmm2
	movss	20(%rsp), %xmm1
	mulss	%xmm1, %xmm1
	movss	44(%rsp), %xmm15
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm5, %xmm5
	mulss	%xmm8, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm10, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm11, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm13, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm5, %xmm5
	mulss	%xmm8, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm10, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm11, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm13, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm5, %xmm5
	mulss	%xmm8, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm10, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm11, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm13, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm5, %xmm5
	mulss	%xmm8, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm10, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm11, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm13, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm5, %xmm5
	mulss	%xmm8, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm10, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm11, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm13, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm5, %xmm5
	mulss	%xmm8, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm10, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm11, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm13, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm5, %xmm5
	mulss	%xmm8, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm10, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm11, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm13, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm5, %xmm5
	mulss	%xmm8, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm10, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm11, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm13, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm5, %xmm5
	mulss	%xmm8, %xmm5
	movss	%xmm5, 4(%rsp)
	mulss	%xmm4, %xmm4
	mulss	%xmm10, %xmm4
	movss	%xmm4, 8(%rsp)
	mulss	%xmm3, %xmm3
	mulss	%xmm11, %xmm3
	movss	%xmm3, 12(%rsp)
	mulss	%xmm2, %xmm2
	mulss	%xmm13, %xmm2
	movss	%xmm2, 16(%rsp)
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	movss	%xmm1, 20(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L922
.L921:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE254:
	.size	float_mul_5, .-float_mul_5
	.globl	float_mul_6
	.type	float_mul_6, @function
float_mul_6:
.LFB255:
	.cfi_startproc
	endbr64
	subq	$72, %rsp
	.cfi_def_cfa_offset 80
	movsd	80(%rsi), %xmm3
	pxor	%xmm0, %xmm0
	cvtsd2ss	%xmm3, %xmm0
	movss	.LC2(%rip), %xmm1
	mulss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	4(%rsi), %xmm2
	cvtss2sd	%xmm2, %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm4, %xmm6
	movss	%xmm6, 12(%rsp)
	movsd	88(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm1, %xmm6
	movss	%xmm6, 20(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm10, %xmm10
	cvtsd2ss	%xmm4, %xmm10
	movss	%xmm10, 40(%rsp)
	movsd	96(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm9
	movss	%xmm5, 24(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm11, %xmm11
	cvtsd2ss	%xmm4, %xmm11
	movss	%xmm11, 44(%rsp)
	movsd	104(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm8
	movss	%xmm5, 28(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm12, %xmm12
	cvtsd2ss	%xmm4, %xmm12
	movss	%xmm12, 48(%rsp)
	movsd	112(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm7
	mulss	%xmm1, %xmm7
	movss	%xmm7, 32(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm13, %xmm13
	cvtsd2ss	%xmm4, %xmm13
	movss	%xmm13, 52(%rsp)
	movsd	120(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 16(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm14, %xmm14
	cvtsd2ss	%xmm4, %xmm14
	movss	%xmm14, 56(%rsp)
	movsd	128(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 36(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	pxor	%xmm15, %xmm15
	cvtsd2ss	%xmm2, %xmm15
	movss	%xmm15, 60(%rsp)
	testq	%rdi, %rdi
	je	.L926
	movaps	%xmm5, %xmm1
	leaq	-1(%rdi), %rax
	movaps	%xmm7, %xmm5
	movss	16(%rsp), %xmm4
	movaps	%xmm1, %xmm7
.L927:
	mulss	%xmm0, %xmm0
	mulss	12(%rsp), %xmm0
	mulss	%xmm6, %xmm6
	movaps	%xmm6, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm9, %xmm2
	mulss	%xmm9, %xmm2
	mulss	%xmm11, %xmm2
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm1
	mulss	%xmm12, %xmm1
	mulss	%xmm5, %xmm5
	mulss	%xmm13, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm14, %xmm4
	mulss	%xmm7, %xmm7
	mulss	%xmm15, %xmm7
	mulss	%xmm0, %xmm0
	movss	12(%rsp), %xmm8
	mulss	%xmm8, %xmm0
	mulss	%xmm3, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm6
	mulss	%xmm2, %xmm2
	mulss	%xmm11, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm12, %xmm1
	mulss	%xmm5, %xmm5
	mulss	%xmm13, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm14, %xmm4
	mulss	%xmm7, %xmm7
	mulss	%xmm15, %xmm7
	mulss	%xmm0, %xmm0
	mulss	%xmm8, %xmm0
	mulss	%xmm3, %xmm6
	mulss	%xmm10, %xmm6
	mulss	%xmm2, %xmm2
	mulss	%xmm11, %xmm2
	movaps	%xmm2, %xmm9
	mulss	%xmm1, %xmm1
	mulss	%xmm12, %xmm1
	movaps	%xmm1, %xmm8
	mulss	%xmm5, %xmm5
	mulss	%xmm13, %xmm5
	movaps	%xmm5, %xmm3
	mulss	%xmm4, %xmm4
	mulss	%xmm14, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm7, %xmm7
	mulss	%xmm15, %xmm7
	mulss	%xmm0, %xmm0
	mulss	12(%rsp), %xmm0
	mulss	%xmm6, %xmm6
	mulss	%xmm10, %xmm6
	mulss	%xmm9, %xmm9
	movaps	%xmm9, %xmm5
	mulss	%xmm11, %xmm5
	mulss	%xmm1, %xmm8
	movaps	%xmm8, %xmm4
	mulss	%xmm12, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm13, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm14, %xmm2
	mulss	%xmm7, %xmm7
	movaps	%xmm7, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	movss	12(%rsp), %xmm7
	mulss	%xmm7, %xmm0
	mulss	%xmm6, %xmm6
	mulss	%xmm10, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm11, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm12, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm13, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm14, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm6, %xmm6
	mulss	%xmm10, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm11, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm12, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm13, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm14, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm6, %xmm6
	mulss	%xmm10, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm11, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm12, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm13, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm14, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm6, %xmm6
	mulss	%xmm10, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm11, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm12, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm13, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm14, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm6, %xmm6
	mulss	%xmm10, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm11, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm12, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm13, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm14, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm6, %xmm6
	mulss	%xmm10, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm9
	mulss	%xmm4, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm4, %xmm8
	mulss	%xmm3, %xmm3
	mulss	%xmm13, %xmm3
	movaps	%xmm3, %xmm5
	mulss	%xmm2, %xmm2
	mulss	%xmm14, %xmm2
	movaps	%xmm2, %xmm4
	mulss	%xmm1, %xmm1
	movaps	%xmm1, %xmm7
	mulss	%xmm15, %xmm7
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L927
	movss	%xmm6, 20(%rsp)
	movss	%xmm9, 24(%rsp)
	movss	%xmm8, 28(%rsp)
	movss	%xmm3, 32(%rsp)
	movss	%xmm2, 16(%rsp)
	movss	%xmm7, 36(%rsp)
.L926:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE255:
	.size	float_mul_6, .-float_mul_6
	.globl	float_mul_7
	.type	float_mul_7, @function
float_mul_7:
.LFB256:
	.cfi_startproc
	endbr64
	subq	$72, %rsp
	.cfi_def_cfa_offset 80
	movsd	80(%rsi), %xmm3
	pxor	%xmm0, %xmm0
	cvtsd2ss	%xmm3, %xmm0
	movss	.LC2(%rip), %xmm1
	mulss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	4(%rsi), %xmm2
	cvtss2sd	%xmm2, %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm4, %xmm6
	movss	%xmm6, 16(%rsp)
	movsd	88(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm10
	movss	%xmm5, 24(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 20(%rsp)
	movsd	96(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm9
	movss	%xmm5, 28(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm4, %xmm6
	movss	%xmm6, 12(%rsp)
	movsd	104(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm8
	movss	%xmm5, 32(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 8(%rsp)
	movsd	112(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm11
	movss	%xmm5, 36(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 4(%rsp)
	movsd	120(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm7
	movss	%xmm5, 40(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm13, %xmm13
	cvtsd2ss	%xmm4, %xmm13
	movss	%xmm13, 52(%rsp)
	movsd	128(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm6
	movss	%xmm5, 44(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm14, %xmm14
	cvtsd2ss	%xmm4, %xmm14
	movss	%xmm14, 56(%rsp)
	movsd	136(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 48(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	pxor	%xmm15, %xmm15
	cvtsd2ss	%xmm2, %xmm15
	movss	%xmm15, 60(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L931
.L932:
	mulss	%xmm0, %xmm0
	movss	16(%rsp), %xmm12
	mulss	%xmm12, %xmm0
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	movss	20(%rsp), %xmm10
	mulss	%xmm10, %xmm4
	movaps	%xmm9, %xmm3
	mulss	%xmm9, %xmm3
	mulss	12(%rsp), %xmm3
	movaps	%xmm8, %xmm2
	mulss	%xmm8, %xmm2
	mulss	8(%rsp), %xmm2
	mulss	%xmm11, %xmm11
	movaps	%xmm11, %xmm1
	movss	4(%rsp), %xmm11
	mulss	%xmm11, %xmm1
	mulss	%xmm7, %xmm7
	mulss	%xmm13, %xmm7
	mulss	%xmm6, %xmm6
	mulss	%xmm14, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm15, %xmm5
	mulss	%xmm0, %xmm0
	movaps	%xmm12, %xmm8
	mulss	%xmm12, %xmm0
	mulss	%xmm4, %xmm4
	movaps	%xmm10, %xmm9
	mulss	%xmm10, %xmm4
	mulss	%xmm3, %xmm3
	movss	12(%rsp), %xmm10
	mulss	%xmm10, %xmm3
	mulss	%xmm2, %xmm2
	mulss	8(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm11, %xmm1
	mulss	%xmm7, %xmm7
	mulss	%xmm13, %xmm7
	mulss	%xmm6, %xmm6
	mulss	%xmm14, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm15, %xmm5
	movaps	%xmm5, %xmm12
	mulss	%xmm0, %xmm0
	movaps	%xmm8, %xmm5
	mulss	%xmm8, %xmm0
	mulss	%xmm4, %xmm4
	movaps	%xmm4, %xmm11
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm11
	mulss	%xmm3, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	%xmm2, %xmm2
	mulss	8(%rsp), %xmm2
	movaps	%xmm2, %xmm9
	mulss	%xmm1, %xmm1
	mulss	4(%rsp), %xmm1
	movaps	%xmm1, %xmm8
	mulss	%xmm7, %xmm7
	mulss	%xmm13, %xmm7
	movaps	%xmm7, %xmm3
	mulss	%xmm6, %xmm6
	mulss	%xmm14, %xmm6
	movaps	%xmm6, %xmm2
	mulss	%xmm12, %xmm12
	movaps	%xmm12, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	movaps	%xmm5, %xmm12
	mulss	%xmm5, %xmm0
	mulss	%xmm11, %xmm11
	movaps	%xmm11, %xmm7
	movaps	%xmm4, %xmm11
	mulss	%xmm4, %xmm7
	mulss	%xmm10, %xmm10
	movaps	%xmm10, %xmm6
	movss	12(%rsp), %xmm10
	mulss	%xmm10, %xmm6
	mulss	%xmm9, %xmm9
	movaps	%xmm9, %xmm5
	movss	8(%rsp), %xmm9
	mulss	%xmm9, %xmm5
	mulss	%xmm8, %xmm8
	mulss	4(%rsp), %xmm8
	movaps	%xmm8, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm13, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm14, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm12, %xmm0
	mulss	%xmm7, %xmm7
	mulss	%xmm11, %xmm7
	mulss	%xmm6, %xmm6
	movaps	%xmm10, %xmm8
	mulss	%xmm10, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm9, %xmm5
	mulss	%xmm4, %xmm4
	movss	4(%rsp), %xmm10
	mulss	%xmm10, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm13, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm14, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm12, %xmm0
	mulss	%xmm7, %xmm7
	mulss	%xmm11, %xmm7
	mulss	%xmm6, %xmm6
	mulss	%xmm8, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm9, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm10, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm13, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm14, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm12, %xmm0
	mulss	%xmm7, %xmm7
	mulss	%xmm11, %xmm7
	mulss	%xmm6, %xmm6
	mulss	%xmm8, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm9, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm10, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm13, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm14, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm12, %xmm0
	mulss	%xmm7, %xmm7
	mulss	%xmm11, %xmm7
	mulss	%xmm6, %xmm6
	mulss	%xmm8, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm9, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm10, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm13, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm14, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm12, %xmm0
	mulss	%xmm7, %xmm7
	mulss	%xmm11, %xmm7
	mulss	%xmm6, %xmm6
	mulss	%xmm8, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm9, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm10, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm13, %xmm3
	mulss	%xmm2, %xmm2
	mulss	%xmm14, %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm12, %xmm0
	mulss	%xmm7, %xmm7
	mulss	%xmm11, %xmm7
	movaps	%xmm7, %xmm10
	mulss	%xmm6, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm9
	mulss	%xmm5, %xmm5
	mulss	8(%rsp), %xmm5
	movaps	%xmm5, %xmm8
	mulss	%xmm4, %xmm4
	mulss	4(%rsp), %xmm4
	movaps	%xmm4, %xmm11
	mulss	%xmm3, %xmm3
	movaps	%xmm3, %xmm7
	mulss	%xmm13, %xmm7
	mulss	%xmm2, %xmm2
	movaps	%xmm2, %xmm6
	mulss	%xmm14, %xmm6
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	movaps	%xmm1, %xmm5
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L932
	movss	%xmm10, 24(%rsp)
	movss	%xmm9, 28(%rsp)
	movss	%xmm8, 32(%rsp)
	movss	%xmm4, 36(%rsp)
	movss	%xmm7, 40(%rsp)
	movss	%xmm6, 44(%rsp)
	movss	%xmm1, 48(%rsp)
.L931:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE256:
	.size	float_mul_7, .-float_mul_7
	.globl	float_mul_8
	.type	float_mul_8, @function
float_mul_8:
.LFB257:
	.cfi_startproc
	endbr64
	subq	$88, %rsp
	.cfi_def_cfa_offset 96
	movsd	80(%rsi), %xmm3
	pxor	%xmm0, %xmm0
	cvtsd2ss	%xmm3, %xmm0
	movss	.LC2(%rip), %xmm1
	mulss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	4(%rsi), %xmm2
	cvtss2sd	%xmm2, %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 24(%rsp)
	movsd	88(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm7
	mulss	%xmm1, %xmm7
	movss	%xmm7, 44(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 28(%rsp)
	movsd	96(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm14
	movss	%xmm5, 48(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 32(%rsp)
	movsd	104(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm13
	movss	%xmm5, 52(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 40(%rsp)
	movsd	112(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm12
	movss	%xmm5, 56(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 36(%rsp)
	movsd	120(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm11
	movss	%xmm5, 60(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 20(%rsp)
	movsd	128(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm10
	movss	%xmm5, 64(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 16(%rsp)
	movsd	136(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm9
	movss	%xmm5, 68(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 12(%rsp)
	movsd	144(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 72(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	pxor	%xmm3, %xmm3
	cvtsd2ss	%xmm2, %xmm3
	movss	%xmm3, 76(%rsp)
	testq	%rdi, %rdi
	je	.L936
	movaps	%xmm5, %xmm8
	leaq	-1(%rdi), %rax
	movaps	%xmm3, %xmm15
.L937:
	mulss	%xmm0, %xmm0
	mulss	24(%rsp), %xmm0
	mulss	%xmm7, %xmm7
	mulss	28(%rsp), %xmm7
	mulss	%xmm14, %xmm14
	movaps	%xmm14, %xmm6
	movss	32(%rsp), %xmm14
	mulss	%xmm14, %xmm6
	movaps	%xmm13, %xmm5
	mulss	%xmm13, %xmm5
	movss	40(%rsp), %xmm13
	mulss	%xmm13, %xmm5
	movaps	%xmm12, %xmm4
	mulss	%xmm12, %xmm4
	movss	36(%rsp), %xmm12
	mulss	%xmm12, %xmm4
	movaps	%xmm11, %xmm3
	mulss	%xmm11, %xmm3
	mulss	20(%rsp), %xmm3
	mulss	%xmm10, %xmm10
	movaps	%xmm10, %xmm2
	movss	16(%rsp), %xmm10
	mulss	%xmm10, %xmm2
	mulss	%xmm9, %xmm9
	movaps	%xmm9, %xmm1
	movss	12(%rsp), %xmm9
	mulss	%xmm9, %xmm1
	mulss	%xmm8, %xmm8
	mulss	%xmm15, %xmm8
	mulss	%xmm0, %xmm0
	mulss	24(%rsp), %xmm0
	mulss	%xmm7, %xmm7
	mulss	28(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	mulss	%xmm5, %xmm5
	movaps	%xmm5, %xmm6
	movaps	%xmm13, %xmm5
	mulss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	mulss	%xmm4, %xmm4
	movaps	%xmm4, %xmm6
	movaps	%xmm12, %xmm4
	mulss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	mulss	%xmm3, %xmm3
	movaps	%xmm3, %xmm6
	mulss	20(%rsp), %xmm6
	movaps	%xmm6, %xmm11
	mulss	%xmm2, %xmm2
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm2
	movaps	%xmm2, %xmm10
	mulss	%xmm1, %xmm1
	mulss	%xmm9, %xmm1
	movaps	%xmm1, %xmm9
	mulss	%xmm8, %xmm8
	mulss	%xmm15, %xmm8
	mulss	%xmm0, %xmm0
	mulss	24(%rsp), %xmm0
	mulss	%xmm7, %xmm7
	mulss	28(%rsp), %xmm7
	mulss	%xmm14, %xmm14
	mulss	32(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	movaps	%xmm13, %xmm6
	movaps	%xmm5, %xmm13
	mulss	%xmm5, %xmm6
	mulss	%xmm12, %xmm12
	movaps	%xmm12, %xmm5
	mulss	%xmm4, %xmm5
	mulss	%xmm11, %xmm11
	movaps	%xmm11, %xmm12
	mulss	20(%rsp), %xmm12
	movaps	%xmm12, %xmm4
	mulss	%xmm2, %xmm10
	movaps	%xmm10, %xmm1
	mulss	%xmm3, %xmm1
	movaps	%xmm1, %xmm3
	mulss	%xmm9, %xmm9
	movaps	%xmm9, %xmm1
	mulss	12(%rsp), %xmm1
	movaps	%xmm1, %xmm2
	mulss	%xmm8, %xmm8
	movaps	%xmm8, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	movss	24(%rsp), %xmm9
	mulss	%xmm9, %xmm0
	mulss	%xmm7, %xmm7
	movss	28(%rsp), %xmm10
	mulss	%xmm10, %xmm7
	mulss	%xmm14, %xmm14
	movss	32(%rsp), %xmm11
	mulss	%xmm11, %xmm14
	mulss	%xmm6, %xmm6
	movaps	%xmm13, %xmm12
	mulss	%xmm13, %xmm6
	mulss	%xmm5, %xmm5
	movss	36(%rsp), %xmm8
	mulss	%xmm8, %xmm5
	mulss	%xmm4, %xmm4
	movss	20(%rsp), %xmm13
	mulss	%xmm13, %xmm4
	mulss	%xmm3, %xmm3
	mulss	16(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm9, %xmm0
	mulss	%xmm7, %xmm7
	mulss	%xmm10, %xmm7
	mulss	%xmm14, %xmm14
	mulss	%xmm11, %xmm14
	mulss	%xmm6, %xmm6
	mulss	%xmm12, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm8, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm13, %xmm4
	mulss	%xmm3, %xmm3
	mulss	16(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm9, %xmm0
	mulss	%xmm7, %xmm7
	mulss	%xmm10, %xmm7
	mulss	%xmm14, %xmm14
	mulss	%xmm11, %xmm14
	mulss	%xmm6, %xmm6
	mulss	%xmm12, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm8, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm13, %xmm4
	mulss	%xmm3, %xmm3
	mulss	16(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm9, %xmm0
	mulss	%xmm7, %xmm7
	mulss	%xmm10, %xmm7
	mulss	%xmm14, %xmm14
	mulss	%xmm11, %xmm14
	mulss	%xmm6, %xmm6
	mulss	%xmm12, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm8, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm13, %xmm4
	mulss	%xmm3, %xmm3
	mulss	16(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm9, %xmm0
	mulss	%xmm7, %xmm7
	mulss	%xmm10, %xmm7
	mulss	%xmm14, %xmm14
	mulss	%xmm11, %xmm14
	mulss	%xmm6, %xmm6
	mulss	%xmm12, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm8, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm13, %xmm4
	mulss	%xmm3, %xmm3
	mulss	16(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm9, %xmm0
	mulss	%xmm7, %xmm7
	mulss	%xmm10, %xmm7
	mulss	%xmm14, %xmm14
	mulss	%xmm11, %xmm14
	mulss	%xmm6, %xmm6
	mulss	%xmm12, %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm8, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm13, %xmm4
	mulss	%xmm3, %xmm3
	mulss	16(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm9, %xmm0
	mulss	%xmm7, %xmm7
	mulss	%xmm10, %xmm7
	mulss	%xmm14, %xmm14
	mulss	%xmm11, %xmm14
	mulss	%xmm6, %xmm6
	movaps	%xmm6, %xmm11
	mulss	%xmm12, %xmm11
	movaps	%xmm11, %xmm13
	mulss	%xmm5, %xmm5
	movaps	%xmm5, %xmm12
	mulss	%xmm8, %xmm12
	mulss	%xmm4, %xmm4
	mulss	20(%rsp), %xmm4
	movaps	%xmm4, %xmm11
	mulss	%xmm3, %xmm3
	mulss	16(%rsp), %xmm3
	movaps	%xmm3, %xmm10
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	movaps	%xmm2, %xmm9
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	movaps	%xmm1, %xmm8
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L937
	movss	%xmm7, 44(%rsp)
	movss	%xmm14, 48(%rsp)
	movss	%xmm13, 52(%rsp)
	movss	%xmm12, 56(%rsp)
	movss	%xmm4, 60(%rsp)
	movss	%xmm3, 64(%rsp)
	movss	%xmm2, 68(%rsp)
	movss	%xmm1, 72(%rsp)
.L936:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	68(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	76(%rsp), %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE257:
	.size	float_mul_8, .-float_mul_8
	.globl	float_mul_9
	.type	float_mul_9, @function
float_mul_9:
.LFB258:
	.cfi_startproc
	endbr64
	subq	$88, %rsp
	.cfi_def_cfa_offset 96
	movsd	80(%rsi), %xmm3
	pxor	%xmm0, %xmm0
	cvtsd2ss	%xmm3, %xmm0
	movss	.LC2(%rip), %xmm1
	mulss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	4(%rsi), %xmm2
	cvtss2sd	%xmm2, %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm4, %xmm6
	movss	%xmm6, 20(%rsp)
	movsd	88(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm1, %xmm6
	movss	%xmm6, 40(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 24(%rsp)
	movsd	96(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm12
	movss	%xmm5, 44(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 28(%rsp)
	movsd	104(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm13
	movss	%xmm5, 48(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 32(%rsp)
	movsd	112(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm14
	movss	%xmm5, 52(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 36(%rsp)
	movsd	120(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm11
	movss	%xmm5, 56(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 4(%rsp)
	movsd	128(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm10
	movss	%xmm5, 60(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 16(%rsp)
	movsd	136(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm9
	movss	%xmm5, 64(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 8(%rsp)
	movsd	144(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm8
	movss	%xmm5, 68(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 12(%rsp)
	movsd	152(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm7
	mulss	%xmm1, %xmm7
	movss	%xmm7, 72(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	pxor	%xmm3, %xmm3
	cvtsd2ss	%xmm2, %xmm3
	movss	%xmm3, 76(%rsp)
	testq	%rdi, %rdi
	je	.L941
	leaq	-1(%rdi), %rax
	movaps	%xmm3, %xmm15
.L942:
	mulss	%xmm0, %xmm0
	mulss	20(%rsp), %xmm0
	mulss	%xmm6, %xmm6
	mulss	24(%rsp), %xmm6
	mulss	%xmm12, %xmm12
	movaps	%xmm12, %xmm5
	movss	28(%rsp), %xmm12
	mulss	%xmm12, %xmm5
	movaps	%xmm13, %xmm4
	mulss	%xmm13, %xmm4
	mulss	32(%rsp), %xmm4
	mulss	%xmm14, %xmm14
	movaps	%xmm14, %xmm3
	movss	36(%rsp), %xmm14
	mulss	%xmm14, %xmm3
	movaps	%xmm11, %xmm2
	mulss	%xmm11, %xmm2
	movss	4(%rsp), %xmm11
	mulss	%xmm11, %xmm2
	mulss	%xmm10, %xmm10
	movaps	%xmm10, %xmm1
	movss	16(%rsp), %xmm10
	mulss	%xmm10, %xmm1
	mulss	%xmm9, %xmm9
	mulss	8(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	12(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	%xmm15, %xmm7
	mulss	%xmm0, %xmm0
	mulss	20(%rsp), %xmm0
	mulss	%xmm6, %xmm6
	mulss	24(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	mulss	%xmm4, %xmm4
	movaps	%xmm4, %xmm5
	movss	32(%rsp), %xmm4
	mulss	%xmm4, %xmm5
	movaps	%xmm5, %xmm13
	mulss	%xmm3, %xmm3
	mulss	%xmm14, %xmm3
	movaps	%xmm3, %xmm14
	mulss	%xmm2, %xmm2
	mulss	%xmm11, %xmm2
	movaps	%xmm2, %xmm11
	mulss	%xmm1, %xmm1
	movaps	%xmm10, %xmm2
	mulss	%xmm10, %xmm1
	movaps	%xmm1, %xmm10
	mulss	%xmm9, %xmm9
	movss	8(%rsp), %xmm3
	mulss	%xmm3, %xmm9
	mulss	%xmm8, %xmm8
	movss	12(%rsp), %xmm1
	mulss	%xmm1, %xmm8
	mulss	%xmm7, %xmm7
	mulss	%xmm15, %xmm7
	mulss	%xmm0, %xmm0
	mulss	20(%rsp), %xmm0
	mulss	%xmm6, %xmm6
	mulss	24(%rsp), %xmm6
	mulss	%xmm12, %xmm12
	mulss	28(%rsp), %xmm12
	mulss	%xmm5, %xmm13
	mulss	%xmm4, %xmm13
	mulss	%xmm14, %xmm14
	mulss	36(%rsp), %xmm14
	mulss	%xmm11, %xmm11
	movaps	%xmm11, %xmm5
	mulss	4(%rsp), %xmm5
	mulss	%xmm10, %xmm10
	movaps	%xmm10, %xmm4
	mulss	%xmm2, %xmm4
	mulss	%xmm9, %xmm9
	mulss	%xmm3, %xmm9
	movaps	%xmm9, %xmm3
	mulss	%xmm8, %xmm8
	mulss	%xmm1, %xmm8
	movaps	%xmm8, %xmm2
	mulss	%xmm7, %xmm7
	mulss	%xmm15, %xmm7
	movaps	%xmm7, %xmm1
	mulss	%xmm0, %xmm0
	movss	20(%rsp), %xmm7
	mulss	%xmm7, %xmm0
	mulss	%xmm6, %xmm6
	movss	24(%rsp), %xmm11
	mulss	%xmm11, %xmm6
	mulss	%xmm12, %xmm12
	movss	28(%rsp), %xmm10
	mulss	%xmm10, %xmm12
	mulss	%xmm13, %xmm13
	movss	32(%rsp), %xmm8
	mulss	%xmm8, %xmm13
	mulss	%xmm14, %xmm14
	movss	36(%rsp), %xmm9
	mulss	%xmm9, %xmm14
	mulss	%xmm5, %xmm5
	mulss	4(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	16(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	8(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm6, %xmm6
	mulss	%xmm11, %xmm6
	mulss	%xmm12, %xmm12
	mulss	%xmm10, %xmm12
	mulss	%xmm13, %xmm13
	mulss	%xmm8, %xmm13
	mulss	%xmm14, %xmm14
	mulss	%xmm9, %xmm14
	mulss	%xmm5, %xmm5
	mulss	4(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	16(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	8(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm6, %xmm6
	mulss	%xmm11, %xmm6
	mulss	%xmm12, %xmm12
	mulss	%xmm10, %xmm12
	mulss	%xmm13, %xmm13
	mulss	%xmm8, %xmm13
	mulss	%xmm14, %xmm14
	mulss	%xmm9, %xmm14
	mulss	%xmm5, %xmm5
	mulss	4(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	16(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	8(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm6, %xmm6
	mulss	%xmm11, %xmm6
	mulss	%xmm12, %xmm12
	mulss	%xmm10, %xmm12
	mulss	%xmm13, %xmm13
	mulss	%xmm8, %xmm13
	mulss	%xmm14, %xmm14
	mulss	%xmm9, %xmm14
	mulss	%xmm5, %xmm5
	mulss	4(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	16(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	8(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm6, %xmm6
	mulss	%xmm11, %xmm6
	mulss	%xmm12, %xmm12
	mulss	%xmm10, %xmm12
	mulss	%xmm13, %xmm13
	mulss	%xmm8, %xmm13
	mulss	%xmm14, %xmm14
	mulss	%xmm9, %xmm14
	mulss	%xmm5, %xmm5
	mulss	4(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	16(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	8(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm6, %xmm6
	mulss	%xmm11, %xmm6
	mulss	%xmm12, %xmm12
	mulss	%xmm10, %xmm12
	mulss	%xmm13, %xmm13
	mulss	%xmm8, %xmm13
	mulss	%xmm14, %xmm14
	mulss	%xmm9, %xmm14
	mulss	%xmm5, %xmm5
	mulss	4(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	16(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	8(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm7, %xmm0
	mulss	%xmm6, %xmm6
	mulss	%xmm11, %xmm6
	mulss	%xmm12, %xmm12
	mulss	%xmm10, %xmm12
	mulss	%xmm13, %xmm13
	mulss	%xmm8, %xmm13
	mulss	%xmm14, %xmm14
	mulss	%xmm9, %xmm14
	mulss	%xmm5, %xmm5
	movaps	%xmm5, %xmm11
	mulss	4(%rsp), %xmm11
	mulss	%xmm4, %xmm4
	mulss	16(%rsp), %xmm4
	movaps	%xmm4, %xmm10
	mulss	%xmm3, %xmm3
	movaps	%xmm3, %xmm9
	mulss	8(%rsp), %xmm9
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	movaps	%xmm2, %xmm8
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	movaps	%xmm1, %xmm7
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L942
	movss	%xmm6, 40(%rsp)
	movss	%xmm12, 44(%rsp)
	movss	%xmm13, 48(%rsp)
	movss	%xmm14, 52(%rsp)
	movss	%xmm11, 56(%rsp)
	movss	%xmm4, 60(%rsp)
	movss	%xmm9, 64(%rsp)
	movss	%xmm2, 68(%rsp)
	movss	%xmm1, 72(%rsp)
.L941:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	68(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	76(%rsp), %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE258:
	.size	float_mul_9, .-float_mul_9
	.globl	float_mul_10
	.type	float_mul_10, @function
float_mul_10:
.LFB259:
	.cfi_startproc
	endbr64
	subq	$104, %rsp
	.cfi_def_cfa_offset 112
	movsd	80(%rsi), %xmm3
	pxor	%xmm0, %xmm0
	cvtsd2ss	%xmm3, %xmm0
	movss	.LC2(%rip), %xmm1
	mulss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	4(%rsi), %xmm2
	cvtss2sd	%xmm2, %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 36(%rsp)
	movsd	88(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm6
	movss	%xmm5, 52(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 40(%rsp)
	movsd	96(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm11
	movss	%xmm5, 56(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 44(%rsp)
	movsd	104(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm12
	movss	%xmm5, 60(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 48(%rsp)
	movsd	112(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm13
	movss	%xmm5, 64(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 12(%rsp)
	movsd	120(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm14
	movss	%xmm5, 68(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 16(%rsp)
	movsd	128(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm10
	movss	%xmm5, 72(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 20(%rsp)
	movsd	136(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm9
	movss	%xmm5, 76(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 24(%rsp)
	movsd	144(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm8
	movss	%xmm5, 80(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 28(%rsp)
	movsd	152(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm7
	movss	%xmm5, 84(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 32(%rsp)
	movsd	160(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm1
	movss	%xmm5, 88(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm2, %xmm5
	movss	%xmm5, 92(%rsp)
	testq	%rdi, %rdi
	je	.L946
	leaq	-1(%rdi), %rax
	movaps	%xmm5, %xmm15
	movaps	%xmm6, %xmm5
	movaps	%xmm1, %xmm6
.L947:
	mulss	%xmm0, %xmm0
	mulss	36(%rsp), %xmm0
	mulss	%xmm5, %xmm5
	mulss	40(%rsp), %xmm5
	movaps	%xmm11, %xmm4
	mulss	%xmm11, %xmm4
	mulss	44(%rsp), %xmm4
	movaps	%xmm12, %xmm3
	mulss	%xmm12, %xmm3
	mulss	48(%rsp), %xmm3
	movaps	%xmm13, %xmm2
	mulss	%xmm13, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm14, %xmm14
	movaps	%xmm14, %xmm1
	mulss	16(%rsp), %xmm1
	mulss	%xmm10, %xmm10
	mulss	20(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	24(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	28(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	32(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	%xmm15, %xmm6
	mulss	%xmm0, %xmm0
	movss	36(%rsp), %xmm11
	mulss	%xmm11, %xmm0
	mulss	%xmm5, %xmm5
	movss	40(%rsp), %xmm12
	mulss	%xmm12, %xmm5
	mulss	%xmm4, %xmm4
	movss	44(%rsp), %xmm13
	mulss	%xmm13, %xmm4
	mulss	%xmm3, %xmm3
	movss	48(%rsp), %xmm14
	mulss	%xmm14, %xmm3
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	16(%rsp), %xmm1
	mulss	%xmm10, %xmm10
	mulss	20(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	24(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	28(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	32(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	%xmm15, %xmm6
	mulss	%xmm0, %xmm0
	mulss	%xmm11, %xmm0
	mulss	%xmm5, %xmm5
	mulss	%xmm12, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm13, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm14, %xmm3
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	16(%rsp), %xmm1
	mulss	%xmm10, %xmm10
	mulss	20(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	24(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	28(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	32(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	%xmm15, %xmm6
	mulss	%xmm0, %xmm0
	mulss	%xmm11, %xmm0
	mulss	%xmm5, %xmm5
	mulss	%xmm12, %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm4, %xmm11
	mulss	%xmm3, %xmm3
	mulss	%xmm14, %xmm3
	movaps	%xmm3, %xmm12
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	movaps	%xmm2, %xmm13
	mulss	%xmm1, %xmm1
	mulss	16(%rsp), %xmm1
	movaps	%xmm1, %xmm14
	mulss	%xmm10, %xmm10
	mulss	20(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	movaps	%xmm9, %xmm4
	mulss	24(%rsp), %xmm4
	mulss	%xmm8, %xmm8
	mulss	28(%rsp), %xmm8
	movaps	%xmm8, %xmm3
	mulss	%xmm7, %xmm7
	mulss	32(%rsp), %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm6, %xmm6
	mulss	%xmm15, %xmm6
	movaps	%xmm6, %xmm1
	mulss	%xmm0, %xmm0
	movss	36(%rsp), %xmm6
	mulss	%xmm6, %xmm0
	mulss	%xmm5, %xmm5
	movss	40(%rsp), %xmm7
	mulss	%xmm7, %xmm5
	mulss	%xmm11, %xmm11
	movss	44(%rsp), %xmm8
	mulss	%xmm8, %xmm11
	mulss	%xmm12, %xmm12
	movss	48(%rsp), %xmm9
	mulss	%xmm9, %xmm12
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm10, %xmm10
	mulss	20(%rsp), %xmm10
	mulss	%xmm4, %xmm4
	mulss	24(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	28(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	32(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm6, %xmm0
	mulss	%xmm5, %xmm5
	mulss	%xmm7, %xmm5
	mulss	%xmm11, %xmm11
	mulss	%xmm8, %xmm11
	mulss	%xmm12, %xmm12
	mulss	%xmm9, %xmm12
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm10, %xmm10
	mulss	20(%rsp), %xmm10
	mulss	%xmm4, %xmm4
	mulss	24(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	28(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	32(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm6, %xmm0
	mulss	%xmm5, %xmm5
	mulss	%xmm7, %xmm5
	mulss	%xmm11, %xmm11
	mulss	%xmm8, %xmm11
	mulss	%xmm12, %xmm12
	mulss	%xmm9, %xmm12
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm10, %xmm10
	mulss	20(%rsp), %xmm10
	mulss	%xmm4, %xmm4
	mulss	24(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	28(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	32(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm6, %xmm0
	mulss	%xmm5, %xmm5
	mulss	%xmm7, %xmm5
	mulss	%xmm11, %xmm11
	mulss	%xmm8, %xmm11
	mulss	%xmm12, %xmm12
	mulss	%xmm9, %xmm12
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm10, %xmm10
	mulss	20(%rsp), %xmm10
	mulss	%xmm4, %xmm4
	mulss	24(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	28(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	32(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm6, %xmm0
	mulss	%xmm5, %xmm5
	mulss	%xmm7, %xmm5
	mulss	%xmm11, %xmm11
	mulss	%xmm8, %xmm11
	mulss	%xmm12, %xmm12
	mulss	%xmm9, %xmm12
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm10, %xmm10
	mulss	20(%rsp), %xmm10
	mulss	%xmm4, %xmm4
	mulss	24(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	28(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	32(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm6, %xmm0
	mulss	%xmm5, %xmm5
	mulss	%xmm7, %xmm5
	mulss	%xmm11, %xmm11
	mulss	%xmm8, %xmm11
	mulss	%xmm12, %xmm12
	mulss	%xmm9, %xmm12
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm10, %xmm10
	mulss	20(%rsp), %xmm10
	mulss	%xmm4, %xmm4
	movaps	%xmm4, %xmm9
	mulss	24(%rsp), %xmm9
	mulss	%xmm3, %xmm3
	movaps	%xmm3, %xmm8
	mulss	28(%rsp), %xmm8
	mulss	%xmm2, %xmm2
	movaps	%xmm2, %xmm7
	mulss	32(%rsp), %xmm7
	mulss	%xmm1, %xmm1
	movaps	%xmm1, %xmm6
	mulss	%xmm15, %xmm6
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L947
	movss	%xmm5, 52(%rsp)
	movss	%xmm11, 56(%rsp)
	movss	%xmm12, 60(%rsp)
	movss	%xmm13, 64(%rsp)
	movss	%xmm14, 68(%rsp)
	movss	%xmm10, 72(%rsp)
	movss	%xmm9, 76(%rsp)
	movss	%xmm8, 80(%rsp)
	movss	%xmm7, 84(%rsp)
	movss	%xmm6, 88(%rsp)
.L946:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	68(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	76(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	84(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	92(%rsp), %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE259:
	.size	float_mul_10, .-float_mul_10
	.globl	float_mul_11
	.type	float_mul_11, @function
float_mul_11:
.LFB260:
	.cfi_startproc
	endbr64
	subq	$104, %rsp
	.cfi_def_cfa_offset 112
	movsd	80(%rsi), %xmm3
	pxor	%xmm0, %xmm0
	cvtsd2ss	%xmm3, %xmm0
	movss	.LC2(%rip), %xmm1
	mulss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	4(%rsi), %xmm2
	cvtss2sd	%xmm2, %xmm2
	mulsd	.LC3(%rip), %xmm2
	movapd	%xmm3, %xmm4
	mulsd	%xmm2, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 36(%rsp)
	movsd	88(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm7
	mulss	%xmm1, %xmm7
	movss	%xmm7, 52(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 40(%rsp)
	movsd	96(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm12
	movss	%xmm5, 56(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 44(%rsp)
	movsd	104(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm13
	movss	%xmm5, 60(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 4(%rsp)
	movsd	112(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm14
	movss	%xmm5, 64(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 8(%rsp)
	movsd	120(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm11
	movss	%xmm5, 68(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 12(%rsp)
	movsd	128(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm10
	movss	%xmm5, 72(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 16(%rsp)
	movsd	136(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm9
	movss	%xmm5, 76(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 20(%rsp)
	movsd	144(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm8
	movss	%xmm5, 80(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 24(%rsp)
	movsd	152(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm6
	movss	%xmm5, 84(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 28(%rsp)
	movsd	160(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 48(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 32(%rsp)
	movsd	168(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 88(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	pxor	%xmm4, %xmm4
	cvtsd2ss	%xmm2, %xmm4
	movss	%xmm4, 92(%rsp)
	testq	%rdi, %rdi
	je	.L951
	leaq	-1(%rdi), %rax
	movaps	%xmm4, %xmm15
	movaps	%xmm7, %xmm4
	movaps	%xmm6, %xmm7
	movss	48(%rsp), %xmm6
.L952:
	mulss	%xmm0, %xmm0
	mulss	36(%rsp), %xmm0
	mulss	%xmm4, %xmm4
	mulss	40(%rsp), %xmm4
	movaps	%xmm12, %xmm3
	mulss	%xmm12, %xmm3
	mulss	44(%rsp), %xmm3
	movaps	%xmm13, %xmm2
	mulss	%xmm13, %xmm2
	mulss	4(%rsp), %xmm2
	mulss	%xmm14, %xmm14
	movaps	%xmm14, %xmm1
	mulss	8(%rsp), %xmm1
	mulss	%xmm11, %xmm11
	mulss	12(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	16(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	20(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	24(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	28(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	32(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm15, %xmm5
	mulss	%xmm0, %xmm0
	movss	36(%rsp), %xmm12
	mulss	%xmm12, %xmm0
	mulss	%xmm4, %xmm4
	movss	40(%rsp), %xmm13
	mulss	%xmm13, %xmm4
	mulss	%xmm3, %xmm3
	movss	44(%rsp), %xmm14
	mulss	%xmm14, %xmm3
	mulss	%xmm2, %xmm2
	mulss	4(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	8(%rsp), %xmm1
	mulss	%xmm11, %xmm11
	mulss	12(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	16(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	20(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	24(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	28(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	32(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm15, %xmm5
	mulss	%xmm0, %xmm0
	mulss	%xmm12, %xmm0
	mulss	%xmm4, %xmm4
	mulss	%xmm13, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm14, %xmm3
	mulss	%xmm2, %xmm2
	mulss	4(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	8(%rsp), %xmm1
	mulss	%xmm11, %xmm11
	mulss	12(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	16(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	20(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	24(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	28(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	32(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	%xmm15, %xmm5
	mulss	%xmm0, %xmm0
	mulss	%xmm12, %xmm0
	mulss	%xmm4, %xmm4
	mulss	%xmm13, %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm14, %xmm3
	movaps	%xmm3, %xmm12
	mulss	%xmm2, %xmm2
	mulss	4(%rsp), %xmm2
	movaps	%xmm2, %xmm13
	mulss	%xmm1, %xmm1
	mulss	8(%rsp), %xmm1
	movaps	%xmm1, %xmm14
	mulss	%xmm11, %xmm11
	mulss	12(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	16(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	20(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	24(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	movaps	%xmm7, %xmm3
	mulss	28(%rsp), %xmm3
	mulss	%xmm6, %xmm6
	mulss	32(%rsp), %xmm6
	movaps	%xmm6, %xmm2
	mulss	%xmm5, %xmm5
	mulss	%xmm15, %xmm5
	movaps	%xmm5, %xmm1
	mulss	%xmm0, %xmm0
	movss	36(%rsp), %xmm6
	mulss	%xmm6, %xmm0
	mulss	%xmm4, %xmm4
	movss	40(%rsp), %xmm7
	mulss	%xmm7, %xmm4
	mulss	%xmm12, %xmm12
	movss	44(%rsp), %xmm5
	mulss	%xmm5, %xmm12
	mulss	%xmm13, %xmm13
	mulss	4(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	8(%rsp), %xmm14
	mulss	%xmm11, %xmm11
	mulss	12(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	16(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	20(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	24(%rsp), %xmm8
	mulss	%xmm3, %xmm3
	mulss	28(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	32(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm6, %xmm0
	mulss	%xmm4, %xmm4
	mulss	%xmm7, %xmm4
	mulss	%xmm12, %xmm12
	mulss	%xmm5, %xmm12
	mulss	%xmm13, %xmm13
	mulss	4(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	8(%rsp), %xmm14
	mulss	%xmm11, %xmm11
	mulss	12(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	16(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	20(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	24(%rsp), %xmm8
	mulss	%xmm3, %xmm3
	mulss	28(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	32(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm6, %xmm0
	mulss	%xmm4, %xmm4
	mulss	%xmm7, %xmm4
	mulss	%xmm12, %xmm12
	mulss	%xmm5, %xmm12
	mulss	%xmm13, %xmm13
	mulss	4(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	8(%rsp), %xmm14
	mulss	%xmm11, %xmm11
	mulss	12(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	16(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	20(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	24(%rsp), %xmm8
	mulss	%xmm3, %xmm3
	mulss	28(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	32(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm6, %xmm0
	mulss	%xmm4, %xmm4
	mulss	%xmm7, %xmm4
	mulss	%xmm12, %xmm12
	mulss	%xmm5, %xmm12
	mulss	%xmm13, %xmm13
	mulss	4(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	8(%rsp), %xmm14
	mulss	%xmm11, %xmm11
	mulss	12(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	16(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	20(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	24(%rsp), %xmm8
	mulss	%xmm3, %xmm3
	mulss	28(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	32(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm6, %xmm0
	mulss	%xmm4, %xmm4
	mulss	%xmm7, %xmm4
	mulss	%xmm12, %xmm12
	mulss	%xmm5, %xmm12
	mulss	%xmm13, %xmm13
	mulss	4(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	8(%rsp), %xmm14
	mulss	%xmm11, %xmm11
	mulss	12(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	16(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	20(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	24(%rsp), %xmm8
	mulss	%xmm3, %xmm3
	mulss	28(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	32(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm6, %xmm0
	mulss	%xmm4, %xmm4
	mulss	%xmm7, %xmm4
	mulss	%xmm12, %xmm12
	mulss	%xmm5, %xmm12
	mulss	%xmm13, %xmm13
	mulss	4(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	8(%rsp), %xmm14
	mulss	%xmm11, %xmm11
	mulss	12(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	16(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	20(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	24(%rsp), %xmm8
	mulss	%xmm3, %xmm3
	movaps	%xmm3, %xmm7
	mulss	28(%rsp), %xmm7
	mulss	%xmm2, %xmm2
	movaps	%xmm2, %xmm6
	mulss	32(%rsp), %xmm6
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	movaps	%xmm1, %xmm5
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L952
	movss	%xmm4, 52(%rsp)
	movss	%xmm12, 56(%rsp)
	movss	%xmm13, 60(%rsp)
	movss	%xmm14, 64(%rsp)
	movss	%xmm11, 68(%rsp)
	movss	%xmm10, 72(%rsp)
	movss	%xmm9, 76(%rsp)
	movss	%xmm8, 80(%rsp)
	movss	%xmm7, 84(%rsp)
	movss	%xmm6, 48(%rsp)
	movss	%xmm1, 88(%rsp)
.L951:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	68(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	76(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	84(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	92(%rsp), %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE260:
	.size	float_mul_11, .-float_mul_11
	.globl	float_mul_12
	.type	float_mul_12, @function
float_mul_12:
.LFB261:
	.cfi_startproc
	endbr64
	subq	$120, %rsp
	.cfi_def_cfa_offset 128
	movsd	80(%rsi), %xmm3
	pxor	%xmm0, %xmm0
	cvtsd2ss	%xmm3, %xmm0
	movss	.LC2(%rip), %xmm1
	mulss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	4(%rsi), %xmm2
	cvtss2sd	%xmm2, %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm4, %xmm6
	movss	%xmm6, 52(%rsp)
	movsd	88(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm1, %xmm6
	movss	%xmm6, 68(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 56(%rsp)
	movsd	96(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm13
	movss	%xmm5, 72(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 12(%rsp)
	movsd	104(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm14
	movss	%xmm5, 76(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 16(%rsp)
	movsd	112(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm12
	movss	%xmm5, 80(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 20(%rsp)
	movsd	120(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm11
	movss	%xmm5, 84(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 24(%rsp)
	movsd	128(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm10
	movss	%xmm5, 88(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 28(%rsp)
	movsd	136(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm9
	movss	%xmm5, 92(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 32(%rsp)
	movsd	144(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm8
	movss	%xmm5, 96(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 36(%rsp)
	movsd	152(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm7
	mulss	%xmm1, %xmm7
	movss	%xmm7, 100(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 40(%rsp)
	movsd	160(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 60(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 44(%rsp)
	movsd	168(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 64(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 48(%rsp)
	movsd	176(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm15
	mulss	%xmm1, %xmm15
	movss	%xmm15, 104(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	pxor	%xmm3, %xmm3
	cvtsd2ss	%xmm2, %xmm3
	movss	%xmm3, 108(%rsp)
	testq	%rdi, %rdi
	je	.L956
	movaps	%xmm15, %xmm1
	leaq	-1(%rdi), %rax
	movaps	%xmm3, %xmm15
	movaps	%xmm6, %xmm3
	movss	60(%rsp), %xmm6
	movss	64(%rsp), %xmm5
	movaps	%xmm1, %xmm4
.L957:
	mulss	%xmm0, %xmm0
	mulss	52(%rsp), %xmm0
	mulss	%xmm3, %xmm3
	mulss	56(%rsp), %xmm3
	movaps	%xmm13, %xmm2
	mulss	%xmm13, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm14, %xmm14
	movaps	%xmm14, %xmm1
	mulss	16(%rsp), %xmm1
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	48(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm15, %xmm4
	mulss	%xmm0, %xmm0
	movss	52(%rsp), %xmm13
	mulss	%xmm13, %xmm0
	mulss	%xmm3, %xmm3
	movss	56(%rsp), %xmm14
	mulss	%xmm14, %xmm3
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	16(%rsp), %xmm1
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	48(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm15, %xmm4
	mulss	%xmm0, %xmm0
	mulss	%xmm13, %xmm0
	mulss	%xmm3, %xmm3
	mulss	%xmm14, %xmm3
	mulss	%xmm2, %xmm2
	mulss	12(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	16(%rsp), %xmm1
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	48(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	%xmm15, %xmm4
	mulss	%xmm0, %xmm0
	mulss	%xmm13, %xmm0
	mulss	%xmm3, %xmm3
	mulss	%xmm14, %xmm3
	mulss	%xmm2, %xmm2
	movaps	%xmm2, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm1, %xmm1
	mulss	16(%rsp), %xmm1
	movaps	%xmm1, %xmm14
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	movaps	%xmm5, %xmm2
	mulss	48(%rsp), %xmm2
	mulss	%xmm4, %xmm4
	mulss	%xmm15, %xmm4
	movaps	%xmm4, %xmm1
	mulss	%xmm0, %xmm0
	movss	52(%rsp), %xmm5
	mulss	%xmm5, %xmm0
	mulss	%xmm3, %xmm3
	movss	56(%rsp), %xmm4
	mulss	%xmm4, %xmm3
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm2, %xmm2
	mulss	48(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm5, %xmm0
	mulss	%xmm3, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm2, %xmm2
	mulss	48(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm5, %xmm0
	mulss	%xmm3, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm2, %xmm2
	mulss	48(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm5, %xmm0
	mulss	%xmm3, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm2, %xmm2
	mulss	48(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm5, %xmm0
	mulss	%xmm3, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm2, %xmm2
	mulss	48(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm5, %xmm0
	mulss	%xmm3, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm2, %xmm2
	mulss	48(%rsp), %xmm2
	movaps	%xmm2, %xmm5
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	movaps	%xmm1, %xmm4
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L957
	movss	%xmm3, 68(%rsp)
	movss	%xmm13, 72(%rsp)
	movss	%xmm14, 76(%rsp)
	movss	%xmm12, 80(%rsp)
	movss	%xmm11, 84(%rsp)
	movss	%xmm10, 88(%rsp)
	movss	%xmm9, 92(%rsp)
	movss	%xmm8, 96(%rsp)
	movss	%xmm7, 100(%rsp)
	movss	%xmm6, 60(%rsp)
	movss	%xmm2, 64(%rsp)
	movss	%xmm1, 104(%rsp)
.L956:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	68(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	76(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	84(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	92(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	100(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	108(%rsp), %edi
	call	use_int@PLT
	addq	$120, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE261:
	.size	float_mul_12, .-float_mul_12
	.globl	float_mul_13
	.type	float_mul_13, @function
float_mul_13:
.LFB262:
	.cfi_startproc
	endbr64
	subq	$120, %rsp
	.cfi_def_cfa_offset 128
	movsd	80(%rsi), %xmm3
	pxor	%xmm0, %xmm0
	cvtsd2ss	%xmm3, %xmm0
	movss	.LC2(%rip), %xmm1
	mulss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	4(%rsi), %xmm2
	cvtss2sd	%xmm2, %xmm2
	mulsd	.LC3(%rip), %xmm2
	movapd	%xmm3, %xmm4
	mulsd	%xmm2, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm4, %xmm6
	movss	%xmm6, 52(%rsp)
	movsd	88(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm1, %xmm6
	movss	%xmm6, 68(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 4(%rsp)
	movsd	96(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm14
	movss	%xmm5, 72(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 8(%rsp)
	movsd	104(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm13
	movss	%xmm5, 76(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 12(%rsp)
	movsd	112(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm12
	movss	%xmm5, 80(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 16(%rsp)
	movsd	120(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm11
	movss	%xmm5, 84(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 20(%rsp)
	movsd	128(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm10
	movss	%xmm5, 88(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 24(%rsp)
	movsd	136(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm9
	movss	%xmm5, 92(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 28(%rsp)
	movsd	144(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm8
	movss	%xmm5, 96(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 32(%rsp)
	movsd	152(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm7
	mulss	%xmm1, %xmm7
	movss	%xmm7, 100(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 36(%rsp)
	movsd	160(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 56(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 40(%rsp)
	movsd	168(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 60(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 44(%rsp)
	movsd	176(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm15
	mulss	%xmm1, %xmm15
	movss	%xmm15, 64(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 48(%rsp)
	movsd	184(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm15
	mulss	%xmm1, %xmm15
	movss	%xmm15, 104(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	cvtsd2ss	%xmm2, %xmm2
	movss	%xmm2, 108(%rsp)
	testq	%rdi, %rdi
	je	.L961
	movaps	%xmm15, %xmm1
	leaq	-1(%rdi), %rax
	movaps	%xmm2, %xmm15
	movaps	%xmm6, %xmm2
	movss	56(%rsp), %xmm6
	movss	60(%rsp), %xmm5
	movss	64(%rsp), %xmm4
	movaps	%xmm1, %xmm3
.L962:
	mulss	%xmm0, %xmm0
	mulss	52(%rsp), %xmm0
	mulss	%xmm2, %xmm2
	mulss	4(%rsp), %xmm2
	mulss	%xmm14, %xmm14
	movaps	%xmm14, %xmm1
	mulss	8(%rsp), %xmm1
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	16(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	20(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	24(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	28(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	32(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	36(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	40(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	44(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	48(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm15, %xmm3
	mulss	%xmm0, %xmm0
	movss	52(%rsp), %xmm14
	mulss	%xmm14, %xmm0
	mulss	%xmm2, %xmm2
	mulss	4(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	8(%rsp), %xmm1
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	16(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	20(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	24(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	28(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	32(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	36(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	40(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	44(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	48(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm15, %xmm3
	mulss	%xmm0, %xmm0
	mulss	%xmm14, %xmm0
	mulss	%xmm2, %xmm2
	mulss	4(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	8(%rsp), %xmm1
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	16(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	20(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	24(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	28(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	32(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	36(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	40(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	44(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	48(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm15, %xmm3
	mulss	%xmm0, %xmm0
	mulss	%xmm14, %xmm0
	mulss	%xmm2, %xmm2
	mulss	4(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	movaps	%xmm1, %xmm14
	mulss	8(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	16(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	20(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	24(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	28(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	32(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	36(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	40(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	44(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	48(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	%xmm15, %xmm3
	movaps	%xmm3, %xmm1
	mulss	%xmm0, %xmm0
	movss	52(%rsp), %xmm3
	mulss	%xmm3, %xmm0
	mulss	%xmm2, %xmm2
	mulss	4(%rsp), %xmm2
	mulss	%xmm14, %xmm14
	mulss	8(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	16(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	20(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	24(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	28(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	32(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	36(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	40(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	44(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	48(%rsp), %xmm4
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm3, %xmm0
	mulss	%xmm2, %xmm2
	mulss	4(%rsp), %xmm2
	mulss	%xmm14, %xmm14
	mulss	8(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	16(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	20(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	24(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	28(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	32(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	36(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	40(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	44(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	48(%rsp), %xmm4
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm3, %xmm0
	mulss	%xmm2, %xmm2
	mulss	4(%rsp), %xmm2
	mulss	%xmm14, %xmm14
	mulss	8(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	16(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	20(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	24(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	28(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	32(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	36(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	40(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	44(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	48(%rsp), %xmm4
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm3, %xmm0
	mulss	%xmm2, %xmm2
	mulss	4(%rsp), %xmm2
	mulss	%xmm14, %xmm14
	mulss	8(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	16(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	20(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	24(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	28(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	32(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	36(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	40(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	44(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	48(%rsp), %xmm4
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm3, %xmm0
	mulss	%xmm2, %xmm2
	mulss	4(%rsp), %xmm2
	mulss	%xmm14, %xmm14
	mulss	8(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	16(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	20(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	24(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	28(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	32(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	36(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	40(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	44(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	48(%rsp), %xmm4
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	%xmm3, %xmm0
	mulss	%xmm2, %xmm2
	mulss	4(%rsp), %xmm2
	mulss	%xmm14, %xmm14
	mulss	8(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	12(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	16(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	20(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	24(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	28(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	32(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	36(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	40(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	44(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	48(%rsp), %xmm4
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	movaps	%xmm1, %xmm3
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L962
	movss	%xmm2, 68(%rsp)
	movss	%xmm14, 72(%rsp)
	movss	%xmm13, 76(%rsp)
	movss	%xmm12, 80(%rsp)
	movss	%xmm11, 84(%rsp)
	movss	%xmm10, 88(%rsp)
	movss	%xmm9, 92(%rsp)
	movss	%xmm8, 96(%rsp)
	movss	%xmm7, 100(%rsp)
	movss	%xmm6, 56(%rsp)
	movss	%xmm5, 60(%rsp)
	movss	%xmm4, 64(%rsp)
	movss	%xmm1, 104(%rsp)
.L961:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	68(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	76(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	84(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	92(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	100(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	108(%rsp), %edi
	call	use_int@PLT
	addq	$120, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE262:
	.size	float_mul_13, .-float_mul_13
	.globl	float_mul_14
	.type	float_mul_14, @function
float_mul_14:
.LFB263:
	.cfi_startproc
	endbr64
	subq	$136, %rsp
	.cfi_def_cfa_offset 144
	movsd	80(%rsi), %xmm3
	pxor	%xmm0, %xmm0
	cvtsd2ss	%xmm3, %xmm0
	movss	.LC2(%rip), %xmm1
	mulss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	4(%rsi), %xmm2
	cvtss2sd	%xmm2, %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm4, %xmm6
	movss	%xmm6, 12(%rsp)
	movsd	88(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm14
	movss	%xmm5, 84(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 16(%rsp)
	movsd	96(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm13
	movss	%xmm5, 88(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm4, %xmm6
	movss	%xmm6, 20(%rsp)
	movsd	104(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm12
	movss	%xmm5, 92(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 24(%rsp)
	movsd	112(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm11
	movss	%xmm5, 96(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm4, %xmm6
	movss	%xmm6, 28(%rsp)
	movsd	120(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm10
	movss	%xmm5, 100(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 32(%rsp)
	movsd	128(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm9
	movss	%xmm5, 104(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm4, %xmm6
	movss	%xmm6, 36(%rsp)
	movsd	136(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm8
	movss	%xmm5, 108(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 40(%rsp)
	movsd	144(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm7
	mulss	%xmm1, %xmm7
	movss	%xmm7, 112(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm4, %xmm6
	movss	%xmm6, 44(%rsp)
	movsd	152(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm1, %xmm6
	movss	%xmm6, 116(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 48(%rsp)
	movsd	160(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 68(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 52(%rsp)
	movsd	168(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm15
	mulss	%xmm1, %xmm15
	movss	%xmm15, 72(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 56(%rsp)
	movsd	176(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 76(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 60(%rsp)
	movsd	184(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm15
	mulss	%xmm1, %xmm15
	movss	%xmm15, 80(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 64(%rsp)
	movsd	192(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 120(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	pxor	%xmm3, %xmm3
	cvtsd2ss	%xmm2, %xmm3
	movss	%xmm3, 124(%rsp)
	testq	%rdi, %rdi
	je	.L966
	movaps	%xmm5, %xmm1
	leaq	-1(%rdi), %rax
	movaps	%xmm3, %xmm15
	movss	68(%rsp), %xmm5
	movss	72(%rsp), %xmm4
	movss	76(%rsp), %xmm3
	movss	80(%rsp), %xmm2
.L967:
	mulss	%xmm0, %xmm0
	mulss	12(%rsp), %xmm0
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	20(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	24(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	28(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	32(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	36(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	40(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	44(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	48(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	52(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	56(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	60(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	64(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	12(%rsp), %xmm0
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	20(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	24(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	28(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	32(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	36(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	40(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	44(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	48(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	52(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	56(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	60(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	64(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	12(%rsp), %xmm0
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	20(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	24(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	28(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	32(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	36(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	40(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	44(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	48(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	52(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	56(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	60(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	64(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	12(%rsp), %xmm0
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	20(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	24(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	28(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	32(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	36(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	40(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	44(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	48(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	52(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	56(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	60(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	64(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	12(%rsp), %xmm0
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	20(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	24(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	28(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	32(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	36(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	40(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	44(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	48(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	52(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	56(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	60(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	64(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	12(%rsp), %xmm0
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	20(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	24(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	28(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	32(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	36(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	40(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	44(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	48(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	52(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	56(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	60(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	64(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	12(%rsp), %xmm0
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	20(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	24(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	28(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	32(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	36(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	40(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	44(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	48(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	52(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	56(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	60(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	64(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	12(%rsp), %xmm0
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	20(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	24(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	28(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	32(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	36(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	40(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	44(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	48(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	52(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	56(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	60(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	64(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	12(%rsp), %xmm0
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	20(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	24(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	28(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	32(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	36(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	40(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	44(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	48(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	52(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	56(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	60(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	64(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	mulss	%xmm0, %xmm0
	mulss	12(%rsp), %xmm0
	mulss	%xmm14, %xmm14
	mulss	16(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	20(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	24(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	28(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	32(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	36(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	40(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	44(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	48(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	52(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	56(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	60(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	64(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	%xmm15, %xmm1
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L967
	movss	%xmm14, 84(%rsp)
	movss	%xmm13, 88(%rsp)
	movss	%xmm12, 92(%rsp)
	movss	%xmm11, 96(%rsp)
	movss	%xmm10, 100(%rsp)
	movss	%xmm9, 104(%rsp)
	movss	%xmm8, 108(%rsp)
	movss	%xmm7, 112(%rsp)
	movss	%xmm6, 116(%rsp)
	movss	%xmm5, 68(%rsp)
	movss	%xmm4, 72(%rsp)
	movss	%xmm3, 76(%rsp)
	movss	%xmm2, 80(%rsp)
	movss	%xmm1, 120(%rsp)
.L966:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	84(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	92(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	100(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	108(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	116(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	68(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	76(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	124(%rsp), %edi
	call	use_int@PLT
	addq	$136, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE263:
	.size	float_mul_14, .-float_mul_14
	.globl	float_mul_15
	.type	float_mul_15, @function
float_mul_15:
.LFB264:
	.cfi_startproc
	endbr64
	subq	$136, %rsp
	.cfi_def_cfa_offset 144
	movsd	80(%rsi), %xmm3
	pxor	%xmm0, %xmm0
	cvtsd2ss	%xmm3, %xmm0
	movss	.LC2(%rip), %xmm1
	mulss	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2ssl	4(%rsi), %xmm2
	cvtss2sd	%xmm2, %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm4, %xmm6
	movss	%xmm6, 4(%rsp)
	movsd	88(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm15
	movss	%xmm5, 84(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 8(%rsp)
	movsd	96(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm14
	movss	%xmm5, 88(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm4, %xmm6
	movss	%xmm6, 12(%rsp)
	movsd	104(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm13
	movss	%xmm5, 92(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 16(%rsp)
	movsd	112(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm12
	movss	%xmm5, 96(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm4, %xmm6
	movss	%xmm6, 20(%rsp)
	movsd	120(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm11
	movss	%xmm5, 100(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 24(%rsp)
	movsd	128(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm10
	movss	%xmm5, 104(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm4, %xmm6
	movss	%xmm6, 28(%rsp)
	movsd	136(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm9
	movss	%xmm5, 108(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm4, %xmm7
	movss	%xmm7, 32(%rsp)
	movsd	144(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm8
	movss	%xmm5, 112(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm4, %xmm6
	movss	%xmm6, 36(%rsp)
	movsd	152(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm7
	mulss	%xmm1, %xmm7
	movss	%xmm7, 116(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm4, %xmm6
	movss	%xmm6, 40(%rsp)
	movsd	160(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm1, %xmm6
	movss	%xmm6, 120(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	movss	%xmm5, 44(%rsp)
	movsd	168(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 68(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 48(%rsp)
	movsd	176(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 72(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 52(%rsp)
	movsd	184(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 76(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 56(%rsp)
	movsd	192(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 80(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	cvtsd2ss	%xmm4, %xmm4
	movss	%xmm4, 60(%rsp)
	movsd	200(%rsi), %xmm4
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movss	%xmm5, 124(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	pxor	%xmm3, %xmm3
	cvtsd2ss	%xmm2, %xmm3
	movss	%xmm3, 64(%rsp)
	testq	%rdi, %rdi
	je	.L971
	movaps	%xmm5, %xmm1
	leaq	-1(%rdi), %rax
	movss	68(%rsp), %xmm5
	movss	72(%rsp), %xmm4
	movss	76(%rsp), %xmm3
	movss	80(%rsp), %xmm2
.L972:
	mulss	%xmm0, %xmm0
	mulss	4(%rsp), %xmm0
	mulss	%xmm15, %xmm15
	mulss	8(%rsp), %xmm15
	mulss	%xmm14, %xmm14
	mulss	12(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	16(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	48(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	52(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	56(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	60(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	64(%rsp), %xmm1
	mulss	%xmm0, %xmm0
	mulss	4(%rsp), %xmm0
	mulss	%xmm15, %xmm15
	mulss	8(%rsp), %xmm15
	mulss	%xmm14, %xmm14
	mulss	12(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	16(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	48(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	52(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	56(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	60(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	64(%rsp), %xmm1
	mulss	%xmm0, %xmm0
	mulss	4(%rsp), %xmm0
	mulss	%xmm15, %xmm15
	mulss	8(%rsp), %xmm15
	mulss	%xmm14, %xmm14
	mulss	12(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	16(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	48(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	52(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	56(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	60(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	64(%rsp), %xmm1
	mulss	%xmm0, %xmm0
	mulss	4(%rsp), %xmm0
	mulss	%xmm15, %xmm15
	mulss	8(%rsp), %xmm15
	mulss	%xmm14, %xmm14
	mulss	12(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	16(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	48(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	52(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	56(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	60(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	64(%rsp), %xmm1
	mulss	%xmm0, %xmm0
	mulss	4(%rsp), %xmm0
	mulss	%xmm15, %xmm15
	mulss	8(%rsp), %xmm15
	mulss	%xmm14, %xmm14
	mulss	12(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	16(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	48(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	52(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	56(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	60(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	64(%rsp), %xmm1
	mulss	%xmm0, %xmm0
	mulss	4(%rsp), %xmm0
	mulss	%xmm15, %xmm15
	mulss	8(%rsp), %xmm15
	mulss	%xmm14, %xmm14
	mulss	12(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	16(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	48(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	52(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	56(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	60(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	64(%rsp), %xmm1
	mulss	%xmm0, %xmm0
	mulss	4(%rsp), %xmm0
	mulss	%xmm15, %xmm15
	mulss	8(%rsp), %xmm15
	mulss	%xmm14, %xmm14
	mulss	12(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	16(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	48(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	52(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	56(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	60(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	64(%rsp), %xmm1
	mulss	%xmm0, %xmm0
	mulss	4(%rsp), %xmm0
	mulss	%xmm15, %xmm15
	mulss	8(%rsp), %xmm15
	mulss	%xmm14, %xmm14
	mulss	12(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	16(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	48(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	52(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	56(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	60(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	64(%rsp), %xmm1
	mulss	%xmm0, %xmm0
	mulss	4(%rsp), %xmm0
	mulss	%xmm15, %xmm15
	mulss	8(%rsp), %xmm15
	mulss	%xmm14, %xmm14
	mulss	12(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	16(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	48(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	52(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	56(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	60(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	64(%rsp), %xmm1
	mulss	%xmm0, %xmm0
	mulss	4(%rsp), %xmm0
	mulss	%xmm15, %xmm15
	mulss	8(%rsp), %xmm15
	mulss	%xmm14, %xmm14
	mulss	12(%rsp), %xmm14
	mulss	%xmm13, %xmm13
	mulss	16(%rsp), %xmm13
	mulss	%xmm12, %xmm12
	mulss	20(%rsp), %xmm12
	mulss	%xmm11, %xmm11
	mulss	24(%rsp), %xmm11
	mulss	%xmm10, %xmm10
	mulss	28(%rsp), %xmm10
	mulss	%xmm9, %xmm9
	mulss	32(%rsp), %xmm9
	mulss	%xmm8, %xmm8
	mulss	36(%rsp), %xmm8
	mulss	%xmm7, %xmm7
	mulss	40(%rsp), %xmm7
	mulss	%xmm6, %xmm6
	mulss	44(%rsp), %xmm6
	mulss	%xmm5, %xmm5
	mulss	48(%rsp), %xmm5
	mulss	%xmm4, %xmm4
	mulss	52(%rsp), %xmm4
	mulss	%xmm3, %xmm3
	mulss	56(%rsp), %xmm3
	mulss	%xmm2, %xmm2
	mulss	60(%rsp), %xmm2
	mulss	%xmm1, %xmm1
	mulss	64(%rsp), %xmm1
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L972
	movss	%xmm15, 84(%rsp)
	movss	%xmm14, 88(%rsp)
	movss	%xmm13, 92(%rsp)
	movss	%xmm12, 96(%rsp)
	movss	%xmm11, 100(%rsp)
	movss	%xmm10, 104(%rsp)
	movss	%xmm9, 108(%rsp)
	movss	%xmm8, 112(%rsp)
	movss	%xmm7, 116(%rsp)
	movss	%xmm6, 120(%rsp)
	movss	%xmm5, 68(%rsp)
	movss	%xmm4, 72(%rsp)
	movss	%xmm3, 76(%rsp)
	movss	%xmm2, 80(%rsp)
	movss	%xmm1, 124(%rsp)
.L971:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	84(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	92(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	100(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	108(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	116(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	68(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	76(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	124(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	64(%rsp), %edi
	call	use_int@PLT
	addq	$136, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE264:
	.size	float_mul_15, .-float_mul_15
	.globl	float_div_0
	.type	float_div_0, @function
float_div_0:
.LFB265:
	.cfi_startproc
	endbr64
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	pxor	%xmm0, %xmm0
	cvtsd2ss	80(%rsi), %xmm0
	mulss	.LC5(%rip), %xmm0
	movl	12(%rsi), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	mulss	.LC6(%rip), %xmm1
	movss	%xmm1, 12(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L976
.L977:
	movss	12(%rsp), %xmm2
	movaps	%xmm2, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm2, %xmm0
	divss	%xmm1, %xmm0
	movaps	%xmm2, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm2, %xmm0
	divss	%xmm1, %xmm0
	movaps	%xmm2, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm2, %xmm0
	divss	%xmm1, %xmm0
	movaps	%xmm2, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm2, %xmm0
	divss	%xmm1, %xmm0
	movaps	%xmm2, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm2, %xmm0
	divss	%xmm1, %xmm0
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L977
.L976:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE265:
	.size	float_div_0, .-float_div_0
	.globl	float_div_1
	.type	float_div_1, @function
float_div_1:
.LFB266:
	.cfi_startproc
	endbr64
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	pxor	%xmm0, %xmm0
	cvtsd2ss	80(%rsi), %xmm0
	movss	.LC5(%rip), %xmm2
	mulss	%xmm2, %xmm0
	movl	12(%rsi), %eax
	addl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movss	.LC6(%rip), %xmm1
	mulss	%xmm1, %xmm3
	movss	%xmm3, 8(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rsi), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 4(%rsp)
	pxor	%xmm2, %xmm2
	cvtsi2ssl	16(%rsi), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 12(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L981
.L982:
	movss	8(%rsp), %xmm4
	movaps	%xmm4, %xmm3
	divss	%xmm0, %xmm3
	movss	12(%rsp), %xmm6
	movaps	%xmm6, %xmm2
	divss	4(%rsp), %xmm2
	movaps	%xmm4, %xmm1
	divss	%xmm3, %xmm1
	movaps	%xmm6, %xmm0
	divss	%xmm2, %xmm0
	movaps	%xmm4, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm6, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm4, %xmm0
	divss	%xmm2, %xmm0
	movaps	%xmm6, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm4, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm6, %xmm0
	divss	%xmm2, %xmm0
	movaps	%xmm4, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm6, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm4, %xmm0
	divss	%xmm2, %xmm0
	movaps	%xmm6, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm4, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm6, %xmm0
	divss	%xmm2, %xmm0
	movaps	%xmm4, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm6, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm4, %xmm0
	divss	%xmm2, %xmm0
	divss	%xmm1, %xmm6
	movss	%xmm6, 4(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L982
.L981:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE266:
	.size	float_div_1, .-float_div_1
	.globl	float_div_2
	.type	float_div_2, @function
float_div_2:
.LFB267:
	.cfi_startproc
	endbr64
	subq	$40, %rsp
	.cfi_def_cfa_offset 48
	pxor	%xmm0, %xmm0
	cvtsd2ss	80(%rsi), %xmm0
	movss	.LC5(%rip), %xmm2
	mulss	%xmm2, %xmm0
	movl	12(%rsi), %eax
	addl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movss	.LC6(%rip), %xmm1
	mulss	%xmm1, %xmm3
	movss	%xmm3, 20(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rsi), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 12(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2ssl	16(%rsi), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 24(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	96(%rsi), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 16(%rsp)
	movl	20(%rsi), %eax
	subl	$1, %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 28(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L986
.L987:
	movss	20(%rsp), %xmm4
	movaps	%xmm4, %xmm3
	divss	%xmm0, %xmm3
	movss	24(%rsp), %xmm5
	movaps	%xmm5, %xmm2
	divss	12(%rsp), %xmm2
	movss	28(%rsp), %xmm7
	movaps	%xmm7, %xmm1
	divss	16(%rsp), %xmm1
	movaps	%xmm4, %xmm0
	divss	%xmm3, %xmm0
	movaps	%xmm5, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm7, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm4, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm5, %xmm6
	movaps	%xmm5, %xmm0
	divss	%xmm3, %xmm0
	movaps	%xmm7, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm4, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm5, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm7, %xmm5
	movaps	%xmm7, %xmm0
	divss	%xmm3, %xmm0
	movaps	%xmm4, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm6, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm7, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm4, %xmm0
	divss	%xmm3, %xmm0
	movaps	%xmm6, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm7, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm4, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm6, %xmm0
	divss	%xmm3, %xmm0
	movaps	%xmm7, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm4, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm6, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm7, %xmm0
	divss	%xmm3, %xmm0
	movaps	%xmm4, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm6, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm7, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm4, %xmm0
	divss	%xmm3, %xmm0
	divss	%xmm2, %xmm6
	movss	%xmm6, 12(%rsp)
	divss	%xmm1, %xmm5
	movss	%xmm5, 16(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L987
.L986:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE267:
	.size	float_div_2, .-float_div_2
	.globl	float_div_3
	.type	float_div_3, @function
float_div_3:
.LFB268:
	.cfi_startproc
	endbr64
	subq	$40, %rsp
	.cfi_def_cfa_offset 48
	pxor	%xmm0, %xmm0
	cvtsd2ss	80(%rsi), %xmm0
	movss	.LC5(%rip), %xmm2
	mulss	%xmm2, %xmm0
	movl	12(%rsi), %eax
	addl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movss	.LC6(%rip), %xmm1
	mulss	%xmm1, %xmm3
	movss	%xmm3, 16(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rsi), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 4(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2ssl	16(%rsi), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 20(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	96(%rsi), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 8(%rsp)
	movl	20(%rsi), %eax
	subl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 24(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	104(%rsi), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 12(%rsp)
	movl	24(%rsi), %eax
	subl	$2, %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 28(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L991
.L992:
	movss	16(%rsp), %xmm5
	movaps	%xmm5, %xmm1
	divss	%xmm0, %xmm1
	movss	20(%rsp), %xmm6
	movaps	%xmm6, %xmm0
	divss	4(%rsp), %xmm0
	movss	24(%rsp), %xmm8
	movaps	%xmm8, %xmm4
	divss	8(%rsp), %xmm4
	movss	28(%rsp), %xmm10
	movaps	%xmm10, %xmm3
	divss	12(%rsp), %xmm3
	movaps	%xmm5, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm6, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm8, %xmm0
	divss	%xmm4, %xmm0
	movaps	%xmm10, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm5, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm6, %xmm7
	movaps	%xmm6, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm8, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm10, %xmm0
	divss	%xmm4, %xmm0
	movaps	%xmm5, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm6, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm8, %xmm6
	movaps	%xmm8, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm10, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm5, %xmm0
	divss	%xmm4, %xmm0
	movaps	%xmm7, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm8, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm10, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm5, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm7, %xmm0
	divss	%xmm4, %xmm0
	movaps	%xmm8, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm10, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm5, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm7, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm8, %xmm0
	divss	%xmm4, %xmm0
	movaps	%xmm10, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm5, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm7, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm8, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm10, %xmm0
	divss	%xmm4, %xmm0
	movaps	%xmm5, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm7, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm8, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm10, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm5, %xmm0
	divss	%xmm4, %xmm0
	divss	%xmm3, %xmm7
	movss	%xmm7, 4(%rsp)
	divss	%xmm2, %xmm6
	movss	%xmm6, 8(%rsp)
	divss	%xmm1, %xmm10
	movss	%xmm10, 12(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L992
.L991:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE268:
	.size	float_div_3, .-float_div_3
	.globl	float_div_4
	.type	float_div_4, @function
float_div_4:
.LFB269:
	.cfi_startproc
	endbr64
	subq	$56, %rsp
	.cfi_def_cfa_offset 64
	pxor	%xmm0, %xmm0
	cvtsd2ss	80(%rsi), %xmm0
	movss	.LC5(%rip), %xmm2
	mulss	%xmm2, %xmm0
	movl	12(%rsi), %eax
	addl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movss	.LC6(%rip), %xmm1
	mulss	%xmm1, %xmm3
	movss	%xmm3, 28(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rsi), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 12(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2ssl	16(%rsi), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 32(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	96(%rsi), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 16(%rsp)
	movl	20(%rsi), %eax
	subl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 36(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	104(%rsi), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 20(%rsp)
	movl	24(%rsi), %eax
	subl	$2, %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 40(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	112(%rsi), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 24(%rsp)
	movl	28(%rsi), %eax
	subl	$3, %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 44(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L996
.L997:
	movss	28(%rsp), %xmm6
	movaps	%xmm6, %xmm3
	divss	%xmm0, %xmm3
	movss	32(%rsp), %xmm7
	movaps	%xmm7, %xmm2
	divss	12(%rsp), %xmm2
	movss	36(%rsp), %xmm9
	movaps	%xmm9, %xmm1
	divss	16(%rsp), %xmm1
	movss	40(%rsp), %xmm11
	movaps	%xmm11, %xmm0
	divss	20(%rsp), %xmm0
	movss	44(%rsp), %xmm13
	movaps	%xmm13, %xmm5
	divss	24(%rsp), %xmm5
	movaps	%xmm6, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm7, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm9, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm11, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm13, %xmm0
	divss	%xmm5, %xmm0
	movaps	%xmm6, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm7, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm9, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm11, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm13, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm6, %xmm0
	divss	%xmm5, %xmm0
	movaps	%xmm7, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm9, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm11, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm13, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm6, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm7, %xmm0
	divss	%xmm5, %xmm0
	movaps	%xmm9, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm11, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm13, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm6, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm7, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm9, %xmm0
	divss	%xmm5, %xmm0
	movaps	%xmm11, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm13, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm6, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm7, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm9, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm11, %xmm0
	divss	%xmm5, %xmm0
	movaps	%xmm13, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm6, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm7, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm9, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm11, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm13, %xmm0
	divss	%xmm5, %xmm0
	movaps	%xmm6, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm7, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm9, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm11, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm13, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm6, %xmm0
	divss	%xmm5, %xmm0
	divss	%xmm4, %xmm7
	movss	%xmm7, 12(%rsp)
	divss	%xmm3, %xmm9
	movss	%xmm9, 16(%rsp)
	divss	%xmm2, %xmm11
	movss	%xmm11, 20(%rsp)
	divss	%xmm1, %xmm13
	movss	%xmm13, 24(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L997
.L996:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE269:
	.size	float_div_4, .-float_div_4
	.globl	float_div_5
	.type	float_div_5, @function
float_div_5:
.LFB270:
	.cfi_startproc
	endbr64
	subq	$56, %rsp
	.cfi_def_cfa_offset 64
	movq	%rsi, %rax
	pxor	%xmm0, %xmm0
	cvtsd2ss	80(%rsi), %xmm0
	movss	.LC5(%rip), %xmm2
	mulss	%xmm2, %xmm0
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movss	.LC6(%rip), %xmm1
	mulss	%xmm1, %xmm3
	movss	%xmm3, 24(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rax), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 4(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2ssl	16(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 28(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	96(%rax), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 8(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 32(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	104(%rax), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 12(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 36(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	112(%rax), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 16(%rsp)
	movl	28(%rax), %ecx
	subl	$3, %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 40(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	120(%rax), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 20(%rsp)
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 44(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1001
.L1002:
	movss	24(%rsp), %xmm7
	movaps	%xmm7, %xmm5
	divss	%xmm0, %xmm5
	movss	28(%rsp), %xmm9
	movaps	%xmm9, %xmm4
	divss	4(%rsp), %xmm4
	movss	32(%rsp), %xmm11
	movaps	%xmm11, %xmm3
	divss	8(%rsp), %xmm3
	movss	36(%rsp), %xmm13
	movaps	%xmm13, %xmm2
	divss	12(%rsp), %xmm2
	movss	40(%rsp), %xmm15
	movaps	%xmm15, %xmm1
	divss	16(%rsp), %xmm1
	movss	44(%rsp), %xmm8
	movaps	%xmm8, %xmm0
	divss	20(%rsp), %xmm0
	movaps	%xmm7, %xmm6
	divss	%xmm5, %xmm6
	movaps	%xmm9, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm11, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm13, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm15, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm8, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm7, %xmm0
	divss	%xmm6, %xmm0
	movaps	%xmm9, %xmm6
	divss	%xmm5, %xmm6
	movaps	%xmm11, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm13, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm15, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm8, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm7, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm9, %xmm0
	divss	%xmm6, %xmm0
	movaps	%xmm11, %xmm6
	divss	%xmm5, %xmm6
	movaps	%xmm13, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm15, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm8, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm7, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm9, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm11, %xmm0
	divss	%xmm6, %xmm0
	movaps	%xmm13, %xmm6
	divss	%xmm5, %xmm6
	movaps	%xmm15, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm8, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm7, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm9, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm11, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm13, %xmm0
	divss	%xmm6, %xmm0
	movaps	%xmm15, %xmm6
	divss	%xmm5, %xmm6
	movaps	%xmm8, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm7, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm9, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm11, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm13, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm15, %xmm0
	divss	%xmm6, %xmm0
	movaps	%xmm8, %xmm6
	divss	%xmm5, %xmm6
	movaps	%xmm7, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm9, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm11, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm13, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm15, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm8, %xmm0
	divss	%xmm6, %xmm0
	movaps	%xmm7, %xmm6
	divss	%xmm5, %xmm6
	movaps	%xmm9, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm11, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm13, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm15, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm8, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm7, %xmm0
	divss	%xmm6, %xmm0
	divss	%xmm5, %xmm9
	movss	%xmm9, 4(%rsp)
	divss	%xmm4, %xmm11
	movss	%xmm11, 8(%rsp)
	divss	%xmm3, %xmm13
	movss	%xmm13, 12(%rsp)
	divss	%xmm2, %xmm15
	movss	%xmm15, 16(%rsp)
	divss	%xmm1, %xmm8
	movss	%xmm8, 20(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1002
.L1001:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE270:
	.size	float_div_5, .-float_div_5
	.globl	float_div_6
	.type	float_div_6, @function
float_div_6:
.LFB271:
	.cfi_startproc
	endbr64
	subq	$72, %rsp
	.cfi_def_cfa_offset 80
	movq	%rsi, %rax
	pxor	%xmm0, %xmm0
	cvtsd2ss	80(%rsi), %xmm0
	movss	.LC5(%rip), %xmm2
	mulss	%xmm2, %xmm0
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movss	.LC6(%rip), %xmm1
	mulss	%xmm1, %xmm3
	movss	%xmm3, 36(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rax), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 12(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2ssl	16(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 40(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	96(%rax), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 16(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 44(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	104(%rax), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 20(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 48(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	112(%rax), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 24(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 52(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	120(%rax), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 28(%rsp)
	movl	32(%rax), %ecx
	subl	$4, %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 56(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	128(%rax), %xmm3
	mulss	%xmm2, %xmm3
	movss	%xmm3, 32(%rsp)
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 60(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1006
.L1007:
	movss	36(%rsp), %xmm8
	movaps	%xmm8, %xmm7
	divss	%xmm0, %xmm7
	movss	40(%rsp), %xmm10
	movaps	%xmm10, %xmm6
	divss	12(%rsp), %xmm6
	movss	44(%rsp), %xmm12
	movaps	%xmm12, %xmm5
	divss	16(%rsp), %xmm5
	movss	48(%rsp), %xmm14
	movaps	%xmm14, %xmm4
	divss	20(%rsp), %xmm4
	movss	52(%rsp), %xmm9
	movaps	%xmm9, %xmm3
	divss	24(%rsp), %xmm3
	movss	56(%rsp), %xmm11
	movaps	%xmm11, %xmm2
	divss	28(%rsp), %xmm2
	movss	60(%rsp), %xmm13
	movaps	%xmm13, %xmm1
	divss	32(%rsp), %xmm1
	movaps	%xmm8, %xmm0
	divss	%xmm7, %xmm0
	movaps	%xmm10, %xmm7
	divss	%xmm6, %xmm7
	movaps	%xmm12, %xmm6
	divss	%xmm5, %xmm6
	movaps	%xmm14, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm9, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm11, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm13, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm8, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm10, %xmm0
	divss	%xmm7, %xmm0
	movaps	%xmm12, %xmm7
	divss	%xmm6, %xmm7
	movaps	%xmm14, %xmm6
	divss	%xmm5, %xmm6
	movaps	%xmm9, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm11, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm13, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm8, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm10, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm12, %xmm0
	divss	%xmm7, %xmm0
	movaps	%xmm14, %xmm7
	divss	%xmm6, %xmm7
	movaps	%xmm9, %xmm6
	divss	%xmm5, %xmm6
	movaps	%xmm11, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm13, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm8, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm10, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm12, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm14, %xmm0
	divss	%xmm7, %xmm0
	movaps	%xmm9, %xmm7
	divss	%xmm6, %xmm7
	movaps	%xmm11, %xmm6
	divss	%xmm5, %xmm6
	movaps	%xmm13, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm8, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm10, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm12, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm14, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm9, %xmm0
	divss	%xmm7, %xmm0
	movaps	%xmm11, %xmm7
	divss	%xmm6, %xmm7
	movaps	%xmm13, %xmm6
	divss	%xmm5, %xmm6
	movaps	%xmm8, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm10, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm12, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm14, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm9, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm11, %xmm0
	divss	%xmm7, %xmm0
	movaps	%xmm13, %xmm7
	divss	%xmm6, %xmm7
	movaps	%xmm8, %xmm6
	divss	%xmm5, %xmm6
	movaps	%xmm10, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm12, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm14, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm9, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm11, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm13, %xmm0
	divss	%xmm7, %xmm0
	movaps	%xmm8, %xmm7
	divss	%xmm6, %xmm7
	movaps	%xmm10, %xmm6
	divss	%xmm5, %xmm6
	movaps	%xmm12, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm14, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm9, %xmm3
	divss	%xmm2, %xmm3
	movaps	%xmm11, %xmm2
	divss	%xmm1, %xmm2
	movaps	%xmm13, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm8, %xmm0
	divss	%xmm7, %xmm0
	divss	%xmm6, %xmm10
	movss	%xmm10, 12(%rsp)
	divss	%xmm5, %xmm12
	movss	%xmm12, 16(%rsp)
	divss	%xmm4, %xmm14
	movss	%xmm14, 20(%rsp)
	divss	%xmm3, %xmm9
	movss	%xmm9, 24(%rsp)
	divss	%xmm2, %xmm11
	movss	%xmm11, 28(%rsp)
	divss	%xmm1, %xmm13
	movss	%xmm13, 32(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1007
.L1006:
	cvttss2sil	%xmm0, %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE271:
	.size	float_div_6, .-float_div_6
	.globl	float_div_7
	.type	float_div_7, @function
float_div_7:
.LFB272:
	.cfi_startproc
	endbr64
	subq	$72, %rsp
	.cfi_def_cfa_offset 80
	movq	%rsi, %rax
	pxor	%xmm2, %xmm2
	cvtsd2ss	80(%rsi), %xmm2
	movss	.LC5(%rip), %xmm1
	mulss	%xmm1, %xmm2
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movss	.LC6(%rip), %xmm0
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm10
	movss	%xmm3, 36(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 8(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2ssl	16(%rax), %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm11
	movss	%xmm3, 40(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	96(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 12(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm12
	movss	%xmm3, 44(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	104(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 16(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm13
	movss	%xmm3, 48(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	112(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 20(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm14
	movss	%xmm3, 52(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	120(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 24(%rsp)
	movl	32(%rax), %esi
	leal	-4(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm15
	movss	%xmm3, 56(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	128(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 28(%rsp)
	movl	36(%rax), %ecx
	subl	$5, %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm9
	movss	%xmm3, 60(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	136(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 32(%rsp)
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	mulss	%xmm0, %xmm1
	movss	%xmm1, 4(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1011
.L1012:
	movaps	%xmm10, %xmm5
	divss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm11, %xmm8
	divss	8(%rsp), %xmm8
	movaps	%xmm12, %xmm0
	divss	12(%rsp), %xmm0
	movaps	%xmm13, %xmm1
	divss	16(%rsp), %xmm1
	movaps	%xmm14, %xmm3
	divss	20(%rsp), %xmm3
	movaps	%xmm15, %xmm4
	divss	24(%rsp), %xmm4
	movaps	%xmm9, %xmm5
	divss	28(%rsp), %xmm5
	movss	4(%rsp), %xmm6
	divss	32(%rsp), %xmm6
	movaps	%xmm10, %xmm7
	divss	%xmm2, %xmm7
	movaps	%xmm11, %xmm2
	divss	%xmm8, %xmm2
	movaps	%xmm12, %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm13, %xmm0
	divss	%xmm1, %xmm0
	movaps	%xmm14, %xmm1
	divss	%xmm3, %xmm1
	movaps	%xmm15, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm9, %xmm4
	divss	%xmm5, %xmm4
	movss	4(%rsp), %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm10, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm11, %xmm7
	divss	%xmm2, %xmm7
	movaps	%xmm12, %xmm2
	divss	%xmm8, %xmm2
	movaps	%xmm13, %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm14, %xmm0
	divss	%xmm1, %xmm0
	movaps	%xmm15, %xmm1
	divss	%xmm3, %xmm1
	movaps	%xmm9, %xmm3
	divss	%xmm4, %xmm3
	movss	4(%rsp), %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm10, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm11, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm12, %xmm7
	divss	%xmm2, %xmm7
	movaps	%xmm13, %xmm2
	divss	%xmm8, %xmm2
	movaps	%xmm14, %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm15, %xmm0
	divss	%xmm1, %xmm0
	movaps	%xmm9, %xmm1
	divss	%xmm3, %xmm1
	movss	4(%rsp), %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm10, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm11, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm12, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm13, %xmm7
	divss	%xmm2, %xmm7
	movaps	%xmm14, %xmm2
	divss	%xmm8, %xmm2
	movaps	%xmm15, %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm9, %xmm0
	divss	%xmm1, %xmm0
	movss	4(%rsp), %xmm1
	divss	%xmm3, %xmm1
	movaps	%xmm10, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm11, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm12, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm13, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm14, %xmm7
	divss	%xmm2, %xmm7
	movaps	%xmm15, %xmm2
	divss	%xmm8, %xmm2
	movaps	%xmm9, %xmm8
	divss	%xmm0, %xmm8
	movss	4(%rsp), %xmm0
	divss	%xmm1, %xmm0
	movaps	%xmm10, %xmm1
	divss	%xmm3, %xmm1
	movaps	%xmm11, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm12, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm13, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm14, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm15, %xmm7
	divss	%xmm2, %xmm7
	movaps	%xmm9, %xmm2
	divss	%xmm8, %xmm2
	movss	4(%rsp), %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm10, %xmm0
	divss	%xmm1, %xmm0
	movaps	%xmm11, %xmm1
	divss	%xmm3, %xmm1
	movaps	%xmm12, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm13, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm14, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm15, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm9, %xmm7
	divss	%xmm2, %xmm7
	movss	4(%rsp), %xmm2
	divss	%xmm8, %xmm2
	movaps	%xmm10, %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm11, %xmm0
	divss	%xmm1, %xmm0
	movaps	%xmm12, %xmm1
	divss	%xmm3, %xmm1
	movaps	%xmm13, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm14, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm15, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm9, %xmm6
	divss	%xmm7, %xmm6
	movss	4(%rsp), %xmm7
	divss	%xmm2, %xmm7
	movaps	%xmm10, %xmm2
	divss	%xmm8, %xmm2
	movaps	%xmm11, %xmm8
	divss	%xmm0, %xmm8
	movss	%xmm8, 8(%rsp)
	movaps	%xmm12, %xmm0
	divss	%xmm1, %xmm0
	movss	%xmm0, 12(%rsp)
	movaps	%xmm13, %xmm1
	divss	%xmm3, %xmm1
	movss	%xmm1, 16(%rsp)
	movaps	%xmm14, %xmm3
	divss	%xmm4, %xmm3
	movss	%xmm3, 20(%rsp)
	movaps	%xmm15, %xmm0
	divss	%xmm5, %xmm0
	movss	%xmm0, 24(%rsp)
	movaps	%xmm9, %xmm5
	divss	%xmm6, %xmm5
	movss	%xmm5, 28(%rsp)
	movss	4(%rsp), %xmm4
	divss	%xmm7, %xmm4
	movss	%xmm4, 32(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1012
.L1011:
	cvttss2sil	%xmm2, %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE272:
	.size	float_div_7, .-float_div_7
	.globl	float_div_8
	.type	float_div_8, @function
float_div_8:
.LFB273:
	.cfi_startproc
	endbr64
	subq	$88, %rsp
	.cfi_def_cfa_offset 96
	movq	%rsi, %rax
	pxor	%xmm2, %xmm2
	cvtsd2ss	80(%rsi), %xmm2
	movss	.LC5(%rip), %xmm1
	mulss	%xmm1, %xmm2
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movss	.LC6(%rip), %xmm0
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm15
	movss	%xmm3, 48(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm13
	movss	%xmm3, 12(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2ssl	16(%rax), %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm14
	movss	%xmm3, 52(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	96(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm12
	movss	%xmm3, 16(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm0, %xmm4
	movss	%xmm4, 56(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	104(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm11
	movss	%xmm3, 36(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movaps	%xmm3, %xmm5
	mulss	%xmm0, %xmm5
	movss	%xmm5, 60(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	112(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm10
	movss	%xmm3, 40(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movaps	%xmm3, %xmm6
	mulss	%xmm0, %xmm6
	movss	%xmm6, 64(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	120(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm9
	movss	%xmm3, 44(%rsp)
	movl	32(%rax), %esi
	leal	-4(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movaps	%xmm3, %xmm7
	mulss	%xmm0, %xmm7
	movss	%xmm7, 68(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	128(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 20(%rsp)
	movl	36(%rax), %esi
	leal	-5(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm8
	movss	%xmm3, 72(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	136(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 24(%rsp)
	movl	40(%rax), %ecx
	subl	$6, %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movss	%xmm3, 32(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	144(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 28(%rsp)
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	mulss	%xmm0, %xmm1
	movss	%xmm1, 76(%rsp)
	testq	%rdi, %rdi
	je	.L1016
	movaps	%xmm1, %xmm0
	leaq	-1(%rdi), %rax
	movss	%xmm15, 12(%rsp)
	movaps	%xmm14, %xmm1
	movaps	%xmm4, %xmm3
	movaps	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	movaps	%xmm7, %xmm6
	movaps	%xmm8, %xmm7
	movss	32(%rsp), %xmm8
	movss	%xmm0, 16(%rsp)
	movaps	%xmm2, %xmm14
	movaps	%xmm9, %xmm2
.L1017:
	movss	12(%rsp), %xmm0
	movaps	%xmm0, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm1, %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm3, %xmm13
	divss	%xmm12, %xmm13
	movaps	%xmm4, %xmm12
	divss	%xmm11, %xmm12
	movaps	%xmm5, %xmm11
	divss	%xmm10, %xmm11
	movaps	%xmm6, %xmm10
	divss	%xmm2, %xmm10
	movaps	%xmm7, %xmm2
	divss	20(%rsp), %xmm2
	movaps	%xmm8, %xmm9
	divss	24(%rsp), %xmm9
	movss	%xmm9, 20(%rsp)
	movss	16(%rsp), %xmm9
	divss	28(%rsp), %xmm9
	movss	%xmm9, 24(%rsp)
	movaps	%xmm0, %xmm9
	divss	%xmm15, %xmm9
	movaps	%xmm1, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm3, %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm4, %xmm13
	divss	%xmm12, %xmm13
	movaps	%xmm5, %xmm12
	divss	%xmm11, %xmm12
	movaps	%xmm6, %xmm11
	divss	%xmm10, %xmm11
	movaps	%xmm7, %xmm10
	divss	%xmm2, %xmm10
	movaps	%xmm8, %xmm2
	divss	20(%rsp), %xmm2
	movaps	%xmm2, %xmm0
	movss	16(%rsp), %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movss	12(%rsp), %xmm2
	divss	%xmm9, %xmm2
	movaps	%xmm2, %xmm9
	movaps	%xmm1, %xmm2
	divss	%xmm15, %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm3, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm4, %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm5, %xmm13
	divss	%xmm12, %xmm13
	movaps	%xmm6, %xmm12
	divss	%xmm11, %xmm12
	movaps	%xmm7, %xmm11
	divss	%xmm10, %xmm11
	movaps	%xmm8, %xmm10
	divss	%xmm0, %xmm10
	movaps	%xmm10, %xmm0
	movss	16(%rsp), %xmm10
	divss	20(%rsp), %xmm10
	movaps	%xmm10, %xmm2
	movss	12(%rsp), %xmm10
	divss	%xmm9, %xmm10
	movss	%xmm10, 20(%rsp)
	movaps	%xmm1, %xmm10
	divss	24(%rsp), %xmm10
	movss	%xmm10, 24(%rsp)
	movaps	%xmm3, %xmm9
	divss	%xmm15, %xmm9
	movss	%xmm9, 28(%rsp)
	movaps	%xmm4, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm5, %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm6, %xmm13
	divss	%xmm12, %xmm13
	movaps	%xmm7, %xmm12
	divss	%xmm11, %xmm12
	movaps	%xmm8, %xmm11
	divss	%xmm0, %xmm11
	movss	16(%rsp), %xmm9
	movaps	%xmm9, %xmm10
	divss	%xmm2, %xmm10
	movss	12(%rsp), %xmm2
	divss	20(%rsp), %xmm2
	movaps	%xmm2, %xmm0
	movaps	%xmm1, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm3, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm4, %xmm2
	divss	%xmm15, %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm5, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm6, %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm7, %xmm13
	divss	%xmm12, %xmm13
	movaps	%xmm8, %xmm12
	divss	%xmm11, %xmm12
	movaps	%xmm9, %xmm11
	divss	%xmm10, %xmm11
	movss	12(%rsp), %xmm10
	divss	%xmm0, %xmm10
	movaps	%xmm10, %xmm0
	movaps	%xmm1, %xmm2
	divss	20(%rsp), %xmm2
	movaps	%xmm3, %xmm10
	divss	24(%rsp), %xmm10
	movss	%xmm10, 20(%rsp)
	movaps	%xmm4, %xmm10
	divss	28(%rsp), %xmm10
	movss	%xmm10, 24(%rsp)
	movaps	%xmm5, %xmm9
	divss	%xmm15, %xmm9
	movaps	%xmm6, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm7, %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm8, %xmm13
	divss	%xmm12, %xmm13
	movss	16(%rsp), %xmm12
	divss	%xmm11, %xmm12
	movss	12(%rsp), %xmm11
	divss	%xmm0, %xmm11
	movaps	%xmm1, %xmm10
	divss	%xmm2, %xmm10
	movaps	%xmm3, %xmm2
	divss	20(%rsp), %xmm2
	movaps	%xmm4, %xmm0
	divss	24(%rsp), %xmm0
	movss	%xmm0, 20(%rsp)
	movaps	%xmm5, %xmm0
	divss	%xmm9, %xmm0
	movaps	%xmm0, %xmm9
	movaps	%xmm6, %xmm0
	divss	%xmm15, %xmm0
	movss	%xmm0, 24(%rsp)
	movaps	%xmm7, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm8, %xmm14
	divss	%xmm13, %xmm14
	movss	16(%rsp), %xmm13
	divss	%xmm12, %xmm13
	movss	12(%rsp), %xmm12
	divss	%xmm11, %xmm12
	movaps	%xmm1, %xmm11
	divss	%xmm10, %xmm11
	movaps	%xmm3, %xmm10
	divss	%xmm2, %xmm10
	movaps	%xmm4, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm5, %xmm0
	divss	%xmm9, %xmm0
	movaps	%xmm0, %xmm9
	movaps	%xmm6, %xmm2
	divss	24(%rsp), %xmm2
	movaps	%xmm2, %xmm0
	movaps	%xmm7, %xmm2
	divss	%xmm15, %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm8, %xmm15
	divss	%xmm14, %xmm15
	movss	16(%rsp), %xmm14
	divss	%xmm13, %xmm14
	movss	12(%rsp), %xmm13
	divss	%xmm12, %xmm13
	movaps	%xmm1, %xmm12
	divss	%xmm11, %xmm12
	movaps	%xmm3, %xmm11
	divss	%xmm10, %xmm11
	movaps	%xmm4, %xmm10
	divss	20(%rsp), %xmm10
	movaps	%xmm5, %xmm2
	divss	%xmm9, %xmm2
	movaps	%xmm6, %xmm9
	divss	%xmm0, %xmm9
	movss	%xmm9, 20(%rsp)
	movaps	%xmm7, %xmm9
	divss	24(%rsp), %xmm9
	movaps	%xmm9, %xmm0
	movaps	%xmm8, %xmm9
	divss	%xmm15, %xmm9
	movss	%xmm9, 24(%rsp)
	movss	16(%rsp), %xmm15
	divss	%xmm14, %xmm15
	movss	12(%rsp), %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm1, %xmm13
	divss	%xmm12, %xmm13
	movaps	%xmm3, %xmm12
	divss	%xmm11, %xmm12
	movaps	%xmm4, %xmm11
	divss	%xmm10, %xmm11
	movaps	%xmm5, %xmm10
	divss	%xmm2, %xmm10
	movaps	%xmm6, %xmm2
	divss	20(%rsp), %xmm2
	movaps	%xmm7, %xmm9
	divss	%xmm0, %xmm9
	movss	%xmm9, 20(%rsp)
	movaps	%xmm8, %xmm0
	divss	24(%rsp), %xmm0
	movss	%xmm0, 24(%rsp)
	movss	16(%rsp), %xmm9
	divss	%xmm15, %xmm9
	movss	%xmm9, 28(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1017
	movss	%xmm13, 12(%rsp)
	movss	%xmm12, 16(%rsp)
	movss	%xmm11, 36(%rsp)
	movss	%xmm10, 40(%rsp)
	movss	%xmm2, 44(%rsp)
	movaps	%xmm14, %xmm2
.L1016:
	cvttss2sil	%xmm2, %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	68(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	76(%rsp), %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE273:
	.size	float_div_8, .-float_div_8
	.globl	float_div_9
	.type	float_div_9, @function
float_div_9:
.LFB274:
	.cfi_startproc
	endbr64
	subq	$88, %rsp
	.cfi_def_cfa_offset 96
	movq	%rsi, %rax
	pxor	%xmm2, %xmm2
	cvtsd2ss	80(%rsi), %xmm2
	movss	.LC5(%rip), %xmm1
	mulss	%xmm1, %xmm2
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movss	.LC6(%rip), %xmm0
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm14
	movss	%xmm3, 44(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 12(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2ssl	16(%rax), %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm15
	movss	%xmm3, 48(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	96(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm12
	movss	%xmm3, 4(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm13
	movss	%xmm3, 52(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	104(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm6
	movss	%xmm3, 8(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm0, %xmm4
	movss	%xmm4, 56(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	112(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 16(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movaps	%xmm3, %xmm5
	mulss	%xmm0, %xmm5
	movss	%xmm5, 60(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	120(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 20(%rsp)
	movl	32(%rax), %esi
	leal	-4(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm7
	movss	%xmm3, 64(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	128(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 24(%rsp)
	movl	36(%rax), %esi
	leal	-5(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm8
	movss	%xmm3, 68(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	136(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 28(%rsp)
	movl	40(%rax), %esi
	leal	-6(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm9
	movss	%xmm3, 72(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	144(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 32(%rsp)
	movl	44(%rax), %ecx
	subl	$7, %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm10
	movss	%xmm3, 76(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	152(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 40(%rsp)
	movl	48(%rax), %eax
	subl	$8, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	mulss	%xmm0, %xmm1
	movss	%xmm1, 36(%rsp)
	testq	%rdi, %rdi
	je	.L1021
	movaps	%xmm3, %xmm11
	leaq	-1(%rdi), %rax
	movaps	%xmm14, %xmm0
	movaps	%xmm15, %xmm1
	movaps	%xmm13, %xmm3
	movss	%xmm7, 4(%rsp)
	movss	%xmm8, 8(%rsp)
	movaps	%xmm9, %xmm8
	movaps	%xmm10, %xmm9
	movss	36(%rsp), %xmm10
	movaps	%xmm2, %xmm13
	movaps	%xmm6, %xmm2
.L1022:
	movaps	%xmm0, %xmm15
	divss	%xmm13, %xmm15
	movaps	%xmm1, %xmm14
	divss	12(%rsp), %xmm14
	movaps	%xmm3, %xmm13
	divss	%xmm12, %xmm13
	movaps	%xmm4, %xmm12
	divss	%xmm2, %xmm12
	movaps	%xmm5, %xmm2
	divss	16(%rsp), %xmm2
	movss	4(%rsp), %xmm6
	movaps	%xmm6, %xmm7
	divss	20(%rsp), %xmm7
	movss	%xmm7, 12(%rsp)
	movss	8(%rsp), %xmm7
	divss	24(%rsp), %xmm7
	movss	%xmm7, 16(%rsp)
	movaps	%xmm8, %xmm7
	divss	28(%rsp), %xmm7
	movss	%xmm7, 20(%rsp)
	movaps	%xmm9, %xmm7
	divss	32(%rsp), %xmm7
	movss	%xmm7, 24(%rsp)
	movaps	%xmm10, %xmm7
	divss	%xmm11, %xmm7
	movaps	%xmm0, %xmm11
	divss	%xmm15, %xmm11
	movaps	%xmm1, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm3, %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm4, %xmm13
	divss	%xmm12, %xmm13
	movaps	%xmm5, %xmm12
	divss	%xmm2, %xmm12
	movaps	%xmm6, %xmm2
	divss	12(%rsp), %xmm2
	movaps	%xmm2, %xmm6
	movss	8(%rsp), %xmm2
	divss	16(%rsp), %xmm2
	movss	%xmm2, 12(%rsp)
	movaps	%xmm8, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 16(%rsp)
	movaps	%xmm9, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm10, %xmm2
	divss	%xmm7, %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm0, %xmm7
	divss	%xmm11, %xmm7
	movss	%xmm7, 28(%rsp)
	movaps	%xmm1, %xmm11
	divss	%xmm15, %xmm11
	movss	%xmm11, 32(%rsp)
	movaps	%xmm3, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm4, %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm5, %xmm13
	divss	%xmm12, %xmm13
	movss	4(%rsp), %xmm12
	divss	%xmm6, %xmm12
	movss	8(%rsp), %xmm7
	divss	12(%rsp), %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm8, %xmm11
	divss	16(%rsp), %xmm11
	movaps	%xmm11, %xmm6
	movaps	%xmm9, %xmm11
	divss	20(%rsp), %xmm11
	movss	%xmm11, 12(%rsp)
	movaps	%xmm10, %xmm11
	divss	24(%rsp), %xmm11
	movss	%xmm11, 16(%rsp)
	movaps	%xmm0, %xmm11
	divss	28(%rsp), %xmm11
	movss	%xmm11, 20(%rsp)
	movaps	%xmm1, %xmm7
	divss	32(%rsp), %xmm7
	movss	%xmm7, 24(%rsp)
	movaps	%xmm3, %xmm11
	divss	%xmm15, %xmm11
	movaps	%xmm4, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm5, %xmm14
	divss	%xmm13, %xmm14
	movss	4(%rsp), %xmm13
	divss	%xmm12, %xmm13
	movss	8(%rsp), %xmm12
	divss	%xmm2, %xmm12
	movaps	%xmm8, %xmm2
	divss	%xmm6, %xmm2
	movaps	%xmm2, %xmm6
	movaps	%xmm9, %xmm2
	divss	12(%rsp), %xmm2
	movss	%xmm2, 12(%rsp)
	movaps	%xmm10, %xmm2
	divss	16(%rsp), %xmm2
	movss	%xmm2, 16(%rsp)
	movaps	%xmm0, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm1, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm3, %xmm2
	divss	%xmm11, %xmm2
	movaps	%xmm2, %xmm7
	movaps	%xmm4, %xmm11
	divss	%xmm15, %xmm11
	movss	%xmm11, 28(%rsp)
	movaps	%xmm5, %xmm15
	divss	%xmm14, %xmm15
	movss	4(%rsp), %xmm14
	divss	%xmm13, %xmm14
	movss	8(%rsp), %xmm13
	divss	%xmm12, %xmm13
	movaps	%xmm8, %xmm12
	divss	%xmm6, %xmm12
	movaps	%xmm9, %xmm2
	divss	12(%rsp), %xmm2
	movaps	%xmm10, %xmm11
	divss	16(%rsp), %xmm11
	movaps	%xmm11, %xmm6
	movaps	%xmm0, %xmm11
	divss	20(%rsp), %xmm11
	movss	%xmm11, 12(%rsp)
	movaps	%xmm1, %xmm11
	divss	24(%rsp), %xmm11
	movss	%xmm11, 16(%rsp)
	movaps	%xmm3, %xmm11
	divss	%xmm7, %xmm11
	movss	%xmm11, 20(%rsp)
	movaps	%xmm4, %xmm7
	divss	28(%rsp), %xmm7
	movss	%xmm7, 24(%rsp)
	movaps	%xmm5, %xmm11
	divss	%xmm15, %xmm11
	movss	4(%rsp), %xmm15
	divss	%xmm14, %xmm15
	movss	8(%rsp), %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm8, %xmm13
	divss	%xmm12, %xmm13
	movaps	%xmm9, %xmm12
	divss	%xmm2, %xmm12
	movaps	%xmm10, %xmm2
	divss	%xmm6, %xmm2
	movaps	%xmm2, %xmm6
	movaps	%xmm0, %xmm2
	divss	12(%rsp), %xmm2
	movss	%xmm2, 12(%rsp)
	movaps	%xmm1, %xmm2
	divss	16(%rsp), %xmm2
	movss	%xmm2, 16(%rsp)
	movaps	%xmm3, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm4, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm5, %xmm2
	divss	%xmm11, %xmm2
	movss	%xmm2, 28(%rsp)
	movss	4(%rsp), %xmm7
	movaps	%xmm7, %xmm11
	divss	%xmm15, %xmm11
	movss	%xmm11, 32(%rsp)
	movss	8(%rsp), %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm8, %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm9, %xmm13
	divss	%xmm12, %xmm13
	movaps	%xmm10, %xmm12
	divss	%xmm6, %xmm12
	movaps	%xmm0, %xmm2
	divss	12(%rsp), %xmm2
	movaps	%xmm1, %xmm11
	divss	16(%rsp), %xmm11
	movaps	%xmm11, %xmm6
	movaps	%xmm3, %xmm11
	divss	20(%rsp), %xmm11
	movss	%xmm11, 12(%rsp)
	movaps	%xmm4, %xmm11
	divss	24(%rsp), %xmm11
	movss	%xmm11, 16(%rsp)
	movaps	%xmm5, %xmm11
	divss	28(%rsp), %xmm11
	movss	%xmm11, 20(%rsp)
	divss	32(%rsp), %xmm7
	movss	%xmm7, 24(%rsp)
	movss	8(%rsp), %xmm11
	divss	%xmm15, %xmm11
	movaps	%xmm8, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm9, %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm10, %xmm13
	divss	%xmm12, %xmm13
	movaps	%xmm0, %xmm12
	divss	%xmm2, %xmm12
	movaps	%xmm1, %xmm2
	divss	%xmm6, %xmm2
	movaps	%xmm2, %xmm6
	movaps	%xmm3, %xmm2
	divss	12(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm4, %xmm2
	divss	16(%rsp), %xmm2
	movss	%xmm2, 16(%rsp)
	movaps	%xmm5, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movss	4(%rsp), %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movss	8(%rsp), %xmm7
	divss	%xmm11, %xmm7
	movaps	%xmm8, %xmm11
	divss	%xmm15, %xmm11
	movaps	%xmm9, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm10, %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm0, %xmm13
	divss	%xmm12, %xmm13
	movaps	%xmm1, %xmm12
	divss	%xmm6, %xmm12
	movss	%xmm12, 12(%rsp)
	movaps	%xmm3, %xmm12
	divss	28(%rsp), %xmm12
	movaps	%xmm4, %xmm2
	divss	16(%rsp), %xmm2
	movaps	%xmm5, %xmm6
	divss	20(%rsp), %xmm6
	movss	%xmm6, 16(%rsp)
	movss	4(%rsp), %xmm6
	divss	24(%rsp), %xmm6
	movss	%xmm6, 20(%rsp)
	movss	8(%rsp), %xmm6
	divss	%xmm7, %xmm6
	movss	%xmm6, 24(%rsp)
	movaps	%xmm8, %xmm6
	divss	%xmm11, %xmm6
	movss	%xmm6, 28(%rsp)
	movaps	%xmm9, %xmm6
	divss	%xmm15, %xmm6
	movss	%xmm6, 32(%rsp)
	movaps	%xmm10, %xmm11
	divss	%xmm14, %xmm11
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1022
	movss	%xmm12, 4(%rsp)
	movss	%xmm2, 8(%rsp)
	movaps	%xmm13, %xmm2
	movss	%xmm11, 40(%rsp)
.L1021:
	cvttss2sil	%xmm2, %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	68(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	76(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE274:
	.size	float_div_9, .-float_div_9
	.globl	float_div_10
	.type	float_div_10, @function
float_div_10:
.LFB275:
	.cfi_startproc
	endbr64
	subq	$104, %rsp
	.cfi_def_cfa_offset 112
	movq	%rsi, %rax
	pxor	%xmm2, %xmm2
	cvtsd2ss	80(%rsi), %xmm2
	movss	.LC5(%rip), %xmm1
	mulss	%xmm1, %xmm2
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movss	.LC6(%rip), %xmm0
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm13
	movss	%xmm3, 52(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 16(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2ssl	16(%rax), %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm14
	movss	%xmm3, 56(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	96(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 20(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm15
	movss	%xmm3, 60(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	104(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 24(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm0, %xmm4
	movss	%xmm4, 64(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	112(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 28(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movaps	%xmm3, %xmm5
	mulss	%xmm0, %xmm5
	movss	%xmm5, 68(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	120(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 32(%rsp)
	movl	32(%rax), %esi
	leal	-4(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm6
	movss	%xmm3, 72(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	128(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 36(%rsp)
	movl	36(%rax), %esi
	leal	-5(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movaps	%xmm3, %xmm7
	mulss	%xmm0, %xmm7
	movss	%xmm7, 76(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	136(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 40(%rsp)
	movl	40(%rax), %esi
	leal	-6(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm8
	movss	%xmm3, 80(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	144(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 44(%rsp)
	movl	44(%rax), %esi
	leal	-7(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm9
	movss	%xmm3, 84(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	152(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 48(%rsp)
	movl	48(%rax), %ecx
	subl	$8, %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm10
	movss	%xmm3, 88(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	160(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 8(%rsp)
	movl	52(%rax), %eax
	subl	$9, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	mulss	%xmm0, %xmm1
	movss	%xmm1, 92(%rsp)
	testq	%rdi, %rdi
	je	.L1026
	movaps	%xmm3, %xmm12
	movaps	%xmm1, %xmm11
	leaq	-1(%rdi), %rax
	movaps	%xmm13, %xmm0
	movaps	%xmm14, %xmm1
	movss	%xmm15, 8(%rsp)
	movss	%xmm6, 12(%rsp)
.L1027:
	movaps	%xmm0, %xmm15
	divss	%xmm2, %xmm15
	movaps	%xmm1, %xmm14
	divss	16(%rsp), %xmm14
	movss	8(%rsp), %xmm3
	movaps	%xmm3, %xmm13
	divss	20(%rsp), %xmm13
	movaps	%xmm4, %xmm2
	divss	24(%rsp), %xmm2
	movaps	%xmm5, %xmm6
	divss	28(%rsp), %xmm6
	movss	%xmm6, 16(%rsp)
	movss	12(%rsp), %xmm6
	divss	32(%rsp), %xmm6
	movss	%xmm6, 20(%rsp)
	movaps	%xmm7, %xmm6
	divss	36(%rsp), %xmm6
	movss	%xmm6, 24(%rsp)
	movaps	%xmm8, %xmm6
	divss	40(%rsp), %xmm6
	movss	%xmm6, 28(%rsp)
	movaps	%xmm9, %xmm6
	divss	44(%rsp), %xmm6
	movss	%xmm6, 32(%rsp)
	movaps	%xmm10, %xmm6
	divss	48(%rsp), %xmm6
	movss	%xmm6, 36(%rsp)
	movaps	%xmm11, %xmm6
	divss	%xmm12, %xmm6
	movaps	%xmm0, %xmm12
	divss	%xmm15, %xmm12
	movaps	%xmm1, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm3, %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm4, %xmm13
	divss	%xmm2, %xmm13
	movaps	%xmm5, %xmm2
	divss	16(%rsp), %xmm2
	movaps	%xmm2, %xmm3
	movss	12(%rsp), %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 16(%rsp)
	movaps	%xmm7, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm8, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm9, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm10, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm11, %xmm2
	divss	%xmm6, %xmm2
	movss	%xmm2, 36(%rsp)
	movaps	%xmm0, %xmm6
	divss	%xmm12, %xmm6
	movss	%xmm6, 40(%rsp)
	movaps	%xmm1, %xmm12
	divss	%xmm15, %xmm12
	movss	%xmm12, 44(%rsp)
	movss	8(%rsp), %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm4, %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm5, %xmm13
	divss	%xmm3, %xmm13
	movss	12(%rsp), %xmm6
	divss	16(%rsp), %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm7, %xmm12
	divss	20(%rsp), %xmm12
	movaps	%xmm12, %xmm3
	movaps	%xmm8, %xmm12
	divss	24(%rsp), %xmm12
	movss	%xmm12, 16(%rsp)
	movaps	%xmm9, %xmm12
	divss	28(%rsp), %xmm12
	movss	%xmm12, 20(%rsp)
	movaps	%xmm10, %xmm12
	divss	32(%rsp), %xmm12
	movss	%xmm12, 24(%rsp)
	movaps	%xmm11, %xmm12
	divss	36(%rsp), %xmm12
	movss	%xmm12, 28(%rsp)
	movaps	%xmm0, %xmm12
	divss	40(%rsp), %xmm12
	movss	%xmm12, 32(%rsp)
	movaps	%xmm1, %xmm6
	divss	44(%rsp), %xmm6
	movss	%xmm6, 36(%rsp)
	movss	8(%rsp), %xmm12
	divss	%xmm15, %xmm12
	movaps	%xmm4, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm5, %xmm14
	divss	%xmm13, %xmm14
	movss	12(%rsp), %xmm13
	divss	%xmm2, %xmm13
	movaps	%xmm7, %xmm2
	divss	%xmm3, %xmm2
	movaps	%xmm2, %xmm3
	movaps	%xmm8, %xmm2
	divss	16(%rsp), %xmm2
	movss	%xmm2, 16(%rsp)
	movaps	%xmm9, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm10, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm11, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm0, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm1, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movss	8(%rsp), %xmm6
	movaps	%xmm6, %xmm2
	divss	%xmm12, %xmm2
	movss	%xmm2, 40(%rsp)
	movaps	%xmm4, %xmm12
	divss	%xmm15, %xmm12
	movss	%xmm12, 44(%rsp)
	movaps	%xmm5, %xmm15
	divss	%xmm14, %xmm15
	movss	12(%rsp), %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm7, %xmm13
	divss	%xmm3, %xmm13
	movaps	%xmm8, %xmm2
	divss	16(%rsp), %xmm2
	movaps	%xmm9, %xmm12
	divss	20(%rsp), %xmm12
	movaps	%xmm12, %xmm3
	movaps	%xmm10, %xmm12
	divss	24(%rsp), %xmm12
	movss	%xmm12, 16(%rsp)
	movaps	%xmm11, %xmm12
	divss	28(%rsp), %xmm12
	movss	%xmm12, 20(%rsp)
	movaps	%xmm0, %xmm12
	divss	32(%rsp), %xmm12
	movss	%xmm12, 24(%rsp)
	movaps	%xmm1, %xmm12
	divss	36(%rsp), %xmm12
	movss	%xmm12, 28(%rsp)
	movaps	%xmm6, %xmm12
	divss	40(%rsp), %xmm12
	movss	%xmm12, 32(%rsp)
	movaps	%xmm4, %xmm6
	divss	44(%rsp), %xmm6
	movss	%xmm6, 36(%rsp)
	movaps	%xmm5, %xmm12
	divss	%xmm15, %xmm12
	movss	12(%rsp), %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm7, %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm8, %xmm13
	divss	%xmm2, %xmm13
	movaps	%xmm9, %xmm2
	divss	%xmm3, %xmm2
	movaps	%xmm2, %xmm3
	movaps	%xmm10, %xmm2
	divss	16(%rsp), %xmm2
	movss	%xmm2, 16(%rsp)
	movaps	%xmm11, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm0, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm1, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movss	8(%rsp), %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm4, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movaps	%xmm5, %xmm2
	divss	%xmm12, %xmm2
	movss	%xmm2, 40(%rsp)
	movss	12(%rsp), %xmm6
	movaps	%xmm6, %xmm12
	divss	%xmm15, %xmm12
	movss	%xmm12, 44(%rsp)
	movaps	%xmm7, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm8, %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm9, %xmm13
	divss	%xmm3, %xmm13
	movaps	%xmm10, %xmm2
	divss	16(%rsp), %xmm2
	movaps	%xmm11, %xmm12
	divss	20(%rsp), %xmm12
	movaps	%xmm12, %xmm3
	movaps	%xmm0, %xmm12
	divss	24(%rsp), %xmm12
	movss	%xmm12, 16(%rsp)
	movaps	%xmm1, %xmm12
	divss	28(%rsp), %xmm12
	movss	%xmm12, 20(%rsp)
	movss	8(%rsp), %xmm12
	divss	32(%rsp), %xmm12
	movss	%xmm12, 24(%rsp)
	movaps	%xmm4, %xmm12
	divss	36(%rsp), %xmm12
	movss	%xmm12, 28(%rsp)
	movaps	%xmm5, %xmm12
	divss	40(%rsp), %xmm12
	movss	%xmm12, 32(%rsp)
	divss	44(%rsp), %xmm6
	movss	%xmm6, 36(%rsp)
	movaps	%xmm7, %xmm12
	divss	%xmm15, %xmm12
	movaps	%xmm8, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm9, %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm10, %xmm13
	divss	%xmm2, %xmm13
	movaps	%xmm11, %xmm2
	divss	%xmm3, %xmm2
	movaps	%xmm2, %xmm3
	movaps	%xmm0, %xmm2
	divss	16(%rsp), %xmm2
	movss	%xmm2, 16(%rsp)
	movaps	%xmm1, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movss	8(%rsp), %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm4, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm5, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movss	12(%rsp), %xmm6
	movaps	%xmm6, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movaps	%xmm7, %xmm2
	divss	%xmm12, %xmm2
	movss	%xmm2, 40(%rsp)
	movaps	%xmm8, %xmm12
	divss	%xmm15, %xmm12
	movaps	%xmm9, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm10, %xmm14
	divss	%xmm13, %xmm14
	movaps	%xmm11, %xmm13
	divss	%xmm3, %xmm13
	movaps	%xmm0, %xmm2
	divss	16(%rsp), %xmm2
	movaps	%xmm1, %xmm3
	divss	20(%rsp), %xmm3
	movss	%xmm3, 16(%rsp)
	movss	8(%rsp), %xmm3
	divss	24(%rsp), %xmm3
	movss	%xmm3, 20(%rsp)
	movaps	%xmm4, %xmm3
	divss	28(%rsp), %xmm3
	movss	%xmm3, 24(%rsp)
	movaps	%xmm5, %xmm3
	divss	32(%rsp), %xmm3
	movss	%xmm3, 28(%rsp)
	movaps	%xmm6, %xmm3
	divss	36(%rsp), %xmm3
	movss	%xmm3, 32(%rsp)
	movaps	%xmm7, %xmm3
	divss	40(%rsp), %xmm3
	movss	%xmm3, 36(%rsp)
	movaps	%xmm8, %xmm3
	divss	%xmm12, %xmm3
	movss	%xmm3, 40(%rsp)
	movaps	%xmm9, %xmm3
	divss	%xmm15, %xmm3
	movss	%xmm3, 44(%rsp)
	movaps	%xmm10, %xmm3
	divss	%xmm14, %xmm3
	movss	%xmm3, 48(%rsp)
	movaps	%xmm11, %xmm12
	divss	%xmm13, %xmm12
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1027
	movss	%xmm12, 8(%rsp)
.L1026:
	cvttss2sil	%xmm2, %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	68(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	76(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	84(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	92(%rsp), %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE275:
	.size	float_div_10, .-float_div_10
	.globl	float_div_11
	.type	float_div_11, @function
float_div_11:
.LFB276:
	.cfi_startproc
	endbr64
	subq	$120, %rsp
	.cfi_def_cfa_offset 128
	movq	%rsi, %rax
	pxor	%xmm2, %xmm2
	cvtsd2ss	80(%rsi), %xmm2
	movss	.LC5(%rip), %xmm1
	mulss	%xmm1, %xmm2
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movss	.LC6(%rip), %xmm0
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm15
	movss	%xmm3, 68(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 20(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2ssl	16(%rax), %xmm3
	movaps	%xmm3, %xmm5
	mulss	%xmm0, %xmm5
	movss	%xmm5, 72(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	96(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 24(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movaps	%xmm3, %xmm6
	mulss	%xmm0, %xmm6
	movss	%xmm6, 76(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	104(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 28(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm4
	movss	%xmm3, 80(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	112(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 32(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movaps	%xmm3, %xmm7
	mulss	%xmm0, %xmm7
	movss	%xmm7, 84(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	120(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 36(%rsp)
	movl	32(%rax), %esi
	leal	-4(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm8
	movss	%xmm3, 88(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	128(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 40(%rsp)
	movl	36(%rax), %esi
	leal	-5(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm9
	movss	%xmm3, 92(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	136(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 44(%rsp)
	movl	40(%rax), %esi
	leal	-6(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm10
	movss	%xmm3, 96(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	144(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 48(%rsp)
	movl	44(%rax), %esi
	leal	-7(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm11
	movss	%xmm3, 100(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	152(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 52(%rsp)
	movl	48(%rax), %esi
	leal	-8(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm12
	movss	%xmm3, 104(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	160(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm13
	movss	%xmm3, 12(%rsp)
	movl	52(%rax), %ecx
	subl	$9, %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm14
	movss	%xmm3, 108(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	168(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 16(%rsp)
	movl	56(%rax), %eax
	subl	$10, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	mulss	%xmm0, %xmm1
	movss	%xmm1, 64(%rsp)
	testq	%rdi, %rdi
	je	.L1031
	leaq	-1(%rdi), %rax
	movaps	%xmm15, %xmm0
	movaps	%xmm5, %xmm1
	movss	%xmm6, 12(%rsp)
	movss	%xmm4, 16(%rsp)
	movaps	%xmm7, %xmm5
	movaps	%xmm8, %xmm6
	movaps	%xmm9, %xmm7
	movaps	%xmm10, %xmm8
	movaps	%xmm11, %xmm9
	movaps	%xmm12, %xmm10
	movaps	%xmm14, %xmm11
	movss	64(%rsp), %xmm12
	movaps	%xmm3, %xmm14
.L1032:
	movaps	%xmm0, %xmm15
	divss	%xmm2, %xmm15
	movaps	%xmm1, %xmm2
	divss	20(%rsp), %xmm2
	movss	12(%rsp), %xmm3
	movaps	%xmm3, %xmm4
	divss	24(%rsp), %xmm4
	movss	%xmm4, 20(%rsp)
	movss	16(%rsp), %xmm4
	divss	28(%rsp), %xmm4
	movss	%xmm4, 24(%rsp)
	movaps	%xmm5, %xmm4
	divss	32(%rsp), %xmm4
	movss	%xmm4, 28(%rsp)
	movaps	%xmm6, %xmm4
	divss	36(%rsp), %xmm4
	movss	%xmm4, 32(%rsp)
	movaps	%xmm7, %xmm4
	divss	40(%rsp), %xmm4
	movss	%xmm4, 36(%rsp)
	movaps	%xmm8, %xmm4
	divss	44(%rsp), %xmm4
	movss	%xmm4, 40(%rsp)
	movaps	%xmm9, %xmm4
	divss	48(%rsp), %xmm4
	movss	%xmm4, 44(%rsp)
	movaps	%xmm10, %xmm4
	divss	52(%rsp), %xmm4
	movss	%xmm4, 48(%rsp)
	movaps	%xmm11, %xmm4
	divss	%xmm13, %xmm4
	movaps	%xmm12, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm0, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm1, %xmm15
	divss	%xmm2, %xmm15
	movaps	%xmm3, %xmm2
	divss	20(%rsp), %xmm2
	movaps	%xmm2, %xmm3
	movss	16(%rsp), %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm5, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm6, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm7, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm8, %xmm2
	divss	40(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movaps	%xmm9, %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 40(%rsp)
	movaps	%xmm10, %xmm2
	divss	48(%rsp), %xmm2
	movss	%xmm2, 44(%rsp)
	movaps	%xmm11, %xmm2
	divss	%xmm4, %xmm2
	movss	%xmm2, 48(%rsp)
	movaps	%xmm12, %xmm4
	divss	%xmm13, %xmm4
	movss	%xmm4, 52(%rsp)
	movaps	%xmm0, %xmm13
	divss	%xmm14, %xmm13
	movss	%xmm13, 56(%rsp)
	movaps	%xmm1, %xmm14
	divss	%xmm15, %xmm14
	movss	12(%rsp), %xmm15
	divss	%xmm3, %xmm15
	movss	16(%rsp), %xmm4
	divss	20(%rsp), %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm5, %xmm13
	divss	24(%rsp), %xmm13
	movaps	%xmm13, %xmm3
	movaps	%xmm6, %xmm13
	divss	28(%rsp), %xmm13
	movss	%xmm13, 20(%rsp)
	movaps	%xmm7, %xmm13
	divss	32(%rsp), %xmm13
	movss	%xmm13, 24(%rsp)
	movaps	%xmm8, %xmm13
	divss	36(%rsp), %xmm13
	movss	%xmm13, 28(%rsp)
	movaps	%xmm9, %xmm13
	divss	40(%rsp), %xmm13
	movss	%xmm13, 32(%rsp)
	movaps	%xmm10, %xmm13
	divss	44(%rsp), %xmm13
	movss	%xmm13, 36(%rsp)
	movaps	%xmm11, %xmm13
	divss	48(%rsp), %xmm13
	movss	%xmm13, 40(%rsp)
	movaps	%xmm12, %xmm13
	divss	52(%rsp), %xmm13
	movss	%xmm13, 44(%rsp)
	movaps	%xmm0, %xmm4
	divss	56(%rsp), %xmm4
	movss	%xmm4, 48(%rsp)
	movaps	%xmm1, %xmm13
	divss	%xmm14, %xmm13
	movss	12(%rsp), %xmm14
	divss	%xmm15, %xmm14
	movss	16(%rsp), %xmm15
	divss	%xmm2, %xmm15
	movaps	%xmm5, %xmm2
	divss	%xmm3, %xmm2
	movaps	%xmm2, %xmm3
	movaps	%xmm6, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm7, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm8, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm9, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm10, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movaps	%xmm11, %xmm2
	divss	40(%rsp), %xmm2
	movss	%xmm2, 40(%rsp)
	movaps	%xmm12, %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 44(%rsp)
	movaps	%xmm0, %xmm2
	divss	48(%rsp), %xmm2
	movss	%xmm2, 48(%rsp)
	movaps	%xmm1, %xmm2
	divss	%xmm13, %xmm2
	movss	%xmm2, 52(%rsp)
	movss	12(%rsp), %xmm4
	movaps	%xmm4, %xmm13
	divss	%xmm14, %xmm13
	movss	%xmm13, 56(%rsp)
	movss	16(%rsp), %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm5, %xmm15
	divss	%xmm3, %xmm15
	movaps	%xmm6, %xmm2
	divss	20(%rsp), %xmm2
	movaps	%xmm7, %xmm13
	divss	24(%rsp), %xmm13
	movaps	%xmm13, %xmm3
	movaps	%xmm8, %xmm13
	divss	28(%rsp), %xmm13
	movss	%xmm13, 20(%rsp)
	movaps	%xmm9, %xmm13
	divss	32(%rsp), %xmm13
	movss	%xmm13, 24(%rsp)
	movaps	%xmm10, %xmm13
	divss	36(%rsp), %xmm13
	movss	%xmm13, 28(%rsp)
	movaps	%xmm11, %xmm13
	divss	40(%rsp), %xmm13
	movss	%xmm13, 32(%rsp)
	movaps	%xmm12, %xmm13
	divss	44(%rsp), %xmm13
	movss	%xmm13, 36(%rsp)
	movaps	%xmm0, %xmm13
	divss	48(%rsp), %xmm13
	movss	%xmm13, 40(%rsp)
	movaps	%xmm1, %xmm13
	divss	52(%rsp), %xmm13
	movss	%xmm13, 44(%rsp)
	divss	56(%rsp), %xmm4
	movss	%xmm4, 48(%rsp)
	movss	16(%rsp), %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm5, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm6, %xmm15
	divss	%xmm2, %xmm15
	movaps	%xmm7, %xmm2
	divss	%xmm3, %xmm2
	movaps	%xmm2, %xmm3
	movaps	%xmm8, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm9, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm10, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm11, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm12, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movaps	%xmm0, %xmm2
	divss	40(%rsp), %xmm2
	movss	%xmm2, 40(%rsp)
	movaps	%xmm1, %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 44(%rsp)
	movss	12(%rsp), %xmm2
	divss	48(%rsp), %xmm2
	movss	%xmm2, 48(%rsp)
	movss	16(%rsp), %xmm4
	divss	%xmm13, %xmm4
	movaps	%xmm5, %xmm13
	divss	%xmm14, %xmm13
	movss	%xmm13, 52(%rsp)
	movaps	%xmm6, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm7, %xmm15
	divss	%xmm3, %xmm15
	movaps	%xmm8, %xmm2
	divss	20(%rsp), %xmm2
	movaps	%xmm9, %xmm13
	divss	24(%rsp), %xmm13
	movaps	%xmm13, %xmm3
	movaps	%xmm10, %xmm13
	divss	28(%rsp), %xmm13
	movss	%xmm13, 20(%rsp)
	movaps	%xmm11, %xmm13
	divss	32(%rsp), %xmm13
	movss	%xmm13, 24(%rsp)
	movaps	%xmm12, %xmm13
	divss	36(%rsp), %xmm13
	movss	%xmm13, 28(%rsp)
	movaps	%xmm0, %xmm13
	divss	40(%rsp), %xmm13
	movss	%xmm13, 32(%rsp)
	movaps	%xmm1, %xmm13
	divss	44(%rsp), %xmm13
	movss	%xmm13, 36(%rsp)
	movss	12(%rsp), %xmm13
	divss	48(%rsp), %xmm13
	movss	%xmm13, 40(%rsp)
	movss	16(%rsp), %xmm13
	divss	%xmm4, %xmm13
	movss	%xmm13, 44(%rsp)
	movaps	%xmm5, %xmm4
	divss	52(%rsp), %xmm4
	movss	%xmm4, 48(%rsp)
	movaps	%xmm6, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm7, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm8, %xmm15
	divss	%xmm2, %xmm15
	movaps	%xmm9, %xmm2
	divss	%xmm3, %xmm2
	movaps	%xmm2, %xmm3
	movaps	%xmm10, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm11, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm12, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm0, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm1, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movss	12(%rsp), %xmm2
	divss	40(%rsp), %xmm2
	movss	%xmm2, 40(%rsp)
	movss	16(%rsp), %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 44(%rsp)
	movaps	%xmm5, %xmm2
	divss	48(%rsp), %xmm2
	movss	%xmm2, 48(%rsp)
	movaps	%xmm6, %xmm4
	divss	%xmm13, %xmm4
	movss	%xmm4, 52(%rsp)
	movaps	%xmm7, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm8, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm9, %xmm15
	divss	%xmm3, %xmm15
	movaps	%xmm10, %xmm4
	divss	20(%rsp), %xmm4
	movss	%xmm4, 56(%rsp)
	movaps	%xmm11, %xmm4
	divss	24(%rsp), %xmm4
	movss	%xmm4, 60(%rsp)
	movaps	%xmm12, %xmm4
	divss	28(%rsp), %xmm4
	movaps	%xmm0, %xmm2
	divss	32(%rsp), %xmm2
	movaps	%xmm1, %xmm3
	divss	36(%rsp), %xmm3
	movss	%xmm3, 20(%rsp)
	movss	12(%rsp), %xmm3
	divss	40(%rsp), %xmm3
	movss	%xmm3, 24(%rsp)
	movss	16(%rsp), %xmm3
	divss	44(%rsp), %xmm3
	movss	%xmm3, 28(%rsp)
	movaps	%xmm5, %xmm3
	divss	48(%rsp), %xmm3
	movss	%xmm3, 32(%rsp)
	movaps	%xmm6, %xmm3
	divss	52(%rsp), %xmm3
	movss	%xmm3, 36(%rsp)
	movaps	%xmm7, %xmm3
	divss	%xmm13, %xmm3
	movss	%xmm3, 40(%rsp)
	movaps	%xmm8, %xmm3
	divss	%xmm14, %xmm3
	movss	%xmm3, 44(%rsp)
	movaps	%xmm9, %xmm3
	divss	%xmm15, %xmm3
	movss	%xmm3, 48(%rsp)
	movaps	%xmm10, %xmm3
	divss	56(%rsp), %xmm3
	movss	%xmm3, 52(%rsp)
	movaps	%xmm11, %xmm13
	divss	60(%rsp), %xmm13
	movaps	%xmm12, %xmm14
	divss	%xmm4, %xmm14
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1032
	movss	%xmm13, 12(%rsp)
	movss	%xmm14, 16(%rsp)
.L1031:
	cvttss2sil	%xmm2, %edi
	call	use_int@PLT
	cvttss2sil	68(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	76(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	84(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	92(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	100(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	108(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	64(%rsp), %edi
	call	use_int@PLT
	addq	$120, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE276:
	.size	float_div_11, .-float_div_11
	.globl	float_div_12
	.type	float_div_12, @function
float_div_12:
.LFB277:
	.cfi_startproc
	endbr64
	subq	$136, %rsp
	.cfi_def_cfa_offset 144
	movq	%rsi, %rax
	pxor	%xmm2, %xmm2
	cvtsd2ss	80(%rsi), %xmm2
	movss	.LC5(%rip), %xmm1
	mulss	%xmm1, %xmm2
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movss	.LC6(%rip), %xmm0
	movaps	%xmm3, %xmm5
	mulss	%xmm0, %xmm5
	movss	%xmm5, 84(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 20(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2ssl	16(%rax), %xmm3
	movaps	%xmm3, %xmm6
	mulss	%xmm0, %xmm6
	movss	%xmm6, 88(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	96(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 24(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movaps	%xmm3, %xmm7
	mulss	%xmm0, %xmm7
	movss	%xmm7, 92(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	104(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 28(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm0, %xmm4
	movss	%xmm4, 96(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	112(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 32(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm8
	movss	%xmm3, 100(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	120(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 36(%rsp)
	movl	32(%rax), %esi
	leal	-4(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm9
	movss	%xmm3, 104(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	128(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 40(%rsp)
	movl	36(%rax), %esi
	leal	-5(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm10
	movss	%xmm3, 108(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	136(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 44(%rsp)
	movl	40(%rax), %esi
	leal	-6(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm11
	movss	%xmm3, 112(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	144(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 48(%rsp)
	movl	44(%rax), %esi
	leal	-7(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm12
	movss	%xmm3, 116(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	152(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 52(%rsp)
	movl	48(%rax), %esi
	leal	-8(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm13
	movss	%xmm3, 120(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	160(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 56(%rsp)
	movl	52(%rax), %esi
	leal	-9(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm15
	movss	%xmm3, 124(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	168(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm14
	movss	%xmm3, 12(%rsp)
	movl	56(%rax), %ecx
	subl	$10, %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movss	%xmm3, 76(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	176(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 16(%rsp)
	movl	60(%rax), %eax
	subl	$11, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	mulss	%xmm0, %xmm1
	movss	%xmm1, 80(%rsp)
	testq	%rdi, %rdi
	je	.L1036
	leaq	-1(%rdi), %rax
	movaps	%xmm5, %xmm0
	movaps	%xmm6, %xmm1
	movss	%xmm7, 16(%rsp)
	movss	%xmm4, 12(%rsp)
	movaps	%xmm8, %xmm5
	movaps	%xmm9, %xmm6
	movaps	%xmm10, %xmm7
	movaps	%xmm11, %xmm8
	movaps	%xmm12, %xmm9
	movaps	%xmm13, %xmm10
	movaps	%xmm15, %xmm11
	movss	76(%rsp), %xmm12
	movss	80(%rsp), %xmm13
	movaps	%xmm3, %xmm15
.L1037:
	movaps	%xmm0, %xmm3
	divss	%xmm2, %xmm3
	movss	%xmm3, 60(%rsp)
	movaps	%xmm1, %xmm2
	divss	20(%rsp), %xmm2
	movss	16(%rsp), %xmm4
	movaps	%xmm4, %xmm3
	divss	24(%rsp), %xmm3
	movss	%xmm3, 16(%rsp)
	movss	12(%rsp), %xmm3
	divss	28(%rsp), %xmm3
	movss	%xmm3, 20(%rsp)
	movaps	%xmm5, %xmm3
	divss	32(%rsp), %xmm3
	movss	%xmm3, 24(%rsp)
	movaps	%xmm6, %xmm3
	divss	36(%rsp), %xmm3
	movss	%xmm3, 28(%rsp)
	movaps	%xmm7, %xmm3
	divss	40(%rsp), %xmm3
	movss	%xmm3, 32(%rsp)
	movaps	%xmm8, %xmm3
	divss	44(%rsp), %xmm3
	movss	%xmm3, 36(%rsp)
	movaps	%xmm9, %xmm3
	divss	48(%rsp), %xmm3
	movss	%xmm3, 40(%rsp)
	movaps	%xmm10, %xmm3
	divss	52(%rsp), %xmm3
	movss	%xmm3, 44(%rsp)
	movaps	%xmm11, %xmm3
	divss	56(%rsp), %xmm3
	movss	%xmm3, 48(%rsp)
	movaps	%xmm12, %xmm3
	divss	%xmm14, %xmm3
	movss	%xmm3, 52(%rsp)
	movaps	%xmm13, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm0, %xmm15
	divss	60(%rsp), %xmm15
	movaps	%xmm1, %xmm3
	divss	%xmm2, %xmm3
	movss	%xmm3, 56(%rsp)
	movaps	%xmm4, %xmm3
	movaps	%xmm4, %xmm2
	divss	16(%rsp), %xmm2
	movaps	%xmm2, %xmm4
	movss	12(%rsp), %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm5, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm6, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm7, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm8, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movaps	%xmm9, %xmm2
	divss	40(%rsp), %xmm2
	movss	%xmm2, 40(%rsp)
	movaps	%xmm10, %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 44(%rsp)
	movaps	%xmm11, %xmm2
	divss	48(%rsp), %xmm2
	movss	%xmm2, 48(%rsp)
	movaps	%xmm12, %xmm2
	divss	52(%rsp), %xmm2
	movss	%xmm2, 52(%rsp)
	movaps	%xmm13, %xmm2
	divss	%xmm14, %xmm2
	movss	%xmm2, 60(%rsp)
	movaps	%xmm0, %xmm14
	divss	%xmm15, %xmm14
	movss	%xmm14, 64(%rsp)
	movaps	%xmm1, %xmm15
	divss	56(%rsp), %xmm15
	movss	%xmm3, 16(%rsp)
	divss	%xmm4, %xmm3
	movss	%xmm3, 56(%rsp)
	movss	12(%rsp), %xmm3
	movaps	%xmm3, %xmm2
	divss	20(%rsp), %xmm2
	movaps	%xmm5, %xmm14
	divss	24(%rsp), %xmm14
	movaps	%xmm14, %xmm4
	movaps	%xmm6, %xmm14
	divss	28(%rsp), %xmm14
	movss	%xmm14, 20(%rsp)
	movaps	%xmm7, %xmm14
	divss	32(%rsp), %xmm14
	movss	%xmm14, 24(%rsp)
	movaps	%xmm8, %xmm14
	divss	36(%rsp), %xmm14
	movss	%xmm14, 28(%rsp)
	movaps	%xmm9, %xmm14
	divss	40(%rsp), %xmm14
	movss	%xmm14, 32(%rsp)
	movaps	%xmm10, %xmm14
	divss	44(%rsp), %xmm14
	movss	%xmm14, 36(%rsp)
	movaps	%xmm11, %xmm14
	divss	48(%rsp), %xmm14
	movss	%xmm14, 40(%rsp)
	movaps	%xmm12, %xmm14
	divss	52(%rsp), %xmm14
	movss	%xmm14, 44(%rsp)
	movaps	%xmm13, %xmm14
	divss	60(%rsp), %xmm14
	movss	%xmm14, 48(%rsp)
	movaps	%xmm0, %xmm14
	divss	64(%rsp), %xmm14
	movss	%xmm14, 52(%rsp)
	movaps	%xmm1, %xmm14
	divss	%xmm15, %xmm14
	movss	16(%rsp), %xmm15
	divss	56(%rsp), %xmm15
	divss	%xmm2, %xmm3
	movss	%xmm3, 56(%rsp)
	movaps	%xmm5, %xmm2
	divss	%xmm4, %xmm2
	movaps	%xmm2, %xmm4
	movaps	%xmm6, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm7, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm8, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm9, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm10, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movaps	%xmm11, %xmm2
	divss	40(%rsp), %xmm2
	movss	%xmm2, 40(%rsp)
	movaps	%xmm12, %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 44(%rsp)
	movaps	%xmm13, %xmm2
	divss	48(%rsp), %xmm2
	movss	%xmm2, 48(%rsp)
	movaps	%xmm0, %xmm2
	divss	52(%rsp), %xmm2
	movss	%xmm2, 52(%rsp)
	movaps	%xmm1, %xmm2
	divss	%xmm14, %xmm2
	movss	%xmm2, 60(%rsp)
	movss	16(%rsp), %xmm3
	movaps	%xmm3, %xmm14
	divss	%xmm15, %xmm14
	movss	%xmm14, 64(%rsp)
	movss	12(%rsp), %xmm15
	divss	56(%rsp), %xmm15
	movaps	%xmm5, %xmm2
	divss	%xmm4, %xmm2
	movss	%xmm2, 56(%rsp)
	movaps	%xmm6, %xmm2
	divss	20(%rsp), %xmm2
	movaps	%xmm7, %xmm14
	divss	24(%rsp), %xmm14
	movaps	%xmm14, %xmm4
	movaps	%xmm8, %xmm14
	divss	28(%rsp), %xmm14
	movss	%xmm14, 20(%rsp)
	movaps	%xmm9, %xmm14
	divss	32(%rsp), %xmm14
	movss	%xmm14, 24(%rsp)
	movaps	%xmm10, %xmm14
	divss	36(%rsp), %xmm14
	movss	%xmm14, 28(%rsp)
	movaps	%xmm11, %xmm14
	divss	40(%rsp), %xmm14
	movss	%xmm14, 32(%rsp)
	movaps	%xmm12, %xmm14
	divss	44(%rsp), %xmm14
	movss	%xmm14, 36(%rsp)
	movaps	%xmm13, %xmm14
	divss	48(%rsp), %xmm14
	movss	%xmm14, 40(%rsp)
	movaps	%xmm0, %xmm14
	divss	52(%rsp), %xmm14
	movss	%xmm14, 44(%rsp)
	movaps	%xmm1, %xmm14
	divss	60(%rsp), %xmm14
	movss	%xmm14, 48(%rsp)
	divss	64(%rsp), %xmm3
	movss	%xmm3, 52(%rsp)
	movss	12(%rsp), %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm5, %xmm15
	divss	56(%rsp), %xmm15
	movaps	%xmm6, %xmm3
	divss	%xmm2, %xmm3
	movss	%xmm3, 56(%rsp)
	movaps	%xmm7, %xmm2
	divss	%xmm4, %xmm2
	movaps	%xmm2, %xmm4
	movaps	%xmm8, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm9, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm10, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm11, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm12, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movaps	%xmm13, %xmm2
	divss	40(%rsp), %xmm2
	movss	%xmm2, 40(%rsp)
	movaps	%xmm0, %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 44(%rsp)
	movaps	%xmm1, %xmm2
	divss	48(%rsp), %xmm2
	movss	%xmm2, 48(%rsp)
	movss	16(%rsp), %xmm2
	divss	52(%rsp), %xmm2
	movss	%xmm2, 52(%rsp)
	movss	12(%rsp), %xmm3
	movaps	%xmm3, %xmm2
	divss	%xmm14, %xmm2
	movss	%xmm2, 60(%rsp)
	movaps	%xmm5, %xmm14
	divss	%xmm15, %xmm14
	movss	%xmm14, 64(%rsp)
	movaps	%xmm6, %xmm15
	divss	56(%rsp), %xmm15
	movaps	%xmm7, %xmm2
	divss	%xmm4, %xmm2
	movss	%xmm2, 56(%rsp)
	movaps	%xmm8, %xmm2
	divss	20(%rsp), %xmm2
	movaps	%xmm9, %xmm14
	divss	24(%rsp), %xmm14
	movaps	%xmm14, %xmm4
	movaps	%xmm10, %xmm14
	divss	28(%rsp), %xmm14
	movss	%xmm14, 20(%rsp)
	movaps	%xmm11, %xmm14
	divss	32(%rsp), %xmm14
	movss	%xmm14, 24(%rsp)
	movaps	%xmm12, %xmm14
	divss	36(%rsp), %xmm14
	movss	%xmm14, 28(%rsp)
	movaps	%xmm13, %xmm14
	divss	40(%rsp), %xmm14
	movss	%xmm14, 32(%rsp)
	movaps	%xmm0, %xmm14
	divss	44(%rsp), %xmm14
	movss	%xmm14, 36(%rsp)
	movaps	%xmm1, %xmm14
	divss	48(%rsp), %xmm14
	movss	%xmm14, 40(%rsp)
	movss	16(%rsp), %xmm14
	divss	52(%rsp), %xmm14
	movss	%xmm14, 44(%rsp)
	movaps	%xmm3, %xmm14
	divss	60(%rsp), %xmm14
	movss	%xmm14, 48(%rsp)
	movaps	%xmm5, %xmm14
	divss	64(%rsp), %xmm14
	movss	%xmm14, 52(%rsp)
	movaps	%xmm6, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm7, %xmm15
	divss	56(%rsp), %xmm15
	movaps	%xmm8, %xmm3
	divss	%xmm2, %xmm3
	movss	%xmm3, 56(%rsp)
	movaps	%xmm9, %xmm2
	divss	%xmm4, %xmm2
	movaps	%xmm2, %xmm4
	movaps	%xmm10, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm11, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm12, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm13, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm0, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movaps	%xmm1, %xmm2
	divss	40(%rsp), %xmm2
	movss	%xmm2, 40(%rsp)
	movss	16(%rsp), %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 44(%rsp)
	movss	12(%rsp), %xmm2
	divss	48(%rsp), %xmm2
	movss	%xmm2, 48(%rsp)
	movaps	%xmm5, %xmm2
	divss	52(%rsp), %xmm2
	movss	%xmm2, 52(%rsp)
	movaps	%xmm6, %xmm3
	divss	%xmm14, %xmm3
	movss	%xmm3, 60(%rsp)
	movaps	%xmm7, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm8, %xmm15
	divss	56(%rsp), %xmm15
	movaps	%xmm9, %xmm3
	divss	%xmm4, %xmm3
	movss	%xmm3, 56(%rsp)
	movaps	%xmm10, %xmm3
	divss	20(%rsp), %xmm3
	movss	%xmm3, 64(%rsp)
	movaps	%xmm11, %xmm3
	divss	24(%rsp), %xmm3
	movss	%xmm3, 68(%rsp)
	movaps	%xmm12, %xmm3
	divss	28(%rsp), %xmm3
	movss	%xmm3, 72(%rsp)
	movaps	%xmm13, %xmm3
	divss	32(%rsp), %xmm3
	movaps	%xmm0, %xmm2
	divss	36(%rsp), %xmm2
	movaps	%xmm1, %xmm4
	divss	40(%rsp), %xmm4
	movss	%xmm4, 20(%rsp)
	movss	16(%rsp), %xmm4
	divss	44(%rsp), %xmm4
	movss	%xmm4, 24(%rsp)
	movss	12(%rsp), %xmm4
	divss	48(%rsp), %xmm4
	movss	%xmm4, 28(%rsp)
	movaps	%xmm5, %xmm4
	divss	52(%rsp), %xmm4
	movss	%xmm4, 32(%rsp)
	movaps	%xmm6, %xmm4
	divss	60(%rsp), %xmm4
	movss	%xmm4, 36(%rsp)
	movaps	%xmm7, %xmm4
	divss	%xmm14, %xmm4
	movss	%xmm4, 40(%rsp)
	movaps	%xmm8, %xmm4
	divss	%xmm15, %xmm4
	movss	%xmm4, 44(%rsp)
	movaps	%xmm9, %xmm4
	divss	56(%rsp), %xmm4
	movss	%xmm4, 48(%rsp)
	movaps	%xmm10, %xmm4
	divss	64(%rsp), %xmm4
	movss	%xmm4, 52(%rsp)
	movaps	%xmm11, %xmm4
	divss	68(%rsp), %xmm4
	movss	%xmm4, 56(%rsp)
	movaps	%xmm12, %xmm14
	divss	72(%rsp), %xmm14
	movaps	%xmm13, %xmm15
	divss	%xmm3, %xmm15
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1037
	movss	%xmm14, 12(%rsp)
	movss	%xmm15, 16(%rsp)
.L1036:
	cvttss2sil	%xmm2, %edi
	call	use_int@PLT
	cvttss2sil	84(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	92(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	100(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	108(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	116(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	124(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	76(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	80(%rsp), %edi
	call	use_int@PLT
	addq	$136, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE277:
	.size	float_div_12, .-float_div_12
	.globl	float_div_13
	.type	float_div_13, @function
float_div_13:
.LFB278:
	.cfi_startproc
	endbr64
	subq	$152, %rsp
	.cfi_def_cfa_offset 160
	movq	%rsi, %rax
	pxor	%xmm2, %xmm2
	cvtsd2ss	80(%rsi), %xmm2
	movss	.LC5(%rip), %xmm1
	mulss	%xmm1, %xmm2
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movss	.LC6(%rip), %xmm0
	mulss	%xmm0, %xmm3
	movss	%xmm3, 16(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	88(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 20(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2ssl	16(%rax), %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm0, %xmm4
	movss	%xmm4, 96(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	96(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 24(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movaps	%xmm3, %xmm6
	mulss	%xmm0, %xmm6
	movss	%xmm6, 100(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	104(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 28(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	movaps	%xmm3, %xmm7
	mulss	%xmm0, %xmm7
	movss	%xmm7, 104(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	112(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 32(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm5
	movss	%xmm3, 108(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	120(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 36(%rsp)
	movl	32(%rax), %esi
	leal	-4(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm8
	movss	%xmm3, 112(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	128(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 40(%rsp)
	movl	36(%rax), %esi
	leal	-5(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm9
	movss	%xmm3, 116(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	136(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 44(%rsp)
	movl	40(%rax), %esi
	leal	-6(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm10
	movss	%xmm3, 120(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	144(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 48(%rsp)
	movl	44(%rax), %esi
	leal	-7(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm11
	movss	%xmm3, 124(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	152(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 52(%rsp)
	movl	48(%rax), %esi
	leal	-8(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm12
	movss	%xmm3, 128(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	160(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 56(%rsp)
	movl	52(%rax), %esi
	leal	-9(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm13
	movss	%xmm3, 132(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	168(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 60(%rsp)
	movl	56(%rax), %esi
	leal	-10(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm15
	movss	%xmm3, 136(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	176(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm14
	movss	%xmm3, 8(%rsp)
	movl	60(%rax), %ecx
	subl	$11, %ecx
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%ecx, %xmm3
	mulss	%xmm0, %xmm3
	movss	%xmm3, 92(%rsp)
	pxor	%xmm3, %xmm3
	cvtsd2ss	184(%rax), %xmm3
	mulss	%xmm1, %xmm3
	movss	%xmm3, 64(%rsp)
	movl	64(%rax), %eax
	subl	$12, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	mulss	%xmm0, %xmm1
	movss	%xmm1, 140(%rsp)
	testq	%rdi, %rdi
	je	.L1041
	movaps	%xmm1, %xmm0
	leaq	-1(%rdi), %rax
	movss	%xmm4, 8(%rsp)
	movaps	%xmm6, %xmm1
	movaps	%xmm7, %xmm3
	movaps	%xmm5, %xmm4
	movss	%xmm8, 12(%rsp)
	movaps	%xmm9, %xmm6
	movaps	%xmm10, %xmm7
	movaps	%xmm11, %xmm8
	movaps	%xmm12, %xmm9
	movaps	%xmm13, %xmm10
	movaps	%xmm15, %xmm11
	movss	92(%rsp), %xmm12
	movaps	%xmm0, %xmm13
	movss	64(%rsp), %xmm15
.L1042:
	movss	16(%rsp), %xmm5
	movaps	%xmm5, %xmm0
	divss	%xmm2, %xmm0
	movss	%xmm0, 16(%rsp)
	movss	8(%rsp), %xmm2
	divss	20(%rsp), %xmm2
	movaps	%xmm1, %xmm0
	divss	24(%rsp), %xmm0
	movss	%xmm0, 20(%rsp)
	movaps	%xmm3, %xmm0
	divss	28(%rsp), %xmm0
	movss	%xmm0, 24(%rsp)
	movaps	%xmm4, %xmm0
	divss	32(%rsp), %xmm0
	movss	%xmm0, 28(%rsp)
	movss	12(%rsp), %xmm0
	divss	36(%rsp), %xmm0
	movss	%xmm0, 32(%rsp)
	movaps	%xmm6, %xmm0
	divss	40(%rsp), %xmm0
	movss	%xmm0, 36(%rsp)
	movaps	%xmm7, %xmm0
	divss	44(%rsp), %xmm0
	movss	%xmm0, 40(%rsp)
	movaps	%xmm8, %xmm0
	divss	48(%rsp), %xmm0
	movss	%xmm0, 44(%rsp)
	movaps	%xmm9, %xmm0
	divss	52(%rsp), %xmm0
	movss	%xmm0, 48(%rsp)
	movaps	%xmm10, %xmm0
	divss	56(%rsp), %xmm0
	movss	%xmm0, 52(%rsp)
	movaps	%xmm11, %xmm0
	divss	60(%rsp), %xmm0
	movss	%xmm0, 56(%rsp)
	movaps	%xmm12, %xmm0
	divss	%xmm14, %xmm0
	movss	%xmm0, 60(%rsp)
	movaps	%xmm13, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm5, %xmm15
	divss	16(%rsp), %xmm15
	movss	8(%rsp), %xmm0
	divss	%xmm2, %xmm0
	movss	%xmm0, 64(%rsp)
	movaps	%xmm1, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm3, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm4, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movss	12(%rsp), %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm6, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movaps	%xmm7, %xmm2
	divss	40(%rsp), %xmm2
	movss	%xmm2, 40(%rsp)
	movaps	%xmm8, %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 44(%rsp)
	movaps	%xmm9, %xmm2
	divss	48(%rsp), %xmm2
	movss	%xmm2, 48(%rsp)
	movaps	%xmm10, %xmm2
	divss	52(%rsp), %xmm2
	movss	%xmm2, 52(%rsp)
	movaps	%xmm11, %xmm2
	divss	56(%rsp), %xmm2
	movss	%xmm2, 56(%rsp)
	movaps	%xmm12, %xmm2
	divss	60(%rsp), %xmm2
	movss	%xmm2, 60(%rsp)
	movaps	%xmm13, %xmm0
	divss	%xmm14, %xmm0
	movss	%xmm0, 68(%rsp)
	movss	%xmm5, 16(%rsp)
	movaps	%xmm5, %xmm14
	divss	%xmm15, %xmm14
	movss	8(%rsp), %xmm2
	divss	64(%rsp), %xmm2
	movaps	%xmm2, %xmm15
	movaps	%xmm1, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm3, %xmm0
	divss	24(%rsp), %xmm0
	movaps	%xmm0, %xmm2
	movaps	%xmm4, %xmm5
	divss	28(%rsp), %xmm5
	movss	%xmm5, 24(%rsp)
	movss	12(%rsp), %xmm5
	divss	32(%rsp), %xmm5
	movss	%xmm5, 28(%rsp)
	movaps	%xmm6, %xmm5
	divss	36(%rsp), %xmm5
	movss	%xmm5, 32(%rsp)
	movaps	%xmm7, %xmm5
	divss	40(%rsp), %xmm5
	movss	%xmm5, 36(%rsp)
	movaps	%xmm8, %xmm5
	divss	44(%rsp), %xmm5
	movss	%xmm5, 40(%rsp)
	movaps	%xmm9, %xmm5
	divss	48(%rsp), %xmm5
	movss	%xmm5, 44(%rsp)
	movaps	%xmm10, %xmm5
	divss	52(%rsp), %xmm5
	movss	%xmm5, 48(%rsp)
	movaps	%xmm11, %xmm5
	divss	56(%rsp), %xmm5
	movss	%xmm5, 52(%rsp)
	movaps	%xmm12, %xmm5
	divss	60(%rsp), %xmm5
	movss	%xmm5, 56(%rsp)
	movaps	%xmm13, %xmm5
	divss	68(%rsp), %xmm5
	movss	%xmm5, 60(%rsp)
	movss	16(%rsp), %xmm0
	divss	%xmm14, %xmm0
	movss	%xmm0, 64(%rsp)
	movss	8(%rsp), %xmm5
	movaps	%xmm5, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm1, %xmm0
	divss	20(%rsp), %xmm0
	movaps	%xmm0, %xmm15
	movaps	%xmm3, %xmm0
	divss	%xmm2, %xmm0
	movss	%xmm0, 20(%rsp)
	movaps	%xmm4, %xmm2
	divss	24(%rsp), %xmm2
	movss	12(%rsp), %xmm0
	divss	28(%rsp), %xmm0
	movss	%xmm0, 24(%rsp)
	movaps	%xmm6, %xmm0
	divss	32(%rsp), %xmm0
	movss	%xmm0, 28(%rsp)
	movaps	%xmm7, %xmm0
	divss	36(%rsp), %xmm0
	movss	%xmm0, 32(%rsp)
	movaps	%xmm8, %xmm0
	divss	40(%rsp), %xmm0
	movss	%xmm0, 36(%rsp)
	movaps	%xmm9, %xmm0
	divss	44(%rsp), %xmm0
	movss	%xmm0, 40(%rsp)
	movaps	%xmm10, %xmm0
	divss	48(%rsp), %xmm0
	movss	%xmm0, 44(%rsp)
	movaps	%xmm11, %xmm0
	divss	52(%rsp), %xmm0
	movss	%xmm0, 48(%rsp)
	movaps	%xmm12, %xmm0
	divss	56(%rsp), %xmm0
	movss	%xmm0, 52(%rsp)
	movaps	%xmm13, %xmm0
	divss	60(%rsp), %xmm0
	movss	%xmm0, 56(%rsp)
	movss	16(%rsp), %xmm0
	divss	64(%rsp), %xmm0
	movss	%xmm0, 60(%rsp)
	divss	%xmm14, %xmm5
	movss	%xmm5, 64(%rsp)
	movaps	%xmm1, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm3, %xmm15
	divss	20(%rsp), %xmm15
	movaps	%xmm4, %xmm5
	divss	%xmm2, %xmm5
	movss	%xmm5, 20(%rsp)
	movss	12(%rsp), %xmm2
	divss	24(%rsp), %xmm2
	movaps	%xmm6, %xmm0
	divss	28(%rsp), %xmm0
	movss	%xmm0, 24(%rsp)
	movaps	%xmm7, %xmm0
	divss	32(%rsp), %xmm0
	movss	%xmm0, 28(%rsp)
	movaps	%xmm8, %xmm0
	divss	36(%rsp), %xmm0
	movss	%xmm0, 32(%rsp)
	movaps	%xmm9, %xmm0
	divss	40(%rsp), %xmm0
	movss	%xmm0, 36(%rsp)
	movaps	%xmm10, %xmm0
	divss	44(%rsp), %xmm0
	movss	%xmm0, 40(%rsp)
	movaps	%xmm11, %xmm0
	divss	48(%rsp), %xmm0
	movss	%xmm0, 44(%rsp)
	movaps	%xmm12, %xmm0
	divss	52(%rsp), %xmm0
	movss	%xmm0, 48(%rsp)
	movaps	%xmm13, %xmm0
	divss	56(%rsp), %xmm0
	movss	%xmm0, 52(%rsp)
	movss	16(%rsp), %xmm0
	divss	60(%rsp), %xmm0
	movss	%xmm0, 56(%rsp)
	movss	8(%rsp), %xmm0
	divss	64(%rsp), %xmm0
	movss	%xmm0, 60(%rsp)
	movaps	%xmm1, %xmm5
	divss	%xmm14, %xmm5
	movss	%xmm5, 64(%rsp)
	movaps	%xmm3, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm4, %xmm15
	divss	20(%rsp), %xmm15
	movss	12(%rsp), %xmm5
	divss	%xmm2, %xmm5
	movss	%xmm5, 20(%rsp)
	movaps	%xmm6, %xmm2
	divss	24(%rsp), %xmm2
	movaps	%xmm7, %xmm0
	divss	28(%rsp), %xmm0
	movss	%xmm0, 24(%rsp)
	movaps	%xmm8, %xmm0
	divss	32(%rsp), %xmm0
	movss	%xmm0, 28(%rsp)
	movaps	%xmm9, %xmm0
	divss	36(%rsp), %xmm0
	movss	%xmm0, 32(%rsp)
	movaps	%xmm10, %xmm0
	divss	40(%rsp), %xmm0
	movss	%xmm0, 36(%rsp)
	movaps	%xmm11, %xmm0
	divss	44(%rsp), %xmm0
	movss	%xmm0, 40(%rsp)
	movaps	%xmm12, %xmm0
	divss	48(%rsp), %xmm0
	movss	%xmm0, 44(%rsp)
	movaps	%xmm13, %xmm0
	divss	52(%rsp), %xmm0
	movss	%xmm0, 48(%rsp)
	movss	16(%rsp), %xmm0
	divss	56(%rsp), %xmm0
	movss	%xmm0, 52(%rsp)
	movss	8(%rsp), %xmm0
	divss	60(%rsp), %xmm0
	movss	%xmm0, 56(%rsp)
	movaps	%xmm1, %xmm0
	divss	64(%rsp), %xmm0
	movss	%xmm0, 60(%rsp)
	movaps	%xmm3, %xmm5
	divss	%xmm14, %xmm5
	movss	%xmm5, 64(%rsp)
	movaps	%xmm4, %xmm14
	divss	%xmm15, %xmm14
	movss	12(%rsp), %xmm15
	divss	20(%rsp), %xmm15
	movaps	%xmm6, %xmm5
	divss	%xmm2, %xmm5
	movss	%xmm5, 20(%rsp)
	movaps	%xmm7, %xmm2
	divss	24(%rsp), %xmm2
	movaps	%xmm8, %xmm0
	divss	28(%rsp), %xmm0
	movss	%xmm0, 24(%rsp)
	movaps	%xmm9, %xmm0
	divss	32(%rsp), %xmm0
	movss	%xmm0, 28(%rsp)
	movaps	%xmm10, %xmm0
	divss	36(%rsp), %xmm0
	movss	%xmm0, 32(%rsp)
	movaps	%xmm11, %xmm0
	divss	40(%rsp), %xmm0
	movss	%xmm0, 36(%rsp)
	movaps	%xmm12, %xmm0
	divss	44(%rsp), %xmm0
	movss	%xmm0, 40(%rsp)
	movaps	%xmm13, %xmm0
	divss	48(%rsp), %xmm0
	movss	%xmm0, 44(%rsp)
	movss	16(%rsp), %xmm0
	divss	52(%rsp), %xmm0
	movss	%xmm0, 48(%rsp)
	movss	8(%rsp), %xmm0
	divss	56(%rsp), %xmm0
	movss	%xmm0, 52(%rsp)
	movaps	%xmm1, %xmm0
	divss	60(%rsp), %xmm0
	movss	%xmm0, 56(%rsp)
	movaps	%xmm3, %xmm0
	divss	64(%rsp), %xmm0
	movss	%xmm0, 60(%rsp)
	movaps	%xmm4, %xmm5
	divss	%xmm14, %xmm5
	movss	%xmm5, 64(%rsp)
	movss	12(%rsp), %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm6, %xmm15
	divss	20(%rsp), %xmm15
	movaps	%xmm7, %xmm5
	divss	%xmm2, %xmm5
	movss	%xmm5, 20(%rsp)
	movaps	%xmm8, %xmm2
	divss	24(%rsp), %xmm2
	movaps	%xmm9, %xmm0
	divss	28(%rsp), %xmm0
	movss	%xmm0, 24(%rsp)
	movaps	%xmm10, %xmm0
	divss	32(%rsp), %xmm0
	movss	%xmm0, 28(%rsp)
	movaps	%xmm11, %xmm0
	divss	36(%rsp), %xmm0
	movss	%xmm0, 32(%rsp)
	movaps	%xmm12, %xmm0
	divss	40(%rsp), %xmm0
	movss	%xmm0, 36(%rsp)
	movaps	%xmm13, %xmm0
	divss	44(%rsp), %xmm0
	movss	%xmm0, 40(%rsp)
	movss	16(%rsp), %xmm0
	divss	48(%rsp), %xmm0
	movss	%xmm0, 44(%rsp)
	movss	8(%rsp), %xmm0
	divss	52(%rsp), %xmm0
	movss	%xmm0, 48(%rsp)
	movaps	%xmm1, %xmm0
	divss	56(%rsp), %xmm0
	movss	%xmm0, 52(%rsp)
	movaps	%xmm3, %xmm0
	divss	60(%rsp), %xmm0
	movss	%xmm0, 56(%rsp)
	movaps	%xmm4, %xmm0
	divss	64(%rsp), %xmm0
	movss	%xmm0, 60(%rsp)
	movss	12(%rsp), %xmm5
	divss	%xmm14, %xmm5
	movss	%xmm5, 64(%rsp)
	movaps	%xmm6, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm7, %xmm15
	divss	20(%rsp), %xmm15
	movaps	%xmm8, %xmm5
	divss	%xmm2, %xmm5
	movss	%xmm5, 68(%rsp)
	movaps	%xmm9, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 72(%rsp)
	movaps	%xmm10, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 76(%rsp)
	movaps	%xmm11, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 80(%rsp)
	movaps	%xmm12, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 84(%rsp)
	movaps	%xmm13, %xmm2
	divss	40(%rsp), %xmm2
	movss	%xmm2, 88(%rsp)
	movss	16(%rsp), %xmm2
	divss	44(%rsp), %xmm2
	movss	8(%rsp), %xmm0
	divss	48(%rsp), %xmm0
	movss	%xmm0, 20(%rsp)
	movaps	%xmm1, %xmm5
	divss	52(%rsp), %xmm5
	movss	%xmm5, 24(%rsp)
	movaps	%xmm3, %xmm5
	divss	56(%rsp), %xmm5
	movss	%xmm5, 28(%rsp)
	movaps	%xmm4, %xmm5
	divss	60(%rsp), %xmm5
	movss	%xmm5, 32(%rsp)
	movss	12(%rsp), %xmm5
	divss	64(%rsp), %xmm5
	movss	%xmm5, 36(%rsp)
	movaps	%xmm6, %xmm0
	divss	%xmm14, %xmm0
	movss	%xmm0, 40(%rsp)
	movaps	%xmm7, %xmm0
	divss	%xmm15, %xmm0
	movss	%xmm0, 44(%rsp)
	movaps	%xmm8, %xmm0
	divss	68(%rsp), %xmm0
	movss	%xmm0, 48(%rsp)
	movaps	%xmm9, %xmm0
	divss	72(%rsp), %xmm0
	movss	%xmm0, 52(%rsp)
	movaps	%xmm10, %xmm0
	divss	76(%rsp), %xmm0
	movss	%xmm0, 56(%rsp)
	movaps	%xmm11, %xmm0
	divss	80(%rsp), %xmm0
	movss	%xmm0, 60(%rsp)
	movaps	%xmm12, %xmm14
	divss	84(%rsp), %xmm14
	movaps	%xmm13, %xmm15
	divss	88(%rsp), %xmm15
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1042
	movss	%xmm14, 8(%rsp)
	movss	%xmm15, 64(%rsp)
.L1041:
	cvttss2sil	%xmm2, %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	100(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	108(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	116(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	124(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	128(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	132(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	136(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	92(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	140(%rsp), %edi
	call	use_int@PLT
	addq	$152, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE278:
	.size	float_div_13, .-float_div_13
	.globl	float_div_14
	.type	float_div_14, @function
float_div_14:
.LFB279:
	.cfi_startproc
	endbr64
	subq	$152, %rsp
	.cfi_def_cfa_offset 160
	movq	%rsi, %rax
	pxor	%xmm15, %xmm15
	cvtsd2ss	80(%rsi), %xmm15
	movss	.LC5(%rip), %xmm1
	mulss	%xmm1, %xmm15
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	movss	.LC6(%rip), %xmm0
	mulss	%xmm0, %xmm2
	movss	%xmm2, (%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	88(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 16(%rsp)
	pxor	%xmm2, %xmm2
	cvtsi2ssl	16(%rax), %xmm2
	mulss	%xmm0, %xmm2
	movss	%xmm2, 4(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	96(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 20(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	movaps	%xmm2, %xmm3
	mulss	%xmm0, %xmm3
	movss	%xmm3, 100(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	104(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 24(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	movaps	%xmm2, %xmm4
	mulss	%xmm0, %xmm4
	movss	%xmm4, 104(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	112(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 28(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	movaps	%xmm2, %xmm5
	mulss	%xmm0, %xmm5
	movss	%xmm5, 108(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	120(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 32(%rsp)
	movl	32(%rax), %esi
	leal	-4(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	movaps	%xmm2, %xmm6
	mulss	%xmm0, %xmm6
	movss	%xmm6, 112(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	128(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 36(%rsp)
	movl	36(%rax), %esi
	leal	-5(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	movaps	%xmm2, %xmm7
	mulss	%xmm0, %xmm7
	movss	%xmm7, 116(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	136(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 40(%rsp)
	movl	40(%rax), %esi
	leal	-6(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm2, %xmm8
	movss	%xmm2, 120(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	144(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 44(%rsp)
	movl	44(%rax), %esi
	leal	-7(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm2, %xmm9
	movss	%xmm2, 124(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	152(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 48(%rsp)
	movl	48(%rax), %esi
	leal	-8(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm2, %xmm10
	movss	%xmm2, 128(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	160(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 52(%rsp)
	movl	52(%rax), %esi
	leal	-9(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm2, %xmm11
	movss	%xmm2, 132(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	168(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 56(%rsp)
	movl	56(%rax), %esi
	leal	-10(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm2, %xmm14
	movss	%xmm2, 136(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	176(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 60(%rsp)
	movl	60(%rax), %esi
	leal	-11(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm2, %xmm13
	movss	%xmm2, 140(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	184(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movaps	%xmm2, %xmm12
	movss	%xmm2, 8(%rsp)
	movl	64(%rax), %ecx
	subl	$12, %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	mulss	%xmm0, %xmm2
	movss	%xmm2, 92(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	192(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 12(%rsp)
	movl	68(%rax), %eax
	subl	$13, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	mulss	%xmm0, %xmm1
	movss	%xmm1, 96(%rsp)
	testq	%rdi, %rdi
	je	.L1046
	leaq	-1(%rdi), %rax
	movaps	%xmm3, %xmm0
	movaps	%xmm4, %xmm1
	movss	%xmm5, 8(%rsp)
	movaps	%xmm6, %xmm3
	movaps	%xmm7, %xmm4
	movaps	%xmm8, %xmm5
	movaps	%xmm9, %xmm6
	movaps	%xmm10, %xmm7
	movaps	%xmm11, %xmm8
	movaps	%xmm14, %xmm9
	movaps	%xmm13, %xmm10
	movss	92(%rsp), %xmm11
	movss	96(%rsp), %xmm14
	movss	%xmm14, 12(%rsp)
	movaps	%xmm12, %xmm13
	movaps	%xmm2, %xmm14
.L1047:
	movss	(%rsp), %xmm2
	divss	%xmm15, %xmm2
	movss	%xmm2, 64(%rsp)
	movss	4(%rsp), %xmm12
	movaps	%xmm12, %xmm15
	divss	16(%rsp), %xmm15
	movaps	%xmm0, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 16(%rsp)
	movaps	%xmm1, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movss	8(%rsp), %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm3, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm4, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm5, %xmm2
	divss	40(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movaps	%xmm6, %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 40(%rsp)
	movaps	%xmm7, %xmm2
	divss	48(%rsp), %xmm2
	movss	%xmm2, 44(%rsp)
	movaps	%xmm8, %xmm2
	divss	52(%rsp), %xmm2
	movss	%xmm2, 48(%rsp)
	movaps	%xmm9, %xmm2
	divss	56(%rsp), %xmm2
	movss	%xmm2, 52(%rsp)
	movaps	%xmm10, %xmm2
	divss	60(%rsp), %xmm2
	movss	%xmm2, 56(%rsp)
	movaps	%xmm11, %xmm2
	divss	%xmm13, %xmm2
	movss	%xmm2, 60(%rsp)
	movss	12(%rsp), %xmm13
	divss	%xmm14, %xmm13
	movss	%xmm13, 68(%rsp)
	movss	(%rsp), %xmm14
	divss	64(%rsp), %xmm14
	movaps	%xmm12, %xmm2
	divss	%xmm15, %xmm2
	movaps	%xmm0, %xmm12
	divss	16(%rsp), %xmm12
	movaps	%xmm12, %xmm15
	movaps	%xmm1, %xmm12
	divss	20(%rsp), %xmm12
	movss	8(%rsp), %xmm13
	divss	24(%rsp), %xmm13
	movss	%xmm13, 16(%rsp)
	movaps	%xmm3, %xmm13
	divss	28(%rsp), %xmm13
	movss	%xmm13, 20(%rsp)
	movaps	%xmm4, %xmm13
	divss	32(%rsp), %xmm13
	movss	%xmm13, 24(%rsp)
	movaps	%xmm5, %xmm13
	divss	36(%rsp), %xmm13
	movss	%xmm13, 28(%rsp)
	movaps	%xmm6, %xmm13
	divss	40(%rsp), %xmm13
	movss	%xmm13, 32(%rsp)
	movaps	%xmm7, %xmm13
	divss	44(%rsp), %xmm13
	movss	%xmm13, 36(%rsp)
	movaps	%xmm8, %xmm13
	divss	48(%rsp), %xmm13
	movss	%xmm13, 40(%rsp)
	movaps	%xmm9, %xmm13
	divss	52(%rsp), %xmm13
	movss	%xmm13, 44(%rsp)
	movaps	%xmm10, %xmm13
	divss	56(%rsp), %xmm13
	movss	%xmm13, 48(%rsp)
	movaps	%xmm11, %xmm13
	divss	60(%rsp), %xmm13
	movss	%xmm13, 52(%rsp)
	movss	12(%rsp), %xmm13
	divss	68(%rsp), %xmm13
	movss	%xmm13, 56(%rsp)
	movss	(%rsp), %xmm13
	divss	%xmm14, %xmm13
	movss	4(%rsp), %xmm14
	divss	%xmm2, %xmm14
	movaps	%xmm0, %xmm2
	divss	%xmm15, %xmm2
	movss	%xmm2, 60(%rsp)
	movaps	%xmm1, %xmm15
	divss	%xmm12, %xmm15
	movss	8(%rsp), %xmm2
	divss	16(%rsp), %xmm2
	movss	%xmm2, 16(%rsp)
	movaps	%xmm3, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm4, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm5, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm6, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm7, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movaps	%xmm8, %xmm2
	divss	40(%rsp), %xmm2
	movss	%xmm2, 40(%rsp)
	movaps	%xmm9, %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 44(%rsp)
	movaps	%xmm10, %xmm2
	divss	48(%rsp), %xmm2
	movss	%xmm2, 48(%rsp)
	movaps	%xmm11, %xmm2
	divss	52(%rsp), %xmm2
	movss	%xmm2, 52(%rsp)
	movss	12(%rsp), %xmm2
	divss	56(%rsp), %xmm2
	movss	%xmm2, 56(%rsp)
	movss	(%rsp), %xmm2
	divss	%xmm13, %xmm2
	movss	%xmm2, 64(%rsp)
	movss	4(%rsp), %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm0, %xmm14
	divss	60(%rsp), %xmm14
	movaps	%xmm1, %xmm12
	divss	%xmm15, %xmm12
	movss	%xmm12, 60(%rsp)
	movss	8(%rsp), %xmm12
	movaps	%xmm12, %xmm15
	divss	16(%rsp), %xmm15
	movaps	%xmm3, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 16(%rsp)
	movaps	%xmm4, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm5, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm6, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm7, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm8, %xmm2
	divss	40(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movaps	%xmm9, %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 40(%rsp)
	movaps	%xmm10, %xmm2
	divss	48(%rsp), %xmm2
	movss	%xmm2, 44(%rsp)
	movaps	%xmm11, %xmm2
	divss	52(%rsp), %xmm2
	movss	%xmm2, 48(%rsp)
	movss	12(%rsp), %xmm2
	divss	56(%rsp), %xmm2
	movss	%xmm2, 52(%rsp)
	movss	(%rsp), %xmm2
	divss	64(%rsp), %xmm2
	movss	%xmm2, 56(%rsp)
	movss	4(%rsp), %xmm2
	divss	%xmm13, %xmm2
	movss	%xmm2, 64(%rsp)
	movaps	%xmm0, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm1, %xmm14
	divss	60(%rsp), %xmm14
	divss	%xmm15, %xmm12
	movss	%xmm12, 60(%rsp)
	movaps	%xmm3, %xmm15
	divss	16(%rsp), %xmm15
	movaps	%xmm4, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 16(%rsp)
	movaps	%xmm5, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm6, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm7, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm8, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm9, %xmm2
	divss	40(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movaps	%xmm10, %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 40(%rsp)
	movaps	%xmm11, %xmm2
	divss	48(%rsp), %xmm2
	movss	%xmm2, 44(%rsp)
	movss	12(%rsp), %xmm2
	divss	52(%rsp), %xmm2
	movss	%xmm2, 48(%rsp)
	movss	(%rsp), %xmm2
	divss	56(%rsp), %xmm2
	movss	%xmm2, 52(%rsp)
	movss	4(%rsp), %xmm2
	divss	64(%rsp), %xmm2
	movss	%xmm2, 56(%rsp)
	movaps	%xmm0, %xmm12
	divss	%xmm13, %xmm12
	movss	%xmm12, 64(%rsp)
	movaps	%xmm1, %xmm13
	divss	%xmm14, %xmm13
	movss	8(%rsp), %xmm14
	divss	60(%rsp), %xmm14
	movaps	%xmm3, %xmm12
	divss	%xmm15, %xmm12
	movss	%xmm12, 60(%rsp)
	movaps	%xmm4, %xmm15
	divss	16(%rsp), %xmm15
	movaps	%xmm5, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 16(%rsp)
	movaps	%xmm6, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm7, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm8, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm9, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm10, %xmm2
	divss	40(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movaps	%xmm11, %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 40(%rsp)
	movss	12(%rsp), %xmm2
	divss	48(%rsp), %xmm2
	movss	%xmm2, 44(%rsp)
	movss	(%rsp), %xmm2
	divss	52(%rsp), %xmm2
	movss	%xmm2, 48(%rsp)
	movss	4(%rsp), %xmm2
	divss	56(%rsp), %xmm2
	movss	%xmm2, 52(%rsp)
	movaps	%xmm0, %xmm2
	divss	64(%rsp), %xmm2
	movss	%xmm2, 56(%rsp)
	movaps	%xmm1, %xmm12
	divss	%xmm13, %xmm12
	movss	%xmm12, 64(%rsp)
	movss	8(%rsp), %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm3, %xmm14
	divss	60(%rsp), %xmm14
	movaps	%xmm4, %xmm12
	divss	%xmm15, %xmm12
	movss	%xmm12, 60(%rsp)
	movaps	%xmm5, %xmm15
	divss	16(%rsp), %xmm15
	movaps	%xmm6, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 16(%rsp)
	movaps	%xmm7, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm8, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm9, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm10, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm11, %xmm2
	divss	40(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movss	12(%rsp), %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 40(%rsp)
	movss	(%rsp), %xmm2
	divss	48(%rsp), %xmm2
	movss	%xmm2, 44(%rsp)
	movss	4(%rsp), %xmm2
	divss	52(%rsp), %xmm2
	movss	%xmm2, 48(%rsp)
	movaps	%xmm0, %xmm2
	divss	56(%rsp), %xmm2
	movss	%xmm2, 52(%rsp)
	movaps	%xmm1, %xmm2
	divss	64(%rsp), %xmm2
	movss	%xmm2, 56(%rsp)
	movss	8(%rsp), %xmm12
	divss	%xmm13, %xmm12
	movss	%xmm12, 64(%rsp)
	movaps	%xmm3, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm4, %xmm14
	divss	60(%rsp), %xmm14
	movaps	%xmm5, %xmm12
	divss	%xmm15, %xmm12
	movss	%xmm12, 60(%rsp)
	movaps	%xmm6, %xmm15
	divss	16(%rsp), %xmm15
	movaps	%xmm7, %xmm2
	divss	20(%rsp), %xmm2
	movss	%xmm2, 16(%rsp)
	movaps	%xmm8, %xmm2
	divss	24(%rsp), %xmm2
	movss	%xmm2, 20(%rsp)
	movaps	%xmm9, %xmm2
	divss	28(%rsp), %xmm2
	movss	%xmm2, 24(%rsp)
	movaps	%xmm10, %xmm2
	divss	32(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm11, %xmm2
	divss	36(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movss	12(%rsp), %xmm2
	divss	40(%rsp), %xmm2
	movss	%xmm2, 36(%rsp)
	movss	(%rsp), %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 40(%rsp)
	movss	4(%rsp), %xmm2
	divss	48(%rsp), %xmm2
	movss	%xmm2, 44(%rsp)
	movaps	%xmm0, %xmm2
	divss	52(%rsp), %xmm2
	movss	%xmm2, 48(%rsp)
	movaps	%xmm1, %xmm2
	divss	56(%rsp), %xmm2
	movss	%xmm2, 52(%rsp)
	movss	8(%rsp), %xmm2
	divss	64(%rsp), %xmm2
	movss	%xmm2, 56(%rsp)
	movaps	%xmm3, %xmm12
	divss	%xmm13, %xmm12
	movss	%xmm12, 64(%rsp)
	movaps	%xmm4, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm5, %xmm14
	divss	60(%rsp), %xmm14
	movaps	%xmm6, %xmm12
	divss	%xmm15, %xmm12
	movss	%xmm12, 60(%rsp)
	movaps	%xmm7, %xmm15
	divss	16(%rsp), %xmm15
	movss	%xmm15, 68(%rsp)
	movaps	%xmm8, %xmm15
	divss	20(%rsp), %xmm15
	movss	%xmm15, 72(%rsp)
	movaps	%xmm9, %xmm15
	divss	24(%rsp), %xmm15
	movss	%xmm15, 76(%rsp)
	movaps	%xmm10, %xmm15
	divss	28(%rsp), %xmm15
	movss	%xmm15, 80(%rsp)
	movaps	%xmm11, %xmm15
	divss	32(%rsp), %xmm15
	movss	%xmm15, 84(%rsp)
	movss	12(%rsp), %xmm12
	divss	36(%rsp), %xmm12
	movss	%xmm12, 88(%rsp)
	movss	(%rsp), %xmm15
	divss	40(%rsp), %xmm15
	movss	4(%rsp), %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 16(%rsp)
	movaps	%xmm0, %xmm12
	divss	48(%rsp), %xmm12
	movss	%xmm12, 20(%rsp)
	movaps	%xmm1, %xmm12
	divss	52(%rsp), %xmm12
	movss	%xmm12, 24(%rsp)
	movss	8(%rsp), %xmm2
	divss	56(%rsp), %xmm2
	movss	%xmm2, 28(%rsp)
	movaps	%xmm3, %xmm2
	divss	64(%rsp), %xmm2
	movss	%xmm2, 32(%rsp)
	movaps	%xmm4, %xmm12
	divss	%xmm13, %xmm12
	movss	%xmm12, 36(%rsp)
	movaps	%xmm5, %xmm13
	divss	%xmm14, %xmm13
	movss	%xmm13, 40(%rsp)
	movaps	%xmm6, %xmm14
	divss	60(%rsp), %xmm14
	movss	%xmm14, 44(%rsp)
	movaps	%xmm7, %xmm14
	divss	68(%rsp), %xmm14
	movss	%xmm14, 48(%rsp)
	movaps	%xmm8, %xmm14
	divss	72(%rsp), %xmm14
	movss	%xmm14, 52(%rsp)
	movaps	%xmm9, %xmm14
	divss	76(%rsp), %xmm14
	movss	%xmm14, 56(%rsp)
	movaps	%xmm10, %xmm14
	divss	80(%rsp), %xmm14
	movss	%xmm14, 60(%rsp)
	movaps	%xmm11, %xmm13
	divss	84(%rsp), %xmm13
	movss	12(%rsp), %xmm14
	divss	88(%rsp), %xmm14
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1047
	movss	%xmm13, 8(%rsp)
	movss	%xmm14, 12(%rsp)
.L1046:
	cvttss2sil	%xmm15, %edi
	call	use_int@PLT
	cvttss2sil	(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	4(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	100(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	108(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	36(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	116(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	124(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	128(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	132(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	136(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	140(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	92(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	96(%rsp), %edi
	call	use_int@PLT
	addq	$152, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE279:
	.size	float_div_14, .-float_div_14
	.globl	float_div_15
	.type	float_div_15, @function
float_div_15:
.LFB280:
	.cfi_startproc
	endbr64
	subq	$168, %rsp
	.cfi_def_cfa_offset 176
	movq	%rsi, %rax
	pxor	%xmm7, %xmm7
	cvtsd2ss	80(%rsi), %xmm7
	movss	.LC5(%rip), %xmm1
	mulss	%xmm1, %xmm7
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	movss	.LC6(%rip), %xmm0
	mulss	%xmm0, %xmm2
	movss	%xmm2, 8(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	88(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 40(%rsp)
	pxor	%xmm2, %xmm2
	cvtsi2ssl	16(%rax), %xmm2
	mulss	%xmm0, %xmm2
	movss	%xmm2, 24(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	96(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 44(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	mulss	%xmm0, %xmm2
	movss	%xmm2, 28(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	104(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 48(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	mulss	%xmm0, %xmm2
	movss	%xmm2, 32(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	112(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 52(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	movaps	%xmm2, %xmm3
	mulss	%xmm0, %xmm3
	movss	%xmm3, 116(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	120(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 56(%rsp)
	movl	32(%rax), %esi
	leal	-4(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	movaps	%xmm2, %xmm4
	mulss	%xmm0, %xmm4
	movss	%xmm4, 120(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	128(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 60(%rsp)
	movl	36(%rax), %esi
	leal	-5(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	movaps	%xmm2, %xmm5
	mulss	%xmm0, %xmm5
	movss	%xmm5, 124(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	136(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 64(%rsp)
	movl	40(%rax), %esi
	leal	-6(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	movaps	%xmm2, %xmm6
	mulss	%xmm0, %xmm6
	movss	%xmm6, 128(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	144(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 68(%rsp)
	movl	44(%rax), %esi
	leal	-7(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm2, %xmm8
	movss	%xmm2, 132(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	152(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 72(%rsp)
	movl	48(%rax), %esi
	leal	-8(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm2, %xmm9
	movss	%xmm2, 136(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	160(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 76(%rsp)
	movl	52(%rax), %esi
	leal	-9(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm2, %xmm10
	movss	%xmm2, 140(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	168(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 80(%rsp)
	movl	56(%rax), %esi
	leal	-10(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm2, %xmm13
	movss	%xmm2, 144(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	176(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 84(%rsp)
	movl	60(%rax), %esi
	leal	-11(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm2, %xmm14
	movss	%xmm2, 148(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	184(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movaps	%xmm2, %xmm15
	movss	%xmm2, 12(%rsp)
	movl	64(%rax), %esi
	leal	-12(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm2, %xmm11
	movss	%xmm2, 152(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	192(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movaps	%xmm2, %xmm12
	movss	%xmm2, 16(%rsp)
	movl	68(%rax), %ecx
	subl	$13, %ecx
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%ecx, %xmm2
	mulss	%xmm0, %xmm2
	movss	%xmm2, 112(%rsp)
	pxor	%xmm2, %xmm2
	cvtsd2ss	200(%rax), %xmm2
	mulss	%xmm1, %xmm2
	movss	%xmm2, 20(%rsp)
	movl	72(%rax), %eax
	subl	$14, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	mulss	%xmm0, %xmm1
	movss	%xmm1, 156(%rsp)
	testq	%rdi, %rdi
	je	.L1051
	leaq	-1(%rdi), %rax
	movaps	%xmm3, %xmm0
	movss	%xmm4, 12(%rsp)
	movss	%xmm5, 16(%rsp)
	movaps	%xmm6, %xmm3
	movaps	%xmm8, %xmm4
	movaps	%xmm9, %xmm5
	movaps	%xmm10, %xmm6
	movaps	%xmm13, %xmm8
	movaps	%xmm14, %xmm9
	movaps	%xmm11, %xmm10
	movss	112(%rsp), %xmm13
	movss	%xmm13, 20(%rsp)
	movss	%xmm1, 36(%rsp)
	movaps	%xmm15, %xmm13
	movaps	%xmm12, %xmm14
	movaps	%xmm2, %xmm15
.L1052:
	movss	8(%rsp), %xmm2
	movaps	%xmm2, %xmm1
	divss	%xmm7, %xmm1
	movss	%xmm1, 88(%rsp)
	movss	24(%rsp), %xmm7
	divss	40(%rsp), %xmm7
	movss	28(%rsp), %xmm11
	divss	44(%rsp), %xmm11
	movss	%xmm11, 40(%rsp)
	movss	32(%rsp), %xmm12
	movaps	%xmm12, %xmm11
	divss	48(%rsp), %xmm11
	movss	%xmm11, 32(%rsp)
	movaps	%xmm0, %xmm1
	divss	52(%rsp), %xmm1
	movss	%xmm1, 44(%rsp)
	movss	12(%rsp), %xmm1
	divss	56(%rsp), %xmm1
	movss	%xmm1, 48(%rsp)
	movss	16(%rsp), %xmm1
	divss	60(%rsp), %xmm1
	movss	%xmm1, 52(%rsp)
	movaps	%xmm3, %xmm1
	divss	64(%rsp), %xmm1
	movss	%xmm1, 56(%rsp)
	movaps	%xmm4, %xmm1
	divss	68(%rsp), %xmm1
	movss	%xmm1, 60(%rsp)
	movaps	%xmm5, %xmm1
	divss	72(%rsp), %xmm1
	movss	%xmm1, 64(%rsp)
	movaps	%xmm6, %xmm1
	divss	76(%rsp), %xmm1
	movss	%xmm1, 68(%rsp)
	movaps	%xmm8, %xmm1
	divss	80(%rsp), %xmm1
	movss	%xmm1, 72(%rsp)
	movaps	%xmm9, %xmm1
	divss	84(%rsp), %xmm1
	movss	%xmm1, 76(%rsp)
	movaps	%xmm10, %xmm1
	divss	%xmm13, %xmm1
	movss	%xmm1, 80(%rsp)
	movss	20(%rsp), %xmm13
	divss	%xmm14, %xmm13
	movss	36(%rsp), %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm2, %xmm15
	divss	88(%rsp), %xmm15
	movss	24(%rsp), %xmm1
	movaps	%xmm1, %xmm11
	divss	%xmm7, %xmm11
	movss	%xmm11, 24(%rsp)
	movss	28(%rsp), %xmm7
	divss	40(%rsp), %xmm7
	movaps	%xmm12, %xmm2
	divss	32(%rsp), %xmm12
	movss	%xmm12, 40(%rsp)
	movaps	%xmm0, %xmm12
	divss	44(%rsp), %xmm12
	movss	%xmm12, 44(%rsp)
	movss	12(%rsp), %xmm12
	divss	48(%rsp), %xmm12
	movss	%xmm12, 48(%rsp)
	movss	16(%rsp), %xmm12
	divss	52(%rsp), %xmm12
	movss	%xmm12, 52(%rsp)
	movaps	%xmm3, %xmm12
	divss	56(%rsp), %xmm12
	movss	%xmm12, 56(%rsp)
	movaps	%xmm4, %xmm12
	divss	60(%rsp), %xmm12
	movss	%xmm12, 60(%rsp)
	movaps	%xmm5, %xmm12
	divss	64(%rsp), %xmm12
	movss	%xmm12, 64(%rsp)
	movaps	%xmm6, %xmm12
	divss	68(%rsp), %xmm12
	movss	%xmm12, 68(%rsp)
	movaps	%xmm8, %xmm12
	divss	72(%rsp), %xmm12
	movss	%xmm12, 72(%rsp)
	movaps	%xmm9, %xmm12
	divss	76(%rsp), %xmm12
	movss	%xmm12, 76(%rsp)
	movaps	%xmm10, %xmm12
	divss	80(%rsp), %xmm12
	movss	%xmm12, 80(%rsp)
	movss	20(%rsp), %xmm11
	divss	%xmm13, %xmm11
	movss	%xmm11, 84(%rsp)
	movss	36(%rsp), %xmm13
	divss	%xmm14, %xmm13
	movss	8(%rsp), %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm1, %xmm15
	divss	24(%rsp), %xmm15
	movss	28(%rsp), %xmm11
	movaps	%xmm11, %xmm12
	divss	%xmm7, %xmm12
	movss	%xmm12, 28(%rsp)
	movss	%xmm2, 32(%rsp)
	movaps	%xmm2, %xmm7
	divss	40(%rsp), %xmm7
	movaps	%xmm0, %xmm2
	divss	44(%rsp), %xmm2
	movss	%xmm2, 40(%rsp)
	movss	12(%rsp), %xmm2
	divss	48(%rsp), %xmm2
	movss	%xmm2, 44(%rsp)
	movss	16(%rsp), %xmm2
	divss	52(%rsp), %xmm2
	movss	%xmm2, 48(%rsp)
	movaps	%xmm3, %xmm2
	divss	56(%rsp), %xmm2
	movss	%xmm2, 52(%rsp)
	movaps	%xmm4, %xmm2
	divss	60(%rsp), %xmm2
	movss	%xmm2, 56(%rsp)
	movaps	%xmm5, %xmm2
	divss	64(%rsp), %xmm2
	movss	%xmm2, 60(%rsp)
	movaps	%xmm6, %xmm2
	divss	68(%rsp), %xmm2
	movss	%xmm2, 64(%rsp)
	movaps	%xmm8, %xmm2
	divss	72(%rsp), %xmm2
	movss	%xmm2, 68(%rsp)
	movaps	%xmm9, %xmm2
	divss	76(%rsp), %xmm2
	movss	%xmm2, 72(%rsp)
	movaps	%xmm10, %xmm2
	divss	80(%rsp), %xmm2
	movss	%xmm2, 76(%rsp)
	movss	20(%rsp), %xmm2
	divss	84(%rsp), %xmm2
	movss	%xmm2, 80(%rsp)
	movss	36(%rsp), %xmm2
	movaps	%xmm2, %xmm12
	divss	%xmm13, %xmm12
	movss	%xmm12, 36(%rsp)
	movss	8(%rsp), %xmm13
	divss	%xmm14, %xmm13
	movss	%xmm1, 24(%rsp)
	movaps	%xmm1, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm11, %xmm15
	divss	28(%rsp), %xmm15
	movss	32(%rsp), %xmm12
	movaps	%xmm12, %xmm1
	divss	%xmm7, %xmm1
	movss	%xmm1, 32(%rsp)
	movaps	%xmm0, %xmm7
	divss	40(%rsp), %xmm7
	movss	12(%rsp), %xmm1
	divss	44(%rsp), %xmm1
	movss	%xmm1, 40(%rsp)
	movss	16(%rsp), %xmm1
	divss	48(%rsp), %xmm1
	movss	%xmm1, 44(%rsp)
	movaps	%xmm3, %xmm1
	divss	52(%rsp), %xmm1
	movss	%xmm1, 48(%rsp)
	movaps	%xmm4, %xmm1
	divss	56(%rsp), %xmm1
	movss	%xmm1, 52(%rsp)
	movaps	%xmm5, %xmm1
	divss	60(%rsp), %xmm1
	movss	%xmm1, 56(%rsp)
	movaps	%xmm6, %xmm1
	divss	64(%rsp), %xmm1
	movss	%xmm1, 60(%rsp)
	movaps	%xmm8, %xmm1
	divss	68(%rsp), %xmm1
	movss	%xmm1, 64(%rsp)
	movaps	%xmm9, %xmm1
	divss	72(%rsp), %xmm1
	movss	%xmm1, 68(%rsp)
	movaps	%xmm10, %xmm1
	divss	76(%rsp), %xmm1
	movss	%xmm1, 72(%rsp)
	movss	20(%rsp), %xmm1
	divss	80(%rsp), %xmm1
	movss	%xmm1, 76(%rsp)
	movaps	%xmm2, %xmm1
	divss	36(%rsp), %xmm1
	movss	%xmm1, 36(%rsp)
	movss	8(%rsp), %xmm1
	divss	%xmm13, %xmm1
	movss	%xmm1, 80(%rsp)
	movss	24(%rsp), %xmm13
	divss	%xmm14, %xmm13
	movss	%xmm11, 28(%rsp)
	movaps	%xmm11, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm12, %xmm15
	divss	32(%rsp), %xmm15
	movaps	%xmm0, %xmm11
	divss	%xmm7, %xmm11
	movss	%xmm11, 84(%rsp)
	movss	12(%rsp), %xmm7
	divss	40(%rsp), %xmm7
	movss	16(%rsp), %xmm11
	divss	44(%rsp), %xmm11
	movss	%xmm11, 40(%rsp)
	movaps	%xmm3, %xmm11
	divss	48(%rsp), %xmm11
	movss	%xmm11, 44(%rsp)
	movaps	%xmm4, %xmm11
	divss	52(%rsp), %xmm11
	movss	%xmm11, 48(%rsp)
	movaps	%xmm5, %xmm11
	divss	56(%rsp), %xmm11
	movss	%xmm11, 52(%rsp)
	movaps	%xmm6, %xmm11
	divss	60(%rsp), %xmm11
	movss	%xmm11, 56(%rsp)
	movaps	%xmm8, %xmm11
	divss	64(%rsp), %xmm11
	movss	%xmm11, 60(%rsp)
	movaps	%xmm9, %xmm11
	divss	68(%rsp), %xmm11
	movss	%xmm11, 64(%rsp)
	movaps	%xmm10, %xmm1
	divss	72(%rsp), %xmm1
	movss	%xmm1, 68(%rsp)
	movss	20(%rsp), %xmm1
	divss	76(%rsp), %xmm1
	movss	%xmm1, 72(%rsp)
	movaps	%xmm2, %xmm1
	divss	36(%rsp), %xmm1
	movss	%xmm1, 36(%rsp)
	movss	8(%rsp), %xmm1
	divss	80(%rsp), %xmm1
	movss	%xmm1, 76(%rsp)
	movss	24(%rsp), %xmm11
	divss	%xmm13, %xmm11
	movss	%xmm11, 80(%rsp)
	movss	28(%rsp), %xmm13
	divss	%xmm14, %xmm13
	movss	%xmm12, 32(%rsp)
	movaps	%xmm12, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm0, %xmm15
	divss	84(%rsp), %xmm15
	movss	12(%rsp), %xmm12
	divss	%xmm7, %xmm12
	movss	%xmm12, 84(%rsp)
	movss	16(%rsp), %xmm12
	divss	40(%rsp), %xmm12
	movaps	%xmm12, %xmm7
	movaps	%xmm3, %xmm12
	divss	44(%rsp), %xmm12
	movss	%xmm12, 40(%rsp)
	movaps	%xmm4, %xmm12
	divss	48(%rsp), %xmm12
	movss	%xmm12, 44(%rsp)
	movaps	%xmm5, %xmm12
	divss	52(%rsp), %xmm12
	movss	%xmm12, 48(%rsp)
	movaps	%xmm6, %xmm12
	divss	56(%rsp), %xmm12
	movss	%xmm12, 52(%rsp)
	movaps	%xmm8, %xmm11
	divss	60(%rsp), %xmm11
	movss	%xmm11, 56(%rsp)
	movaps	%xmm9, %xmm11
	divss	64(%rsp), %xmm11
	movss	%xmm11, 60(%rsp)
	movaps	%xmm10, %xmm11
	divss	68(%rsp), %xmm11
	movss	%xmm11, 64(%rsp)
	movss	20(%rsp), %xmm11
	divss	72(%rsp), %xmm11
	movss	%xmm11, 68(%rsp)
	movaps	%xmm2, %xmm12
	divss	36(%rsp), %xmm12
	movss	%xmm12, 36(%rsp)
	movss	8(%rsp), %xmm1
	divss	76(%rsp), %xmm1
	movss	%xmm1, 72(%rsp)
	movss	24(%rsp), %xmm1
	divss	80(%rsp), %xmm1
	movss	%xmm1, 76(%rsp)
	movss	28(%rsp), %xmm12
	divss	%xmm13, %xmm12
	movss	%xmm12, 80(%rsp)
	movss	32(%rsp), %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm0, %xmm14
	divss	%xmm15, %xmm14
	movss	12(%rsp), %xmm15
	divss	84(%rsp), %xmm15
	movss	16(%rsp), %xmm11
	divss	%xmm7, %xmm11
	movss	%xmm11, 84(%rsp)
	movaps	%xmm3, %xmm7
	divss	40(%rsp), %xmm7
	movaps	%xmm4, %xmm1
	divss	44(%rsp), %xmm1
	movss	%xmm1, 40(%rsp)
	movaps	%xmm5, %xmm12
	divss	48(%rsp), %xmm12
	movss	%xmm12, 44(%rsp)
	movaps	%xmm6, %xmm12
	divss	52(%rsp), %xmm12
	movss	%xmm12, 48(%rsp)
	movaps	%xmm8, %xmm12
	divss	56(%rsp), %xmm12
	movss	%xmm12, 52(%rsp)
	movaps	%xmm9, %xmm11
	divss	60(%rsp), %xmm11
	movss	%xmm11, 56(%rsp)
	movaps	%xmm10, %xmm11
	divss	64(%rsp), %xmm11
	movss	%xmm11, 60(%rsp)
	movss	20(%rsp), %xmm11
	divss	68(%rsp), %xmm11
	movss	%xmm11, 64(%rsp)
	movaps	%xmm2, %xmm11
	divss	36(%rsp), %xmm11
	movss	%xmm11, 68(%rsp)
	movss	8(%rsp), %xmm1
	divss	72(%rsp), %xmm1
	movss	%xmm1, 72(%rsp)
	movss	24(%rsp), %xmm1
	divss	76(%rsp), %xmm1
	movss	%xmm1, 76(%rsp)
	movss	28(%rsp), %xmm11
	divss	80(%rsp), %xmm11
	movss	%xmm11, 80(%rsp)
	movss	32(%rsp), %xmm12
	divss	%xmm13, %xmm12
	movss	%xmm12, 88(%rsp)
	movaps	%xmm0, %xmm13
	divss	%xmm14, %xmm13
	movss	12(%rsp), %xmm14
	divss	%xmm15, %xmm14
	movss	16(%rsp), %xmm15
	divss	84(%rsp), %xmm15
	movaps	%xmm3, %xmm12
	divss	%xmm7, %xmm12
	movss	%xmm12, 84(%rsp)
	movaps	%xmm4, %xmm7
	divss	40(%rsp), %xmm7
	movaps	%xmm5, %xmm11
	divss	44(%rsp), %xmm11
	movss	%xmm11, 40(%rsp)
	movaps	%xmm6, %xmm12
	divss	48(%rsp), %xmm12
	movss	%xmm12, 44(%rsp)
	movaps	%xmm8, %xmm12
	divss	52(%rsp), %xmm12
	movss	%xmm12, 48(%rsp)
	movaps	%xmm9, %xmm12
	divss	56(%rsp), %xmm12
	movss	%xmm12, 52(%rsp)
	movaps	%xmm10, %xmm11
	divss	60(%rsp), %xmm11
	movss	%xmm11, 56(%rsp)
	movss	20(%rsp), %xmm11
	divss	64(%rsp), %xmm11
	movss	%xmm11, 60(%rsp)
	movss	%xmm2, 36(%rsp)
	movaps	%xmm2, %xmm12
	divss	68(%rsp), %xmm12
	movss	%xmm12, 64(%rsp)
	movss	8(%rsp), %xmm1
	divss	72(%rsp), %xmm1
	movss	%xmm1, 68(%rsp)
	movss	24(%rsp), %xmm1
	divss	76(%rsp), %xmm1
	movss	%xmm1, 72(%rsp)
	movss	28(%rsp), %xmm11
	divss	80(%rsp), %xmm11
	movss	%xmm11, 76(%rsp)
	movss	32(%rsp), %xmm12
	movaps	%xmm12, %xmm11
	divss	88(%rsp), %xmm11
	movss	%xmm11, 80(%rsp)
	movaps	%xmm0, %xmm2
	divss	%xmm13, %xmm2
	movss	%xmm2, 88(%rsp)
	movss	12(%rsp), %xmm13
	divss	%xmm14, %xmm13
	movss	16(%rsp), %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm3, %xmm15
	divss	84(%rsp), %xmm15
	movaps	%xmm4, %xmm2
	divss	%xmm7, %xmm2
	movaps	%xmm5, %xmm7
	divss	40(%rsp), %xmm7
	movss	%xmm7, 84(%rsp)
	movaps	%xmm6, %xmm7
	divss	44(%rsp), %xmm7
	movss	%xmm7, 92(%rsp)
	movaps	%xmm8, %xmm7
	divss	48(%rsp), %xmm7
	movss	%xmm7, 96(%rsp)
	movaps	%xmm9, %xmm7
	divss	52(%rsp), %xmm7
	movss	%xmm7, 100(%rsp)
	movaps	%xmm10, %xmm7
	divss	56(%rsp), %xmm7
	movss	%xmm7, 104(%rsp)
	movss	20(%rsp), %xmm7
	divss	60(%rsp), %xmm7
	movss	%xmm7, 108(%rsp)
	movss	36(%rsp), %xmm11
	divss	64(%rsp), %xmm11
	movss	8(%rsp), %xmm7
	divss	68(%rsp), %xmm7
	movss	24(%rsp), %xmm1
	divss	72(%rsp), %xmm1
	movss	%xmm1, 40(%rsp)
	movss	28(%rsp), %xmm1
	divss	76(%rsp), %xmm1
	movss	%xmm1, 44(%rsp)
	divss	80(%rsp), %xmm12
	movss	%xmm12, 48(%rsp)
	movaps	%xmm0, %xmm12
	divss	88(%rsp), %xmm12
	movss	%xmm12, 52(%rsp)
	movss	12(%rsp), %xmm12
	divss	%xmm13, %xmm12
	movss	%xmm12, 56(%rsp)
	movss	16(%rsp), %xmm13
	divss	%xmm14, %xmm13
	movss	%xmm13, 60(%rsp)
	movaps	%xmm3, %xmm14
	divss	%xmm15, %xmm14
	movss	%xmm14, 64(%rsp)
	movaps	%xmm4, %xmm15
	divss	%xmm2, %xmm15
	movss	%xmm15, 68(%rsp)
	movaps	%xmm5, %xmm2
	divss	84(%rsp), %xmm2
	movss	%xmm2, 72(%rsp)
	movaps	%xmm6, %xmm2
	divss	92(%rsp), %xmm2
	movss	%xmm2, 76(%rsp)
	movaps	%xmm8, %xmm2
	divss	96(%rsp), %xmm2
	movss	%xmm2, 80(%rsp)
	movaps	%xmm9, %xmm2
	divss	100(%rsp), %xmm2
	movss	%xmm2, 84(%rsp)
	movaps	%xmm10, %xmm13
	divss	104(%rsp), %xmm13
	movss	20(%rsp), %xmm14
	divss	108(%rsp), %xmm14
	movss	36(%rsp), %xmm15
	divss	%xmm11, %xmm15
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1052
	movss	%xmm13, 12(%rsp)
	movss	%xmm14, 16(%rsp)
	movss	%xmm15, 20(%rsp)
.L1051:
	cvttss2sil	%xmm7, %edi
	call	use_int@PLT
	cvttss2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	44(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	28(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	52(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	116(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	60(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	124(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	128(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	68(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	132(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	136(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	76(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	140(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	144(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	84(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	148(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	12(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	152(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	20(%rsp), %edi
	call	use_int@PLT
	cvttss2sil	156(%rsp), %edi
	call	use_int@PLT
	addq	$168, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE280:
	.size	float_div_15, .-float_div_15
	.globl	double_add_0
	.type	double_add_0, @function
double_add_0:
.LFB281:
	.cfi_startproc
	endbr64
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movsd	.LC7(%rip), %xmm0
	addsd	80(%rsi), %xmm0
	pxor	%xmm1, %xmm1
	cvtsi2sdl	8(%rsi), %xmm1
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1056
.L1057:
	addsd	%xmm1, %xmm0
	addsd	%xmm0, %xmm0
	addsd	%xmm0, %xmm0
	addsd	%xmm0, %xmm0
	addsd	%xmm0, %xmm0
	addsd	%xmm0, %xmm0
	addsd	%xmm0, %xmm0
	addsd	%xmm0, %xmm0
	addsd	%xmm0, %xmm0
	addsd	%xmm0, %xmm0
	addsd	%xmm0, %xmm0
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1057
.L1056:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE281:
	.size	double_add_0, .-double_add_0
	.globl	double_add_1
	.type	double_add_1, @function
double_add_1:
.LFB282:
	.cfi_startproc
	endbr64
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	movsd	.LC7(%rip), %xmm1
	movapd	%xmm1, %xmm0
	addsd	80(%rsi), %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2sdl	8(%rsi), %xmm2
	addsd	88(%rsi), %xmm1
	movsd	%xmm1, 8(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1061
.L1062:
	addsd	%xmm2, %xmm0
	movsd	8(%rsp), %xmm1
	addsd	%xmm2, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	movsd	%xmm1, 8(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1062
.L1061:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE282:
	.size	double_add_1, .-double_add_1
	.globl	double_add_2
	.type	double_add_2, @function
double_add_2:
.LFB283:
	.cfi_startproc
	endbr64
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	movsd	.LC7(%rip), %xmm1
	movapd	%xmm1, %xmm0
	addsd	80(%rsi), %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2sdl	8(%rsi), %xmm2
	movapd	%xmm1, %xmm4
	addsd	88(%rsi), %xmm4
	movsd	%xmm4, (%rsp)
	addsd	96(%rsi), %xmm1
	movsd	%xmm1, 8(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1066
.L1067:
	addsd	%xmm2, %xmm0
	movsd	(%rsp), %xmm3
	addsd	%xmm2, %xmm3
	movsd	8(%rsp), %xmm1
	addsd	%xmm2, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm3, %xmm3
	movsd	%xmm3, (%rsp)
	addsd	%xmm1, %xmm1
	movsd	%xmm1, 8(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1067
.L1066:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE283:
	.size	double_add_2, .-double_add_2
	.globl	double_add_3
	.type	double_add_3, @function
double_add_3:
.LFB284:
	.cfi_startproc
	endbr64
	subq	$40, %rsp
	.cfi_def_cfa_offset 48
	movsd	.LC7(%rip), %xmm1
	movapd	%xmm1, %xmm0
	addsd	80(%rsi), %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2sdl	8(%rsi), %xmm2
	movapd	%xmm1, %xmm5
	addsd	88(%rsi), %xmm5
	movsd	%xmm5, 8(%rsp)
	movapd	%xmm1, %xmm6
	addsd	96(%rsi), %xmm6
	movsd	%xmm6, 16(%rsp)
	addsd	104(%rsi), %xmm1
	movsd	%xmm1, 24(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1071
.L1072:
	addsd	%xmm2, %xmm0
	movsd	8(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	movsd	16(%rsp), %xmm3
	addsd	%xmm2, %xmm3
	movsd	24(%rsp), %xmm1
	addsd	%xmm2, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm4, %xmm4
	movsd	%xmm4, 8(%rsp)
	addsd	%xmm3, %xmm3
	movsd	%xmm3, 16(%rsp)
	addsd	%xmm1, %xmm1
	movsd	%xmm1, 24(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1072
.L1071:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE284:
	.size	double_add_3, .-double_add_3
	.globl	double_add_4
	.type	double_add_4, @function
double_add_4:
.LFB285:
	.cfi_startproc
	endbr64
	subq	$40, %rsp
	.cfi_def_cfa_offset 48
	movsd	.LC7(%rip), %xmm1
	movapd	%xmm1, %xmm0
	addsd	80(%rsi), %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2sdl	8(%rsi), %xmm2
	movapd	%xmm1, %xmm6
	addsd	88(%rsi), %xmm6
	movsd	%xmm6, (%rsp)
	movapd	%xmm1, %xmm7
	addsd	96(%rsi), %xmm7
	movsd	%xmm7, 8(%rsp)
	movapd	%xmm1, %xmm6
	addsd	104(%rsi), %xmm6
	movsd	%xmm6, 16(%rsp)
	addsd	112(%rsi), %xmm1
	movsd	%xmm1, 24(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1076
.L1077:
	addsd	%xmm2, %xmm0
	movsd	(%rsp), %xmm5
	addsd	%xmm2, %xmm5
	movsd	8(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	movsd	16(%rsp), %xmm3
	addsd	%xmm2, %xmm3
	movsd	24(%rsp), %xmm1
	addsd	%xmm2, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	movsd	%xmm5, (%rsp)
	addsd	%xmm4, %xmm4
	movsd	%xmm4, 8(%rsp)
	addsd	%xmm3, %xmm3
	movsd	%xmm3, 16(%rsp)
	addsd	%xmm1, %xmm1
	movsd	%xmm1, 24(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1077
.L1076:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE285:
	.size	double_add_4, .-double_add_4
	.globl	double_add_5
	.type	double_add_5, @function
double_add_5:
.LFB286:
	.cfi_startproc
	endbr64
	subq	$56, %rsp
	.cfi_def_cfa_offset 64
	movsd	.LC7(%rip), %xmm1
	movapd	%xmm1, %xmm0
	addsd	80(%rsi), %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2sdl	8(%rsi), %xmm2
	movapd	%xmm1, %xmm7
	addsd	88(%rsi), %xmm7
	movsd	%xmm7, 8(%rsp)
	movapd	%xmm1, %xmm7
	addsd	96(%rsi), %xmm7
	movsd	%xmm7, 16(%rsp)
	movapd	%xmm1, %xmm7
	addsd	104(%rsi), %xmm7
	movsd	%xmm7, 24(%rsp)
	movapd	%xmm1, %xmm7
	addsd	112(%rsi), %xmm7
	movsd	%xmm7, 32(%rsp)
	addsd	120(%rsi), %xmm1
	movsd	%xmm1, 40(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1081
.L1082:
	addsd	%xmm2, %xmm0
	movsd	8(%rsp), %xmm6
	addsd	%xmm2, %xmm6
	movsd	16(%rsp), %xmm5
	addsd	%xmm2, %xmm5
	movsd	24(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	movsd	32(%rsp), %xmm3
	addsd	%xmm2, %xmm3
	movsd	40(%rsp), %xmm1
	addsd	%xmm2, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm6, %xmm6
	movsd	%xmm6, 8(%rsp)
	addsd	%xmm5, %xmm5
	movsd	%xmm5, 16(%rsp)
	addsd	%xmm4, %xmm4
	movsd	%xmm4, 24(%rsp)
	addsd	%xmm3, %xmm3
	movsd	%xmm3, 32(%rsp)
	addsd	%xmm1, %xmm1
	movsd	%xmm1, 40(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1082
.L1081:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE286:
	.size	double_add_5, .-double_add_5
	.globl	double_add_6
	.type	double_add_6, @function
double_add_6:
.LFB287:
	.cfi_startproc
	endbr64
	subq	$56, %rsp
	.cfi_def_cfa_offset 64
	movsd	.LC7(%rip), %xmm1
	movapd	%xmm1, %xmm0
	addsd	80(%rsi), %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2sdl	8(%rsi), %xmm2
	movapd	%xmm1, %xmm3
	addsd	88(%rsi), %xmm3
	movsd	%xmm3, (%rsp)
	movapd	%xmm1, %xmm4
	addsd	96(%rsi), %xmm4
	movsd	%xmm4, 8(%rsp)
	movapd	%xmm1, %xmm5
	addsd	104(%rsi), %xmm5
	movsd	%xmm5, 16(%rsp)
	movapd	%xmm1, %xmm6
	addsd	112(%rsi), %xmm6
	movsd	%xmm6, 24(%rsp)
	movapd	%xmm1, %xmm7
	addsd	120(%rsi), %xmm7
	movsd	%xmm7, 32(%rsp)
	addsd	128(%rsi), %xmm1
	movsd	%xmm1, 40(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1086
.L1087:
	addsd	%xmm2, %xmm0
	movsd	(%rsp), %xmm7
	addsd	%xmm2, %xmm7
	movsd	8(%rsp), %xmm6
	addsd	%xmm2, %xmm6
	movsd	16(%rsp), %xmm5
	addsd	%xmm2, %xmm5
	movsd	24(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	movsd	32(%rsp), %xmm3
	addsd	%xmm2, %xmm3
	movsd	40(%rsp), %xmm1
	addsd	%xmm2, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm7, %xmm7
	movsd	%xmm7, (%rsp)
	addsd	%xmm6, %xmm6
	movsd	%xmm6, 8(%rsp)
	addsd	%xmm5, %xmm5
	movsd	%xmm5, 16(%rsp)
	addsd	%xmm4, %xmm4
	movsd	%xmm4, 24(%rsp)
	addsd	%xmm3, %xmm3
	movsd	%xmm3, 32(%rsp)
	addsd	%xmm1, %xmm1
	movsd	%xmm1, 40(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1087
.L1086:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE287:
	.size	double_add_6, .-double_add_6
	.globl	double_add_7
	.type	double_add_7, @function
double_add_7:
.LFB288:
	.cfi_startproc
	endbr64
	subq	$72, %rsp
	.cfi_def_cfa_offset 80
	movsd	.LC7(%rip), %xmm1
	movapd	%xmm1, %xmm0
	addsd	80(%rsi), %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2sdl	8(%rsi), %xmm2
	movapd	%xmm1, %xmm3
	addsd	88(%rsi), %xmm3
	movsd	%xmm3, 8(%rsp)
	movapd	%xmm1, %xmm4
	addsd	96(%rsi), %xmm4
	movsd	%xmm4, 16(%rsp)
	movapd	%xmm1, %xmm5
	addsd	104(%rsi), %xmm5
	movsd	%xmm5, 24(%rsp)
	movapd	%xmm1, %xmm6
	addsd	112(%rsi), %xmm6
	movsd	%xmm6, 32(%rsp)
	movapd	%xmm1, %xmm7
	addsd	120(%rsi), %xmm7
	movsd	%xmm7, 40(%rsp)
	movapd	%xmm1, %xmm3
	addsd	128(%rsi), %xmm3
	movsd	%xmm3, 48(%rsp)
	addsd	136(%rsi), %xmm1
	movsd	%xmm1, 56(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1091
.L1092:
	addsd	%xmm2, %xmm0
	movsd	8(%rsp), %xmm8
	addsd	%xmm2, %xmm8
	movsd	16(%rsp), %xmm7
	addsd	%xmm2, %xmm7
	movsd	24(%rsp), %xmm6
	addsd	%xmm2, %xmm6
	movsd	32(%rsp), %xmm5
	addsd	%xmm2, %xmm5
	movsd	40(%rsp), %xmm4
	addsd	%xmm2, %xmm4
	movsd	48(%rsp), %xmm3
	addsd	%xmm2, %xmm3
	movsd	56(%rsp), %xmm1
	addsd	%xmm2, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm8, %xmm8
	movsd	%xmm8, 8(%rsp)
	addsd	%xmm7, %xmm7
	movsd	%xmm7, 16(%rsp)
	addsd	%xmm6, %xmm6
	movsd	%xmm6, 24(%rsp)
	addsd	%xmm5, %xmm5
	movsd	%xmm5, 32(%rsp)
	addsd	%xmm4, %xmm4
	movsd	%xmm4, 40(%rsp)
	addsd	%xmm3, %xmm3
	movsd	%xmm3, 48(%rsp)
	addsd	%xmm1, %xmm1
	movsd	%xmm1, 56(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1092
.L1091:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE288:
	.size	double_add_7, .-double_add_7
	.globl	double_add_8
	.type	double_add_8, @function
double_add_8:
.LFB289:
	.cfi_startproc
	endbr64
	subq	$72, %rsp
	.cfi_def_cfa_offset 80
	movsd	.LC7(%rip), %xmm0
	movapd	%xmm0, %xmm1
	addsd	80(%rsi), %xmm1
	pxor	%xmm4, %xmm4
	cvtsi2sdl	8(%rsi), %xmm4
	movapd	%xmm0, %xmm2
	addsd	88(%rsi), %xmm2
	movsd	%xmm2, (%rsp)
	movapd	%xmm0, %xmm3
	addsd	96(%rsi), %xmm3
	movsd	%xmm3, 8(%rsp)
	movapd	%xmm0, %xmm5
	addsd	104(%rsi), %xmm5
	movsd	%xmm5, 16(%rsp)
	movapd	%xmm0, %xmm6
	addsd	112(%rsi), %xmm6
	movsd	%xmm6, 24(%rsp)
	movapd	%xmm0, %xmm7
	addsd	120(%rsi), %xmm7
	movsd	%xmm7, 32(%rsp)
	movapd	%xmm0, %xmm2
	addsd	128(%rsi), %xmm2
	movsd	%xmm2, 40(%rsp)
	movapd	%xmm0, %xmm3
	addsd	136(%rsi), %xmm3
	movsd	%xmm3, 48(%rsp)
	addsd	144(%rsi), %xmm0
	movsd	%xmm0, 56(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1096
.L1097:
	addsd	%xmm4, %xmm1
	movsd	(%rsp), %xmm5
	addsd	%xmm4, %xmm5
	movsd	8(%rsp), %xmm3
	addsd	%xmm4, %xmm3
	movsd	16(%rsp), %xmm2
	addsd	%xmm4, %xmm2
	movsd	24(%rsp), %xmm0
	addsd	%xmm4, %xmm0
	movsd	32(%rsp), %xmm9
	addsd	%xmm4, %xmm9
	movsd	40(%rsp), %xmm8
	addsd	%xmm4, %xmm8
	movsd	48(%rsp), %xmm7
	addsd	%xmm4, %xmm7
	movsd	56(%rsp), %xmm6
	addsd	%xmm4, %xmm6
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	movsd	%xmm5, (%rsp)
	addsd	%xmm3, %xmm3
	movsd	%xmm3, 8(%rsp)
	addsd	%xmm2, %xmm2
	movsd	%xmm2, 16(%rsp)
	addsd	%xmm0, %xmm0
	movsd	%xmm0, 24(%rsp)
	addsd	%xmm9, %xmm9
	movsd	%xmm9, 32(%rsp)
	addsd	%xmm8, %xmm8
	movsd	%xmm8, 40(%rsp)
	addsd	%xmm7, %xmm7
	movsd	%xmm7, 48(%rsp)
	addsd	%xmm6, %xmm6
	movsd	%xmm6, 56(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1097
.L1096:
	cvttsd2sil	%xmm1, %edi
	call	use_int@PLT
	cvttsd2sil	(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE289:
	.size	double_add_8, .-double_add_8
	.globl	double_add_9
	.type	double_add_9, @function
double_add_9:
.LFB290:
	.cfi_startproc
	endbr64
	subq	$88, %rsp
	.cfi_def_cfa_offset 96
	movsd	.LC7(%rip), %xmm1
	movapd	%xmm1, %xmm0
	addsd	80(%rsi), %xmm0
	pxor	%xmm3, %xmm3
	cvtsi2sdl	8(%rsi), %xmm3
	movapd	%xmm1, %xmm2
	addsd	88(%rsi), %xmm2
	movsd	%xmm2, 8(%rsp)
	movapd	%xmm1, %xmm4
	addsd	96(%rsi), %xmm4
	movsd	%xmm4, 16(%rsp)
	movapd	%xmm1, %xmm5
	addsd	104(%rsi), %xmm5
	movsd	%xmm5, 24(%rsp)
	movapd	%xmm1, %xmm6
	addsd	112(%rsi), %xmm6
	movsd	%xmm6, 32(%rsp)
	movapd	%xmm1, %xmm7
	addsd	120(%rsi), %xmm7
	movsd	%xmm7, 40(%rsp)
	movapd	%xmm1, %xmm2
	addsd	128(%rsi), %xmm2
	movsd	%xmm2, 48(%rsp)
	movapd	%xmm1, %xmm4
	addsd	136(%rsi), %xmm4
	movsd	%xmm4, 56(%rsp)
	movapd	%xmm1, %xmm5
	addsd	144(%rsi), %xmm5
	movsd	%xmm5, 64(%rsp)
	addsd	152(%rsi), %xmm1
	movsd	%xmm1, 72(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1101
.L1102:
	addsd	%xmm3, %xmm0
	movsd	8(%rsp), %xmm1
	addsd	%xmm3, %xmm1
	movsd	16(%rsp), %xmm5
	addsd	%xmm3, %xmm5
	movsd	24(%rsp), %xmm4
	addsd	%xmm3, %xmm4
	movsd	32(%rsp), %xmm2
	addsd	%xmm3, %xmm2
	movsd	40(%rsp), %xmm10
	addsd	%xmm3, %xmm10
	movsd	48(%rsp), %xmm9
	addsd	%xmm3, %xmm9
	movsd	56(%rsp), %xmm8
	addsd	%xmm3, %xmm8
	movsd	64(%rsp), %xmm7
	addsd	%xmm3, %xmm7
	movsd	72(%rsp), %xmm6
	addsd	%xmm3, %xmm6
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm2, %xmm2
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm2, %xmm2
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm2, %xmm2
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm2, %xmm2
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm2, %xmm2
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm2, %xmm2
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm2, %xmm2
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm2, %xmm2
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm2, %xmm2
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm0, %xmm0
	addsd	%xmm1, %xmm1
	movsd	%xmm1, 8(%rsp)
	addsd	%xmm5, %xmm5
	movsd	%xmm5, 16(%rsp)
	addsd	%xmm4, %xmm4
	movsd	%xmm4, 24(%rsp)
	addsd	%xmm2, %xmm2
	movsd	%xmm2, 32(%rsp)
	addsd	%xmm10, %xmm10
	movsd	%xmm10, 40(%rsp)
	addsd	%xmm9, %xmm9
	movsd	%xmm9, 48(%rsp)
	addsd	%xmm8, %xmm8
	movsd	%xmm8, 56(%rsp)
	addsd	%xmm7, %xmm7
	movsd	%xmm7, 64(%rsp)
	addsd	%xmm6, %xmm6
	movsd	%xmm6, 72(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1102
.L1101:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE290:
	.size	double_add_9, .-double_add_9
	.globl	double_add_10
	.type	double_add_10, @function
double_add_10:
.LFB291:
	.cfi_startproc
	endbr64
	subq	$88, %rsp
	.cfi_def_cfa_offset 96
	movsd	.LC7(%rip), %xmm0
	movapd	%xmm0, %xmm7
	addsd	80(%rsi), %xmm7
	pxor	%xmm8, %xmm8
	cvtsi2sdl	8(%rsi), %xmm8
	movapd	%xmm0, %xmm2
	addsd	88(%rsi), %xmm2
	movsd	%xmm2, (%rsp)
	movapd	%xmm0, %xmm3
	addsd	96(%rsi), %xmm3
	movsd	%xmm3, 8(%rsp)
	movapd	%xmm0, %xmm4
	addsd	104(%rsi), %xmm4
	movsd	%xmm4, 16(%rsp)
	movapd	%xmm0, %xmm5
	addsd	112(%rsi), %xmm5
	movsd	%xmm5, 24(%rsp)
	movapd	%xmm0, %xmm6
	addsd	120(%rsi), %xmm6
	movsd	%xmm6, 32(%rsp)
	movapd	%xmm0, %xmm2
	addsd	128(%rsi), %xmm2
	movsd	%xmm2, 40(%rsp)
	movapd	%xmm0, %xmm3
	addsd	136(%rsi), %xmm3
	movsd	%xmm3, 48(%rsp)
	movapd	%xmm0, %xmm4
	addsd	144(%rsi), %xmm4
	movsd	%xmm4, 56(%rsp)
	movapd	%xmm0, %xmm5
	addsd	152(%rsi), %xmm5
	movsd	%xmm5, 64(%rsp)
	addsd	160(%rsi), %xmm0
	movsd	%xmm0, 72(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1106
.L1107:
	addsd	%xmm8, %xmm7
	movsd	(%rsp), %xmm3
	addsd	%xmm8, %xmm3
	movsd	8(%rsp), %xmm2
	addsd	%xmm8, %xmm2
	movsd	16(%rsp), %xmm1
	addsd	%xmm8, %xmm1
	movsd	24(%rsp), %xmm0
	addsd	%xmm8, %xmm0
	movsd	32(%rsp), %xmm11
	addsd	%xmm8, %xmm11
	movsd	40(%rsp), %xmm10
	addsd	%xmm8, %xmm10
	movsd	48(%rsp), %xmm9
	addsd	%xmm8, %xmm9
	movsd	56(%rsp), %xmm6
	addsd	%xmm8, %xmm6
	movsd	64(%rsp), %xmm5
	addsd	%xmm8, %xmm5
	movsd	72(%rsp), %xmm4
	addsd	%xmm8, %xmm4
	addsd	%xmm7, %xmm7
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm7, %xmm7
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm7, %xmm7
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	movapd	%xmm1, %xmm13
	addsd	%xmm0, %xmm0
	movapd	%xmm0, %xmm12
	movapd	%xmm11, %xmm1
	addsd	%xmm11, %xmm1
	movapd	%xmm10, %xmm0
	addsd	%xmm10, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm7, %xmm7
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm7, %xmm7
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm7, %xmm7
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm7, %xmm7
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm7, %xmm7
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm7, %xmm7
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm7, %xmm7
	addsd	%xmm3, %xmm3
	movsd	%xmm3, (%rsp)
	addsd	%xmm2, %xmm2
	movsd	%xmm2, 8(%rsp)
	addsd	%xmm13, %xmm13
	movsd	%xmm13, 16(%rsp)
	addsd	%xmm12, %xmm12
	movsd	%xmm12, 24(%rsp)
	addsd	%xmm1, %xmm1
	movsd	%xmm1, 32(%rsp)
	addsd	%xmm0, %xmm0
	movsd	%xmm0, 40(%rsp)
	addsd	%xmm9, %xmm9
	movsd	%xmm9, 48(%rsp)
	addsd	%xmm6, %xmm6
	movsd	%xmm6, 56(%rsp)
	addsd	%xmm5, %xmm5
	movsd	%xmm5, 64(%rsp)
	addsd	%xmm4, %xmm4
	movsd	%xmm4, 72(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1107
.L1106:
	cvttsd2sil	%xmm7, %edi
	call	use_int@PLT
	cvttsd2sil	(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE291:
	.size	double_add_10, .-double_add_10
	.globl	double_add_11
	.type	double_add_11, @function
double_add_11:
.LFB292:
	.cfi_startproc
	endbr64
	subq	$104, %rsp
	.cfi_def_cfa_offset 112
	movsd	.LC7(%rip), %xmm0
	movapd	%xmm0, %xmm10
	addsd	80(%rsi), %xmm10
	pxor	%xmm8, %xmm8
	cvtsi2sdl	8(%rsi), %xmm8
	movapd	%xmm0, %xmm1
	addsd	88(%rsi), %xmm1
	movsd	%xmm1, 8(%rsp)
	movapd	%xmm0, %xmm2
	addsd	96(%rsi), %xmm2
	movsd	%xmm2, 16(%rsp)
	movapd	%xmm0, %xmm3
	addsd	104(%rsi), %xmm3
	movsd	%xmm3, 24(%rsp)
	movapd	%xmm0, %xmm4
	addsd	112(%rsi), %xmm4
	movsd	%xmm4, 32(%rsp)
	movapd	%xmm0, %xmm5
	addsd	120(%rsi), %xmm5
	movsd	%xmm5, 40(%rsp)
	movapd	%xmm0, %xmm6
	addsd	128(%rsi), %xmm6
	movsd	%xmm6, 48(%rsp)
	movapd	%xmm0, %xmm1
	addsd	136(%rsi), %xmm1
	movsd	%xmm1, 56(%rsp)
	movapd	%xmm0, %xmm2
	addsd	144(%rsi), %xmm2
	movsd	%xmm2, 64(%rsp)
	movapd	%xmm0, %xmm3
	addsd	152(%rsi), %xmm3
	movsd	%xmm3, 72(%rsp)
	movapd	%xmm0, %xmm4
	addsd	160(%rsi), %xmm4
	movsd	%xmm4, 80(%rsp)
	addsd	168(%rsi), %xmm0
	movsd	%xmm0, 88(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1111
.L1112:
	addsd	%xmm8, %xmm10
	movsd	8(%rsp), %xmm2
	addsd	%xmm8, %xmm2
	movsd	16(%rsp), %xmm1
	addsd	%xmm8, %xmm1
	movsd	24(%rsp), %xmm0
	addsd	%xmm8, %xmm0
	movsd	32(%rsp), %xmm12
	addsd	%xmm8, %xmm12
	movsd	40(%rsp), %xmm11
	addsd	%xmm8, %xmm11
	movsd	48(%rsp), %xmm9
	addsd	%xmm8, %xmm9
	movsd	56(%rsp), %xmm6
	addsd	%xmm8, %xmm6
	movsd	64(%rsp), %xmm13
	addsd	%xmm8, %xmm13
	movsd	72(%rsp), %xmm5
	addsd	%xmm8, %xmm5
	movsd	80(%rsp), %xmm4
	addsd	%xmm8, %xmm4
	movsd	88(%rsp), %xmm3
	addsd	%xmm8, %xmm3
	addsd	%xmm10, %xmm10
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm13, %xmm13
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm13, %xmm13
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm7, %xmm7
	movapd	%xmm7, %xmm10
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm12, %xmm12
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm13, %xmm13
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm10, %xmm10
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm12, %xmm12
	addsd	%xmm7, %xmm7
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm13, %xmm13
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm10, %xmm10
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm12, %xmm12
	addsd	%xmm7, %xmm7
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm13, %xmm13
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm10, %xmm10
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm12, %xmm12
	addsd	%xmm7, %xmm7
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm13, %xmm13
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm10, %xmm10
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm12, %xmm12
	addsd	%xmm7, %xmm7
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm13, %xmm13
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm10, %xmm10
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm12, %xmm12
	addsd	%xmm7, %xmm7
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm13, %xmm13
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm10, %xmm10
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm12, %xmm12
	addsd	%xmm7, %xmm7
	addsd	%xmm9, %xmm9
	addsd	%xmm6, %xmm6
	addsd	%xmm13, %xmm13
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm10, %xmm10
	addsd	%xmm2, %xmm2
	movsd	%xmm2, 8(%rsp)
	addsd	%xmm1, %xmm1
	movsd	%xmm1, 16(%rsp)
	addsd	%xmm0, %xmm0
	movsd	%xmm0, 24(%rsp)
	addsd	%xmm12, %xmm12
	movsd	%xmm12, 32(%rsp)
	addsd	%xmm7, %xmm7
	movsd	%xmm7, 40(%rsp)
	addsd	%xmm9, %xmm9
	movsd	%xmm9, 48(%rsp)
	addsd	%xmm6, %xmm6
	movsd	%xmm6, 56(%rsp)
	addsd	%xmm13, %xmm13
	movsd	%xmm13, 64(%rsp)
	addsd	%xmm5, %xmm5
	movsd	%xmm5, 72(%rsp)
	addsd	%xmm4, %xmm4
	movsd	%xmm4, 80(%rsp)
	addsd	%xmm3, %xmm3
	movsd	%xmm3, 88(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1112
.L1111:
	cvttsd2sil	%xmm10, %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE292:
	.size	double_add_11, .-double_add_11
	.globl	double_add_12
	.type	double_add_12, @function
double_add_12:
.LFB293:
	.cfi_startproc
	endbr64
	subq	$104, %rsp
	.cfi_def_cfa_offset 112
	movsd	.LC7(%rip), %xmm0
	movapd	%xmm0, %xmm7
	addsd	80(%rsi), %xmm7
	pxor	%xmm6, %xmm6
	cvtsi2sdl	8(%rsi), %xmm6
	movapd	%xmm0, %xmm1
	addsd	88(%rsi), %xmm1
	movsd	%xmm1, (%rsp)
	movapd	%xmm0, %xmm2
	addsd	96(%rsi), %xmm2
	movsd	%xmm2, 8(%rsp)
	movapd	%xmm0, %xmm3
	addsd	104(%rsi), %xmm3
	movsd	%xmm3, 16(%rsp)
	movapd	%xmm0, %xmm4
	addsd	112(%rsi), %xmm4
	movsd	%xmm4, 24(%rsp)
	movapd	%xmm0, %xmm5
	addsd	120(%rsi), %xmm5
	movsd	%xmm5, 32(%rsp)
	movapd	%xmm0, %xmm1
	addsd	128(%rsi), %xmm1
	movsd	%xmm1, 40(%rsp)
	movapd	%xmm0, %xmm2
	addsd	136(%rsi), %xmm2
	movsd	%xmm2, 48(%rsp)
	movapd	%xmm0, %xmm3
	addsd	144(%rsi), %xmm3
	movsd	%xmm3, 56(%rsp)
	movapd	%xmm0, %xmm4
	addsd	152(%rsi), %xmm4
	movsd	%xmm4, 64(%rsp)
	movapd	%xmm0, %xmm5
	addsd	160(%rsi), %xmm5
	movsd	%xmm5, 72(%rsp)
	movapd	%xmm0, %xmm1
	addsd	168(%rsi), %xmm1
	movsd	%xmm1, 80(%rsp)
	addsd	176(%rsi), %xmm0
	movsd	%xmm0, 88(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1116
.L1117:
	addsd	%xmm6, %xmm7
	movsd	(%rsp), %xmm1
	addsd	%xmm6, %xmm1
	movsd	8(%rsp), %xmm0
	addsd	%xmm6, %xmm0
	movsd	16(%rsp), %xmm5
	addsd	%xmm6, %xmm5
	movsd	24(%rsp), %xmm4
	addsd	%xmm6, %xmm4
	movsd	32(%rsp), %xmm13
	addsd	%xmm6, %xmm13
	movsd	40(%rsp), %xmm12
	addsd	%xmm6, %xmm12
	movsd	48(%rsp), %xmm11
	addsd	%xmm6, %xmm11
	movsd	56(%rsp), %xmm10
	addsd	%xmm6, %xmm10
	movsd	64(%rsp), %xmm9
	addsd	%xmm6, %xmm9
	movsd	72(%rsp), %xmm8
	addsd	%xmm6, %xmm8
	movsd	80(%rsp), %xmm3
	addsd	%xmm6, %xmm3
	movsd	88(%rsp), %xmm2
	addsd	%xmm6, %xmm2
	addsd	%xmm7, %xmm7
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm7, %xmm7
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm7, %xmm7
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm7, %xmm7
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm7, %xmm7
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm7, %xmm7
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm7, %xmm7
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm7, %xmm7
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm7, %xmm7
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm7, %xmm7
	addsd	%xmm1, %xmm1
	movsd	%xmm1, (%rsp)
	addsd	%xmm0, %xmm0
	movsd	%xmm0, 8(%rsp)
	addsd	%xmm5, %xmm5
	movsd	%xmm5, 16(%rsp)
	addsd	%xmm4, %xmm4
	movsd	%xmm4, 24(%rsp)
	addsd	%xmm13, %xmm13
	movsd	%xmm13, 32(%rsp)
	addsd	%xmm12, %xmm12
	movsd	%xmm12, 40(%rsp)
	addsd	%xmm11, %xmm11
	movsd	%xmm11, 48(%rsp)
	addsd	%xmm10, %xmm10
	movsd	%xmm10, 56(%rsp)
	addsd	%xmm9, %xmm9
	movsd	%xmm9, 64(%rsp)
	addsd	%xmm8, %xmm8
	movsd	%xmm8, 72(%rsp)
	addsd	%xmm3, %xmm3
	movsd	%xmm3, 80(%rsp)
	addsd	%xmm2, %xmm2
	movsd	%xmm2, 88(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1117
.L1116:
	cvttsd2sil	%xmm7, %edi
	call	use_int@PLT
	cvttsd2sil	(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE293:
	.size	double_add_12, .-double_add_12
	.globl	double_add_13
	.type	double_add_13, @function
double_add_13:
.LFB294:
	.cfi_startproc
	endbr64
	subq	$120, %rsp
	.cfi_def_cfa_offset 128
	movsd	.LC7(%rip), %xmm0
	movapd	%xmm0, %xmm10
	addsd	80(%rsi), %xmm10
	pxor	%xmm13, %xmm13
	cvtsi2sdl	8(%rsi), %xmm13
	movapd	%xmm0, %xmm1
	addsd	88(%rsi), %xmm1
	movsd	%xmm1, 8(%rsp)
	movapd	%xmm0, %xmm2
	addsd	96(%rsi), %xmm2
	movsd	%xmm2, 16(%rsp)
	movapd	%xmm0, %xmm3
	addsd	104(%rsi), %xmm3
	movsd	%xmm3, 24(%rsp)
	movapd	%xmm0, %xmm4
	addsd	112(%rsi), %xmm4
	movsd	%xmm4, 32(%rsp)
	movapd	%xmm0, %xmm5
	addsd	120(%rsi), %xmm5
	movsd	%xmm5, 40(%rsp)
	movapd	%xmm0, %xmm6
	addsd	128(%rsi), %xmm6
	movsd	%xmm6, 48(%rsp)
	movapd	%xmm0, %xmm7
	addsd	136(%rsi), %xmm7
	movsd	%xmm7, 56(%rsp)
	movapd	%xmm0, %xmm1
	addsd	144(%rsi), %xmm1
	movsd	%xmm1, 64(%rsp)
	movapd	%xmm0, %xmm2
	addsd	152(%rsi), %xmm2
	movsd	%xmm2, 72(%rsp)
	movapd	%xmm0, %xmm3
	addsd	160(%rsi), %xmm3
	movsd	%xmm3, 80(%rsp)
	movapd	%xmm0, %xmm4
	addsd	168(%rsi), %xmm4
	movsd	%xmm4, 88(%rsp)
	movapd	%xmm0, %xmm5
	addsd	176(%rsi), %xmm5
	movsd	%xmm5, 96(%rsp)
	addsd	184(%rsi), %xmm0
	movsd	%xmm0, 104(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1121
.L1122:
	addsd	%xmm13, %xmm10
	movsd	8(%rsp), %xmm0
	addsd	%xmm13, %xmm0
	movsd	16(%rsp), %xmm9
	addsd	%xmm13, %xmm9
	movsd	24(%rsp), %xmm8
	addsd	%xmm13, %xmm8
	movsd	32(%rsp), %xmm7
	addsd	%xmm13, %xmm7
	movsd	40(%rsp), %xmm6
	addsd	%xmm13, %xmm6
	movsd	48(%rsp), %xmm5
	addsd	%xmm13, %xmm5
	movsd	56(%rsp), %xmm4
	addsd	%xmm13, %xmm4
	movsd	64(%rsp), %xmm3
	addsd	%xmm13, %xmm3
	movsd	72(%rsp), %xmm2
	addsd	%xmm13, %xmm2
	movsd	80(%rsp), %xmm14
	addsd	%xmm13, %xmm14
	movsd	88(%rsp), %xmm12
	addsd	%xmm13, %xmm12
	movsd	96(%rsp), %xmm11
	addsd	%xmm13, %xmm11
	movsd	104(%rsp), %xmm1
	addsd	%xmm13, %xmm1
	addsd	%xmm10, %xmm10
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm14, %xmm14
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm1, %xmm1
	addsd	%xmm10, %xmm10
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm14, %xmm14
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm1, %xmm1
	addsd	%xmm10, %xmm10
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm14, %xmm14
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm1, %xmm1
	addsd	%xmm10, %xmm10
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm14, %xmm14
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm1, %xmm1
	addsd	%xmm10, %xmm10
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm14, %xmm14
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm1, %xmm1
	addsd	%xmm10, %xmm10
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm14, %xmm14
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm1, %xmm1
	addsd	%xmm10, %xmm10
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm14, %xmm14
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm1, %xmm1
	addsd	%xmm10, %xmm10
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm14, %xmm14
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm1, %xmm1
	addsd	%xmm10, %xmm10
	addsd	%xmm0, %xmm0
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm14, %xmm14
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm1, %xmm1
	addsd	%xmm10, %xmm10
	addsd	%xmm0, %xmm0
	movsd	%xmm0, 8(%rsp)
	addsd	%xmm9, %xmm9
	movsd	%xmm9, 16(%rsp)
	addsd	%xmm8, %xmm8
	movsd	%xmm8, 24(%rsp)
	addsd	%xmm7, %xmm7
	movsd	%xmm7, 32(%rsp)
	addsd	%xmm6, %xmm6
	movsd	%xmm6, 40(%rsp)
	addsd	%xmm5, %xmm5
	movsd	%xmm5, 48(%rsp)
	addsd	%xmm4, %xmm4
	movsd	%xmm4, 56(%rsp)
	addsd	%xmm3, %xmm3
	movsd	%xmm3, 64(%rsp)
	addsd	%xmm2, %xmm2
	movsd	%xmm2, 72(%rsp)
	addsd	%xmm14, %xmm14
	movsd	%xmm14, 80(%rsp)
	addsd	%xmm12, %xmm12
	movsd	%xmm12, 88(%rsp)
	addsd	%xmm11, %xmm11
	movsd	%xmm11, 96(%rsp)
	addsd	%xmm1, %xmm1
	movsd	%xmm1, 104(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1122
.L1121:
	cvttsd2sil	%xmm10, %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	addq	$120, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE294:
	.size	double_add_13, .-double_add_13
	.globl	double_add_14
	.type	double_add_14, @function
double_add_14:
.LFB295:
	.cfi_startproc
	endbr64
	subq	$120, %rsp
	.cfi_def_cfa_offset 128
	movsd	.LC7(%rip), %xmm0
	movapd	%xmm0, %xmm6
	addsd	80(%rsi), %xmm6
	pxor	%xmm15, %xmm15
	cvtsi2sdl	8(%rsi), %xmm15
	movapd	%xmm0, %xmm1
	addsd	88(%rsi), %xmm1
	movsd	%xmm1, (%rsp)
	movapd	%xmm0, %xmm2
	addsd	96(%rsi), %xmm2
	movsd	%xmm2, 8(%rsp)
	movapd	%xmm0, %xmm3
	addsd	104(%rsi), %xmm3
	movsd	%xmm3, 16(%rsp)
	movapd	%xmm0, %xmm4
	addsd	112(%rsi), %xmm4
	movsd	%xmm4, 24(%rsp)
	movapd	%xmm0, %xmm5
	addsd	120(%rsi), %xmm5
	movsd	%xmm5, 32(%rsp)
	movapd	%xmm0, %xmm7
	addsd	128(%rsi), %xmm7
	movsd	%xmm7, 40(%rsp)
	movapd	%xmm0, %xmm1
	addsd	136(%rsi), %xmm1
	movsd	%xmm1, 48(%rsp)
	movapd	%xmm0, %xmm2
	addsd	144(%rsi), %xmm2
	movsd	%xmm2, 56(%rsp)
	movapd	%xmm0, %xmm3
	addsd	152(%rsi), %xmm3
	movsd	%xmm3, 64(%rsp)
	movapd	%xmm0, %xmm4
	addsd	160(%rsi), %xmm4
	movsd	%xmm4, 72(%rsp)
	movapd	%xmm0, %xmm5
	addsd	168(%rsi), %xmm5
	movsd	%xmm5, 80(%rsp)
	movapd	%xmm0, %xmm7
	addsd	176(%rsi), %xmm7
	movsd	%xmm7, 88(%rsp)
	movapd	%xmm0, %xmm1
	addsd	184(%rsi), %xmm1
	movsd	%xmm1, 96(%rsp)
	addsd	192(%rsi), %xmm0
	movsd	%xmm0, 104(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1126
.L1127:
	addsd	%xmm15, %xmm6
	movsd	(%rsp), %xmm5
	addsd	%xmm15, %xmm5
	movsd	8(%rsp), %xmm4
	addsd	%xmm15, %xmm4
	movsd	16(%rsp), %xmm3
	addsd	%xmm15, %xmm3
	movsd	24(%rsp), %xmm2
	addsd	%xmm15, %xmm2
	movsd	32(%rsp), %xmm1
	addsd	%xmm15, %xmm1
	movsd	40(%rsp), %xmm0
	addsd	%xmm15, %xmm0
	movsd	48(%rsp), %xmm14
	addsd	%xmm15, %xmm14
	movsd	56(%rsp), %xmm13
	addsd	%xmm15, %xmm13
	movsd	64(%rsp), %xmm12
	addsd	%xmm15, %xmm12
	movsd	72(%rsp), %xmm11
	addsd	%xmm15, %xmm11
	movsd	80(%rsp), %xmm10
	addsd	%xmm15, %xmm10
	movsd	88(%rsp), %xmm9
	addsd	%xmm15, %xmm9
	movsd	96(%rsp), %xmm8
	addsd	%xmm15, %xmm8
	movsd	104(%rsp), %xmm7
	addsd	%xmm15, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm5, %xmm5
	movsd	%xmm5, (%rsp)
	addsd	%xmm4, %xmm4
	movsd	%xmm4, 8(%rsp)
	addsd	%xmm3, %xmm3
	movsd	%xmm3, 16(%rsp)
	addsd	%xmm2, %xmm2
	movsd	%xmm2, 24(%rsp)
	addsd	%xmm1, %xmm1
	movsd	%xmm1, 32(%rsp)
	addsd	%xmm0, %xmm0
	movsd	%xmm0, 40(%rsp)
	addsd	%xmm14, %xmm14
	movsd	%xmm14, 48(%rsp)
	addsd	%xmm13, %xmm13
	movsd	%xmm13, 56(%rsp)
	addsd	%xmm12, %xmm12
	movsd	%xmm12, 64(%rsp)
	addsd	%xmm11, %xmm11
	movsd	%xmm11, 72(%rsp)
	addsd	%xmm10, %xmm10
	movsd	%xmm10, 80(%rsp)
	addsd	%xmm9, %xmm9
	movsd	%xmm9, 88(%rsp)
	addsd	%xmm8, %xmm8
	movsd	%xmm8, 96(%rsp)
	addsd	%xmm7, %xmm7
	movsd	%xmm7, 104(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1127
.L1126:
	cvttsd2sil	%xmm6, %edi
	call	use_int@PLT
	cvttsd2sil	(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	addq	$120, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE295:
	.size	double_add_14, .-double_add_14
	.globl	double_add_15
	.type	double_add_15, @function
double_add_15:
.LFB296:
	.cfi_startproc
	endbr64
	subq	$136, %rsp
	.cfi_def_cfa_offset 144
	movsd	.LC7(%rip), %xmm0
	movapd	%xmm0, %xmm5
	addsd	80(%rsi), %xmm5
	pxor	%xmm1, %xmm1
	cvtsi2sdl	8(%rsi), %xmm1
	movsd	%xmm1, (%rsp)
	movapd	%xmm0, %xmm15
	addsd	88(%rsi), %xmm15
	movsd	%xmm15, 8(%rsp)
	movapd	%xmm0, %xmm14
	addsd	96(%rsi), %xmm14
	movsd	%xmm14, 16(%rsp)
	movapd	%xmm0, %xmm13
	addsd	104(%rsi), %xmm13
	movsd	%xmm13, 24(%rsp)
	movapd	%xmm0, %xmm12
	addsd	112(%rsi), %xmm12
	movsd	%xmm12, 32(%rsp)
	movapd	%xmm0, %xmm11
	addsd	120(%rsi), %xmm11
	movsd	%xmm11, 40(%rsp)
	movapd	%xmm0, %xmm10
	addsd	128(%rsi), %xmm10
	movsd	%xmm10, 48(%rsp)
	movapd	%xmm0, %xmm9
	addsd	136(%rsi), %xmm9
	movsd	%xmm9, 56(%rsp)
	movapd	%xmm0, %xmm8
	addsd	144(%rsi), %xmm8
	movsd	%xmm8, 64(%rsp)
	movapd	%xmm0, %xmm7
	addsd	152(%rsi), %xmm7
	movsd	%xmm7, 72(%rsp)
	movapd	%xmm0, %xmm6
	addsd	160(%rsi), %xmm6
	movsd	%xmm6, 80(%rsp)
	movapd	%xmm0, %xmm4
	addsd	168(%rsi), %xmm4
	movsd	%xmm4, 88(%rsp)
	movapd	%xmm0, %xmm3
	addsd	176(%rsi), %xmm3
	movsd	%xmm3, 96(%rsp)
	movapd	%xmm0, %xmm2
	addsd	184(%rsi), %xmm2
	movsd	%xmm2, 104(%rsp)
	movapd	%xmm0, %xmm1
	addsd	192(%rsi), %xmm1
	movsd	%xmm1, 112(%rsp)
	addsd	200(%rsi), %xmm0
	movsd	%xmm0, 120(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1131
.L1132:
	addsd	(%rsp), %xmm5
	addsd	(%rsp), %xmm15
	addsd	(%rsp), %xmm14
	addsd	(%rsp), %xmm13
	addsd	(%rsp), %xmm12
	addsd	(%rsp), %xmm11
	addsd	(%rsp), %xmm10
	addsd	(%rsp), %xmm9
	addsd	(%rsp), %xmm8
	addsd	(%rsp), %xmm7
	addsd	(%rsp), %xmm6
	addsd	(%rsp), %xmm4
	addsd	(%rsp), %xmm3
	addsd	(%rsp), %xmm2
	addsd	(%rsp), %xmm1
	addsd	(%rsp), %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm15, %xmm15
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm15, %xmm15
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm15, %xmm15
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm15, %xmm15
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm15, %xmm15
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm15, %xmm15
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm15, %xmm15
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm15, %xmm15
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm15, %xmm15
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	addsd	%xmm5, %xmm5
	addsd	%xmm15, %xmm15
	addsd	%xmm14, %xmm14
	addsd	%xmm13, %xmm13
	addsd	%xmm12, %xmm12
	addsd	%xmm11, %xmm11
	addsd	%xmm10, %xmm10
	addsd	%xmm9, %xmm9
	addsd	%xmm8, %xmm8
	addsd	%xmm7, %xmm7
	addsd	%xmm6, %xmm6
	addsd	%xmm4, %xmm4
	addsd	%xmm3, %xmm3
	addsd	%xmm2, %xmm2
	addsd	%xmm1, %xmm1
	addsd	%xmm0, %xmm0
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1132
	movsd	%xmm15, 8(%rsp)
	movsd	%xmm14, 16(%rsp)
	movsd	%xmm13, 24(%rsp)
	movsd	%xmm12, 32(%rsp)
	movsd	%xmm11, 40(%rsp)
	movsd	%xmm10, 48(%rsp)
	movsd	%xmm9, 56(%rsp)
	movsd	%xmm8, 64(%rsp)
	movsd	%xmm7, 72(%rsp)
	movsd	%xmm6, 80(%rsp)
	movsd	%xmm4, 88(%rsp)
	movsd	%xmm3, 96(%rsp)
	movsd	%xmm2, 104(%rsp)
	movsd	%xmm1, 112(%rsp)
	movsd	%xmm0, 120(%rsp)
.L1131:
	cvttsd2sil	%xmm5, %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	120(%rsp), %edi
	call	use_int@PLT
	addq	$136, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE296:
	.size	double_add_15, .-double_add_15
	.globl	double_mul_0
	.type	double_mul_0, @function
double_mul_0:
.LFB297:
	.cfi_startproc
	endbr64
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	movsd	80(%rsi), %xmm2
	movapd	%xmm2, %xmm0
	mulsd	.LC8(%rip), %xmm0
	pxor	%xmm1, %xmm1
	cvtsi2sdl	4(%rsi), %xmm1
	mulsd	.LC3(%rip), %xmm1
	mulsd	%xmm2, %xmm1
	divsd	.LC4(%rip), %xmm1
	movsd	%xmm1, 8(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1136
.L1137:
	mulsd	%xmm0, %xmm0
	movsd	8(%rsp), %xmm3
	mulsd	%xmm3, %xmm0
	mulsd	%xmm0, %xmm0
	mulsd	%xmm3, %xmm0
	mulsd	%xmm0, %xmm0
	mulsd	%xmm3, %xmm0
	mulsd	%xmm0, %xmm0
	mulsd	%xmm3, %xmm0
	mulsd	%xmm0, %xmm0
	mulsd	%xmm3, %xmm0
	mulsd	%xmm0, %xmm0
	mulsd	%xmm3, %xmm0
	mulsd	%xmm0, %xmm0
	mulsd	%xmm3, %xmm0
	mulsd	%xmm0, %xmm0
	mulsd	%xmm3, %xmm0
	mulsd	%xmm0, %xmm0
	mulsd	%xmm3, %xmm0
	mulsd	%xmm0, %xmm0
	mulsd	%xmm3, %xmm0
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1137
.L1136:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE297:
	.size	double_mul_0, .-double_mul_0
	.globl	double_mul_1
	.type	double_mul_1, @function
double_mul_1:
.LFB298:
	.cfi_startproc
	endbr64
	subq	$40, %rsp
	.cfi_def_cfa_offset 48
	movsd	80(%rsi), %xmm2
	movsd	.LC8(%rip), %xmm4
	movapd	%xmm2, %xmm0
	mulsd	%xmm4, %xmm0
	pxor	%xmm1, %xmm1
	cvtsi2sdl	4(%rsi), %xmm1
	mulsd	.LC3(%rip), %xmm1
	mulsd	%xmm1, %xmm2
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm2
	movsd	%xmm2, 16(%rsp)
	movsd	88(%rsi), %xmm2
	mulsd	%xmm2, %xmm4
	movsd	%xmm4, 8(%rsp)
	mulsd	%xmm2, %xmm1
	divsd	%xmm3, %xmm1
	movsd	%xmm1, 24(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1141
.L1142:
	mulsd	%xmm0, %xmm0
	movsd	16(%rsp), %xmm6
	mulsd	%xmm6, %xmm0
	movsd	8(%rsp), %xmm1
	mulsd	%xmm1, %xmm1
	movsd	24(%rsp), %xmm7
	mulsd	%xmm7, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm1, %xmm1
	mulsd	%xmm7, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm1, %xmm1
	mulsd	%xmm7, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm1, %xmm1
	mulsd	%xmm7, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm1, %xmm1
	mulsd	%xmm7, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm1, %xmm1
	mulsd	%xmm7, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm1, %xmm1
	mulsd	%xmm7, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm1, %xmm1
	mulsd	%xmm7, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm1, %xmm1
	mulsd	%xmm7, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm1, %xmm1
	mulsd	%xmm7, %xmm1
	movsd	%xmm1, 8(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1142
.L1141:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE298:
	.size	double_mul_1, .-double_mul_1
	.globl	double_mul_2
	.type	double_mul_2, @function
double_mul_2:
.LFB299:
	.cfi_startproc
	endbr64
	subq	$56, %rsp
	.cfi_def_cfa_offset 64
	movsd	80(%rsi), %xmm2
	movsd	.LC8(%rip), %xmm4
	movapd	%xmm2, %xmm0
	mulsd	%xmm4, %xmm0
	pxor	%xmm1, %xmm1
	cvtsi2sdl	4(%rsi), %xmm1
	mulsd	.LC3(%rip), %xmm1
	mulsd	%xmm1, %xmm2
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm2
	movsd	%xmm2, 24(%rsp)
	movsd	88(%rsi), %xmm2
	movapd	%xmm2, %xmm5
	mulsd	%xmm4, %xmm5
	movsd	%xmm5, 8(%rsp)
	mulsd	%xmm1, %xmm2
	divsd	%xmm3, %xmm2
	movsd	%xmm2, 32(%rsp)
	movsd	96(%rsi), %xmm2
	mulsd	%xmm2, %xmm4
	movsd	%xmm4, 16(%rsp)
	mulsd	%xmm2, %xmm1
	divsd	%xmm3, %xmm1
	movsd	%xmm1, 40(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1146
.L1147:
	mulsd	%xmm0, %xmm0
	movsd	24(%rsp), %xmm6
	mulsd	%xmm6, %xmm0
	movsd	8(%rsp), %xmm2
	mulsd	%xmm2, %xmm2
	movsd	32(%rsp), %xmm7
	mulsd	%xmm7, %xmm2
	movsd	16(%rsp), %xmm1
	mulsd	%xmm1, %xmm1
	movsd	40(%rsp), %xmm4
	mulsd	%xmm4, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	%xmm7, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm4, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	%xmm7, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm4, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	%xmm7, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm4, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	%xmm7, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm4, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	%xmm7, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm4, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	%xmm7, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm4, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	%xmm7, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm4, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	%xmm7, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm4, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	%xmm7, %xmm2
	movsd	%xmm2, 8(%rsp)
	mulsd	%xmm1, %xmm1
	mulsd	%xmm4, %xmm1
	movsd	%xmm1, 16(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1147
.L1146:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE299:
	.size	double_mul_2, .-double_mul_2
	.globl	double_mul_3
	.type	double_mul_3, @function
double_mul_3:
.LFB300:
	.cfi_startproc
	endbr64
	subq	$72, %rsp
	.cfi_def_cfa_offset 80
	movsd	80(%rsi), %xmm2
	movsd	.LC8(%rip), %xmm3
	movapd	%xmm2, %xmm0
	mulsd	%xmm3, %xmm0
	pxor	%xmm1, %xmm1
	cvtsi2sdl	4(%rsi), %xmm1
	mulsd	.LC3(%rip), %xmm1
	movapd	%xmm2, %xmm4
	mulsd	%xmm1, %xmm4
	movsd	.LC4(%rip), %xmm2
	divsd	%xmm2, %xmm4
	movsd	%xmm4, 32(%rsp)
	movsd	88(%rsi), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm3, %xmm6
	movsd	%xmm6, 8(%rsp)
	mulsd	%xmm1, %xmm4
	divsd	%xmm2, %xmm4
	movsd	%xmm4, 40(%rsp)
	movsd	96(%rsi), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm3, %xmm6
	movsd	%xmm6, 16(%rsp)
	mulsd	%xmm1, %xmm4
	divsd	%xmm2, %xmm4
	movsd	%xmm4, 48(%rsp)
	movsd	104(%rsi), %xmm4
	mulsd	%xmm4, %xmm3
	movsd	%xmm3, 24(%rsp)
	mulsd	%xmm4, %xmm1
	divsd	%xmm2, %xmm1
	movsd	%xmm1, 56(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1151
.L1152:
	mulsd	%xmm0, %xmm0
	movsd	32(%rsp), %xmm6
	mulsd	%xmm6, %xmm0
	movsd	8(%rsp), %xmm3
	mulsd	%xmm3, %xmm3
	movsd	40(%rsp), %xmm4
	mulsd	%xmm4, %xmm3
	movsd	16(%rsp), %xmm2
	mulsd	%xmm2, %xmm2
	movsd	48(%rsp), %xmm8
	mulsd	%xmm8, %xmm2
	movsd	24(%rsp), %xmm1
	mulsd	%xmm1, %xmm1
	movsd	56(%rsp), %xmm9
	mulsd	%xmm9, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm3, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm8, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm9, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm3, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm8, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm9, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm3, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm8, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm9, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm3, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm8, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm9, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm3, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm8, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm9, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm3, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm8, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm9, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm3, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm8, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm9, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm3, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm8, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm9, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm3, %xmm3
	mulsd	%xmm4, %xmm3
	movsd	%xmm3, 8(%rsp)
	mulsd	%xmm2, %xmm2
	mulsd	%xmm8, %xmm2
	movsd	%xmm2, 16(%rsp)
	mulsd	%xmm1, %xmm1
	mulsd	%xmm9, %xmm1
	movsd	%xmm1, 24(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1152
.L1151:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE300:
	.size	double_mul_3, .-double_mul_3
	.globl	double_mul_4
	.type	double_mul_4, @function
double_mul_4:
.LFB301:
	.cfi_startproc
	endbr64
	subq	$88, %rsp
	.cfi_def_cfa_offset 96
	movsd	80(%rsi), %xmm2
	movsd	.LC8(%rip), %xmm3
	movapd	%xmm2, %xmm0
	mulsd	%xmm3, %xmm0
	pxor	%xmm1, %xmm1
	cvtsi2sdl	4(%rsi), %xmm1
	mulsd	.LC3(%rip), %xmm1
	mulsd	%xmm1, %xmm2
	movapd	%xmm2, %xmm4
	movsd	.LC4(%rip), %xmm2
	divsd	%xmm2, %xmm4
	movsd	%xmm4, 40(%rsp)
	movsd	88(%rsi), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm3, %xmm6
	movsd	%xmm6, 8(%rsp)
	mulsd	%xmm1, %xmm4
	divsd	%xmm2, %xmm4
	movsd	%xmm4, 48(%rsp)
	movsd	96(%rsi), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm3, %xmm6
	movsd	%xmm6, 16(%rsp)
	mulsd	%xmm1, %xmm4
	divsd	%xmm2, %xmm4
	movsd	%xmm4, 56(%rsp)
	movsd	104(%rsi), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm3, %xmm6
	movsd	%xmm6, 24(%rsp)
	mulsd	%xmm1, %xmm4
	divsd	%xmm2, %xmm4
	movsd	%xmm4, 64(%rsp)
	movsd	112(%rsi), %xmm4
	mulsd	%xmm4, %xmm3
	movsd	%xmm3, 32(%rsp)
	mulsd	%xmm4, %xmm1
	divsd	%xmm2, %xmm1
	movsd	%xmm1, 72(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1156
.L1157:
	mulsd	%xmm0, %xmm0
	movsd	40(%rsp), %xmm6
	mulsd	%xmm6, %xmm0
	movsd	8(%rsp), %xmm4
	mulsd	%xmm4, %xmm4
	movsd	48(%rsp), %xmm7
	mulsd	%xmm7, %xmm4
	movsd	16(%rsp), %xmm3
	mulsd	%xmm3, %xmm3
	movsd	56(%rsp), %xmm8
	mulsd	%xmm8, %xmm3
	movsd	24(%rsp), %xmm2
	mulsd	%xmm2, %xmm2
	movsd	64(%rsp), %xmm9
	mulsd	%xmm9, %xmm2
	movsd	32(%rsp), %xmm1
	mulsd	%xmm1, %xmm1
	movsd	72(%rsp), %xmm11
	mulsd	%xmm11, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm4, %xmm4
	mulsd	%xmm7, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm8, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm9, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm11, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm4, %xmm4
	mulsd	%xmm7, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm8, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm9, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm11, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm4, %xmm4
	mulsd	%xmm7, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm8, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm9, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm11, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm4, %xmm4
	mulsd	%xmm7, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm8, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm9, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm11, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm4, %xmm4
	mulsd	%xmm7, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm8, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm9, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm11, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm4, %xmm4
	mulsd	%xmm7, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm8, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm9, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm11, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm4, %xmm4
	mulsd	%xmm7, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm8, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm9, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm11, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm4, %xmm4
	mulsd	%xmm7, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm8, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm9, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm11, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm4, %xmm4
	mulsd	%xmm7, %xmm4
	movsd	%xmm4, 8(%rsp)
	mulsd	%xmm3, %xmm3
	mulsd	%xmm8, %xmm3
	movsd	%xmm3, 16(%rsp)
	mulsd	%xmm2, %xmm2
	mulsd	%xmm9, %xmm2
	movsd	%xmm2, 24(%rsp)
	mulsd	%xmm1, %xmm1
	mulsd	%xmm11, %xmm1
	movsd	%xmm1, 32(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1157
.L1156:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE301:
	.size	double_mul_4, .-double_mul_4
	.globl	double_mul_5
	.type	double_mul_5, @function
double_mul_5:
.LFB302:
	.cfi_startproc
	endbr64
	subq	$104, %rsp
	.cfi_def_cfa_offset 112
	movsd	80(%rsi), %xmm2
	movsd	.LC8(%rip), %xmm3
	movapd	%xmm2, %xmm0
	mulsd	%xmm3, %xmm0
	pxor	%xmm1, %xmm1
	cvtsi2sdl	4(%rsi), %xmm1
	mulsd	.LC3(%rip), %xmm1
	mulsd	%xmm1, %xmm2
	movapd	%xmm2, %xmm4
	movsd	.LC4(%rip), %xmm2
	divsd	%xmm2, %xmm4
	movsd	%xmm4, 48(%rsp)
	movsd	88(%rsi), %xmm4
	movapd	%xmm4, %xmm7
	mulsd	%xmm3, %xmm7
	movsd	%xmm7, 8(%rsp)
	mulsd	%xmm1, %xmm4
	divsd	%xmm2, %xmm4
	movsd	%xmm4, 56(%rsp)
	movsd	96(%rsi), %xmm4
	movapd	%xmm4, %xmm7
	mulsd	%xmm3, %xmm7
	movsd	%xmm7, 16(%rsp)
	mulsd	%xmm1, %xmm4
	divsd	%xmm2, %xmm4
	movsd	%xmm4, 64(%rsp)
	movsd	104(%rsi), %xmm4
	movapd	%xmm4, %xmm7
	mulsd	%xmm3, %xmm7
	movsd	%xmm7, 24(%rsp)
	mulsd	%xmm1, %xmm4
	divsd	%xmm2, %xmm4
	movsd	%xmm4, 72(%rsp)
	movsd	112(%rsi), %xmm4
	movapd	%xmm4, %xmm7
	mulsd	%xmm3, %xmm7
	movsd	%xmm7, 32(%rsp)
	mulsd	%xmm1, %xmm4
	divsd	%xmm2, %xmm4
	movsd	%xmm4, 80(%rsp)
	movsd	120(%rsi), %xmm4
	mulsd	%xmm4, %xmm3
	movsd	%xmm3, 40(%rsp)
	mulsd	%xmm4, %xmm1
	divsd	%xmm2, %xmm1
	movsd	%xmm1, 88(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1161
.L1162:
	mulsd	%xmm0, %xmm0
	movsd	48(%rsp), %xmm7
	mulsd	%xmm7, %xmm0
	movsd	8(%rsp), %xmm5
	mulsd	%xmm5, %xmm5
	movsd	56(%rsp), %xmm8
	mulsd	%xmm8, %xmm5
	movsd	16(%rsp), %xmm4
	mulsd	%xmm4, %xmm4
	movsd	64(%rsp), %xmm10
	mulsd	%xmm10, %xmm4
	movsd	24(%rsp), %xmm3
	mulsd	%xmm3, %xmm3
	movsd	72(%rsp), %xmm11
	mulsd	%xmm11, %xmm3
	movsd	32(%rsp), %xmm2
	mulsd	%xmm2, %xmm2
	movsd	80(%rsp), %xmm13
	mulsd	%xmm13, %xmm2
	movsd	40(%rsp), %xmm1
	mulsd	%xmm1, %xmm1
	movsd	88(%rsp), %xmm15
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	%xmm5, %xmm5
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm10, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm11, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm13, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	%xmm5, %xmm5
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm10, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm11, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm13, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	%xmm5, %xmm5
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm10, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm11, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm13, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	%xmm5, %xmm5
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm10, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm11, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm13, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	%xmm5, %xmm5
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm10, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm11, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm13, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	%xmm5, %xmm5
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm10, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm11, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm13, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	%xmm5, %xmm5
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm10, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm11, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm13, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	%xmm5, %xmm5
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm10, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm11, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm13, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	%xmm5, %xmm5
	mulsd	%xmm8, %xmm5
	movsd	%xmm5, 8(%rsp)
	mulsd	%xmm4, %xmm4
	mulsd	%xmm10, %xmm4
	movsd	%xmm4, 16(%rsp)
	mulsd	%xmm3, %xmm3
	mulsd	%xmm11, %xmm3
	movsd	%xmm3, 24(%rsp)
	mulsd	%xmm2, %xmm2
	mulsd	%xmm13, %xmm2
	movsd	%xmm2, 32(%rsp)
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	movsd	%xmm1, 40(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1162
.L1161:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE302:
	.size	double_mul_5, .-double_mul_5
	.globl	double_mul_6
	.type	double_mul_6, @function
double_mul_6:
.LFB303:
	.cfi_startproc
	endbr64
	subq	$120, %rsp
	.cfi_def_cfa_offset 128
	movsd	80(%rsi), %xmm3
	movsd	.LC8(%rip), %xmm1
	movapd	%xmm3, %xmm0
	mulsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2sdl	4(%rsi), %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 64(%rsp)
	movsd	88(%rsi), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm1, %xmm6
	movsd	%xmm6, 16(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 72(%rsp)
	movsd	96(%rsi), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm1, %xmm6
	movsd	%xmm6, 24(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 80(%rsp)
	movsd	104(%rsi), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm1, %xmm6
	movsd	%xmm6, 32(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 88(%rsp)
	movsd	112(%rsi), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm1, %xmm6
	movsd	%xmm6, 40(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 96(%rsp)
	movsd	120(%rsi), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm1, %xmm6
	movsd	%xmm6, 48(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 104(%rsp)
	movsd	128(%rsi), %xmm4
	mulsd	%xmm4, %xmm1
	movsd	%xmm1, 56(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	movsd	%xmm2, 8(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1166
.L1167:
	mulsd	%xmm0, %xmm0
	movsd	64(%rsp), %xmm10
	mulsd	%xmm10, %xmm0
	movsd	16(%rsp), %xmm3
	mulsd	%xmm3, %xmm3
	movsd	72(%rsp), %xmm11
	mulsd	%xmm11, %xmm3
	movsd	24(%rsp), %xmm2
	mulsd	%xmm2, %xmm2
	movsd	80(%rsp), %xmm12
	mulsd	%xmm12, %xmm2
	movsd	32(%rsp), %xmm1
	mulsd	%xmm1, %xmm1
	movsd	88(%rsp), %xmm13
	mulsd	%xmm13, %xmm1
	movsd	40(%rsp), %xmm5
	mulsd	%xmm5, %xmm5
	movsd	96(%rsp), %xmm14
	mulsd	%xmm14, %xmm5
	movsd	48(%rsp), %xmm4
	mulsd	%xmm4, %xmm4
	movsd	104(%rsp), %xmm15
	mulsd	%xmm15, %xmm4
	movsd	56(%rsp), %xmm7
	mulsd	%xmm7, %xmm7
	movsd	8(%rsp), %xmm9
	mulsd	%xmm9, %xmm7
	mulsd	%xmm0, %xmm0
	mulsd	%xmm10, %xmm0
	mulsd	%xmm3, %xmm3
	movapd	%xmm3, %xmm6
	mulsd	%xmm11, %xmm6
	mulsd	%xmm2, %xmm2
	mulsd	%xmm12, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm13, %xmm1
	mulsd	%xmm5, %xmm5
	mulsd	%xmm14, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm15, %xmm4
	mulsd	%xmm7, %xmm7
	mulsd	%xmm9, %xmm7
	mulsd	%xmm0, %xmm0
	mulsd	%xmm10, %xmm0
	mulsd	%xmm6, %xmm6
	mulsd	%xmm11, %xmm6
	mulsd	%xmm2, %xmm2
	mulsd	%xmm12, %xmm2
	movapd	%xmm2, %xmm9
	mulsd	%xmm1, %xmm1
	mulsd	%xmm13, %xmm1
	movapd	%xmm1, %xmm8
	mulsd	%xmm5, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm14, %xmm1
	mulsd	%xmm14, %xmm3
	mulsd	%xmm4, %xmm4
	movapd	%xmm15, %xmm14
	mulsd	%xmm15, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm7, %xmm7
	mulsd	8(%rsp), %xmm7
	mulsd	%xmm0, %xmm0
	mulsd	%xmm10, %xmm0
	mulsd	%xmm6, %xmm6
	mulsd	%xmm11, %xmm6
	mulsd	%xmm9, %xmm9
	movapd	%xmm9, %xmm5
	mulsd	%xmm12, %xmm5
	mulsd	%xmm8, %xmm8
	movapd	%xmm8, %xmm4
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	movapd	%xmm1, %xmm15
	mulsd	%xmm1, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm14, %xmm2
	mulsd	%xmm7, %xmm7
	movsd	8(%rsp), %xmm9
	mulsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm10, %xmm0
	mulsd	%xmm6, %xmm6
	mulsd	%xmm11, %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm12, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm15, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm14, %xmm2
	mulsd	%xmm7, %xmm1
	mulsd	%xmm9, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm10, %xmm0
	mulsd	%xmm6, %xmm6
	mulsd	%xmm11, %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm12, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm15, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm14, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm9, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm10, %xmm0
	mulsd	%xmm6, %xmm6
	mulsd	%xmm11, %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm12, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm15, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm14, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm9, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm10, %xmm0
	mulsd	%xmm6, %xmm6
	mulsd	%xmm11, %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm12, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm15, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm14, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm9, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm10, %xmm0
	mulsd	%xmm6, %xmm6
	mulsd	%xmm11, %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm12, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm15, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm14, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm9, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm10, %xmm0
	mulsd	%xmm6, %xmm6
	mulsd	%xmm11, %xmm6
	movsd	%xmm6, 16(%rsp)
	mulsd	%xmm5, %xmm5
	mulsd	%xmm12, %xmm5
	movsd	%xmm5, 24(%rsp)
	mulsd	%xmm4, %xmm4
	mulsd	%xmm13, %xmm4
	movsd	%xmm4, 32(%rsp)
	mulsd	%xmm3, %xmm3
	mulsd	%xmm15, %xmm3
	movsd	%xmm3, 40(%rsp)
	mulsd	%xmm2, %xmm2
	mulsd	%xmm14, %xmm2
	movsd	%xmm2, 48(%rsp)
	mulsd	%xmm1, %xmm1
	mulsd	%xmm9, %xmm1
	movsd	%xmm1, 56(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1167
.L1166:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	addq	$120, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE303:
	.size	double_mul_6, .-double_mul_6
	.globl	double_mul_7
	.type	double_mul_7, @function
double_mul_7:
.LFB304:
	.cfi_startproc
	endbr64
	subq	$136, %rsp
	.cfi_def_cfa_offset 144
	movsd	80(%rsi), %xmm3
	movsd	.LC8(%rip), %xmm1
	movapd	%xmm3, %xmm0
	mulsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2sdl	4(%rsi), %xmm2
	mulsd	.LC3(%rip), %xmm2
	movapd	%xmm3, %xmm4
	mulsd	%xmm2, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 32(%rsp)
	movsd	88(%rsi), %xmm4
	movapd	%xmm4, %xmm8
	mulsd	%xmm1, %xmm8
	movsd	%xmm8, 64(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 56(%rsp)
	movsd	96(%rsi), %xmm4
	movapd	%xmm4, %xmm7
	mulsd	%xmm1, %xmm7
	movsd	%xmm7, 72(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 40(%rsp)
	movsd	104(%rsi), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm1, %xmm6
	movsd	%xmm6, 80(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 48(%rsp)
	movsd	112(%rsi), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm1, %xmm5
	movsd	%xmm5, 88(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 24(%rsp)
	movsd	120(%rsi), %xmm4
	movapd	%xmm4, %xmm13
	mulsd	%xmm1, %xmm13
	movsd	%xmm13, 96(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 16(%rsp)
	movsd	128(%rsi), %xmm4
	movapd	%xmm4, %xmm14
	mulsd	%xmm1, %xmm14
	movsd	%xmm14, 104(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 8(%rsp)
	movsd	136(%rsi), %xmm4
	mulsd	%xmm4, %xmm1
	movsd	%xmm1, 112(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	movsd	%xmm2, 120(%rsp)
	testq	%rdi, %rdi
	je	.L1171
	leaq	-1(%rdi), %rax
	movapd	%xmm2, %xmm15
	movapd	%xmm13, %xmm4
	movapd	%xmm14, %xmm3
	movapd	%xmm1, %xmm2
.L1172:
	mulsd	%xmm0, %xmm0
	movsd	32(%rsp), %xmm9
	mulsd	%xmm9, %xmm0
	movapd	%xmm8, %xmm1
	mulsd	%xmm8, %xmm1
	movsd	56(%rsp), %xmm11
	mulsd	%xmm11, %xmm1
	mulsd	%xmm7, %xmm7
	movsd	40(%rsp), %xmm12
	mulsd	%xmm12, %xmm7
	mulsd	%xmm6, %xmm6
	movsd	48(%rsp), %xmm10
	mulsd	%xmm10, %xmm6
	mulsd	%xmm5, %xmm5
	movsd	24(%rsp), %xmm8
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm4
	movsd	16(%rsp), %xmm13
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	movsd	8(%rsp), %xmm14
	mulsd	%xmm14, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm15, %xmm2
	mulsd	%xmm0, %xmm0
	mulsd	%xmm9, %xmm0
	mulsd	%xmm1, %xmm1
	mulsd	%xmm11, %xmm1
	mulsd	%xmm7, %xmm7
	mulsd	%xmm12, %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	%xmm10, %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm14, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm15, %xmm2
	movapd	%xmm2, %xmm8
	mulsd	%xmm0, %xmm0
	movapd	%xmm9, %xmm2
	mulsd	%xmm9, %xmm0
	mulsd	%xmm1, %xmm1
	movapd	%xmm1, %xmm9
	movapd	%xmm11, %xmm1
	mulsd	%xmm11, %xmm9
	movapd	%xmm9, %xmm14
	mulsd	%xmm7, %xmm7
	mulsd	%xmm12, %xmm7
	movapd	%xmm7, %xmm13
	mulsd	%xmm6, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm12
	mulsd	%xmm5, %xmm5
	mulsd	24(%rsp), %xmm5
	movapd	%xmm5, %xmm11
	mulsd	%xmm4, %xmm4
	mulsd	16(%rsp), %xmm4
	movapd	%xmm4, %xmm10
	mulsd	%xmm3, %xmm3
	mulsd	8(%rsp), %xmm3
	movapd	%xmm3, %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	%xmm15, %xmm8
	mulsd	%xmm0, %xmm0
	mulsd	%xmm2, %xmm0
	mulsd	%xmm14, %xmm14
	movapd	%xmm14, %xmm6
	movapd	%xmm1, %xmm14
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm13, %xmm13
	movapd	%xmm13, %xmm6
	movsd	40(%rsp), %xmm13
	mulsd	%xmm13, %xmm6
	mulsd	%xmm12, %xmm12
	movapd	%xmm12, %xmm5
	movsd	48(%rsp), %xmm12
	mulsd	%xmm12, %xmm5
	mulsd	%xmm11, %xmm11
	movapd	%xmm11, %xmm4
	movsd	24(%rsp), %xmm11
	mulsd	%xmm11, %xmm4
	mulsd	%xmm10, %xmm10
	movapd	%xmm10, %xmm3
	mulsd	16(%rsp), %xmm3
	mulsd	%xmm9, %xmm9
	mulsd	8(%rsp), %xmm9
	movapd	%xmm9, %xmm2
	mulsd	%xmm8, %xmm8
	movapd	%xmm8, %xmm9
	mulsd	%xmm15, %xmm9
	movapd	%xmm9, %xmm1
	mulsd	%xmm0, %xmm0
	movsd	32(%rsp), %xmm10
	mulsd	%xmm10, %xmm0
	mulsd	%xmm7, %xmm7
	mulsd	%xmm14, %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	%xmm13, %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm12, %xmm5
	mulsd	%xmm4, %xmm4
	movapd	%xmm11, %xmm9
	mulsd	%xmm11, %xmm4
	mulsd	%xmm3, %xmm3
	movsd	16(%rsp), %xmm11
	mulsd	%xmm11, %xmm3
	mulsd	%xmm2, %xmm2
	movsd	8(%rsp), %xmm8
	mulsd	%xmm8, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm10, %xmm0
	mulsd	%xmm7, %xmm7
	mulsd	%xmm14, %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	%xmm13, %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm12, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm9, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm11, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm8, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm10, %xmm0
	mulsd	%xmm7, %xmm7
	mulsd	%xmm14, %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	%xmm13, %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm12, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm9, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm11, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm8, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm10, %xmm0
	mulsd	%xmm7, %xmm7
	mulsd	%xmm14, %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	%xmm13, %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm12, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm9, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm11, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm8, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm10, %xmm0
	mulsd	%xmm7, %xmm7
	mulsd	%xmm14, %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	%xmm13, %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm12, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm9, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm11, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	%xmm8, %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm10, %xmm0
	mulsd	%xmm7, %xmm7
	mulsd	%xmm14, %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm6, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm13, %xmm7
	mulsd	%xmm5, %xmm5
	mulsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm6
	mulsd	%xmm4, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm3, %xmm3
	mulsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	movapd	%xmm2, %xmm3
	mulsd	%xmm1, %xmm1
	movapd	%xmm1, %xmm2
	mulsd	%xmm15, %xmm2
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1172
	movsd	%xmm8, 64(%rsp)
	movsd	%xmm7, 72(%rsp)
	movsd	%xmm6, 80(%rsp)
	movsd	%xmm5, 88(%rsp)
	movsd	%xmm4, 96(%rsp)
	movsd	%xmm3, 104(%rsp)
	movsd	%xmm2, 112(%rsp)
.L1171:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	120(%rsp), %edi
	call	use_int@PLT
	addq	$136, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE304:
	.size	double_mul_7, .-double_mul_7
	.globl	double_mul_8
	.type	double_mul_8, @function
double_mul_8:
.LFB305:
	.cfi_startproc
	endbr64
	subq	$152, %rsp
	.cfi_def_cfa_offset 160
	movsd	80(%rsi), %xmm3
	movsd	.LC8(%rip), %xmm1
	movapd	%xmm3, %xmm0
	mulsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2sdl	4(%rsi), %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 32(%rsp)
	movsd	88(%rsi), %xmm4
	movapd	%xmm4, %xmm7
	mulsd	%xmm1, %xmm7
	movsd	%xmm7, 72(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 40(%rsp)
	movsd	96(%rsi), %xmm4
	movapd	%xmm4, %xmm14
	mulsd	%xmm1, %xmm14
	movsd	%xmm14, 80(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 48(%rsp)
	movsd	104(%rsi), %xmm4
	movapd	%xmm4, %xmm13
	mulsd	%xmm1, %xmm13
	movsd	%xmm13, 88(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 56(%rsp)
	movsd	112(%rsi), %xmm4
	movapd	%xmm4, %xmm12
	mulsd	%xmm1, %xmm12
	movsd	%xmm12, 96(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 64(%rsp)
	movsd	120(%rsi), %xmm4
	movapd	%xmm4, %xmm11
	mulsd	%xmm1, %xmm11
	movsd	%xmm11, 104(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 24(%rsp)
	movsd	128(%rsi), %xmm4
	movapd	%xmm4, %xmm10
	mulsd	%xmm1, %xmm10
	movsd	%xmm10, 112(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 8(%rsp)
	movsd	136(%rsi), %xmm4
	movapd	%xmm4, %xmm9
	mulsd	%xmm1, %xmm9
	movsd	%xmm9, 120(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 16(%rsp)
	movsd	144(%rsi), %xmm4
	mulsd	%xmm4, %xmm1
	movsd	%xmm1, 128(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	movsd	%xmm2, 136(%rsp)
	testq	%rdi, %rdi
	je	.L1176
	movapd	%xmm1, %xmm8
	leaq	-1(%rdi), %rax
	movapd	%xmm2, %xmm15
.L1177:
	mulsd	%xmm0, %xmm0
	mulsd	32(%rsp), %xmm0
	mulsd	%xmm7, %xmm7
	mulsd	40(%rsp), %xmm7
	mulsd	%xmm14, %xmm14
	movapd	%xmm14, %xmm6
	movsd	48(%rsp), %xmm14
	mulsd	%xmm14, %xmm6
	mulsd	%xmm13, %xmm13
	movapd	%xmm13, %xmm5
	movsd	56(%rsp), %xmm13
	mulsd	%xmm13, %xmm5
	movapd	%xmm12, %xmm4
	mulsd	%xmm12, %xmm4
	movsd	64(%rsp), %xmm12
	mulsd	%xmm12, %xmm4
	movapd	%xmm11, %xmm3
	mulsd	%xmm11, %xmm3
	mulsd	24(%rsp), %xmm3
	mulsd	%xmm10, %xmm10
	movapd	%xmm10, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm9, %xmm9
	movapd	%xmm9, %xmm1
	mulsd	16(%rsp), %xmm1
	mulsd	%xmm8, %xmm8
	mulsd	%xmm15, %xmm8
	mulsd	%xmm0, %xmm0
	mulsd	32(%rsp), %xmm0
	mulsd	%xmm7, %xmm7
	mulsd	40(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	mulsd	%xmm5, %xmm5
	mulsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	mulsd	%xmm4, %xmm4
	movapd	%xmm4, %xmm6
	movapd	%xmm12, %xmm4
	mulsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	mulsd	%xmm3, %xmm3
	movapd	%xmm3, %xmm5
	mulsd	24(%rsp), %xmm5
	movapd	%xmm5, %xmm11
	mulsd	%xmm2, %xmm2
	movapd	%xmm2, %xmm10
	movsd	8(%rsp), %xmm2
	mulsd	%xmm2, %xmm10
	mulsd	%xmm1, %xmm1
	movapd	%xmm1, %xmm9
	movsd	16(%rsp), %xmm1
	mulsd	%xmm1, %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	%xmm15, %xmm8
	mulsd	%xmm0, %xmm0
	mulsd	32(%rsp), %xmm0
	mulsd	%xmm7, %xmm7
	mulsd	40(%rsp), %xmm7
	mulsd	%xmm14, %xmm14
	mulsd	48(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	movapd	%xmm13, %xmm6
	movsd	56(%rsp), %xmm13
	mulsd	%xmm13, %xmm6
	mulsd	%xmm12, %xmm12
	mulsd	%xmm4, %xmm12
	movapd	%xmm12, %xmm5
	mulsd	%xmm11, %xmm11
	movapd	%xmm11, %xmm12
	mulsd	24(%rsp), %xmm12
	movapd	%xmm12, %xmm4
	mulsd	%xmm10, %xmm10
	movapd	%xmm10, %xmm12
	mulsd	%xmm2, %xmm12
	movapd	%xmm12, %xmm3
	mulsd	%xmm9, %xmm9
	movapd	%xmm9, %xmm2
	mulsd	%xmm1, %xmm2
	mulsd	%xmm8, %xmm8
	movapd	%xmm8, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	movsd	32(%rsp), %xmm9
	mulsd	%xmm9, %xmm0
	mulsd	%xmm7, %xmm7
	movsd	40(%rsp), %xmm10
	mulsd	%xmm10, %xmm7
	mulsd	%xmm14, %xmm14
	movsd	48(%rsp), %xmm11
	mulsd	%xmm11, %xmm14
	mulsd	%xmm6, %xmm6
	movapd	%xmm13, %xmm12
	mulsd	%xmm13, %xmm6
	mulsd	%xmm5, %xmm5
	movsd	64(%rsp), %xmm8
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm4
	movsd	24(%rsp), %xmm13
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	8(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	16(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm9, %xmm0
	mulsd	%xmm7, %xmm7
	mulsd	%xmm10, %xmm7
	mulsd	%xmm14, %xmm14
	mulsd	%xmm11, %xmm14
	mulsd	%xmm6, %xmm6
	mulsd	%xmm12, %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	8(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	16(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm9, %xmm0
	mulsd	%xmm7, %xmm7
	mulsd	%xmm10, %xmm7
	mulsd	%xmm14, %xmm14
	mulsd	%xmm11, %xmm14
	mulsd	%xmm6, %xmm6
	mulsd	%xmm12, %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	8(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	16(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm9, %xmm0
	mulsd	%xmm7, %xmm7
	mulsd	%xmm10, %xmm7
	mulsd	%xmm14, %xmm14
	mulsd	%xmm11, %xmm14
	mulsd	%xmm6, %xmm6
	mulsd	%xmm12, %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	8(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	16(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm9, %xmm0
	mulsd	%xmm7, %xmm7
	mulsd	%xmm10, %xmm7
	mulsd	%xmm14, %xmm14
	mulsd	%xmm11, %xmm14
	mulsd	%xmm6, %xmm6
	mulsd	%xmm12, %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	8(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	16(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm9, %xmm0
	mulsd	%xmm7, %xmm7
	mulsd	%xmm10, %xmm7
	mulsd	%xmm14, %xmm14
	mulsd	%xmm11, %xmm14
	mulsd	%xmm6, %xmm6
	mulsd	%xmm12, %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm8, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	8(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	16(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm9, %xmm0
	mulsd	%xmm7, %xmm7
	mulsd	%xmm10, %xmm7
	mulsd	%xmm14, %xmm14
	mulsd	%xmm11, %xmm14
	mulsd	%xmm6, %xmm6
	mulsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm13
	mulsd	%xmm5, %xmm5
	movapd	%xmm5, %xmm12
	mulsd	%xmm8, %xmm12
	mulsd	%xmm4, %xmm4
	mulsd	24(%rsp), %xmm4
	movapd	%xmm4, %xmm11
	mulsd	%xmm3, %xmm3
	mulsd	8(%rsp), %xmm3
	movapd	%xmm3, %xmm10
	mulsd	%xmm2, %xmm2
	mulsd	16(%rsp), %xmm2
	movapd	%xmm2, %xmm9
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	movapd	%xmm1, %xmm8
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1177
	movsd	%xmm7, 72(%rsp)
	movsd	%xmm14, 80(%rsp)
	movsd	%xmm6, 88(%rsp)
	movsd	%xmm12, 96(%rsp)
	movsd	%xmm4, 104(%rsp)
	movsd	%xmm3, 112(%rsp)
	movsd	%xmm2, 120(%rsp)
	movsd	%xmm1, 128(%rsp)
.L1176:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	128(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	136(%rsp), %edi
	call	use_int@PLT
	addq	$152, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE305:
	.size	double_mul_8, .-double_mul_8
	.globl	double_mul_9
	.type	double_mul_9, @function
double_mul_9:
.LFB306:
	.cfi_startproc
	endbr64
	subq	$168, %rsp
	.cfi_def_cfa_offset 176
	movsd	80(%rsi), %xmm3
	movsd	.LC8(%rip), %xmm1
	movapd	%xmm3, %xmm0
	mulsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2sdl	4(%rsi), %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 40(%rsp)
	movsd	88(%rsi), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm1, %xmm6
	movsd	%xmm6, 80(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 48(%rsp)
	movsd	96(%rsi), %xmm4
	movapd	%xmm4, %xmm12
	mulsd	%xmm1, %xmm12
	movsd	%xmm12, 88(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 64(%rsp)
	movsd	104(%rsi), %xmm4
	movapd	%xmm4, %xmm13
	mulsd	%xmm1, %xmm13
	movsd	%xmm13, 96(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 72(%rsp)
	movsd	112(%rsi), %xmm4
	movapd	%xmm4, %xmm14
	mulsd	%xmm1, %xmm14
	movsd	%xmm14, 104(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 56(%rsp)
	movsd	120(%rsi), %xmm4
	movapd	%xmm4, %xmm11
	mulsd	%xmm1, %xmm11
	movsd	%xmm11, 112(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 8(%rsp)
	movsd	128(%rsi), %xmm4
	movapd	%xmm4, %xmm10
	mulsd	%xmm1, %xmm10
	movsd	%xmm10, 120(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 16(%rsp)
	movsd	136(%rsi), %xmm4
	movapd	%xmm4, %xmm9
	mulsd	%xmm1, %xmm9
	movsd	%xmm9, 128(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 24(%rsp)
	movsd	144(%rsi), %xmm4
	movapd	%xmm4, %xmm8
	mulsd	%xmm1, %xmm8
	movsd	%xmm8, 136(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 32(%rsp)
	movsd	152(%rsi), %xmm4
	mulsd	%xmm4, %xmm1
	movsd	%xmm1, 144(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	movsd	%xmm2, 152(%rsp)
	testq	%rdi, %rdi
	je	.L1181
	movapd	%xmm1, %xmm7
	leaq	-1(%rdi), %rax
	movapd	%xmm2, %xmm15
.L1182:
	mulsd	%xmm0, %xmm0
	mulsd	40(%rsp), %xmm0
	mulsd	%xmm6, %xmm6
	mulsd	48(%rsp), %xmm6
	mulsd	%xmm12, %xmm12
	movapd	%xmm12, %xmm5
	movsd	64(%rsp), %xmm12
	mulsd	%xmm12, %xmm5
	movapd	%xmm13, %xmm4
	mulsd	%xmm13, %xmm4
	movsd	72(%rsp), %xmm13
	mulsd	%xmm13, %xmm4
	mulsd	%xmm14, %xmm14
	movapd	%xmm14, %xmm3
	movsd	56(%rsp), %xmm14
	mulsd	%xmm14, %xmm3
	movapd	%xmm11, %xmm2
	mulsd	%xmm11, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm10, %xmm10
	movapd	%xmm10, %xmm1
	mulsd	16(%rsp), %xmm1
	mulsd	%xmm9, %xmm9
	mulsd	24(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	32(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	%xmm15, %xmm7
	mulsd	%xmm0, %xmm0
	mulsd	40(%rsp), %xmm0
	mulsd	%xmm6, %xmm6
	mulsd	48(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	movapd	%xmm5, %xmm11
	movapd	%xmm12, %xmm5
	mulsd	%xmm12, %xmm11
	movapd	%xmm11, %xmm12
	mulsd	%xmm4, %xmm4
	movapd	%xmm4, %xmm11
	movapd	%xmm13, %xmm4
	mulsd	%xmm13, %xmm11
	movapd	%xmm11, %xmm13
	mulsd	%xmm3, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm14
	mulsd	%xmm2, %xmm2
	movapd	%xmm2, %xmm3
	mulsd	8(%rsp), %xmm3
	movapd	%xmm3, %xmm11
	mulsd	%xmm1, %xmm1
	movsd	16(%rsp), %xmm3
	mulsd	%xmm3, %xmm1
	movapd	%xmm1, %xmm10
	mulsd	%xmm9, %xmm9
	movsd	24(%rsp), %xmm2
	mulsd	%xmm2, %xmm9
	mulsd	%xmm8, %xmm8
	movsd	32(%rsp), %xmm1
	mulsd	%xmm1, %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	%xmm15, %xmm7
	mulsd	%xmm0, %xmm0
	mulsd	40(%rsp), %xmm0
	mulsd	%xmm6, %xmm6
	mulsd	48(%rsp), %xmm6
	mulsd	%xmm12, %xmm12
	mulsd	%xmm5, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	%xmm4, %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	56(%rsp), %xmm14
	mulsd	%xmm11, %xmm11
	movapd	%xmm11, %xmm5
	mulsd	8(%rsp), %xmm5
	mulsd	%xmm10, %xmm10
	movapd	%xmm10, %xmm4
	mulsd	%xmm3, %xmm4
	mulsd	%xmm9, %xmm9
	mulsd	%xmm2, %xmm9
	movapd	%xmm9, %xmm3
	mulsd	%xmm8, %xmm8
	mulsd	%xmm1, %xmm8
	movapd	%xmm8, %xmm2
	mulsd	%xmm7, %xmm7
	mulsd	%xmm15, %xmm7
	movapd	%xmm7, %xmm1
	mulsd	%xmm0, %xmm0
	movsd	40(%rsp), %xmm7
	mulsd	%xmm7, %xmm0
	mulsd	%xmm6, %xmm6
	movsd	48(%rsp), %xmm11
	mulsd	%xmm11, %xmm6
	mulsd	%xmm12, %xmm12
	movsd	64(%rsp), %xmm10
	mulsd	%xmm10, %xmm12
	mulsd	%xmm13, %xmm13
	movsd	72(%rsp), %xmm8
	mulsd	%xmm8, %xmm13
	mulsd	%xmm14, %xmm14
	movsd	56(%rsp), %xmm9
	mulsd	%xmm9, %xmm14
	mulsd	%xmm5, %xmm5
	mulsd	8(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	16(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	24(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	32(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	%xmm6, %xmm6
	mulsd	%xmm11, %xmm6
	mulsd	%xmm12, %xmm12
	mulsd	%xmm10, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	%xmm8, %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	%xmm9, %xmm14
	mulsd	%xmm5, %xmm5
	mulsd	8(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	16(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	24(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	32(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	%xmm6, %xmm6
	mulsd	%xmm11, %xmm6
	mulsd	%xmm12, %xmm12
	mulsd	%xmm10, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	%xmm8, %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	%xmm9, %xmm14
	mulsd	%xmm5, %xmm5
	mulsd	8(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	16(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	24(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	32(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	%xmm6, %xmm6
	mulsd	%xmm11, %xmm6
	mulsd	%xmm12, %xmm12
	mulsd	%xmm10, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	%xmm8, %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	%xmm9, %xmm14
	mulsd	%xmm5, %xmm5
	mulsd	8(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	16(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	24(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	32(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	%xmm6, %xmm6
	mulsd	%xmm11, %xmm6
	mulsd	%xmm12, %xmm12
	mulsd	%xmm10, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	%xmm8, %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	%xmm9, %xmm14
	mulsd	%xmm5, %xmm5
	mulsd	8(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	16(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	24(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	32(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	%xmm6, %xmm6
	mulsd	%xmm11, %xmm6
	mulsd	%xmm12, %xmm12
	mulsd	%xmm10, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	%xmm8, %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	%xmm9, %xmm14
	mulsd	%xmm5, %xmm5
	mulsd	8(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	16(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	24(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	32(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	%xmm6, %xmm6
	mulsd	%xmm11, %xmm6
	mulsd	%xmm12, %xmm12
	mulsd	%xmm10, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	%xmm8, %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	%xmm9, %xmm14
	mulsd	%xmm5, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	8(%rsp), %xmm11
	mulsd	%xmm4, %xmm4
	mulsd	16(%rsp), %xmm4
	movapd	%xmm4, %xmm10
	mulsd	%xmm3, %xmm3
	movapd	%xmm3, %xmm9
	mulsd	24(%rsp), %xmm9
	mulsd	%xmm2, %xmm2
	mulsd	32(%rsp), %xmm2
	movapd	%xmm2, %xmm8
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	movapd	%xmm1, %xmm7
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1182
	movsd	%xmm6, 80(%rsp)
	movsd	%xmm12, 88(%rsp)
	movsd	%xmm13, 96(%rsp)
	movsd	%xmm14, 104(%rsp)
	movsd	%xmm11, 112(%rsp)
	movsd	%xmm4, 120(%rsp)
	movsd	%xmm9, 128(%rsp)
	movsd	%xmm2, 136(%rsp)
	movsd	%xmm1, 144(%rsp)
.L1181:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	128(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	136(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	144(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	152(%rsp), %edi
	call	use_int@PLT
	addq	$168, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE306:
	.size	double_mul_9, .-double_mul_9
	.globl	double_mul_10
	.type	double_mul_10, @function
double_mul_10:
.LFB307:
	.cfi_startproc
	endbr64
	subq	$184, %rsp
	.cfi_def_cfa_offset 192
	movsd	80(%rsi), %xmm3
	movsd	.LC8(%rip), %xmm1
	movapd	%xmm3, %xmm0
	mulsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2sdl	4(%rsi), %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 56(%rsp)
	movsd	88(%rsi), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm1, %xmm5
	movsd	%xmm5, 88(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 64(%rsp)
	movsd	96(%rsi), %xmm4
	movapd	%xmm4, %xmm11
	mulsd	%xmm1, %xmm11
	movsd	%xmm11, 96(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 72(%rsp)
	movsd	104(%rsi), %xmm4
	movapd	%xmm4, %xmm12
	mulsd	%xmm1, %xmm12
	movsd	%xmm12, 104(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 80(%rsp)
	movsd	112(%rsi), %xmm4
	movapd	%xmm4, %xmm13
	mulsd	%xmm1, %xmm13
	movsd	%xmm13, 112(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 8(%rsp)
	movsd	120(%rsi), %xmm4
	movapd	%xmm4, %xmm14
	mulsd	%xmm1, %xmm14
	movsd	%xmm14, 120(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 16(%rsp)
	movsd	128(%rsi), %xmm4
	movapd	%xmm4, %xmm10
	mulsd	%xmm1, %xmm10
	movsd	%xmm10, 128(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 24(%rsp)
	movsd	136(%rsi), %xmm4
	movapd	%xmm4, %xmm9
	mulsd	%xmm1, %xmm9
	movsd	%xmm9, 136(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 32(%rsp)
	movsd	144(%rsi), %xmm4
	movapd	%xmm4, %xmm8
	mulsd	%xmm1, %xmm8
	movsd	%xmm8, 144(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 40(%rsp)
	movsd	152(%rsi), %xmm4
	movapd	%xmm4, %xmm7
	mulsd	%xmm1, %xmm7
	movsd	%xmm7, 152(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 48(%rsp)
	movsd	160(%rsi), %xmm4
	mulsd	%xmm4, %xmm1
	movsd	%xmm1, 160(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	movsd	%xmm2, 168(%rsp)
	testq	%rdi, %rdi
	je	.L1186
	movapd	%xmm1, %xmm6
	leaq	-1(%rdi), %rax
	movapd	%xmm2, %xmm15
.L1187:
	mulsd	%xmm0, %xmm0
	mulsd	56(%rsp), %xmm0
	mulsd	%xmm5, %xmm5
	mulsd	64(%rsp), %xmm5
	movapd	%xmm11, %xmm4
	mulsd	%xmm11, %xmm4
	mulsd	72(%rsp), %xmm4
	movapd	%xmm12, %xmm3
	mulsd	%xmm12, %xmm3
	mulsd	80(%rsp), %xmm3
	movapd	%xmm13, %xmm2
	mulsd	%xmm13, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm14, %xmm14
	movapd	%xmm14, %xmm1
	mulsd	16(%rsp), %xmm1
	mulsd	%xmm10, %xmm10
	mulsd	24(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	32(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	40(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	48(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	%xmm15, %xmm6
	mulsd	%xmm0, %xmm0
	movsd	56(%rsp), %xmm11
	mulsd	%xmm11, %xmm0
	mulsd	%xmm5, %xmm5
	movsd	64(%rsp), %xmm12
	mulsd	%xmm12, %xmm5
	mulsd	%xmm4, %xmm4
	movsd	72(%rsp), %xmm13
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	movsd	80(%rsp), %xmm14
	mulsd	%xmm14, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	16(%rsp), %xmm1
	mulsd	%xmm10, %xmm10
	mulsd	24(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	32(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	40(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	48(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	%xmm15, %xmm6
	mulsd	%xmm0, %xmm0
	mulsd	%xmm11, %xmm0
	mulsd	%xmm5, %xmm5
	mulsd	%xmm12, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm14, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	16(%rsp), %xmm1
	mulsd	%xmm10, %xmm10
	mulsd	24(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	32(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	40(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	48(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	%xmm15, %xmm6
	mulsd	%xmm0, %xmm0
	mulsd	%xmm11, %xmm0
	mulsd	%xmm5, %xmm5
	mulsd	%xmm12, %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm11
	mulsd	%xmm3, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm12
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	movapd	%xmm2, %xmm13
	mulsd	%xmm1, %xmm1
	mulsd	16(%rsp), %xmm1
	movapd	%xmm1, %xmm14
	mulsd	%xmm10, %xmm10
	mulsd	24(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	movapd	%xmm9, %xmm4
	mulsd	32(%rsp), %xmm4
	mulsd	%xmm8, %xmm8
	mulsd	40(%rsp), %xmm8
	movapd	%xmm8, %xmm3
	mulsd	%xmm7, %xmm7
	mulsd	48(%rsp), %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm6, %xmm6
	mulsd	%xmm15, %xmm6
	movapd	%xmm6, %xmm1
	mulsd	%xmm0, %xmm0
	movsd	56(%rsp), %xmm6
	mulsd	%xmm6, %xmm0
	mulsd	%xmm5, %xmm5
	movsd	64(%rsp), %xmm7
	mulsd	%xmm7, %xmm5
	mulsd	%xmm11, %xmm11
	movsd	72(%rsp), %xmm8
	mulsd	%xmm8, %xmm11
	mulsd	%xmm12, %xmm12
	movsd	80(%rsp), %xmm9
	mulsd	%xmm9, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm10, %xmm10
	mulsd	24(%rsp), %xmm10
	mulsd	%xmm4, %xmm4
	mulsd	32(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	40(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	48(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm5, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	%xmm11, %xmm11
	mulsd	%xmm8, %xmm11
	mulsd	%xmm12, %xmm12
	mulsd	%xmm9, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm10, %xmm10
	mulsd	24(%rsp), %xmm10
	mulsd	%xmm4, %xmm4
	mulsd	32(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	40(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	48(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm5, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	%xmm11, %xmm11
	mulsd	%xmm8, %xmm11
	mulsd	%xmm12, %xmm12
	mulsd	%xmm9, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm10, %xmm10
	mulsd	24(%rsp), %xmm10
	mulsd	%xmm4, %xmm4
	mulsd	32(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	40(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	48(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm5, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	%xmm11, %xmm11
	mulsd	%xmm8, %xmm11
	mulsd	%xmm12, %xmm12
	mulsd	%xmm9, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm10, %xmm10
	mulsd	24(%rsp), %xmm10
	mulsd	%xmm4, %xmm4
	mulsd	32(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	40(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	48(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm5, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	%xmm11, %xmm11
	mulsd	%xmm8, %xmm11
	mulsd	%xmm12, %xmm12
	mulsd	%xmm9, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm10, %xmm10
	mulsd	24(%rsp), %xmm10
	mulsd	%xmm4, %xmm4
	mulsd	32(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	40(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	48(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm6, %xmm0
	mulsd	%xmm5, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	%xmm11, %xmm11
	mulsd	%xmm8, %xmm11
	mulsd	%xmm12, %xmm12
	mulsd	%xmm9, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm10, %xmm10
	mulsd	24(%rsp), %xmm10
	mulsd	%xmm4, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	32(%rsp), %xmm9
	mulsd	%xmm3, %xmm3
	movapd	%xmm3, %xmm8
	mulsd	40(%rsp), %xmm8
	mulsd	%xmm2, %xmm2
	movapd	%xmm2, %xmm7
	mulsd	48(%rsp), %xmm7
	mulsd	%xmm1, %xmm1
	movapd	%xmm1, %xmm6
	mulsd	%xmm15, %xmm6
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1187
	movsd	%xmm5, 88(%rsp)
	movsd	%xmm11, 96(%rsp)
	movsd	%xmm12, 104(%rsp)
	movsd	%xmm13, 112(%rsp)
	movsd	%xmm14, 120(%rsp)
	movsd	%xmm10, 128(%rsp)
	movsd	%xmm9, 136(%rsp)
	movsd	%xmm8, 144(%rsp)
	movsd	%xmm7, 152(%rsp)
	movsd	%xmm6, 160(%rsp)
.L1186:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	128(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	136(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	144(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	152(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	160(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	168(%rsp), %edi
	call	use_int@PLT
	addq	$184, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE307:
	.size	double_mul_10, .-double_mul_10
	.globl	double_mul_11
	.type	double_mul_11, @function
double_mul_11:
.LFB308:
	.cfi_startproc
	endbr64
	subq	$200, %rsp
	.cfi_def_cfa_offset 208
	movsd	80(%rsi), %xmm3
	movsd	.LC8(%rip), %xmm1
	movapd	%xmm3, %xmm0
	mulsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2sdl	4(%rsi), %xmm2
	mulsd	.LC3(%rip), %xmm2
	movapd	%xmm3, %xmm4
	mulsd	%xmm2, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 72(%rsp)
	movsd	88(%rsi), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm1, %xmm5
	movsd	%xmm5, 96(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 80(%rsp)
	movsd	96(%rsi), %xmm4
	movapd	%xmm4, %xmm12
	mulsd	%xmm1, %xmm12
	movsd	%xmm12, 104(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 88(%rsp)
	movsd	104(%rsi), %xmm4
	movapd	%xmm4, %xmm13
	mulsd	%xmm1, %xmm13
	movsd	%xmm13, 112(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 8(%rsp)
	movsd	112(%rsi), %xmm4
	movapd	%xmm4, %xmm14
	mulsd	%xmm1, %xmm14
	movsd	%xmm14, 120(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 16(%rsp)
	movsd	120(%rsi), %xmm4
	movapd	%xmm4, %xmm11
	mulsd	%xmm1, %xmm11
	movsd	%xmm11, 128(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 24(%rsp)
	movsd	128(%rsi), %xmm4
	movapd	%xmm4, %xmm10
	mulsd	%xmm1, %xmm10
	movsd	%xmm10, 136(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 32(%rsp)
	movsd	136(%rsi), %xmm4
	movapd	%xmm4, %xmm9
	mulsd	%xmm1, %xmm9
	movsd	%xmm9, 144(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 40(%rsp)
	movsd	144(%rsi), %xmm4
	movapd	%xmm4, %xmm8
	mulsd	%xmm1, %xmm8
	movsd	%xmm8, 152(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 48(%rsp)
	movsd	152(%rsi), %xmm4
	movapd	%xmm4, %xmm7
	mulsd	%xmm1, %xmm7
	movsd	%xmm7, 160(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 56(%rsp)
	movsd	160(%rsi), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm1, %xmm6
	movsd	%xmm6, 168(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 64(%rsp)
	movsd	168(%rsi), %xmm4
	mulsd	%xmm4, %xmm1
	movsd	%xmm1, 176(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	movsd	%xmm2, 184(%rsp)
	testq	%rdi, %rdi
	je	.L1191
	leaq	-1(%rdi), %rax
	movapd	%xmm2, %xmm15
	movapd	%xmm5, %xmm4
	movapd	%xmm1, %xmm5
.L1192:
	mulsd	%xmm0, %xmm0
	mulsd	72(%rsp), %xmm0
	mulsd	%xmm4, %xmm4
	mulsd	80(%rsp), %xmm4
	movapd	%xmm12, %xmm3
	mulsd	%xmm12, %xmm3
	mulsd	88(%rsp), %xmm3
	movapd	%xmm13, %xmm2
	mulsd	%xmm13, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm14, %xmm14
	movapd	%xmm14, %xmm1
	mulsd	16(%rsp), %xmm1
	mulsd	%xmm11, %xmm11
	mulsd	24(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	32(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	40(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	48(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	56(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	64(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm15, %xmm5
	mulsd	%xmm0, %xmm0
	movsd	72(%rsp), %xmm12
	mulsd	%xmm12, %xmm0
	mulsd	%xmm4, %xmm4
	movsd	80(%rsp), %xmm13
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	movsd	88(%rsp), %xmm14
	mulsd	%xmm14, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	16(%rsp), %xmm1
	mulsd	%xmm11, %xmm11
	mulsd	24(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	32(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	40(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	48(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	56(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	64(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm15, %xmm5
	mulsd	%xmm0, %xmm0
	mulsd	%xmm12, %xmm0
	mulsd	%xmm4, %xmm4
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm14, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	16(%rsp), %xmm1
	mulsd	%xmm11, %xmm11
	mulsd	24(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	32(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	40(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	48(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	56(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	64(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	%xmm15, %xmm5
	mulsd	%xmm0, %xmm0
	mulsd	%xmm12, %xmm0
	mulsd	%xmm4, %xmm4
	mulsd	%xmm13, %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm12
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	movapd	%xmm2, %xmm13
	mulsd	%xmm1, %xmm1
	mulsd	16(%rsp), %xmm1
	movapd	%xmm1, %xmm14
	mulsd	%xmm11, %xmm11
	mulsd	24(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	32(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	40(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	48(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	movapd	%xmm7, %xmm3
	mulsd	56(%rsp), %xmm3
	mulsd	%xmm6, %xmm6
	mulsd	64(%rsp), %xmm6
	movapd	%xmm6, %xmm2
	mulsd	%xmm5, %xmm5
	mulsd	%xmm15, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	%xmm0, %xmm0
	movsd	72(%rsp), %xmm5
	mulsd	%xmm5, %xmm0
	mulsd	%xmm4, %xmm4
	movsd	80(%rsp), %xmm6
	mulsd	%xmm6, %xmm4
	mulsd	%xmm12, %xmm12
	movsd	88(%rsp), %xmm7
	mulsd	%xmm7, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm11, %xmm11
	mulsd	24(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	32(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	40(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	48(%rsp), %xmm8
	mulsd	%xmm3, %xmm3
	mulsd	56(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	64(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm5, %xmm0
	mulsd	%xmm4, %xmm4
	mulsd	%xmm6, %xmm4
	mulsd	%xmm12, %xmm12
	mulsd	%xmm7, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm11, %xmm11
	mulsd	24(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	32(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	40(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	48(%rsp), %xmm8
	mulsd	%xmm3, %xmm3
	mulsd	56(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	64(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm5, %xmm0
	mulsd	%xmm4, %xmm4
	mulsd	%xmm6, %xmm4
	mulsd	%xmm12, %xmm12
	mulsd	%xmm7, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm11, %xmm11
	mulsd	24(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	32(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	40(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	48(%rsp), %xmm8
	mulsd	%xmm3, %xmm3
	mulsd	56(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	64(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm5, %xmm0
	mulsd	%xmm4, %xmm4
	mulsd	%xmm6, %xmm4
	mulsd	%xmm12, %xmm12
	mulsd	%xmm7, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm11, %xmm11
	mulsd	24(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	32(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	40(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	48(%rsp), %xmm8
	mulsd	%xmm3, %xmm3
	mulsd	56(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	64(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm5, %xmm0
	mulsd	%xmm4, %xmm4
	mulsd	%xmm6, %xmm4
	mulsd	%xmm12, %xmm12
	mulsd	%xmm7, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm11, %xmm11
	mulsd	24(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	32(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	40(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	48(%rsp), %xmm8
	mulsd	%xmm3, %xmm3
	mulsd	56(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	64(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm5, %xmm0
	mulsd	%xmm4, %xmm4
	mulsd	%xmm6, %xmm4
	mulsd	%xmm12, %xmm12
	mulsd	%xmm7, %xmm12
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm11, %xmm11
	mulsd	24(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	32(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	40(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	48(%rsp), %xmm8
	mulsd	%xmm3, %xmm3
	movapd	%xmm3, %xmm7
	mulsd	56(%rsp), %xmm7
	mulsd	%xmm2, %xmm2
	movapd	%xmm2, %xmm6
	mulsd	64(%rsp), %xmm6
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	movapd	%xmm1, %xmm5
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1192
	movsd	%xmm4, 96(%rsp)
	movsd	%xmm12, 104(%rsp)
	movsd	%xmm13, 112(%rsp)
	movsd	%xmm14, 120(%rsp)
	movsd	%xmm11, 128(%rsp)
	movsd	%xmm10, 136(%rsp)
	movsd	%xmm9, 144(%rsp)
	movsd	%xmm8, 152(%rsp)
	movsd	%xmm7, 160(%rsp)
	movsd	%xmm6, 168(%rsp)
	movsd	%xmm1, 176(%rsp)
.L1191:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	128(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	136(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	144(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	152(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	160(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	168(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	176(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	184(%rsp), %edi
	call	use_int@PLT
	addq	$200, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE308:
	.size	double_mul_11, .-double_mul_11
	.globl	double_mul_12
	.type	double_mul_12, @function
double_mul_12:
.LFB309:
	.cfi_startproc
	endbr64
	subq	$216, %rsp
	.cfi_def_cfa_offset 224
	movsd	80(%rsi), %xmm3
	movsd	.LC8(%rip), %xmm1
	movapd	%xmm3, %xmm0
	mulsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2sdl	4(%rsi), %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 88(%rsp)
	movsd	88(%rsi), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm1, %xmm6
	movsd	%xmm6, 112(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 96(%rsp)
	movsd	96(%rsi), %xmm4
	movapd	%xmm4, %xmm13
	mulsd	%xmm1, %xmm13
	movsd	%xmm13, 120(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 8(%rsp)
	movsd	104(%rsi), %xmm4
	movapd	%xmm4, %xmm14
	mulsd	%xmm1, %xmm14
	movsd	%xmm14, 128(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 16(%rsp)
	movsd	112(%rsi), %xmm4
	movapd	%xmm4, %xmm12
	mulsd	%xmm1, %xmm12
	movsd	%xmm12, 136(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 24(%rsp)
	movsd	120(%rsi), %xmm4
	movapd	%xmm4, %xmm11
	mulsd	%xmm1, %xmm11
	movsd	%xmm11, 144(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 32(%rsp)
	movsd	128(%rsi), %xmm4
	movapd	%xmm4, %xmm10
	mulsd	%xmm1, %xmm10
	movsd	%xmm10, 152(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 40(%rsp)
	movsd	136(%rsi), %xmm4
	movapd	%xmm4, %xmm9
	mulsd	%xmm1, %xmm9
	movsd	%xmm9, 160(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 48(%rsp)
	movsd	144(%rsi), %xmm4
	movapd	%xmm4, %xmm8
	mulsd	%xmm1, %xmm8
	movsd	%xmm8, 168(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 56(%rsp)
	movsd	152(%rsi), %xmm4
	movapd	%xmm4, %xmm7
	mulsd	%xmm1, %xmm7
	movsd	%xmm7, 176(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 64(%rsp)
	movsd	160(%rsi), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm1, %xmm5
	movsd	%xmm5, 184(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 72(%rsp)
	movsd	168(%rsi), %xmm4
	movapd	%xmm4, %xmm15
	mulsd	%xmm1, %xmm15
	movsd	%xmm15, 104(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 80(%rsp)
	movsd	176(%rsi), %xmm4
	mulsd	%xmm4, %xmm1
	movsd	%xmm1, 192(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	movsd	%xmm2, 200(%rsp)
	testq	%rdi, %rdi
	je	.L1196
	leaq	-1(%rdi), %rax
	movapd	%xmm2, %xmm15
	movapd	%xmm6, %xmm3
	movapd	%xmm5, %xmm6
	movsd	104(%rsp), %xmm5
	movapd	%xmm1, %xmm4
.L1197:
	mulsd	%xmm0, %xmm0
	mulsd	88(%rsp), %xmm0
	mulsd	%xmm3, %xmm3
	mulsd	96(%rsp), %xmm3
	movapd	%xmm13, %xmm2
	mulsd	%xmm13, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm14, %xmm14
	movapd	%xmm14, %xmm1
	mulsd	16(%rsp), %xmm1
	mulsd	%xmm12, %xmm12
	mulsd	24(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	32(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	40(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	48(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	56(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	64(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	72(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	80(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm15, %xmm4
	mulsd	%xmm0, %xmm0
	movsd	88(%rsp), %xmm13
	mulsd	%xmm13, %xmm0
	mulsd	%xmm3, %xmm3
	movsd	96(%rsp), %xmm14
	mulsd	%xmm14, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	16(%rsp), %xmm1
	mulsd	%xmm12, %xmm12
	mulsd	24(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	32(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	40(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	48(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	56(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	64(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	72(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	80(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm15, %xmm4
	mulsd	%xmm0, %xmm0
	mulsd	%xmm13, %xmm0
	mulsd	%xmm3, %xmm3
	mulsd	%xmm14, %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	16(%rsp), %xmm1
	mulsd	%xmm12, %xmm12
	mulsd	24(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	32(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	40(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	48(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	56(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	64(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	72(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	80(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	%xmm15, %xmm4
	mulsd	%xmm0, %xmm0
	mulsd	%xmm13, %xmm0
	mulsd	%xmm3, %xmm3
	mulsd	%xmm14, %xmm3
	mulsd	%xmm2, %xmm2
	movapd	%xmm2, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm1, %xmm1
	mulsd	16(%rsp), %xmm1
	movapd	%xmm1, %xmm14
	mulsd	%xmm12, %xmm12
	mulsd	24(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	32(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	40(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	48(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	56(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	64(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	72(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	80(%rsp), %xmm2
	mulsd	%xmm4, %xmm4
	mulsd	%xmm15, %xmm4
	movapd	%xmm4, %xmm1
	mulsd	%xmm0, %xmm0
	movsd	88(%rsp), %xmm5
	mulsd	%xmm5, %xmm0
	mulsd	%xmm3, %xmm3
	movsd	96(%rsp), %xmm4
	mulsd	%xmm4, %xmm3
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm12, %xmm12
	mulsd	24(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	32(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	40(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	48(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	56(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	64(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	72(%rsp), %xmm6
	mulsd	%xmm2, %xmm2
	mulsd	80(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm5, %xmm0
	mulsd	%xmm3, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm12, %xmm12
	mulsd	24(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	32(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	40(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	48(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	56(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	64(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	72(%rsp), %xmm6
	mulsd	%xmm2, %xmm2
	mulsd	80(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm5, %xmm0
	mulsd	%xmm3, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm12, %xmm12
	mulsd	24(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	32(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	40(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	48(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	56(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	64(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	72(%rsp), %xmm6
	mulsd	%xmm2, %xmm2
	mulsd	80(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm5, %xmm0
	mulsd	%xmm3, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm12, %xmm12
	mulsd	24(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	32(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	40(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	48(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	56(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	64(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	72(%rsp), %xmm6
	mulsd	%xmm2, %xmm2
	mulsd	80(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm5, %xmm0
	mulsd	%xmm3, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm12, %xmm12
	mulsd	24(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	32(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	40(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	48(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	56(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	64(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	72(%rsp), %xmm6
	mulsd	%xmm2, %xmm2
	mulsd	80(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm5, %xmm0
	mulsd	%xmm3, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm13, %xmm13
	mulsd	8(%rsp), %xmm13
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm12, %xmm12
	mulsd	24(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	32(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	40(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	48(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	56(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	64(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	72(%rsp), %xmm6
	mulsd	%xmm2, %xmm2
	mulsd	80(%rsp), %xmm2
	movapd	%xmm2, %xmm5
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	movapd	%xmm1, %xmm4
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1197
	movsd	%xmm3, 112(%rsp)
	movsd	%xmm13, 120(%rsp)
	movsd	%xmm14, 128(%rsp)
	movsd	%xmm12, 136(%rsp)
	movsd	%xmm11, 144(%rsp)
	movsd	%xmm10, 152(%rsp)
	movsd	%xmm9, 160(%rsp)
	movsd	%xmm8, 168(%rsp)
	movsd	%xmm7, 176(%rsp)
	movsd	%xmm6, 184(%rsp)
	movsd	%xmm2, 104(%rsp)
	movsd	%xmm1, 192(%rsp)
.L1196:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	128(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	136(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	144(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	152(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	160(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	168(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	176(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	184(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	192(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	200(%rsp), %edi
	call	use_int@PLT
	addq	$216, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE309:
	.size	double_mul_12, .-double_mul_12
	.globl	double_mul_13
	.type	double_mul_13, @function
double_mul_13:
.LFB310:
	.cfi_startproc
	endbr64
	subq	$232, %rsp
	.cfi_def_cfa_offset 240
	movsd	80(%rsi), %xmm3
	movsd	.LC8(%rip), %xmm1
	movapd	%xmm3, %xmm0
	mulsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2sdl	4(%rsi), %xmm2
	mulsd	.LC3(%rip), %xmm2
	movapd	%xmm3, %xmm4
	mulsd	%xmm2, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 104(%rsp)
	movsd	88(%rsi), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm1, %xmm5
	movsd	%xmm5, 128(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 8(%rsp)
	movsd	96(%rsi), %xmm4
	movapd	%xmm4, %xmm14
	mulsd	%xmm1, %xmm14
	movsd	%xmm14, 136(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 16(%rsp)
	movsd	104(%rsi), %xmm4
	movapd	%xmm4, %xmm13
	mulsd	%xmm1, %xmm13
	movsd	%xmm13, 144(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 24(%rsp)
	movsd	112(%rsi), %xmm4
	movapd	%xmm4, %xmm12
	mulsd	%xmm1, %xmm12
	movsd	%xmm12, 152(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 32(%rsp)
	movsd	120(%rsi), %xmm4
	movapd	%xmm4, %xmm11
	mulsd	%xmm1, %xmm11
	movsd	%xmm11, 160(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 40(%rsp)
	movsd	128(%rsi), %xmm4
	movapd	%xmm4, %xmm10
	mulsd	%xmm1, %xmm10
	movsd	%xmm10, 168(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 48(%rsp)
	movsd	136(%rsi), %xmm4
	movapd	%xmm4, %xmm9
	mulsd	%xmm1, %xmm9
	movsd	%xmm9, 176(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 56(%rsp)
	movsd	144(%rsi), %xmm4
	movapd	%xmm4, %xmm8
	mulsd	%xmm1, %xmm8
	movsd	%xmm8, 184(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 64(%rsp)
	movsd	152(%rsi), %xmm4
	movapd	%xmm4, %xmm7
	mulsd	%xmm1, %xmm7
	movsd	%xmm7, 192(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 72(%rsp)
	movsd	160(%rsi), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm1, %xmm6
	movsd	%xmm6, 200(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 80(%rsp)
	movsd	168(%rsi), %xmm4
	movapd	%xmm4, %xmm15
	mulsd	%xmm1, %xmm15
	movsd	%xmm15, 112(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 88(%rsp)
	movsd	176(%rsi), %xmm4
	movapd	%xmm4, %xmm15
	mulsd	%xmm1, %xmm15
	movsd	%xmm15, 120(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 96(%rsp)
	movsd	184(%rsi), %xmm4
	mulsd	%xmm4, %xmm1
	movsd	%xmm1, 208(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	movsd	%xmm2, 216(%rsp)
	testq	%rdi, %rdi
	je	.L1201
	leaq	-1(%rdi), %rax
	movapd	%xmm2, %xmm15
	movapd	%xmm5, %xmm2
	movsd	112(%rsp), %xmm5
	movsd	120(%rsp), %xmm4
	movapd	%xmm1, %xmm3
.L1202:
	mulsd	%xmm0, %xmm0
	mulsd	104(%rsp), %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm14, %xmm14
	movapd	%xmm14, %xmm1
	mulsd	16(%rsp), %xmm1
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm15, %xmm3
	mulsd	%xmm0, %xmm0
	movsd	104(%rsp), %xmm14
	mulsd	%xmm14, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	16(%rsp), %xmm1
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm15, %xmm3
	mulsd	%xmm0, %xmm0
	mulsd	%xmm14, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	16(%rsp), %xmm1
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm15, %xmm3
	mulsd	%xmm0, %xmm0
	mulsd	%xmm14, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	movapd	%xmm1, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	%xmm15, %xmm3
	movapd	%xmm3, %xmm1
	mulsd	%xmm0, %xmm0
	movsd	104(%rsp), %xmm3
	mulsd	%xmm3, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm3, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm3, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm3, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm3, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	%xmm3, %xmm0
	mulsd	%xmm2, %xmm2
	mulsd	8(%rsp), %xmm2
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	movapd	%xmm1, %xmm3
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1202
	movsd	%xmm2, 128(%rsp)
	movsd	%xmm14, 136(%rsp)
	movsd	%xmm13, 144(%rsp)
	movsd	%xmm12, 152(%rsp)
	movsd	%xmm11, 160(%rsp)
	movsd	%xmm10, 168(%rsp)
	movsd	%xmm9, 176(%rsp)
	movsd	%xmm8, 184(%rsp)
	movsd	%xmm7, 192(%rsp)
	movsd	%xmm6, 200(%rsp)
	movsd	%xmm5, 112(%rsp)
	movsd	%xmm4, 120(%rsp)
	movsd	%xmm1, 208(%rsp)
.L1201:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	128(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	136(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	144(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	152(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	160(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	168(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	176(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	184(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	192(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	200(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	208(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	216(%rsp), %edi
	call	use_int@PLT
	addq	$232, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE310:
	.size	double_mul_13, .-double_mul_13
	.globl	double_mul_14
	.type	double_mul_14, @function
double_mul_14:
.LFB311:
	.cfi_startproc
	endbr64
	subq	$248, %rsp
	.cfi_def_cfa_offset 256
	movsd	80(%rsi), %xmm3
	movsd	.LC8(%rip), %xmm1
	movapd	%xmm3, %xmm0
	mulsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsi2sdl	4(%rsi), %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 8(%rsp)
	movsd	88(%rsi), %xmm4
	movapd	%xmm4, %xmm14
	mulsd	%xmm1, %xmm14
	movsd	%xmm14, 144(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 16(%rsp)
	movsd	96(%rsi), %xmm4
	movapd	%xmm4, %xmm13
	mulsd	%xmm1, %xmm13
	movsd	%xmm13, 152(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 24(%rsp)
	movsd	104(%rsi), %xmm4
	movapd	%xmm4, %xmm12
	mulsd	%xmm1, %xmm12
	movsd	%xmm12, 160(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 32(%rsp)
	movsd	112(%rsi), %xmm4
	movapd	%xmm4, %xmm11
	mulsd	%xmm1, %xmm11
	movsd	%xmm11, 168(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 40(%rsp)
	movsd	120(%rsi), %xmm4
	movapd	%xmm4, %xmm10
	mulsd	%xmm1, %xmm10
	movsd	%xmm10, 176(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 48(%rsp)
	movsd	128(%rsi), %xmm4
	movapd	%xmm4, %xmm9
	mulsd	%xmm1, %xmm9
	movsd	%xmm9, 184(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 56(%rsp)
	movsd	136(%rsi), %xmm4
	movapd	%xmm4, %xmm8
	mulsd	%xmm1, %xmm8
	movsd	%xmm8, 192(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 64(%rsp)
	movsd	144(%rsi), %xmm4
	movapd	%xmm4, %xmm7
	mulsd	%xmm1, %xmm7
	movsd	%xmm7, 200(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 72(%rsp)
	movsd	152(%rsi), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm1, %xmm6
	movsd	%xmm6, 208(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 80(%rsp)
	movsd	160(%rsi), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm1, %xmm5
	movsd	%xmm5, 216(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 88(%rsp)
	movsd	168(%rsi), %xmm4
	movapd	%xmm4, %xmm15
	mulsd	%xmm1, %xmm15
	movsd	%xmm15, 120(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 96(%rsp)
	movsd	176(%rsi), %xmm4
	movapd	%xmm4, %xmm15
	mulsd	%xmm1, %xmm15
	movsd	%xmm15, 128(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 104(%rsp)
	movsd	184(%rsi), %xmm4
	movapd	%xmm4, %xmm15
	mulsd	%xmm1, %xmm15
	movsd	%xmm15, 136(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 112(%rsp)
	movsd	192(%rsi), %xmm4
	mulsd	%xmm4, %xmm1
	movsd	%xmm1, 224(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	movsd	%xmm2, 232(%rsp)
	testq	%rdi, %rdi
	je	.L1206
	leaq	-1(%rdi), %rax
	movapd	%xmm2, %xmm15
	movsd	120(%rsp), %xmm4
	movsd	128(%rsp), %xmm3
	movsd	136(%rsp), %xmm2
.L1207:
	mulsd	%xmm0, %xmm0
	mulsd	8(%rsp), %xmm0
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	8(%rsp), %xmm0
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	8(%rsp), %xmm0
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	8(%rsp), %xmm0
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	8(%rsp), %xmm0
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	8(%rsp), %xmm0
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	8(%rsp), %xmm0
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	8(%rsp), %xmm0
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	8(%rsp), %xmm0
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	8(%rsp), %xmm0
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	%xmm15, %xmm1
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1207
	movsd	%xmm14, 144(%rsp)
	movsd	%xmm13, 152(%rsp)
	movsd	%xmm12, 160(%rsp)
	movsd	%xmm11, 168(%rsp)
	movsd	%xmm10, 176(%rsp)
	movsd	%xmm9, 184(%rsp)
	movsd	%xmm8, 192(%rsp)
	movsd	%xmm7, 200(%rsp)
	movsd	%xmm6, 208(%rsp)
	movsd	%xmm5, 216(%rsp)
	movsd	%xmm4, 120(%rsp)
	movsd	%xmm3, 128(%rsp)
	movsd	%xmm2, 136(%rsp)
	movsd	%xmm1, 224(%rsp)
.L1206:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	144(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	152(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	160(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	168(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	176(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	184(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	192(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	200(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	208(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	216(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	128(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	136(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	224(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	232(%rsp), %edi
	call	use_int@PLT
	addq	$248, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE311:
	.size	double_mul_14, .-double_mul_14
	.globl	double_mul_15
	.type	double_mul_15, @function
double_mul_15:
.LFB312:
	.cfi_startproc
	endbr64
	subq	$264, %rsp
	.cfi_def_cfa_offset 272
	movsd	80(%rsi), %xmm3
	movsd	.LC8(%rip), %xmm1
	movapd	%xmm3, %xmm0
	mulsd	%xmm1, %xmm0
	movsd	%xmm0, 128(%rsp)
	pxor	%xmm2, %xmm2
	cvtsi2sdl	4(%rsi), %xmm2
	mulsd	.LC3(%rip), %xmm2
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm4
	movsd	.LC4(%rip), %xmm3
	divsd	%xmm3, %xmm4
	movsd	%xmm4, (%rsp)
	movsd	88(%rsi), %xmm4
	movapd	%xmm4, %xmm15
	mulsd	%xmm1, %xmm15
	movsd	%xmm15, 160(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 8(%rsp)
	movsd	96(%rsi), %xmm4
	movapd	%xmm4, %xmm14
	mulsd	%xmm1, %xmm14
	movsd	%xmm14, 168(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 16(%rsp)
	movsd	104(%rsi), %xmm4
	movapd	%xmm4, %xmm13
	mulsd	%xmm1, %xmm13
	movsd	%xmm13, 176(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 24(%rsp)
	movsd	112(%rsi), %xmm4
	movapd	%xmm4, %xmm12
	mulsd	%xmm1, %xmm12
	movsd	%xmm12, 184(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 32(%rsp)
	movsd	120(%rsi), %xmm4
	movapd	%xmm4, %xmm11
	mulsd	%xmm1, %xmm11
	movsd	%xmm11, 192(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 40(%rsp)
	movsd	128(%rsi), %xmm4
	movapd	%xmm4, %xmm10
	mulsd	%xmm1, %xmm10
	movsd	%xmm10, 200(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 48(%rsp)
	movsd	136(%rsi), %xmm4
	movapd	%xmm4, %xmm9
	mulsd	%xmm1, %xmm9
	movsd	%xmm9, 208(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 56(%rsp)
	movsd	144(%rsi), %xmm4
	movapd	%xmm4, %xmm8
	mulsd	%xmm1, %xmm8
	movsd	%xmm8, 216(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 64(%rsp)
	movsd	152(%rsi), %xmm4
	movapd	%xmm4, %xmm7
	mulsd	%xmm1, %xmm7
	movsd	%xmm7, 224(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 72(%rsp)
	movsd	160(%rsi), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm1, %xmm6
	movsd	%xmm6, 232(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 80(%rsp)
	movsd	168(%rsi), %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm1, %xmm5
	movsd	%xmm5, 240(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 88(%rsp)
	movsd	176(%rsi), %xmm4
	movapd	%xmm4, %xmm0
	mulsd	%xmm1, %xmm0
	movsd	%xmm0, 136(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 96(%rsp)
	movsd	184(%rsi), %xmm4
	movapd	%xmm4, %xmm0
	mulsd	%xmm1, %xmm0
	movsd	%xmm0, 144(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 104(%rsp)
	movsd	192(%rsi), %xmm4
	movapd	%xmm4, %xmm0
	mulsd	%xmm1, %xmm0
	movsd	%xmm0, 152(%rsp)
	mulsd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movsd	%xmm4, 112(%rsp)
	movsd	200(%rsi), %xmm4
	mulsd	%xmm4, %xmm1
	movsd	%xmm1, 248(%rsp)
	mulsd	%xmm4, %xmm2
	divsd	%xmm3, %xmm2
	movsd	%xmm2, 120(%rsp)
	testq	%rdi, %rdi
	je	.L1211
	leaq	-1(%rdi), %rax
	movsd	128(%rsp), %xmm0
	movsd	136(%rsp), %xmm4
	movsd	144(%rsp), %xmm3
	movsd	152(%rsp), %xmm2
.L1212:
	mulsd	%xmm0, %xmm0
	mulsd	(%rsp), %xmm0
	mulsd	%xmm15, %xmm15
	mulsd	8(%rsp), %xmm15
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	120(%rsp), %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	(%rsp), %xmm0
	mulsd	%xmm15, %xmm15
	mulsd	8(%rsp), %xmm15
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	120(%rsp), %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	(%rsp), %xmm0
	mulsd	%xmm15, %xmm15
	mulsd	8(%rsp), %xmm15
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	120(%rsp), %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	(%rsp), %xmm0
	mulsd	%xmm15, %xmm15
	mulsd	8(%rsp), %xmm15
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	120(%rsp), %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	(%rsp), %xmm0
	mulsd	%xmm15, %xmm15
	mulsd	8(%rsp), %xmm15
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	120(%rsp), %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	(%rsp), %xmm0
	mulsd	%xmm15, %xmm15
	mulsd	8(%rsp), %xmm15
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	120(%rsp), %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	(%rsp), %xmm0
	mulsd	%xmm15, %xmm15
	mulsd	8(%rsp), %xmm15
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	120(%rsp), %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	(%rsp), %xmm0
	mulsd	%xmm15, %xmm15
	mulsd	8(%rsp), %xmm15
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	120(%rsp), %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	(%rsp), %xmm0
	mulsd	%xmm15, %xmm15
	mulsd	8(%rsp), %xmm15
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	120(%rsp), %xmm1
	mulsd	%xmm0, %xmm0
	mulsd	(%rsp), %xmm0
	mulsd	%xmm15, %xmm15
	mulsd	8(%rsp), %xmm15
	mulsd	%xmm14, %xmm14
	mulsd	16(%rsp), %xmm14
	mulsd	%xmm13, %xmm13
	mulsd	24(%rsp), %xmm13
	mulsd	%xmm12, %xmm12
	mulsd	32(%rsp), %xmm12
	mulsd	%xmm11, %xmm11
	mulsd	40(%rsp), %xmm11
	mulsd	%xmm10, %xmm10
	mulsd	48(%rsp), %xmm10
	mulsd	%xmm9, %xmm9
	mulsd	56(%rsp), %xmm9
	mulsd	%xmm8, %xmm8
	mulsd	64(%rsp), %xmm8
	mulsd	%xmm7, %xmm7
	mulsd	72(%rsp), %xmm7
	mulsd	%xmm6, %xmm6
	mulsd	80(%rsp), %xmm6
	mulsd	%xmm5, %xmm5
	mulsd	88(%rsp), %xmm5
	mulsd	%xmm4, %xmm4
	mulsd	96(%rsp), %xmm4
	mulsd	%xmm3, %xmm3
	mulsd	104(%rsp), %xmm3
	mulsd	%xmm2, %xmm2
	mulsd	112(%rsp), %xmm2
	mulsd	%xmm1, %xmm1
	mulsd	120(%rsp), %xmm1
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1212
	movsd	%xmm0, 128(%rsp)
	movsd	%xmm15, 160(%rsp)
	movsd	%xmm14, 168(%rsp)
	movsd	%xmm13, 176(%rsp)
	movsd	%xmm12, 184(%rsp)
	movsd	%xmm11, 192(%rsp)
	movsd	%xmm10, 200(%rsp)
	movsd	%xmm9, 208(%rsp)
	movsd	%xmm8, 216(%rsp)
	movsd	%xmm7, 224(%rsp)
	movsd	%xmm6, 232(%rsp)
	movsd	%xmm5, 240(%rsp)
	movsd	%xmm4, 136(%rsp)
	movsd	%xmm3, 144(%rsp)
	movsd	%xmm2, 152(%rsp)
	movsd	%xmm1, 248(%rsp)
.L1211:
	cvttsd2sil	128(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	160(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	168(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	176(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	184(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	192(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	200(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	208(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	216(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	224(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	232(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	240(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	136(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	144(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	152(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	248(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	120(%rsp), %edi
	call	use_int@PLT
	addq	$264, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE312:
	.size	double_mul_15, .-double_mul_15
	.globl	double_div_0
	.type	double_div_0, @function
double_div_0:
.LFB313:
	.cfi_startproc
	endbr64
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	movsd	.LC9(%rip), %xmm0
	mulsd	80(%rsi), %xmm0
	movl	12(%rsi), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	mulsd	.LC10(%rip), %xmm1
	movsd	%xmm1, 8(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1216
.L1217:
	movsd	8(%rsp), %xmm2
	movapd	%xmm2, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm2, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm2, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm2, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm2, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm2, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm2, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm2, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm2, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm2, %xmm0
	divsd	%xmm1, %xmm0
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1217
.L1216:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE313:
	.size	double_div_0, .-double_div_0
	.globl	double_div_1
	.type	double_div_1, @function
double_div_1:
.LFB314:
	.cfi_startproc
	endbr64
	subq	$40, %rsp
	.cfi_def_cfa_offset 48
	movsd	.LC9(%rip), %xmm2
	movapd	%xmm2, %xmm0
	mulsd	80(%rsi), %xmm0
	movl	12(%rsi), %eax
	addl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	.LC10(%rip), %xmm1
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 16(%rsp)
	mulsd	88(%rsi), %xmm2
	movsd	%xmm2, 8(%rsp)
	pxor	%xmm2, %xmm2
	cvtsi2sdl	16(%rsi), %xmm2
	mulsd	%xmm1, %xmm2
	movsd	%xmm2, 24(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1221
.L1222:
	movsd	16(%rsp), %xmm4
	movapd	%xmm4, %xmm3
	divsd	%xmm0, %xmm3
	movsd	24(%rsp), %xmm6
	movapd	%xmm6, %xmm2
	divsd	8(%rsp), %xmm2
	movapd	%xmm4, %xmm1
	divsd	%xmm3, %xmm1
	movapd	%xmm6, %xmm0
	divsd	%xmm2, %xmm0
	movapd	%xmm4, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm6, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm4, %xmm0
	divsd	%xmm2, %xmm0
	movapd	%xmm6, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm4, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm6, %xmm0
	divsd	%xmm2, %xmm0
	movapd	%xmm4, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm6, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm4, %xmm0
	divsd	%xmm2, %xmm0
	movapd	%xmm6, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm4, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm6, %xmm0
	divsd	%xmm2, %xmm0
	movapd	%xmm4, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm6, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm4, %xmm0
	divsd	%xmm2, %xmm0
	divsd	%xmm1, %xmm6
	movsd	%xmm6, 8(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1222
.L1221:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	addq	$40, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE314:
	.size	double_div_1, .-double_div_1
	.globl	double_div_2
	.type	double_div_2, @function
double_div_2:
.LFB315:
	.cfi_startproc
	endbr64
	subq	$56, %rsp
	.cfi_def_cfa_offset 64
	movsd	.LC9(%rip), %xmm2
	movapd	%xmm2, %xmm0
	mulsd	80(%rsi), %xmm0
	movl	12(%rsi), %eax
	addl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	.LC10(%rip), %xmm1
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 24(%rsp)
	movapd	%xmm2, %xmm7
	mulsd	88(%rsi), %xmm7
	movsd	%xmm7, 8(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2sdl	16(%rsi), %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 32(%rsp)
	mulsd	96(%rsi), %xmm2
	movsd	%xmm2, 16(%rsp)
	movl	20(%rsi), %eax
	subl	$1, %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	%xmm2, 40(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1226
.L1227:
	movsd	24(%rsp), %xmm4
	movapd	%xmm4, %xmm3
	divsd	%xmm0, %xmm3
	movsd	32(%rsp), %xmm5
	movapd	%xmm5, %xmm2
	divsd	8(%rsp), %xmm2
	movsd	40(%rsp), %xmm7
	movapd	%xmm7, %xmm1
	divsd	16(%rsp), %xmm1
	movapd	%xmm4, %xmm0
	divsd	%xmm3, %xmm0
	movapd	%xmm5, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm7, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm4, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm5, %xmm6
	movapd	%xmm5, %xmm0
	divsd	%xmm3, %xmm0
	movapd	%xmm7, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm4, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm5, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm7, %xmm5
	movapd	%xmm7, %xmm0
	divsd	%xmm3, %xmm0
	movapd	%xmm4, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm6, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm7, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm4, %xmm0
	divsd	%xmm3, %xmm0
	movapd	%xmm6, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm7, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm4, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm6, %xmm0
	divsd	%xmm3, %xmm0
	movapd	%xmm7, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm4, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm6, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm7, %xmm0
	divsd	%xmm3, %xmm0
	movapd	%xmm4, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm6, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm7, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm4, %xmm0
	divsd	%xmm3, %xmm0
	divsd	%xmm2, %xmm6
	movsd	%xmm6, 8(%rsp)
	divsd	%xmm1, %xmm5
	movsd	%xmm5, 16(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1227
.L1226:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	addq	$56, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE315:
	.size	double_div_2, .-double_div_2
	.globl	double_div_3
	.type	double_div_3, @function
double_div_3:
.LFB316:
	.cfi_startproc
	endbr64
	subq	$72, %rsp
	.cfi_def_cfa_offset 80
	movsd	.LC9(%rip), %xmm2
	movapd	%xmm2, %xmm0
	mulsd	80(%rsi), %xmm0
	movl	12(%rsi), %eax
	addl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	.LC10(%rip), %xmm1
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 32(%rsp)
	movapd	%xmm2, %xmm5
	mulsd	88(%rsi), %xmm5
	movsd	%xmm5, 8(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2sdl	16(%rsi), %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 40(%rsp)
	movapd	%xmm2, %xmm5
	mulsd	96(%rsi), %xmm5
	movsd	%xmm5, 16(%rsp)
	movl	20(%rsi), %eax
	subl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 48(%rsp)
	mulsd	104(%rsi), %xmm2
	movsd	%xmm2, 24(%rsp)
	movl	24(%rsi), %eax
	subl	$2, %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	%xmm2, 56(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1231
.L1232:
	movsd	32(%rsp), %xmm5
	movapd	%xmm5, %xmm1
	divsd	%xmm0, %xmm1
	movsd	40(%rsp), %xmm6
	movapd	%xmm6, %xmm0
	divsd	8(%rsp), %xmm0
	movsd	48(%rsp), %xmm8
	movapd	%xmm8, %xmm4
	divsd	16(%rsp), %xmm4
	movsd	56(%rsp), %xmm10
	movapd	%xmm10, %xmm3
	divsd	24(%rsp), %xmm3
	movapd	%xmm5, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm6, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm8, %xmm0
	divsd	%xmm4, %xmm0
	movapd	%xmm10, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm5, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm6, %xmm7
	movapd	%xmm6, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm8, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm10, %xmm0
	divsd	%xmm4, %xmm0
	movapd	%xmm5, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm6, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm8, %xmm6
	movapd	%xmm8, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm10, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm5, %xmm0
	divsd	%xmm4, %xmm0
	movapd	%xmm7, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm8, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm10, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm5, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm7, %xmm0
	divsd	%xmm4, %xmm0
	movapd	%xmm8, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm10, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm5, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm7, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm8, %xmm0
	divsd	%xmm4, %xmm0
	movapd	%xmm10, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm5, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm7, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm8, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm10, %xmm0
	divsd	%xmm4, %xmm0
	movapd	%xmm5, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm7, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm8, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm10, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm5, %xmm0
	divsd	%xmm4, %xmm0
	divsd	%xmm3, %xmm7
	movsd	%xmm7, 8(%rsp)
	divsd	%xmm2, %xmm6
	movsd	%xmm6, 16(%rsp)
	divsd	%xmm1, %xmm10
	movsd	%xmm10, 24(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1232
.L1231:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	addq	$72, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE316:
	.size	double_div_3, .-double_div_3
	.globl	double_div_4
	.type	double_div_4, @function
double_div_4:
.LFB317:
	.cfi_startproc
	endbr64
	subq	$88, %rsp
	.cfi_def_cfa_offset 96
	movsd	.LC9(%rip), %xmm2
	movapd	%xmm2, %xmm0
	mulsd	80(%rsi), %xmm0
	movl	12(%rsi), %eax
	addl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	.LC10(%rip), %xmm1
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 40(%rsp)
	movapd	%xmm2, %xmm6
	mulsd	88(%rsi), %xmm6
	movsd	%xmm6, 8(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2sdl	16(%rsi), %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 48(%rsp)
	movapd	%xmm2, %xmm6
	mulsd	96(%rsi), %xmm6
	movsd	%xmm6, 16(%rsp)
	movl	20(%rsi), %eax
	subl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 56(%rsp)
	movapd	%xmm2, %xmm6
	mulsd	104(%rsi), %xmm6
	movsd	%xmm6, 24(%rsp)
	movl	24(%rsi), %eax
	subl	$2, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 64(%rsp)
	mulsd	112(%rsi), %xmm2
	movsd	%xmm2, 32(%rsp)
	movl	28(%rsi), %eax
	subl	$3, %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	%xmm2, 72(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1236
.L1237:
	movsd	40(%rsp), %xmm6
	movapd	%xmm6, %xmm3
	divsd	%xmm0, %xmm3
	movsd	48(%rsp), %xmm7
	movapd	%xmm7, %xmm2
	divsd	8(%rsp), %xmm2
	movsd	56(%rsp), %xmm9
	movapd	%xmm9, %xmm1
	divsd	16(%rsp), %xmm1
	movsd	64(%rsp), %xmm11
	movapd	%xmm11, %xmm0
	divsd	24(%rsp), %xmm0
	movsd	72(%rsp), %xmm13
	movapd	%xmm13, %xmm5
	divsd	32(%rsp), %xmm5
	movapd	%xmm6, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm7, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm9, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm11, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm13, %xmm0
	divsd	%xmm5, %xmm0
	movapd	%xmm6, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm7, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm9, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm11, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm13, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm6, %xmm0
	divsd	%xmm5, %xmm0
	movapd	%xmm7, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm9, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm11, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm13, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm6, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm7, %xmm0
	divsd	%xmm5, %xmm0
	movapd	%xmm9, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm11, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm13, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm6, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm7, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm9, %xmm0
	divsd	%xmm5, %xmm0
	movapd	%xmm11, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm13, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm6, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm7, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm9, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm11, %xmm0
	divsd	%xmm5, %xmm0
	movapd	%xmm13, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm6, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm7, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm9, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm11, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm13, %xmm0
	divsd	%xmm5, %xmm0
	movapd	%xmm6, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm7, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm9, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm11, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm13, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm6, %xmm0
	divsd	%xmm5, %xmm0
	divsd	%xmm4, %xmm7
	movsd	%xmm7, 8(%rsp)
	divsd	%xmm3, %xmm9
	movsd	%xmm9, 16(%rsp)
	divsd	%xmm2, %xmm11
	movsd	%xmm11, 24(%rsp)
	divsd	%xmm1, %xmm13
	movsd	%xmm13, 32(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1237
.L1236:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	addq	$88, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE317:
	.size	double_div_4, .-double_div_4
	.globl	double_div_5
	.type	double_div_5, @function
double_div_5:
.LFB318:
	.cfi_startproc
	endbr64
	subq	$104, %rsp
	.cfi_def_cfa_offset 112
	movsd	.LC9(%rip), %xmm2
	movapd	%xmm2, %xmm0
	mulsd	80(%rsi), %xmm0
	movl	12(%rsi), %eax
	addl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	.LC10(%rip), %xmm1
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 48(%rsp)
	movapd	%xmm2, %xmm7
	mulsd	88(%rsi), %xmm7
	movsd	%xmm7, 8(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2sdl	16(%rsi), %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 56(%rsp)
	movapd	%xmm2, %xmm7
	mulsd	96(%rsi), %xmm7
	movsd	%xmm7, 16(%rsp)
	movl	20(%rsi), %eax
	subl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 64(%rsp)
	movapd	%xmm2, %xmm7
	mulsd	104(%rsi), %xmm7
	movsd	%xmm7, 24(%rsp)
	movl	24(%rsi), %eax
	subl	$2, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 72(%rsp)
	movapd	%xmm2, %xmm7
	mulsd	112(%rsi), %xmm7
	movsd	%xmm7, 32(%rsp)
	movl	28(%rsi), %eax
	subl	$3, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 80(%rsp)
	mulsd	120(%rsi), %xmm2
	movsd	%xmm2, 40(%rsp)
	movl	32(%rsi), %eax
	subl	$4, %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	%xmm2, 88(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1241
.L1242:
	movsd	48(%rsp), %xmm7
	movapd	%xmm7, %xmm5
	divsd	%xmm0, %xmm5
	movsd	56(%rsp), %xmm9
	movapd	%xmm9, %xmm4
	divsd	8(%rsp), %xmm4
	movsd	64(%rsp), %xmm11
	movapd	%xmm11, %xmm3
	divsd	16(%rsp), %xmm3
	movsd	72(%rsp), %xmm13
	movapd	%xmm13, %xmm2
	divsd	24(%rsp), %xmm2
	movsd	80(%rsp), %xmm15
	movapd	%xmm15, %xmm1
	divsd	32(%rsp), %xmm1
	movsd	88(%rsp), %xmm8
	movapd	%xmm8, %xmm0
	divsd	40(%rsp), %xmm0
	movapd	%xmm7, %xmm6
	divsd	%xmm5, %xmm6
	movapd	%xmm9, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm11, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm13, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm15, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm8, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm7, %xmm0
	divsd	%xmm6, %xmm0
	movapd	%xmm9, %xmm6
	divsd	%xmm5, %xmm6
	movapd	%xmm11, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm13, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm15, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm8, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm7, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm9, %xmm0
	divsd	%xmm6, %xmm0
	movapd	%xmm11, %xmm6
	divsd	%xmm5, %xmm6
	movapd	%xmm13, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm15, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm8, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm7, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm9, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm11, %xmm0
	divsd	%xmm6, %xmm0
	movapd	%xmm13, %xmm6
	divsd	%xmm5, %xmm6
	movapd	%xmm15, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm8, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm7, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm9, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm11, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm13, %xmm0
	divsd	%xmm6, %xmm0
	movapd	%xmm15, %xmm6
	divsd	%xmm5, %xmm6
	movapd	%xmm8, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm7, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm9, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm11, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm13, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm15, %xmm0
	divsd	%xmm6, %xmm0
	movapd	%xmm8, %xmm6
	divsd	%xmm5, %xmm6
	movapd	%xmm7, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm9, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm11, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm13, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm15, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm8, %xmm0
	divsd	%xmm6, %xmm0
	movapd	%xmm7, %xmm6
	divsd	%xmm5, %xmm6
	movapd	%xmm9, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm11, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm13, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm15, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm8, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm7, %xmm0
	divsd	%xmm6, %xmm0
	divsd	%xmm5, %xmm9
	movsd	%xmm9, 8(%rsp)
	divsd	%xmm4, %xmm11
	movsd	%xmm11, 16(%rsp)
	divsd	%xmm3, %xmm13
	movsd	%xmm13, 24(%rsp)
	divsd	%xmm2, %xmm15
	movsd	%xmm15, 32(%rsp)
	divsd	%xmm1, %xmm8
	movsd	%xmm8, 40(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1242
.L1241:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE318:
	.size	double_div_5, .-double_div_5
	.globl	double_div_6
	.type	double_div_6, @function
double_div_6:
.LFB319:
	.cfi_startproc
	endbr64
	subq	$120, %rsp
	.cfi_def_cfa_offset 128
	movsd	.LC9(%rip), %xmm2
	movapd	%xmm2, %xmm0
	mulsd	80(%rsi), %xmm0
	movl	12(%rsi), %eax
	addl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	.LC10(%rip), %xmm1
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 56(%rsp)
	movapd	%xmm2, %xmm5
	mulsd	88(%rsi), %xmm5
	movsd	%xmm5, 8(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2sdl	16(%rsi), %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 64(%rsp)
	movapd	%xmm2, %xmm6
	mulsd	96(%rsi), %xmm6
	movsd	%xmm6, 16(%rsp)
	movl	20(%rsi), %eax
	subl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 72(%rsp)
	movapd	%xmm2, %xmm7
	mulsd	104(%rsi), %xmm7
	movsd	%xmm7, 24(%rsp)
	movl	24(%rsi), %eax
	subl	$2, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 80(%rsp)
	movapd	%xmm2, %xmm5
	mulsd	112(%rsi), %xmm5
	movsd	%xmm5, 32(%rsp)
	movl	28(%rsi), %eax
	subl	$3, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 88(%rsp)
	movapd	%xmm2, %xmm6
	mulsd	120(%rsi), %xmm6
	movsd	%xmm6, 40(%rsp)
	movl	32(%rsi), %eax
	subl	$4, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 96(%rsp)
	mulsd	128(%rsi), %xmm2
	movsd	%xmm2, 48(%rsp)
	movl	36(%rsi), %eax
	subl	$5, %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	%xmm2, 104(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1246
.L1247:
	movsd	56(%rsp), %xmm8
	movapd	%xmm8, %xmm7
	divsd	%xmm0, %xmm7
	movsd	64(%rsp), %xmm10
	movapd	%xmm10, %xmm6
	divsd	8(%rsp), %xmm6
	movsd	72(%rsp), %xmm12
	movapd	%xmm12, %xmm5
	divsd	16(%rsp), %xmm5
	movsd	80(%rsp), %xmm14
	movapd	%xmm14, %xmm4
	divsd	24(%rsp), %xmm4
	movsd	88(%rsp), %xmm9
	movapd	%xmm9, %xmm3
	divsd	32(%rsp), %xmm3
	movsd	96(%rsp), %xmm11
	movapd	%xmm11, %xmm2
	divsd	40(%rsp), %xmm2
	movsd	104(%rsp), %xmm13
	movapd	%xmm13, %xmm1
	divsd	48(%rsp), %xmm1
	movapd	%xmm8, %xmm0
	divsd	%xmm7, %xmm0
	movapd	%xmm10, %xmm7
	divsd	%xmm6, %xmm7
	movapd	%xmm12, %xmm6
	divsd	%xmm5, %xmm6
	movapd	%xmm14, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm9, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm11, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm13, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm8, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm10, %xmm0
	divsd	%xmm7, %xmm0
	movapd	%xmm12, %xmm7
	divsd	%xmm6, %xmm7
	movapd	%xmm14, %xmm6
	divsd	%xmm5, %xmm6
	movapd	%xmm9, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm11, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm13, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm8, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm10, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm12, %xmm0
	divsd	%xmm7, %xmm0
	movapd	%xmm14, %xmm7
	divsd	%xmm6, %xmm7
	movapd	%xmm9, %xmm6
	divsd	%xmm5, %xmm6
	movapd	%xmm11, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm13, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm8, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm10, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm12, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm14, %xmm0
	divsd	%xmm7, %xmm0
	movapd	%xmm9, %xmm7
	divsd	%xmm6, %xmm7
	movapd	%xmm11, %xmm6
	divsd	%xmm5, %xmm6
	movapd	%xmm13, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm8, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm10, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm12, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm14, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm9, %xmm0
	divsd	%xmm7, %xmm0
	movapd	%xmm11, %xmm7
	divsd	%xmm6, %xmm7
	movapd	%xmm13, %xmm6
	divsd	%xmm5, %xmm6
	movapd	%xmm8, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm10, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm12, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm14, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm9, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm11, %xmm0
	divsd	%xmm7, %xmm0
	movapd	%xmm13, %xmm7
	divsd	%xmm6, %xmm7
	movapd	%xmm8, %xmm6
	divsd	%xmm5, %xmm6
	movapd	%xmm10, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm12, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm14, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm9, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm11, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm13, %xmm0
	divsd	%xmm7, %xmm0
	movapd	%xmm8, %xmm7
	divsd	%xmm6, %xmm7
	movapd	%xmm10, %xmm6
	divsd	%xmm5, %xmm6
	movapd	%xmm12, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm14, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm9, %xmm3
	divsd	%xmm2, %xmm3
	movapd	%xmm11, %xmm2
	divsd	%xmm1, %xmm2
	movapd	%xmm13, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm8, %xmm0
	divsd	%xmm7, %xmm0
	divsd	%xmm6, %xmm10
	movsd	%xmm10, 8(%rsp)
	divsd	%xmm5, %xmm12
	movsd	%xmm12, 16(%rsp)
	divsd	%xmm4, %xmm14
	movsd	%xmm14, 24(%rsp)
	divsd	%xmm3, %xmm9
	movsd	%xmm9, 32(%rsp)
	divsd	%xmm2, %xmm11
	movsd	%xmm11, 40(%rsp)
	divsd	%xmm1, %xmm13
	movsd	%xmm13, 48(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1247
.L1246:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	addq	$120, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE319:
	.size	double_div_6, .-double_div_6
	.globl	double_div_7
	.type	double_div_7, @function
double_div_7:
.LFB320:
	.cfi_startproc
	endbr64
	subq	$136, %rsp
	.cfi_def_cfa_offset 144
	movsd	.LC9(%rip), %xmm2
	movapd	%xmm2, %xmm0
	mulsd	80(%rsi), %xmm0
	movl	12(%rsi), %eax
	addl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	.LC10(%rip), %xmm1
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm10
	movsd	%xmm3, 72(%rsp)
	movapd	%xmm2, %xmm4
	mulsd	88(%rsi), %xmm4
	movsd	%xmm4, 16(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2sdl	16(%rsi), %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm11
	movsd	%xmm3, 80(%rsp)
	movapd	%xmm2, %xmm4
	mulsd	96(%rsi), %xmm4
	movsd	%xmm4, 24(%rsp)
	movl	20(%rsi), %eax
	subl	$1, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm12
	movsd	%xmm3, 88(%rsp)
	movapd	%xmm2, %xmm4
	mulsd	104(%rsi), %xmm4
	movsd	%xmm4, 32(%rsp)
	movl	24(%rsi), %eax
	subl	$2, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm13
	movsd	%xmm3, 96(%rsp)
	movapd	%xmm2, %xmm4
	mulsd	112(%rsi), %xmm4
	movsd	%xmm4, 40(%rsp)
	movl	28(%rsi), %eax
	subl	$3, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm14
	movsd	%xmm3, 104(%rsp)
	movapd	%xmm2, %xmm4
	mulsd	120(%rsi), %xmm4
	movsd	%xmm4, 48(%rsp)
	movl	32(%rsi), %eax
	subl	$4, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm15
	movsd	%xmm3, 112(%rsp)
	movapd	%xmm2, %xmm4
	mulsd	128(%rsi), %xmm4
	movsd	%xmm4, 56(%rsp)
	movl	36(%rsi), %eax
	subl	$5, %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm9
	movsd	%xmm3, 120(%rsp)
	mulsd	136(%rsi), %xmm2
	movsd	%xmm2, 64(%rsp)
	movl	40(%rsi), %eax
	subl	$6, %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	%xmm2, 8(%rsp)
	leaq	-1(%rdi), %rax
	testq	%rdi, %rdi
	je	.L1251
.L1252:
	movapd	%xmm10, %xmm5
	divsd	%xmm0, %xmm5
	movapd	%xmm5, %xmm0
	movapd	%xmm11, %xmm8
	divsd	16(%rsp), %xmm8
	movapd	%xmm12, %xmm1
	divsd	24(%rsp), %xmm1
	movapd	%xmm13, %xmm2
	divsd	32(%rsp), %xmm2
	movapd	%xmm14, %xmm3
	divsd	40(%rsp), %xmm3
	movapd	%xmm15, %xmm4
	divsd	48(%rsp), %xmm4
	movapd	%xmm9, %xmm5
	divsd	56(%rsp), %xmm5
	movsd	8(%rsp), %xmm6
	divsd	64(%rsp), %xmm6
	movapd	%xmm10, %xmm7
	divsd	%xmm0, %xmm7
	movapd	%xmm11, %xmm0
	divsd	%xmm8, %xmm0
	movapd	%xmm12, %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm13, %xmm1
	divsd	%xmm2, %xmm1
	movapd	%xmm14, %xmm2
	divsd	%xmm3, %xmm2
	movapd	%xmm15, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm9, %xmm4
	divsd	%xmm5, %xmm4
	movsd	8(%rsp), %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm10, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm11, %xmm7
	divsd	%xmm0, %xmm7
	movapd	%xmm12, %xmm0
	divsd	%xmm8, %xmm0
	movapd	%xmm13, %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm14, %xmm1
	divsd	%xmm2, %xmm1
	movapd	%xmm15, %xmm2
	divsd	%xmm3, %xmm2
	movapd	%xmm9, %xmm3
	divsd	%xmm4, %xmm3
	movsd	8(%rsp), %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm10, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm11, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm12, %xmm7
	divsd	%xmm0, %xmm7
	movapd	%xmm13, %xmm0
	divsd	%xmm8, %xmm0
	movapd	%xmm14, %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm15, %xmm1
	divsd	%xmm2, %xmm1
	movapd	%xmm9, %xmm2
	divsd	%xmm3, %xmm2
	movsd	8(%rsp), %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm10, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm11, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm12, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm13, %xmm7
	divsd	%xmm0, %xmm7
	movapd	%xmm14, %xmm0
	divsd	%xmm8, %xmm0
	movapd	%xmm15, %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm9, %xmm1
	divsd	%xmm2, %xmm1
	movsd	8(%rsp), %xmm2
	divsd	%xmm3, %xmm2
	movapd	%xmm10, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm11, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm12, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm13, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm14, %xmm7
	divsd	%xmm0, %xmm7
	movapd	%xmm15, %xmm0
	divsd	%xmm8, %xmm0
	movapd	%xmm9, %xmm8
	divsd	%xmm1, %xmm8
	movsd	8(%rsp), %xmm1
	divsd	%xmm2, %xmm1
	movapd	%xmm10, %xmm2
	divsd	%xmm3, %xmm2
	movapd	%xmm11, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm12, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm13, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm14, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm15, %xmm7
	divsd	%xmm0, %xmm7
	movapd	%xmm9, %xmm0
	divsd	%xmm8, %xmm0
	movsd	8(%rsp), %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm10, %xmm1
	divsd	%xmm2, %xmm1
	movapd	%xmm11, %xmm2
	divsd	%xmm3, %xmm2
	movapd	%xmm12, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm13, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm14, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm15, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm9, %xmm7
	divsd	%xmm0, %xmm7
	movsd	8(%rsp), %xmm0
	divsd	%xmm8, %xmm0
	movapd	%xmm10, %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm11, %xmm1
	divsd	%xmm2, %xmm1
	movapd	%xmm12, %xmm2
	divsd	%xmm3, %xmm2
	movapd	%xmm13, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm14, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm15, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm9, %xmm6
	divsd	%xmm7, %xmm6
	movsd	8(%rsp), %xmm7
	divsd	%xmm0, %xmm7
	movapd	%xmm10, %xmm0
	divsd	%xmm8, %xmm0
	movapd	%xmm11, %xmm8
	divsd	%xmm1, %xmm8
	movsd	%xmm8, 16(%rsp)
	movapd	%xmm12, %xmm1
	divsd	%xmm2, %xmm1
	movsd	%xmm1, 24(%rsp)
	movapd	%xmm13, %xmm2
	divsd	%xmm3, %xmm2
	movsd	%xmm2, 32(%rsp)
	movapd	%xmm14, %xmm3
	divsd	%xmm4, %xmm3
	movsd	%xmm3, 40(%rsp)
	movapd	%xmm15, %xmm1
	divsd	%xmm5, %xmm1
	movsd	%xmm1, 48(%rsp)
	movapd	%xmm9, %xmm5
	divsd	%xmm6, %xmm5
	movsd	%xmm5, 56(%rsp)
	movsd	8(%rsp), %xmm4
	divsd	%xmm7, %xmm4
	movsd	%xmm4, 64(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1252
.L1251:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	addq	$136, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE320:
	.size	double_div_7, .-double_div_7
	.globl	double_div_8
	.type	double_div_8, @function
double_div_8:
.LFB321:
	.cfi_startproc
	endbr64
	subq	$152, %rsp
	.cfi_def_cfa_offset 160
	movq	%rsi, %rax
	movsd	.LC9(%rip), %xmm2
	movapd	%xmm2, %xmm0
	mulsd	80(%rsi), %xmm0
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movsd	.LC10(%rip), %xmm1
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm15
	movsd	%xmm3, 80(%rsp)
	movapd	%xmm2, %xmm13
	mulsd	88(%rax), %xmm13
	movsd	%xmm13, 8(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2sdl	16(%rax), %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm14
	movsd	%xmm3, 88(%rsp)
	movapd	%xmm2, %xmm12
	mulsd	96(%rax), %xmm12
	movsd	%xmm12, 16(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm1, %xmm4
	movsd	%xmm4, 96(%rsp)
	movapd	%xmm2, %xmm11
	mulsd	104(%rax), %xmm11
	movsd	%xmm11, 56(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movapd	%xmm3, %xmm5
	mulsd	%xmm1, %xmm5
	movsd	%xmm5, 104(%rsp)
	movapd	%xmm2, %xmm10
	mulsd	112(%rax), %xmm10
	movsd	%xmm10, 64(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movapd	%xmm3, %xmm6
	mulsd	%xmm1, %xmm6
	movsd	%xmm6, 112(%rsp)
	movapd	%xmm2, %xmm9
	mulsd	120(%rax), %xmm9
	movsd	%xmm9, 72(%rsp)
	movl	32(%rax), %esi
	leal	-4(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movapd	%xmm3, %xmm7
	mulsd	%xmm1, %xmm7
	movsd	%xmm7, 120(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	128(%rax), %xmm3
	movsd	%xmm3, 24(%rsp)
	movl	36(%rax), %esi
	leal	-5(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm8
	movsd	%xmm3, 128(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	136(%rax), %xmm3
	movsd	%xmm3, 32(%rsp)
	movl	40(%rax), %ecx
	subl	$6, %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 48(%rsp)
	mulsd	144(%rax), %xmm2
	movsd	%xmm2, 40(%rsp)
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	%xmm2, 136(%rsp)
	testq	%rdi, %rdi
	je	.L1256
	movapd	%xmm2, %xmm1
	leaq	-1(%rdi), %rax
	movsd	%xmm15, 8(%rsp)
	movapd	%xmm14, %xmm2
	movapd	%xmm4, %xmm3
	movapd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	movapd	%xmm7, %xmm6
	movapd	%xmm8, %xmm7
	movsd	48(%rsp), %xmm8
	movsd	%xmm1, 16(%rsp)
	movapd	%xmm0, %xmm14
	movapd	%xmm9, %xmm0
.L1257:
	movsd	8(%rsp), %xmm1
	movapd	%xmm1, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm2, %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm3, %xmm13
	divsd	%xmm12, %xmm13
	movapd	%xmm4, %xmm12
	divsd	%xmm11, %xmm12
	movapd	%xmm5, %xmm11
	divsd	%xmm10, %xmm11
	movapd	%xmm6, %xmm10
	divsd	%xmm0, %xmm10
	movapd	%xmm7, %xmm0
	divsd	24(%rsp), %xmm0
	movapd	%xmm8, %xmm9
	divsd	32(%rsp), %xmm9
	movsd	%xmm9, 24(%rsp)
	movsd	16(%rsp), %xmm9
	divsd	40(%rsp), %xmm9
	movsd	%xmm9, 32(%rsp)
	movapd	%xmm1, %xmm9
	divsd	%xmm15, %xmm9
	movapd	%xmm2, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm3, %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm4, %xmm13
	divsd	%xmm12, %xmm13
	movapd	%xmm5, %xmm12
	divsd	%xmm11, %xmm12
	movapd	%xmm6, %xmm11
	divsd	%xmm10, %xmm11
	movapd	%xmm7, %xmm10
	divsd	%xmm0, %xmm10
	movapd	%xmm8, %xmm0
	divsd	24(%rsp), %xmm0
	movapd	%xmm0, %xmm1
	movsd	16(%rsp), %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movsd	8(%rsp), %xmm0
	divsd	%xmm9, %xmm0
	movapd	%xmm0, %xmm9
	movapd	%xmm2, %xmm0
	divsd	%xmm15, %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm3, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm4, %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm5, %xmm13
	divsd	%xmm12, %xmm13
	movapd	%xmm6, %xmm12
	divsd	%xmm11, %xmm12
	movapd	%xmm7, %xmm11
	divsd	%xmm10, %xmm11
	movapd	%xmm8, %xmm10
	divsd	%xmm1, %xmm10
	movapd	%xmm10, %xmm1
	movsd	16(%rsp), %xmm10
	divsd	24(%rsp), %xmm10
	movapd	%xmm10, %xmm0
	movsd	8(%rsp), %xmm10
	divsd	%xmm9, %xmm10
	movsd	%xmm10, 24(%rsp)
	movapd	%xmm2, %xmm10
	divsd	32(%rsp), %xmm10
	movsd	%xmm10, 32(%rsp)
	movapd	%xmm3, %xmm9
	divsd	%xmm15, %xmm9
	movsd	%xmm9, 40(%rsp)
	movapd	%xmm4, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm5, %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm6, %xmm13
	divsd	%xmm12, %xmm13
	movapd	%xmm7, %xmm12
	divsd	%xmm11, %xmm12
	movapd	%xmm8, %xmm11
	divsd	%xmm1, %xmm11
	movsd	16(%rsp), %xmm9
	movapd	%xmm9, %xmm10
	divsd	%xmm0, %xmm10
	movsd	8(%rsp), %xmm0
	divsd	24(%rsp), %xmm0
	movapd	%xmm0, %xmm1
	movapd	%xmm2, %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm3, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm4, %xmm0
	divsd	%xmm15, %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm5, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm6, %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm7, %xmm13
	divsd	%xmm12, %xmm13
	movapd	%xmm8, %xmm12
	divsd	%xmm11, %xmm12
	movapd	%xmm9, %xmm11
	divsd	%xmm10, %xmm11
	movsd	8(%rsp), %xmm10
	divsd	%xmm1, %xmm10
	movapd	%xmm10, %xmm1
	movapd	%xmm2, %xmm0
	divsd	24(%rsp), %xmm0
	movapd	%xmm3, %xmm10
	divsd	32(%rsp), %xmm10
	movsd	%xmm10, 24(%rsp)
	movapd	%xmm4, %xmm10
	divsd	40(%rsp), %xmm10
	movsd	%xmm10, 32(%rsp)
	movapd	%xmm5, %xmm9
	divsd	%xmm15, %xmm9
	movapd	%xmm6, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm7, %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm8, %xmm13
	divsd	%xmm12, %xmm13
	movsd	16(%rsp), %xmm12
	divsd	%xmm11, %xmm12
	movsd	8(%rsp), %xmm11
	divsd	%xmm1, %xmm11
	movapd	%xmm2, %xmm10
	divsd	%xmm0, %xmm10
	movapd	%xmm3, %xmm0
	divsd	24(%rsp), %xmm0
	movapd	%xmm4, %xmm1
	divsd	32(%rsp), %xmm1
	movsd	%xmm1, 24(%rsp)
	movapd	%xmm5, %xmm1
	divsd	%xmm9, %xmm1
	movapd	%xmm1, %xmm9
	movapd	%xmm6, %xmm1
	divsd	%xmm15, %xmm1
	movsd	%xmm1, 32(%rsp)
	movapd	%xmm7, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm8, %xmm14
	divsd	%xmm13, %xmm14
	movsd	16(%rsp), %xmm13
	divsd	%xmm12, %xmm13
	movsd	8(%rsp), %xmm12
	divsd	%xmm11, %xmm12
	movapd	%xmm2, %xmm11
	divsd	%xmm10, %xmm11
	movapd	%xmm3, %xmm10
	divsd	%xmm0, %xmm10
	movapd	%xmm4, %xmm0
	divsd	24(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm5, %xmm1
	divsd	%xmm9, %xmm1
	movapd	%xmm1, %xmm9
	movapd	%xmm6, %xmm0
	divsd	32(%rsp), %xmm0
	movapd	%xmm0, %xmm1
	movapd	%xmm7, %xmm0
	divsd	%xmm15, %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm8, %xmm15
	divsd	%xmm14, %xmm15
	movsd	16(%rsp), %xmm14
	divsd	%xmm13, %xmm14
	movsd	8(%rsp), %xmm13
	divsd	%xmm12, %xmm13
	movapd	%xmm2, %xmm12
	divsd	%xmm11, %xmm12
	movapd	%xmm3, %xmm11
	divsd	%xmm10, %xmm11
	movapd	%xmm4, %xmm10
	divsd	24(%rsp), %xmm10
	movapd	%xmm5, %xmm0
	divsd	%xmm9, %xmm0
	movapd	%xmm6, %xmm9
	divsd	%xmm1, %xmm9
	movsd	%xmm9, 24(%rsp)
	movapd	%xmm7, %xmm9
	divsd	32(%rsp), %xmm9
	movapd	%xmm9, %xmm1
	movapd	%xmm8, %xmm9
	divsd	%xmm15, %xmm9
	movsd	%xmm9, 32(%rsp)
	movsd	16(%rsp), %xmm15
	divsd	%xmm14, %xmm15
	movsd	8(%rsp), %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm2, %xmm13
	divsd	%xmm12, %xmm13
	movapd	%xmm3, %xmm12
	divsd	%xmm11, %xmm12
	movapd	%xmm4, %xmm11
	divsd	%xmm10, %xmm11
	movapd	%xmm5, %xmm10
	divsd	%xmm0, %xmm10
	movapd	%xmm6, %xmm0
	divsd	24(%rsp), %xmm0
	movapd	%xmm7, %xmm9
	divsd	%xmm1, %xmm9
	movsd	%xmm9, 24(%rsp)
	movapd	%xmm8, %xmm1
	divsd	32(%rsp), %xmm1
	movsd	%xmm1, 32(%rsp)
	movsd	16(%rsp), %xmm9
	divsd	%xmm15, %xmm9
	movsd	%xmm9, 40(%rsp)
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1257
	movsd	%xmm13, 8(%rsp)
	movsd	%xmm12, 16(%rsp)
	movsd	%xmm11, 56(%rsp)
	movsd	%xmm10, 64(%rsp)
	movsd	%xmm0, 72(%rsp)
	movapd	%xmm14, %xmm0
.L1256:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	128(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	136(%rsp), %edi
	call	use_int@PLT
	addq	$152, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE321:
	.size	double_div_8, .-double_div_8
	.globl	double_div_9
	.type	double_div_9, @function
double_div_9:
.LFB322:
	.cfi_startproc
	endbr64
	subq	$168, %rsp
	.cfi_def_cfa_offset 176
	movq	%rsi, %rax
	movsd	.LC9(%rip), %xmm2
	movapd	%xmm2, %xmm0
	mulsd	80(%rsi), %xmm0
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movsd	.LC10(%rip), %xmm1
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm14
	movsd	%xmm3, 88(%rsp)
	movapd	%xmm2, %xmm4
	mulsd	88(%rax), %xmm4
	movsd	%xmm4, 24(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2sdl	16(%rax), %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm15
	movsd	%xmm3, 96(%rsp)
	movapd	%xmm2, %xmm12
	mulsd	96(%rax), %xmm12
	movsd	%xmm12, 8(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm13
	movsd	%xmm3, 104(%rsp)
	movapd	%xmm2, %xmm6
	mulsd	104(%rax), %xmm6
	movsd	%xmm6, 16(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm1, %xmm4
	movsd	%xmm4, 112(%rsp)
	movapd	%xmm2, %xmm5
	mulsd	112(%rax), %xmm5
	movsd	%xmm5, 32(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movapd	%xmm3, %xmm5
	mulsd	%xmm1, %xmm5
	movsd	%xmm5, 120(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	120(%rax), %xmm3
	movsd	%xmm3, 40(%rsp)
	movl	32(%rax), %esi
	leal	-4(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm7
	movsd	%xmm3, 128(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	128(%rax), %xmm3
	movsd	%xmm3, 48(%rsp)
	movl	36(%rax), %esi
	leal	-5(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm8
	movsd	%xmm3, 136(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	136(%rax), %xmm3
	movsd	%xmm3, 56(%rsp)
	movl	40(%rax), %esi
	leal	-6(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm9
	movsd	%xmm3, 144(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	144(%rax), %xmm3
	movsd	%xmm3, 64(%rsp)
	movl	44(%rax), %ecx
	subl	$7, %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 152(%rsp)
	mulsd	152(%rax), %xmm2
	movapd	%xmm2, %xmm11
	movsd	%xmm2, 80(%rsp)
	movl	48(%rax), %eax
	subl	$8, %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	%xmm2, 72(%rsp)
	testq	%rdi, %rdi
	je	.L1261
	movapd	%xmm3, %xmm10
	leaq	-1(%rdi), %rax
	movapd	%xmm14, %xmm1
	movapd	%xmm15, %xmm2
	movapd	%xmm13, %xmm3
	movsd	%xmm7, 8(%rsp)
	movsd	%xmm8, 16(%rsp)
	movapd	%xmm9, %xmm8
	movapd	%xmm10, %xmm9
	movsd	72(%rsp), %xmm10
	movapd	%xmm0, %xmm13
	movapd	%xmm6, %xmm0
.L1262:
	movapd	%xmm1, %xmm15
	divsd	%xmm13, %xmm15
	movapd	%xmm2, %xmm14
	divsd	24(%rsp), %xmm14
	movapd	%xmm3, %xmm13
	divsd	%xmm12, %xmm13
	movapd	%xmm4, %xmm12
	divsd	%xmm0, %xmm12
	movapd	%xmm5, %xmm0
	divsd	32(%rsp), %xmm0
	movsd	8(%rsp), %xmm6
	movapd	%xmm6, %xmm7
	divsd	40(%rsp), %xmm7
	movsd	%xmm7, 24(%rsp)
	movsd	16(%rsp), %xmm7
	divsd	48(%rsp), %xmm7
	movsd	%xmm7, 32(%rsp)
	movapd	%xmm8, %xmm7
	divsd	56(%rsp), %xmm7
	movsd	%xmm7, 40(%rsp)
	movapd	%xmm9, %xmm7
	divsd	64(%rsp), %xmm7
	movsd	%xmm7, 48(%rsp)
	movapd	%xmm10, %xmm7
	divsd	%xmm11, %xmm7
	movapd	%xmm1, %xmm11
	divsd	%xmm15, %xmm11
	movapd	%xmm2, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm3, %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm4, %xmm13
	divsd	%xmm12, %xmm13
	movapd	%xmm5, %xmm12
	divsd	%xmm0, %xmm12
	movapd	%xmm6, %xmm0
	divsd	24(%rsp), %xmm0
	movapd	%xmm0, %xmm6
	movsd	16(%rsp), %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm8, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm9, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm10, %xmm0
	divsd	%xmm7, %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm1, %xmm7
	divsd	%xmm11, %xmm7
	movsd	%xmm7, 56(%rsp)
	movapd	%xmm2, %xmm11
	divsd	%xmm15, %xmm11
	movsd	%xmm11, 64(%rsp)
	movapd	%xmm3, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm4, %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm5, %xmm13
	divsd	%xmm12, %xmm13
	movsd	8(%rsp), %xmm12
	divsd	%xmm6, %xmm12
	movsd	16(%rsp), %xmm7
	divsd	24(%rsp), %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm8, %xmm11
	divsd	32(%rsp), %xmm11
	movapd	%xmm11, %xmm6
	movapd	%xmm9, %xmm11
	divsd	40(%rsp), %xmm11
	movsd	%xmm11, 24(%rsp)
	movapd	%xmm10, %xmm11
	divsd	48(%rsp), %xmm11
	movsd	%xmm11, 32(%rsp)
	movapd	%xmm1, %xmm11
	divsd	56(%rsp), %xmm11
	movsd	%xmm11, 40(%rsp)
	movapd	%xmm2, %xmm7
	divsd	64(%rsp), %xmm7
	movsd	%xmm7, 48(%rsp)
	movapd	%xmm3, %xmm11
	divsd	%xmm15, %xmm11
	movapd	%xmm4, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm5, %xmm14
	divsd	%xmm13, %xmm14
	movsd	8(%rsp), %xmm13
	divsd	%xmm12, %xmm13
	movsd	16(%rsp), %xmm12
	divsd	%xmm0, %xmm12
	movapd	%xmm8, %xmm0
	divsd	%xmm6, %xmm0
	movapd	%xmm0, %xmm6
	movapd	%xmm9, %xmm0
	divsd	24(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm10, %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm1, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm2, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm3, %xmm0
	divsd	%xmm11, %xmm0
	movapd	%xmm0, %xmm7
	movapd	%xmm4, %xmm11
	divsd	%xmm15, %xmm11
	movsd	%xmm11, 56(%rsp)
	movapd	%xmm5, %xmm15
	divsd	%xmm14, %xmm15
	movsd	8(%rsp), %xmm14
	divsd	%xmm13, %xmm14
	movsd	16(%rsp), %xmm13
	divsd	%xmm12, %xmm13
	movapd	%xmm8, %xmm12
	divsd	%xmm6, %xmm12
	movapd	%xmm9, %xmm0
	divsd	24(%rsp), %xmm0
	movapd	%xmm10, %xmm11
	divsd	32(%rsp), %xmm11
	movapd	%xmm11, %xmm6
	movapd	%xmm1, %xmm11
	divsd	40(%rsp), %xmm11
	movsd	%xmm11, 24(%rsp)
	movapd	%xmm2, %xmm11
	divsd	48(%rsp), %xmm11
	movsd	%xmm11, 32(%rsp)
	movapd	%xmm3, %xmm11
	divsd	%xmm7, %xmm11
	movsd	%xmm11, 40(%rsp)
	movapd	%xmm4, %xmm7
	divsd	56(%rsp), %xmm7
	movsd	%xmm7, 48(%rsp)
	movapd	%xmm5, %xmm11
	divsd	%xmm15, %xmm11
	movsd	8(%rsp), %xmm15
	divsd	%xmm14, %xmm15
	movsd	16(%rsp), %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm8, %xmm13
	divsd	%xmm12, %xmm13
	movapd	%xmm9, %xmm12
	divsd	%xmm0, %xmm12
	movapd	%xmm10, %xmm0
	divsd	%xmm6, %xmm0
	movapd	%xmm0, %xmm6
	movapd	%xmm1, %xmm0
	divsd	24(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm2, %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm3, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm4, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm5, %xmm0
	divsd	%xmm11, %xmm0
	movsd	%xmm0, 56(%rsp)
	movsd	8(%rsp), %xmm7
	movapd	%xmm7, %xmm11
	divsd	%xmm15, %xmm11
	movsd	%xmm11, 64(%rsp)
	movsd	16(%rsp), %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm8, %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm9, %xmm13
	divsd	%xmm12, %xmm13
	movapd	%xmm10, %xmm12
	divsd	%xmm6, %xmm12
	movapd	%xmm1, %xmm0
	divsd	24(%rsp), %xmm0
	movapd	%xmm2, %xmm11
	divsd	32(%rsp), %xmm11
	movapd	%xmm11, %xmm6
	movapd	%xmm3, %xmm11
	divsd	40(%rsp), %xmm11
	movsd	%xmm11, 24(%rsp)
	movapd	%xmm4, %xmm11
	divsd	48(%rsp), %xmm11
	movsd	%xmm11, 32(%rsp)
	movapd	%xmm5, %xmm11
	divsd	56(%rsp), %xmm11
	movsd	%xmm11, 40(%rsp)
	divsd	64(%rsp), %xmm7
	movsd	%xmm7, 48(%rsp)
	movsd	16(%rsp), %xmm11
	divsd	%xmm15, %xmm11
	movapd	%xmm8, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm9, %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm10, %xmm13
	divsd	%xmm12, %xmm13
	movapd	%xmm1, %xmm12
	divsd	%xmm0, %xmm12
	movapd	%xmm2, %xmm0
	divsd	%xmm6, %xmm0
	movapd	%xmm0, %xmm6
	movapd	%xmm3, %xmm0
	divsd	24(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movapd	%xmm4, %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm5, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movsd	8(%rsp), %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movsd	16(%rsp), %xmm7
	divsd	%xmm11, %xmm7
	movapd	%xmm8, %xmm11
	divsd	%xmm15, %xmm11
	movapd	%xmm9, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm10, %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm1, %xmm13
	divsd	%xmm12, %xmm13
	movapd	%xmm2, %xmm12
	divsd	%xmm6, %xmm12
	movsd	%xmm12, 24(%rsp)
	movapd	%xmm3, %xmm12
	divsd	56(%rsp), %xmm12
	movapd	%xmm4, %xmm0
	divsd	32(%rsp), %xmm0
	movapd	%xmm5, %xmm6
	divsd	40(%rsp), %xmm6
	movsd	%xmm6, 32(%rsp)
	movsd	8(%rsp), %xmm6
	divsd	48(%rsp), %xmm6
	movsd	%xmm6, 40(%rsp)
	movsd	16(%rsp), %xmm6
	divsd	%xmm7, %xmm6
	movsd	%xmm6, 48(%rsp)
	movapd	%xmm8, %xmm6
	divsd	%xmm11, %xmm6
	movsd	%xmm6, 56(%rsp)
	movapd	%xmm9, %xmm6
	divsd	%xmm15, %xmm6
	movsd	%xmm6, 64(%rsp)
	movapd	%xmm10, %xmm11
	divsd	%xmm14, %xmm11
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1262
	movsd	%xmm12, 8(%rsp)
	movsd	%xmm0, 16(%rsp)
	movapd	%xmm13, %xmm0
	movsd	%xmm11, 80(%rsp)
.L1261:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	128(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	136(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	144(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	152(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	addq	$168, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE322:
	.size	double_div_9, .-double_div_9
	.globl	double_div_10
	.type	double_div_10, @function
double_div_10:
.LFB323:
	.cfi_startproc
	endbr64
	subq	$184, %rsp
	.cfi_def_cfa_offset 192
	movq	%rsi, %rax
	movsd	.LC9(%rip), %xmm2
	movapd	%xmm2, %xmm0
	mulsd	80(%rsi), %xmm0
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movsd	.LC10(%rip), %xmm1
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm13
	movsd	%xmm3, 88(%rsp)
	movapd	%xmm2, %xmm4
	mulsd	88(%rax), %xmm4
	movsd	%xmm4, 16(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2sdl	16(%rax), %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm14
	movsd	%xmm3, 96(%rsp)
	movapd	%xmm2, %xmm5
	mulsd	96(%rax), %xmm5
	movsd	%xmm5, 24(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm15
	movsd	%xmm3, 104(%rsp)
	movapd	%xmm2, %xmm7
	mulsd	104(%rax), %xmm7
	movsd	%xmm7, 32(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm1, %xmm4
	movsd	%xmm4, 112(%rsp)
	movapd	%xmm2, %xmm5
	mulsd	112(%rax), %xmm5
	movsd	%xmm5, 40(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movapd	%xmm3, %xmm5
	mulsd	%xmm1, %xmm5
	movsd	%xmm5, 120(%rsp)
	movapd	%xmm2, %xmm7
	mulsd	120(%rax), %xmm7
	movsd	%xmm7, 48(%rsp)
	movl	32(%rax), %esi
	leal	-4(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm6
	movsd	%xmm3, 128(%rsp)
	movapd	%xmm2, %xmm7
	mulsd	128(%rax), %xmm7
	movsd	%xmm7, 56(%rsp)
	movl	36(%rax), %esi
	leal	-5(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movapd	%xmm3, %xmm7
	mulsd	%xmm1, %xmm7
	movsd	%xmm7, 136(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	136(%rax), %xmm3
	movsd	%xmm3, 64(%rsp)
	movl	40(%rax), %esi
	leal	-6(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm8
	movsd	%xmm3, 144(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	144(%rax), %xmm3
	movsd	%xmm3, 72(%rsp)
	movl	44(%rax), %esi
	leal	-7(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm9
	movsd	%xmm3, 152(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	152(%rax), %xmm3
	movsd	%xmm3, 80(%rsp)
	movl	48(%rax), %ecx
	subl	$8, %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 160(%rsp)
	mulsd	160(%rax), %xmm2
	movapd	%xmm2, %xmm12
	movsd	%xmm2, (%rsp)
	movl	52(%rax), %eax
	subl	$9, %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	%xmm2, 168(%rsp)
	testq	%rdi, %rdi
	je	.L1266
	movapd	%xmm3, %xmm10
	movapd	%xmm2, %xmm11
	leaq	-1(%rdi), %rax
	movapd	%xmm13, %xmm1
	movapd	%xmm14, %xmm2
	movsd	%xmm15, (%rsp)
	movsd	%xmm6, 8(%rsp)
.L1267:
	movapd	%xmm1, %xmm15
	divsd	%xmm0, %xmm15
	movapd	%xmm2, %xmm14
	divsd	16(%rsp), %xmm14
	movsd	(%rsp), %xmm3
	movapd	%xmm3, %xmm13
	divsd	24(%rsp), %xmm13
	movapd	%xmm4, %xmm0
	divsd	32(%rsp), %xmm0
	movapd	%xmm5, %xmm6
	divsd	40(%rsp), %xmm6
	movsd	%xmm6, 16(%rsp)
	movsd	8(%rsp), %xmm6
	divsd	48(%rsp), %xmm6
	movsd	%xmm6, 24(%rsp)
	movapd	%xmm7, %xmm6
	divsd	56(%rsp), %xmm6
	movsd	%xmm6, 32(%rsp)
	movapd	%xmm8, %xmm6
	divsd	64(%rsp), %xmm6
	movsd	%xmm6, 40(%rsp)
	movapd	%xmm9, %xmm6
	divsd	72(%rsp), %xmm6
	movsd	%xmm6, 48(%rsp)
	movapd	%xmm10, %xmm6
	divsd	80(%rsp), %xmm6
	movsd	%xmm6, 56(%rsp)
	movapd	%xmm11, %xmm6
	divsd	%xmm12, %xmm6
	movapd	%xmm1, %xmm12
	divsd	%xmm15, %xmm12
	movapd	%xmm2, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm3, %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm4, %xmm13
	divsd	%xmm0, %xmm13
	movapd	%xmm5, %xmm0
	divsd	16(%rsp), %xmm0
	movapd	%xmm0, %xmm3
	movsd	8(%rsp), %xmm0
	divsd	24(%rsp), %xmm0
	movsd	%xmm0, 16(%rsp)
	movapd	%xmm7, %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm8, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm9, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm10, %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm11, %xmm0
	divsd	%xmm6, %xmm0
	movsd	%xmm0, 56(%rsp)
	movapd	%xmm1, %xmm6
	divsd	%xmm12, %xmm6
	movsd	%xmm6, 64(%rsp)
	movapd	%xmm2, %xmm12
	divsd	%xmm15, %xmm12
	movsd	%xmm12, 72(%rsp)
	movsd	(%rsp), %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm4, %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm5, %xmm13
	divsd	%xmm3, %xmm13
	movsd	8(%rsp), %xmm6
	divsd	16(%rsp), %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm7, %xmm12
	divsd	24(%rsp), %xmm12
	movapd	%xmm12, %xmm3
	movapd	%xmm8, %xmm12
	divsd	32(%rsp), %xmm12
	movsd	%xmm12, 16(%rsp)
	movapd	%xmm9, %xmm12
	divsd	40(%rsp), %xmm12
	movsd	%xmm12, 24(%rsp)
	movapd	%xmm10, %xmm12
	divsd	48(%rsp), %xmm12
	movsd	%xmm12, 32(%rsp)
	movapd	%xmm11, %xmm12
	divsd	56(%rsp), %xmm12
	movsd	%xmm12, 40(%rsp)
	movapd	%xmm1, %xmm12
	divsd	64(%rsp), %xmm12
	movsd	%xmm12, 48(%rsp)
	movapd	%xmm2, %xmm6
	divsd	72(%rsp), %xmm6
	movsd	%xmm6, 56(%rsp)
	movsd	(%rsp), %xmm12
	divsd	%xmm15, %xmm12
	movapd	%xmm4, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm5, %xmm14
	divsd	%xmm13, %xmm14
	movsd	8(%rsp), %xmm13
	divsd	%xmm0, %xmm13
	movapd	%xmm7, %xmm0
	divsd	%xmm3, %xmm0
	movapd	%xmm0, %xmm3
	movapd	%xmm8, %xmm0
	divsd	16(%rsp), %xmm0
	movsd	%xmm0, 16(%rsp)
	movapd	%xmm9, %xmm0
	divsd	24(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm10, %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm11, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm1, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm2, %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movsd	(%rsp), %xmm6
	movapd	%xmm6, %xmm0
	divsd	%xmm12, %xmm0
	movsd	%xmm0, 64(%rsp)
	movapd	%xmm4, %xmm12
	divsd	%xmm15, %xmm12
	movsd	%xmm12, 72(%rsp)
	movapd	%xmm5, %xmm15
	divsd	%xmm14, %xmm15
	movsd	8(%rsp), %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm7, %xmm13
	divsd	%xmm3, %xmm13
	movapd	%xmm8, %xmm0
	divsd	16(%rsp), %xmm0
	movapd	%xmm9, %xmm12
	divsd	24(%rsp), %xmm12
	movapd	%xmm12, %xmm3
	movapd	%xmm10, %xmm12
	divsd	32(%rsp), %xmm12
	movsd	%xmm12, 16(%rsp)
	movapd	%xmm11, %xmm12
	divsd	40(%rsp), %xmm12
	movsd	%xmm12, 24(%rsp)
	movapd	%xmm1, %xmm12
	divsd	48(%rsp), %xmm12
	movsd	%xmm12, 32(%rsp)
	movapd	%xmm2, %xmm12
	divsd	56(%rsp), %xmm12
	movsd	%xmm12, 40(%rsp)
	movapd	%xmm6, %xmm12
	divsd	64(%rsp), %xmm12
	movsd	%xmm12, 48(%rsp)
	movapd	%xmm4, %xmm6
	divsd	72(%rsp), %xmm6
	movsd	%xmm6, 56(%rsp)
	movapd	%xmm5, %xmm12
	divsd	%xmm15, %xmm12
	movsd	8(%rsp), %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm7, %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm8, %xmm13
	divsd	%xmm0, %xmm13
	movapd	%xmm9, %xmm0
	divsd	%xmm3, %xmm0
	movapd	%xmm0, %xmm3
	movapd	%xmm10, %xmm0
	divsd	16(%rsp), %xmm0
	movsd	%xmm0, 16(%rsp)
	movapd	%xmm11, %xmm0
	divsd	24(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm1, %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm2, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movsd	(%rsp), %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm4, %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movapd	%xmm5, %xmm0
	divsd	%xmm12, %xmm0
	movsd	%xmm0, 64(%rsp)
	movsd	8(%rsp), %xmm6
	movapd	%xmm6, %xmm12
	divsd	%xmm15, %xmm12
	movsd	%xmm12, 72(%rsp)
	movapd	%xmm7, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm8, %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm9, %xmm13
	divsd	%xmm3, %xmm13
	movapd	%xmm10, %xmm0
	divsd	16(%rsp), %xmm0
	movapd	%xmm11, %xmm12
	divsd	24(%rsp), %xmm12
	movapd	%xmm12, %xmm3
	movapd	%xmm1, %xmm12
	divsd	32(%rsp), %xmm12
	movsd	%xmm12, 16(%rsp)
	movapd	%xmm2, %xmm12
	divsd	40(%rsp), %xmm12
	movsd	%xmm12, 24(%rsp)
	movsd	(%rsp), %xmm12
	divsd	48(%rsp), %xmm12
	movsd	%xmm12, 32(%rsp)
	movapd	%xmm4, %xmm12
	divsd	56(%rsp), %xmm12
	movsd	%xmm12, 40(%rsp)
	movapd	%xmm5, %xmm12
	divsd	64(%rsp), %xmm12
	movsd	%xmm12, 48(%rsp)
	divsd	72(%rsp), %xmm6
	movsd	%xmm6, 56(%rsp)
	movapd	%xmm7, %xmm12
	divsd	%xmm15, %xmm12
	movapd	%xmm8, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm9, %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm10, %xmm13
	divsd	%xmm0, %xmm13
	movapd	%xmm11, %xmm0
	divsd	%xmm3, %xmm0
	movapd	%xmm0, %xmm3
	movapd	%xmm1, %xmm0
	divsd	16(%rsp), %xmm0
	movsd	%xmm0, 16(%rsp)
	movapd	%xmm2, %xmm0
	divsd	24(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movsd	(%rsp), %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm4, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm5, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movsd	8(%rsp), %xmm6
	movapd	%xmm6, %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movapd	%xmm7, %xmm0
	divsd	%xmm12, %xmm0
	movsd	%xmm0, 64(%rsp)
	movapd	%xmm8, %xmm12
	divsd	%xmm15, %xmm12
	movapd	%xmm9, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm10, %xmm14
	divsd	%xmm13, %xmm14
	movapd	%xmm11, %xmm13
	divsd	%xmm3, %xmm13
	movapd	%xmm1, %xmm0
	divsd	16(%rsp), %xmm0
	movapd	%xmm2, %xmm3
	divsd	24(%rsp), %xmm3
	movsd	%xmm3, 16(%rsp)
	movsd	(%rsp), %xmm3
	divsd	32(%rsp), %xmm3
	movsd	%xmm3, 24(%rsp)
	movapd	%xmm4, %xmm3
	divsd	40(%rsp), %xmm3
	movsd	%xmm3, 32(%rsp)
	movapd	%xmm5, %xmm3
	divsd	48(%rsp), %xmm3
	movsd	%xmm3, 40(%rsp)
	movapd	%xmm6, %xmm3
	divsd	56(%rsp), %xmm3
	movsd	%xmm3, 48(%rsp)
	movapd	%xmm7, %xmm3
	divsd	64(%rsp), %xmm3
	movsd	%xmm3, 56(%rsp)
	movapd	%xmm8, %xmm3
	divsd	%xmm12, %xmm3
	movsd	%xmm3, 64(%rsp)
	movapd	%xmm9, %xmm3
	divsd	%xmm15, %xmm3
	movsd	%xmm3, 72(%rsp)
	movapd	%xmm10, %xmm3
	divsd	%xmm14, %xmm3
	movsd	%xmm3, 80(%rsp)
	movapd	%xmm11, %xmm12
	divsd	%xmm13, %xmm12
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1267
	movsd	%xmm12, (%rsp)
.L1266:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	128(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	136(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	144(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	152(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	160(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	168(%rsp), %edi
	call	use_int@PLT
	addq	$184, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE323:
	.size	double_div_10, .-double_div_10
	.globl	double_div_11
	.type	double_div_11, @function
double_div_11:
.LFB324:
	.cfi_startproc
	endbr64
	subq	$216, %rsp
	.cfi_def_cfa_offset 224
	movq	%rsi, %rax
	movsd	.LC9(%rip), %xmm2
	movapd	%xmm2, %xmm0
	mulsd	80(%rsi), %xmm0
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movsd	.LC10(%rip), %xmm1
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm15
	movsd	%xmm3, 120(%rsp)
	movapd	%xmm2, %xmm5
	mulsd	88(%rax), %xmm5
	movsd	%xmm5, 24(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2sdl	16(%rax), %xmm3
	movapd	%xmm3, %xmm5
	mulsd	%xmm1, %xmm5
	movsd	%xmm5, 128(%rsp)
	movapd	%xmm2, %xmm6
	mulsd	96(%rax), %xmm6
	movsd	%xmm6, 32(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movapd	%xmm3, %xmm6
	mulsd	%xmm1, %xmm6
	movsd	%xmm6, 136(%rsp)
	movapd	%xmm2, %xmm7
	mulsd	104(%rax), %xmm7
	movsd	%xmm7, 40(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm4
	movsd	%xmm3, 144(%rsp)
	movapd	%xmm2, %xmm7
	mulsd	112(%rax), %xmm7
	movsd	%xmm7, 48(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movapd	%xmm3, %xmm7
	mulsd	%xmm1, %xmm7
	movsd	%xmm7, 152(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	120(%rax), %xmm3
	movsd	%xmm3, 56(%rsp)
	movl	32(%rax), %esi
	leal	-4(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm8
	movsd	%xmm3, 160(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	128(%rax), %xmm3
	movsd	%xmm3, 64(%rsp)
	movl	36(%rax), %esi
	leal	-5(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm9
	movsd	%xmm3, 168(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	136(%rax), %xmm3
	movsd	%xmm3, 72(%rsp)
	movl	40(%rax), %esi
	leal	-6(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm10
	movsd	%xmm3, 176(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	144(%rax), %xmm3
	movsd	%xmm3, 80(%rsp)
	movl	44(%rax), %esi
	leal	-7(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm11
	movsd	%xmm3, 184(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	152(%rax), %xmm3
	movsd	%xmm3, 88(%rsp)
	movl	48(%rax), %esi
	leal	-8(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm12
	movsd	%xmm3, 192(%rsp)
	movapd	%xmm2, %xmm13
	mulsd	160(%rax), %xmm13
	movsd	%xmm13, 8(%rsp)
	movl	52(%rax), %ecx
	subl	$9, %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 200(%rsp)
	mulsd	168(%rax), %xmm2
	movapd	%xmm2, %xmm14
	movsd	%xmm2, 16(%rsp)
	movl	56(%rax), %eax
	subl	$10, %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	%xmm2, 112(%rsp)
	testq	%rdi, %rdi
	je	.L1271
	leaq	-1(%rdi), %rax
	movapd	%xmm15, %xmm1
	movapd	%xmm5, %xmm2
	movsd	%xmm6, 8(%rsp)
	movsd	%xmm4, 16(%rsp)
	movapd	%xmm7, %xmm5
	movapd	%xmm8, %xmm6
	movapd	%xmm9, %xmm7
	movapd	%xmm10, %xmm8
	movapd	%xmm11, %xmm9
	movapd	%xmm12, %xmm10
	movapd	%xmm3, %xmm11
	movsd	112(%rsp), %xmm12
.L1272:
	movapd	%xmm1, %xmm15
	divsd	%xmm0, %xmm15
	movapd	%xmm2, %xmm0
	divsd	24(%rsp), %xmm0
	movsd	8(%rsp), %xmm3
	movapd	%xmm3, %xmm4
	divsd	32(%rsp), %xmm4
	movsd	%xmm4, 24(%rsp)
	movsd	16(%rsp), %xmm4
	divsd	40(%rsp), %xmm4
	movsd	%xmm4, 32(%rsp)
	movapd	%xmm5, %xmm4
	divsd	48(%rsp), %xmm4
	movsd	%xmm4, 40(%rsp)
	movapd	%xmm6, %xmm4
	divsd	56(%rsp), %xmm4
	movsd	%xmm4, 48(%rsp)
	movapd	%xmm7, %xmm4
	divsd	64(%rsp), %xmm4
	movsd	%xmm4, 56(%rsp)
	movapd	%xmm8, %xmm4
	divsd	72(%rsp), %xmm4
	movsd	%xmm4, 64(%rsp)
	movapd	%xmm9, %xmm4
	divsd	80(%rsp), %xmm4
	movsd	%xmm4, 72(%rsp)
	movapd	%xmm10, %xmm4
	divsd	88(%rsp), %xmm4
	movsd	%xmm4, 80(%rsp)
	movapd	%xmm11, %xmm4
	divsd	%xmm13, %xmm4
	movapd	%xmm12, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm1, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm2, %xmm15
	divsd	%xmm0, %xmm15
	movapd	%xmm3, %xmm0
	divsd	24(%rsp), %xmm0
	movapd	%xmm0, %xmm3
	movsd	16(%rsp), %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm5, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm6, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm7, %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm8, %xmm0
	divsd	64(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movapd	%xmm9, %xmm0
	divsd	72(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	movapd	%xmm10, %xmm0
	divsd	80(%rsp), %xmm0
	movsd	%xmm0, 72(%rsp)
	movapd	%xmm11, %xmm0
	divsd	%xmm4, %xmm0
	movsd	%xmm0, 80(%rsp)
	movapd	%xmm12, %xmm4
	divsd	%xmm13, %xmm4
	movsd	%xmm4, 88(%rsp)
	movapd	%xmm1, %xmm13
	divsd	%xmm14, %xmm13
	movsd	%xmm13, 96(%rsp)
	movapd	%xmm2, %xmm14
	divsd	%xmm15, %xmm14
	movsd	8(%rsp), %xmm15
	divsd	%xmm3, %xmm15
	movsd	16(%rsp), %xmm4
	divsd	24(%rsp), %xmm4
	movapd	%xmm4, %xmm0
	movapd	%xmm5, %xmm13
	divsd	32(%rsp), %xmm13
	movapd	%xmm13, %xmm3
	movapd	%xmm6, %xmm13
	divsd	40(%rsp), %xmm13
	movsd	%xmm13, 24(%rsp)
	movapd	%xmm7, %xmm13
	divsd	48(%rsp), %xmm13
	movsd	%xmm13, 32(%rsp)
	movapd	%xmm8, %xmm13
	divsd	56(%rsp), %xmm13
	movsd	%xmm13, 40(%rsp)
	movapd	%xmm9, %xmm13
	divsd	64(%rsp), %xmm13
	movsd	%xmm13, 48(%rsp)
	movapd	%xmm10, %xmm13
	divsd	72(%rsp), %xmm13
	movsd	%xmm13, 56(%rsp)
	movapd	%xmm11, %xmm13
	divsd	80(%rsp), %xmm13
	movsd	%xmm13, 64(%rsp)
	movapd	%xmm12, %xmm13
	divsd	88(%rsp), %xmm13
	movsd	%xmm13, 72(%rsp)
	movapd	%xmm1, %xmm4
	divsd	96(%rsp), %xmm4
	movsd	%xmm4, 80(%rsp)
	movapd	%xmm2, %xmm13
	divsd	%xmm14, %xmm13
	movsd	8(%rsp), %xmm14
	divsd	%xmm15, %xmm14
	movsd	16(%rsp), %xmm15
	divsd	%xmm0, %xmm15
	movapd	%xmm5, %xmm0
	divsd	%xmm3, %xmm0
	movapd	%xmm0, %xmm3
	movapd	%xmm6, %xmm0
	divsd	24(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm7, %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm8, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm9, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm10, %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movapd	%xmm11, %xmm0
	divsd	64(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	movapd	%xmm12, %xmm0
	divsd	72(%rsp), %xmm0
	movsd	%xmm0, 72(%rsp)
	movapd	%xmm1, %xmm0
	divsd	80(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	movapd	%xmm2, %xmm0
	divsd	%xmm13, %xmm0
	movsd	%xmm0, 88(%rsp)
	movsd	8(%rsp), %xmm4
	movapd	%xmm4, %xmm13
	divsd	%xmm14, %xmm13
	movsd	%xmm13, 96(%rsp)
	movsd	16(%rsp), %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm5, %xmm15
	divsd	%xmm3, %xmm15
	movapd	%xmm6, %xmm0
	divsd	24(%rsp), %xmm0
	movapd	%xmm7, %xmm13
	divsd	32(%rsp), %xmm13
	movapd	%xmm13, %xmm3
	movapd	%xmm8, %xmm13
	divsd	40(%rsp), %xmm13
	movsd	%xmm13, 24(%rsp)
	movapd	%xmm9, %xmm13
	divsd	48(%rsp), %xmm13
	movsd	%xmm13, 32(%rsp)
	movapd	%xmm10, %xmm13
	divsd	56(%rsp), %xmm13
	movsd	%xmm13, 40(%rsp)
	movapd	%xmm11, %xmm13
	divsd	64(%rsp), %xmm13
	movsd	%xmm13, 48(%rsp)
	movapd	%xmm12, %xmm13
	divsd	72(%rsp), %xmm13
	movsd	%xmm13, 56(%rsp)
	movapd	%xmm1, %xmm13
	divsd	80(%rsp), %xmm13
	movsd	%xmm13, 64(%rsp)
	movapd	%xmm2, %xmm13
	divsd	88(%rsp), %xmm13
	movsd	%xmm13, 72(%rsp)
	divsd	96(%rsp), %xmm4
	movsd	%xmm4, 80(%rsp)
	movsd	16(%rsp), %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm5, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm6, %xmm15
	divsd	%xmm0, %xmm15
	movapd	%xmm7, %xmm0
	divsd	%xmm3, %xmm0
	movapd	%xmm0, %xmm3
	movapd	%xmm8, %xmm0
	divsd	24(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm9, %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm10, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm11, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm12, %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movapd	%xmm1, %xmm0
	divsd	64(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	movapd	%xmm2, %xmm0
	divsd	72(%rsp), %xmm0
	movsd	%xmm0, 72(%rsp)
	movsd	8(%rsp), %xmm0
	divsd	80(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	movsd	16(%rsp), %xmm4
	divsd	%xmm13, %xmm4
	movapd	%xmm5, %xmm13
	divsd	%xmm14, %xmm13
	movsd	%xmm13, 88(%rsp)
	movapd	%xmm6, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm7, %xmm15
	divsd	%xmm3, %xmm15
	movapd	%xmm8, %xmm0
	divsd	24(%rsp), %xmm0
	movapd	%xmm9, %xmm13
	divsd	32(%rsp), %xmm13
	movapd	%xmm13, %xmm3
	movapd	%xmm10, %xmm13
	divsd	40(%rsp), %xmm13
	movsd	%xmm13, 24(%rsp)
	movapd	%xmm11, %xmm13
	divsd	48(%rsp), %xmm13
	movsd	%xmm13, 32(%rsp)
	movapd	%xmm12, %xmm13
	divsd	56(%rsp), %xmm13
	movsd	%xmm13, 40(%rsp)
	movapd	%xmm1, %xmm13
	divsd	64(%rsp), %xmm13
	movsd	%xmm13, 48(%rsp)
	movapd	%xmm2, %xmm13
	divsd	72(%rsp), %xmm13
	movsd	%xmm13, 56(%rsp)
	movsd	8(%rsp), %xmm13
	divsd	80(%rsp), %xmm13
	movsd	%xmm13, 64(%rsp)
	movsd	16(%rsp), %xmm13
	divsd	%xmm4, %xmm13
	movsd	%xmm13, 72(%rsp)
	movapd	%xmm5, %xmm4
	divsd	88(%rsp), %xmm4
	movsd	%xmm4, 80(%rsp)
	movapd	%xmm6, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm7, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm8, %xmm15
	divsd	%xmm0, %xmm15
	movapd	%xmm9, %xmm0
	divsd	%xmm3, %xmm0
	movapd	%xmm0, %xmm3
	movapd	%xmm10, %xmm0
	divsd	24(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm11, %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm12, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm1, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm2, %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movsd	8(%rsp), %xmm0
	divsd	64(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	movsd	16(%rsp), %xmm0
	divsd	72(%rsp), %xmm0
	movsd	%xmm0, 72(%rsp)
	movapd	%xmm5, %xmm0
	divsd	80(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	movapd	%xmm6, %xmm4
	divsd	%xmm13, %xmm4
	movsd	%xmm4, 88(%rsp)
	movapd	%xmm7, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm15
	divsd	%xmm3, %xmm15
	movapd	%xmm10, %xmm4
	divsd	24(%rsp), %xmm4
	movsd	%xmm4, 96(%rsp)
	movapd	%xmm11, %xmm4
	divsd	32(%rsp), %xmm4
	movsd	%xmm4, 104(%rsp)
	movapd	%xmm12, %xmm4
	divsd	40(%rsp), %xmm4
	movapd	%xmm1, %xmm0
	divsd	48(%rsp), %xmm0
	movapd	%xmm2, %xmm3
	divsd	56(%rsp), %xmm3
	movsd	%xmm3, 24(%rsp)
	movsd	8(%rsp), %xmm3
	divsd	64(%rsp), %xmm3
	movsd	%xmm3, 32(%rsp)
	movsd	16(%rsp), %xmm3
	divsd	72(%rsp), %xmm3
	movsd	%xmm3, 40(%rsp)
	movapd	%xmm5, %xmm3
	divsd	80(%rsp), %xmm3
	movsd	%xmm3, 48(%rsp)
	movapd	%xmm6, %xmm3
	divsd	88(%rsp), %xmm3
	movsd	%xmm3, 56(%rsp)
	movapd	%xmm7, %xmm3
	divsd	%xmm13, %xmm3
	movsd	%xmm3, 64(%rsp)
	movapd	%xmm8, %xmm3
	divsd	%xmm14, %xmm3
	movsd	%xmm3, 72(%rsp)
	movapd	%xmm9, %xmm3
	divsd	%xmm15, %xmm3
	movsd	%xmm3, 80(%rsp)
	movapd	%xmm10, %xmm3
	divsd	96(%rsp), %xmm3
	movsd	%xmm3, 88(%rsp)
	movapd	%xmm11, %xmm13
	divsd	104(%rsp), %xmm13
	movapd	%xmm12, %xmm14
	divsd	%xmm4, %xmm14
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1272
	movsd	%xmm13, 8(%rsp)
	movsd	%xmm14, 16(%rsp)
.L1271:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	128(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	136(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	144(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	152(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	160(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	168(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	176(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	184(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	192(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	200(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	addq	$216, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE324:
	.size	double_div_11, .-double_div_11
	.globl	double_div_12
	.type	double_div_12, @function
double_div_12:
.LFB325:
	.cfi_startproc
	endbr64
	subq	$248, %rsp
	.cfi_def_cfa_offset 256
	movq	%rsi, %rax
	movsd	.LC9(%rip), %xmm2
	movapd	%xmm2, %xmm0
	mulsd	80(%rsi), %xmm0
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movsd	.LC10(%rip), %xmm1
	movapd	%xmm3, %xmm5
	mulsd	%xmm1, %xmm5
	movsd	%xmm5, 144(%rsp)
	movapd	%xmm2, %xmm6
	mulsd	88(%rax), %xmm6
	movsd	%xmm6, 24(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2sdl	16(%rax), %xmm3
	movapd	%xmm3, %xmm6
	mulsd	%xmm1, %xmm6
	movsd	%xmm6, 152(%rsp)
	movapd	%xmm2, %xmm7
	mulsd	96(%rax), %xmm7
	movsd	%xmm7, 32(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movapd	%xmm3, %xmm7
	mulsd	%xmm1, %xmm7
	movsd	%xmm7, 160(%rsp)
	movapd	%xmm2, %xmm4
	mulsd	104(%rax), %xmm4
	movsd	%xmm4, 40(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm1, %xmm4
	movsd	%xmm4, 168(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	112(%rax), %xmm3
	movsd	%xmm3, 48(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm8
	movsd	%xmm3, 176(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	120(%rax), %xmm3
	movsd	%xmm3, 56(%rsp)
	movl	32(%rax), %esi
	leal	-4(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm9
	movsd	%xmm3, 184(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	128(%rax), %xmm3
	movsd	%xmm3, 64(%rsp)
	movl	36(%rax), %esi
	leal	-5(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm10
	movsd	%xmm3, 192(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	136(%rax), %xmm3
	movsd	%xmm3, 72(%rsp)
	movl	40(%rax), %esi
	leal	-6(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm11
	movsd	%xmm3, 200(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	144(%rax), %xmm3
	movsd	%xmm3, 80(%rsp)
	movl	44(%rax), %esi
	leal	-7(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm12
	movsd	%xmm3, 208(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	152(%rax), %xmm3
	movsd	%xmm3, 88(%rsp)
	movl	48(%rax), %esi
	leal	-8(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm13
	movsd	%xmm3, 216(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	160(%rax), %xmm3
	movsd	%xmm3, 96(%rsp)
	movl	52(%rax), %esi
	leal	-9(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm15
	movsd	%xmm3, 224(%rsp)
	movapd	%xmm2, %xmm14
	mulsd	168(%rax), %xmm14
	movsd	%xmm14, 8(%rsp)
	movl	56(%rax), %ecx
	subl	$10, %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm1, %xmm3
	movsd	%xmm3, 232(%rsp)
	mulsd	176(%rax), %xmm2
	movsd	%xmm2, 104(%rsp)
	movl	60(%rax), %eax
	subl	$11, %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	mulsd	%xmm1, %xmm2
	movsd	%xmm2, 136(%rsp)
	testq	%rdi, %rdi
	je	.L1276
	leaq	-1(%rdi), %rax
	movapd	%xmm5, %xmm1
	movapd	%xmm6, %xmm2
	movsd	%xmm7, 16(%rsp)
	movsd	%xmm4, 8(%rsp)
	movapd	%xmm8, %xmm5
	movapd	%xmm9, %xmm6
	movapd	%xmm10, %xmm7
	movapd	%xmm11, %xmm8
	movapd	%xmm12, %xmm9
	movapd	%xmm13, %xmm10
	movapd	%xmm15, %xmm11
	movapd	%xmm3, %xmm12
	movsd	136(%rsp), %xmm13
	movsd	104(%rsp), %xmm15
.L1277:
	movapd	%xmm1, %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 104(%rsp)
	movapd	%xmm2, %xmm0
	divsd	24(%rsp), %xmm0
	movsd	16(%rsp), %xmm4
	movapd	%xmm4, %xmm3
	divsd	32(%rsp), %xmm3
	movsd	%xmm3, 16(%rsp)
	movsd	8(%rsp), %xmm3
	divsd	40(%rsp), %xmm3
	movsd	%xmm3, 24(%rsp)
	movapd	%xmm5, %xmm3
	divsd	48(%rsp), %xmm3
	movsd	%xmm3, 32(%rsp)
	movapd	%xmm6, %xmm3
	divsd	56(%rsp), %xmm3
	movsd	%xmm3, 40(%rsp)
	movapd	%xmm7, %xmm3
	divsd	64(%rsp), %xmm3
	movsd	%xmm3, 48(%rsp)
	movapd	%xmm8, %xmm3
	divsd	72(%rsp), %xmm3
	movsd	%xmm3, 56(%rsp)
	movapd	%xmm9, %xmm3
	divsd	80(%rsp), %xmm3
	movsd	%xmm3, 64(%rsp)
	movapd	%xmm10, %xmm3
	divsd	88(%rsp), %xmm3
	movsd	%xmm3, 72(%rsp)
	movapd	%xmm11, %xmm3
	divsd	96(%rsp), %xmm3
	movsd	%xmm3, 80(%rsp)
	movapd	%xmm12, %xmm3
	divsd	%xmm14, %xmm3
	movsd	%xmm3, 88(%rsp)
	movapd	%xmm13, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm1, %xmm15
	divsd	104(%rsp), %xmm15
	movapd	%xmm2, %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 96(%rsp)
	movapd	%xmm4, %xmm3
	movapd	%xmm4, %xmm0
	divsd	16(%rsp), %xmm0
	movapd	%xmm0, %xmm4
	movsd	8(%rsp), %xmm0
	divsd	24(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm5, %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm6, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm7, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm8, %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movapd	%xmm9, %xmm0
	divsd	64(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	movapd	%xmm10, %xmm0
	divsd	72(%rsp), %xmm0
	movsd	%xmm0, 72(%rsp)
	movapd	%xmm11, %xmm0
	divsd	80(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	movapd	%xmm12, %xmm0
	divsd	88(%rsp), %xmm0
	movsd	%xmm0, 88(%rsp)
	movapd	%xmm13, %xmm0
	divsd	%xmm14, %xmm0
	movsd	%xmm0, 104(%rsp)
	movapd	%xmm1, %xmm14
	divsd	%xmm15, %xmm14
	movsd	%xmm14, 112(%rsp)
	movapd	%xmm2, %xmm15
	divsd	96(%rsp), %xmm15
	movsd	%xmm3, 16(%rsp)
	divsd	%xmm4, %xmm3
	movsd	%xmm3, 96(%rsp)
	movsd	8(%rsp), %xmm3
	movapd	%xmm3, %xmm0
	divsd	24(%rsp), %xmm0
	movapd	%xmm5, %xmm14
	divsd	32(%rsp), %xmm14
	movapd	%xmm14, %xmm4
	movapd	%xmm6, %xmm14
	divsd	40(%rsp), %xmm14
	movsd	%xmm14, 24(%rsp)
	movapd	%xmm7, %xmm14
	divsd	48(%rsp), %xmm14
	movsd	%xmm14, 32(%rsp)
	movapd	%xmm8, %xmm14
	divsd	56(%rsp), %xmm14
	movsd	%xmm14, 40(%rsp)
	movapd	%xmm9, %xmm14
	divsd	64(%rsp), %xmm14
	movsd	%xmm14, 48(%rsp)
	movapd	%xmm10, %xmm14
	divsd	72(%rsp), %xmm14
	movsd	%xmm14, 56(%rsp)
	movapd	%xmm11, %xmm14
	divsd	80(%rsp), %xmm14
	movsd	%xmm14, 64(%rsp)
	movapd	%xmm12, %xmm14
	divsd	88(%rsp), %xmm14
	movsd	%xmm14, 72(%rsp)
	movapd	%xmm13, %xmm14
	divsd	104(%rsp), %xmm14
	movsd	%xmm14, 80(%rsp)
	movapd	%xmm1, %xmm14
	divsd	112(%rsp), %xmm14
	movsd	%xmm14, 88(%rsp)
	movapd	%xmm2, %xmm14
	divsd	%xmm15, %xmm14
	movsd	16(%rsp), %xmm15
	divsd	96(%rsp), %xmm15
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 96(%rsp)
	movapd	%xmm5, %xmm0
	divsd	%xmm4, %xmm0
	movapd	%xmm0, %xmm4
	movapd	%xmm6, %xmm0
	divsd	24(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm7, %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm8, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm9, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm10, %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movapd	%xmm11, %xmm0
	divsd	64(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	movapd	%xmm12, %xmm0
	divsd	72(%rsp), %xmm0
	movsd	%xmm0, 72(%rsp)
	movapd	%xmm13, %xmm0
	divsd	80(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	movapd	%xmm1, %xmm0
	divsd	88(%rsp), %xmm0
	movsd	%xmm0, 88(%rsp)
	movapd	%xmm2, %xmm0
	divsd	%xmm14, %xmm0
	movsd	%xmm0, 104(%rsp)
	movsd	16(%rsp), %xmm3
	movapd	%xmm3, %xmm14
	divsd	%xmm15, %xmm14
	movsd	%xmm14, 112(%rsp)
	movsd	8(%rsp), %xmm15
	divsd	96(%rsp), %xmm15
	movapd	%xmm5, %xmm0
	divsd	%xmm4, %xmm0
	movsd	%xmm0, 96(%rsp)
	movapd	%xmm6, %xmm0
	divsd	24(%rsp), %xmm0
	movapd	%xmm7, %xmm14
	divsd	32(%rsp), %xmm14
	movapd	%xmm14, %xmm4
	movapd	%xmm8, %xmm14
	divsd	40(%rsp), %xmm14
	movsd	%xmm14, 24(%rsp)
	movapd	%xmm9, %xmm14
	divsd	48(%rsp), %xmm14
	movsd	%xmm14, 32(%rsp)
	movapd	%xmm10, %xmm14
	divsd	56(%rsp), %xmm14
	movsd	%xmm14, 40(%rsp)
	movapd	%xmm11, %xmm14
	divsd	64(%rsp), %xmm14
	movsd	%xmm14, 48(%rsp)
	movapd	%xmm12, %xmm14
	divsd	72(%rsp), %xmm14
	movsd	%xmm14, 56(%rsp)
	movapd	%xmm13, %xmm14
	divsd	80(%rsp), %xmm14
	movsd	%xmm14, 64(%rsp)
	movapd	%xmm1, %xmm14
	divsd	88(%rsp), %xmm14
	movsd	%xmm14, 72(%rsp)
	movapd	%xmm2, %xmm14
	divsd	104(%rsp), %xmm14
	movsd	%xmm14, 80(%rsp)
	divsd	112(%rsp), %xmm3
	movsd	%xmm3, 88(%rsp)
	movsd	8(%rsp), %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm5, %xmm15
	divsd	96(%rsp), %xmm15
	movapd	%xmm6, %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 96(%rsp)
	movapd	%xmm7, %xmm0
	divsd	%xmm4, %xmm0
	movapd	%xmm0, %xmm4
	movapd	%xmm8, %xmm0
	divsd	24(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm9, %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm10, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm11, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm12, %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movapd	%xmm13, %xmm0
	divsd	64(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	movapd	%xmm1, %xmm0
	divsd	72(%rsp), %xmm0
	movsd	%xmm0, 72(%rsp)
	movapd	%xmm2, %xmm0
	divsd	80(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	movsd	16(%rsp), %xmm0
	divsd	88(%rsp), %xmm0
	movsd	%xmm0, 88(%rsp)
	movsd	8(%rsp), %xmm3
	movapd	%xmm3, %xmm0
	divsd	%xmm14, %xmm0
	movsd	%xmm0, 104(%rsp)
	movapd	%xmm5, %xmm14
	divsd	%xmm15, %xmm14
	movsd	%xmm14, 112(%rsp)
	movapd	%xmm6, %xmm15
	divsd	96(%rsp), %xmm15
	movapd	%xmm7, %xmm0
	divsd	%xmm4, %xmm0
	movsd	%xmm0, 96(%rsp)
	movapd	%xmm8, %xmm0
	divsd	24(%rsp), %xmm0
	movapd	%xmm9, %xmm14
	divsd	32(%rsp), %xmm14
	movapd	%xmm14, %xmm4
	movapd	%xmm10, %xmm14
	divsd	40(%rsp), %xmm14
	movsd	%xmm14, 24(%rsp)
	movapd	%xmm11, %xmm14
	divsd	48(%rsp), %xmm14
	movsd	%xmm14, 32(%rsp)
	movapd	%xmm12, %xmm14
	divsd	56(%rsp), %xmm14
	movsd	%xmm14, 40(%rsp)
	movapd	%xmm13, %xmm14
	divsd	64(%rsp), %xmm14
	movsd	%xmm14, 48(%rsp)
	movapd	%xmm1, %xmm14
	divsd	72(%rsp), %xmm14
	movsd	%xmm14, 56(%rsp)
	movapd	%xmm2, %xmm14
	divsd	80(%rsp), %xmm14
	movsd	%xmm14, 64(%rsp)
	movsd	16(%rsp), %xmm14
	divsd	88(%rsp), %xmm14
	movsd	%xmm14, 72(%rsp)
	movapd	%xmm3, %xmm14
	divsd	104(%rsp), %xmm14
	movsd	%xmm14, 80(%rsp)
	movapd	%xmm5, %xmm14
	divsd	112(%rsp), %xmm14
	movsd	%xmm14, 88(%rsp)
	movapd	%xmm6, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm7, %xmm15
	divsd	96(%rsp), %xmm15
	movapd	%xmm8, %xmm3
	divsd	%xmm0, %xmm3
	movsd	%xmm3, 96(%rsp)
	movapd	%xmm9, %xmm0
	divsd	%xmm4, %xmm0
	movapd	%xmm0, %xmm4
	movapd	%xmm10, %xmm0
	divsd	24(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm11, %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm12, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm13, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm1, %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movapd	%xmm2, %xmm0
	divsd	64(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	movsd	16(%rsp), %xmm0
	divsd	72(%rsp), %xmm0
	movsd	%xmm0, 72(%rsp)
	movsd	8(%rsp), %xmm0
	divsd	80(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	movapd	%xmm5, %xmm0
	divsd	88(%rsp), %xmm0
	movsd	%xmm0, 88(%rsp)
	movapd	%xmm6, %xmm3
	divsd	%xmm14, %xmm3
	movsd	%xmm3, 104(%rsp)
	movapd	%xmm7, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm8, %xmm15
	divsd	96(%rsp), %xmm15
	movapd	%xmm9, %xmm3
	divsd	%xmm4, %xmm3
	movsd	%xmm3, 96(%rsp)
	movapd	%xmm10, %xmm3
	divsd	24(%rsp), %xmm3
	movsd	%xmm3, 112(%rsp)
	movapd	%xmm11, %xmm3
	divsd	32(%rsp), %xmm3
	movsd	%xmm3, 120(%rsp)
	movapd	%xmm12, %xmm3
	divsd	40(%rsp), %xmm3
	movsd	%xmm3, 128(%rsp)
	movapd	%xmm13, %xmm3
	divsd	48(%rsp), %xmm3
	movapd	%xmm1, %xmm0
	divsd	56(%rsp), %xmm0
	movapd	%xmm2, %xmm4
	divsd	64(%rsp), %xmm4
	movsd	%xmm4, 24(%rsp)
	movsd	16(%rsp), %xmm4
	divsd	72(%rsp), %xmm4
	movsd	%xmm4, 32(%rsp)
	movsd	8(%rsp), %xmm4
	divsd	80(%rsp), %xmm4
	movsd	%xmm4, 40(%rsp)
	movapd	%xmm5, %xmm4
	divsd	88(%rsp), %xmm4
	movsd	%xmm4, 48(%rsp)
	movapd	%xmm6, %xmm4
	divsd	104(%rsp), %xmm4
	movsd	%xmm4, 56(%rsp)
	movapd	%xmm7, %xmm4
	divsd	%xmm14, %xmm4
	movsd	%xmm4, 64(%rsp)
	movapd	%xmm8, %xmm4
	divsd	%xmm15, %xmm4
	movsd	%xmm4, 72(%rsp)
	movapd	%xmm9, %xmm4
	divsd	96(%rsp), %xmm4
	movsd	%xmm4, 80(%rsp)
	movapd	%xmm10, %xmm4
	divsd	112(%rsp), %xmm4
	movsd	%xmm4, 88(%rsp)
	movapd	%xmm11, %xmm4
	divsd	120(%rsp), %xmm4
	movsd	%xmm4, 96(%rsp)
	movapd	%xmm12, %xmm14
	divsd	128(%rsp), %xmm14
	movapd	%xmm13, %xmm15
	divsd	%xmm3, %xmm15
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1277
	movsd	%xmm14, 8(%rsp)
	movsd	%xmm15, 104(%rsp)
.L1276:
	cvttsd2sil	%xmm0, %edi
	call	use_int@PLT
	cvttsd2sil	144(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	152(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	160(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	168(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	176(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	184(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	192(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	200(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	208(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	216(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	224(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	232(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	136(%rsp), %edi
	call	use_int@PLT
	addq	$248, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE325:
	.size	double_div_12, .-double_div_12
	.globl	double_div_13
	.type	double_div_13, @function
double_div_13:
.LFB326:
	.cfi_startproc
	endbr64
	subq	$280, %rsp
	.cfi_def_cfa_offset 288
	movq	%rsi, %rax
	movsd	.LC9(%rip), %xmm2
	movapd	%xmm2, %xmm1
	mulsd	80(%rsi), %xmm1
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm3
	movsd	%xmm3, 16(%rsp)
	movapd	%xmm2, %xmm4
	mulsd	88(%rax), %xmm4
	movsd	%xmm4, 24(%rsp)
	pxor	%xmm3, %xmm3
	cvtsi2sdl	16(%rax), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm0, %xmm4
	movsd	%xmm4, 176(%rsp)
	movapd	%xmm2, %xmm6
	mulsd	96(%rax), %xmm6
	movsd	%xmm6, 32(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movapd	%xmm3, %xmm6
	mulsd	%xmm0, %xmm6
	movsd	%xmm6, 184(%rsp)
	movapd	%xmm2, %xmm7
	mulsd	104(%rax), %xmm7
	movsd	%xmm7, 40(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	movapd	%xmm3, %xmm7
	mulsd	%xmm0, %xmm7
	movsd	%xmm7, 192(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	112(%rax), %xmm3
	movsd	%xmm3, 48(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm5
	movsd	%xmm3, 200(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	120(%rax), %xmm3
	movsd	%xmm3, 56(%rsp)
	movl	32(%rax), %esi
	leal	-4(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm8
	movsd	%xmm3, 208(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	128(%rax), %xmm3
	movsd	%xmm3, 64(%rsp)
	movl	36(%rax), %esi
	leal	-5(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm9
	movsd	%xmm3, 216(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	136(%rax), %xmm3
	movsd	%xmm3, 72(%rsp)
	movl	40(%rax), %esi
	leal	-6(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm10
	movsd	%xmm3, 224(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	144(%rax), %xmm3
	movsd	%xmm3, 80(%rsp)
	movl	44(%rax), %esi
	leal	-7(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm11
	movsd	%xmm3, 232(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	152(%rax), %xmm3
	movsd	%xmm3, 88(%rsp)
	movl	48(%rax), %esi
	leal	-8(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm12
	movsd	%xmm3, 240(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	160(%rax), %xmm3
	movsd	%xmm3, 96(%rsp)
	movl	52(%rax), %esi
	leal	-9(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm13
	movsd	%xmm3, 248(%rsp)
	movapd	%xmm2, %xmm3
	mulsd	168(%rax), %xmm3
	movsd	%xmm3, 104(%rsp)
	movl	56(%rax), %esi
	leal	-10(%rsi), %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm15
	movsd	%xmm3, 256(%rsp)
	movapd	%xmm2, %xmm14
	mulsd	176(%rax), %xmm14
	movsd	%xmm14, (%rsp)
	movl	60(%rax), %ecx
	subl	$11, %ecx
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%ecx, %xmm3
	mulsd	%xmm0, %xmm3
	movsd	%xmm3, 168(%rsp)
	mulsd	184(%rax), %xmm2
	movsd	%xmm2, 112(%rsp)
	movl	64(%rax), %eax
	subl	$12, %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	mulsd	%xmm0, %xmm2
	movsd	%xmm2, 264(%rsp)
	testq	%rdi, %rdi
	je	.L1281
	movapd	%xmm2, %xmm0
	leaq	-1(%rdi), %rax
	movsd	%xmm4, (%rsp)
	movapd	%xmm6, %xmm2
	movapd	%xmm7, %xmm3
	movapd	%xmm5, %xmm4
	movsd	%xmm8, 8(%rsp)
	movapd	%xmm9, %xmm6
	movapd	%xmm10, %xmm7
	movapd	%xmm11, %xmm8
	movapd	%xmm12, %xmm9
	movapd	%xmm13, %xmm10
	movapd	%xmm15, %xmm11
	movsd	168(%rsp), %xmm12
	movapd	%xmm0, %xmm13
	movsd	112(%rsp), %xmm15
.L1282:
	movsd	16(%rsp), %xmm5
	movapd	%xmm5, %xmm0
	divsd	%xmm1, %xmm0
	movsd	%xmm0, 16(%rsp)
	movsd	(%rsp), %xmm1
	divsd	24(%rsp), %xmm1
	movapd	%xmm2, %xmm0
	divsd	32(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm3, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm4, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movsd	8(%rsp), %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm6, %xmm0
	divsd	64(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movapd	%xmm7, %xmm0
	divsd	72(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	movapd	%xmm8, %xmm0
	divsd	80(%rsp), %xmm0
	movsd	%xmm0, 72(%rsp)
	movapd	%xmm9, %xmm0
	divsd	88(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	movapd	%xmm10, %xmm0
	divsd	96(%rsp), %xmm0
	movsd	%xmm0, 88(%rsp)
	movapd	%xmm11, %xmm0
	divsd	104(%rsp), %xmm0
	movsd	%xmm0, 96(%rsp)
	movapd	%xmm12, %xmm0
	divsd	%xmm14, %xmm0
	movsd	%xmm0, 104(%rsp)
	movapd	%xmm13, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm5, %xmm15
	divsd	16(%rsp), %xmm15
	movsd	(%rsp), %xmm0
	divsd	%xmm1, %xmm0
	movsd	%xmm0, 112(%rsp)
	movapd	%xmm2, %xmm1
	divsd	24(%rsp), %xmm1
	movsd	%xmm1, 24(%rsp)
	movapd	%xmm3, %xmm1
	divsd	32(%rsp), %xmm1
	movsd	%xmm1, 32(%rsp)
	movapd	%xmm4, %xmm1
	divsd	40(%rsp), %xmm1
	movsd	%xmm1, 40(%rsp)
	movsd	8(%rsp), %xmm1
	divsd	48(%rsp), %xmm1
	movsd	%xmm1, 48(%rsp)
	movapd	%xmm6, %xmm1
	divsd	56(%rsp), %xmm1
	movsd	%xmm1, 56(%rsp)
	movapd	%xmm7, %xmm1
	divsd	64(%rsp), %xmm1
	movsd	%xmm1, 64(%rsp)
	movapd	%xmm8, %xmm1
	divsd	72(%rsp), %xmm1
	movsd	%xmm1, 72(%rsp)
	movapd	%xmm9, %xmm1
	divsd	80(%rsp), %xmm1
	movsd	%xmm1, 80(%rsp)
	movapd	%xmm10, %xmm1
	divsd	88(%rsp), %xmm1
	movsd	%xmm1, 88(%rsp)
	movapd	%xmm11, %xmm1
	divsd	96(%rsp), %xmm1
	movsd	%xmm1, 96(%rsp)
	movapd	%xmm12, %xmm1
	divsd	104(%rsp), %xmm1
	movsd	%xmm1, 104(%rsp)
	movapd	%xmm13, %xmm0
	divsd	%xmm14, %xmm0
	movsd	%xmm0, 120(%rsp)
	movsd	%xmm5, 16(%rsp)
	movapd	%xmm5, %xmm14
	divsd	%xmm15, %xmm14
	movsd	(%rsp), %xmm1
	divsd	112(%rsp), %xmm1
	movapd	%xmm1, %xmm15
	movapd	%xmm2, %xmm1
	divsd	24(%rsp), %xmm1
	movsd	%xmm1, 24(%rsp)
	movapd	%xmm3, %xmm0
	divsd	32(%rsp), %xmm0
	movapd	%xmm0, %xmm1
	movapd	%xmm4, %xmm5
	divsd	40(%rsp), %xmm5
	movsd	%xmm5, 32(%rsp)
	movsd	8(%rsp), %xmm5
	divsd	48(%rsp), %xmm5
	movsd	%xmm5, 40(%rsp)
	movapd	%xmm6, %xmm5
	divsd	56(%rsp), %xmm5
	movsd	%xmm5, 48(%rsp)
	movapd	%xmm7, %xmm5
	divsd	64(%rsp), %xmm5
	movsd	%xmm5, 56(%rsp)
	movapd	%xmm8, %xmm5
	divsd	72(%rsp), %xmm5
	movsd	%xmm5, 64(%rsp)
	movapd	%xmm9, %xmm5
	divsd	80(%rsp), %xmm5
	movsd	%xmm5, 72(%rsp)
	movapd	%xmm10, %xmm5
	divsd	88(%rsp), %xmm5
	movsd	%xmm5, 80(%rsp)
	movapd	%xmm11, %xmm5
	divsd	96(%rsp), %xmm5
	movsd	%xmm5, 88(%rsp)
	movapd	%xmm12, %xmm5
	divsd	104(%rsp), %xmm5
	movsd	%xmm5, 96(%rsp)
	movapd	%xmm13, %xmm5
	divsd	120(%rsp), %xmm5
	movsd	%xmm5, 104(%rsp)
	movsd	16(%rsp), %xmm0
	divsd	%xmm14, %xmm0
	movsd	%xmm0, 112(%rsp)
	movsd	(%rsp), %xmm5
	movapd	%xmm5, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm2, %xmm0
	divsd	24(%rsp), %xmm0
	movapd	%xmm0, %xmm15
	movapd	%xmm3, %xmm0
	divsd	%xmm1, %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm4, %xmm1
	divsd	32(%rsp), %xmm1
	movsd	8(%rsp), %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm6, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm7, %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm8, %xmm0
	divsd	64(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movapd	%xmm9, %xmm0
	divsd	72(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	movapd	%xmm10, %xmm0
	divsd	80(%rsp), %xmm0
	movsd	%xmm0, 72(%rsp)
	movapd	%xmm11, %xmm0
	divsd	88(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	movapd	%xmm12, %xmm0
	divsd	96(%rsp), %xmm0
	movsd	%xmm0, 88(%rsp)
	movapd	%xmm13, %xmm0
	divsd	104(%rsp), %xmm0
	movsd	%xmm0, 96(%rsp)
	movsd	16(%rsp), %xmm0
	divsd	112(%rsp), %xmm0
	movsd	%xmm0, 104(%rsp)
	divsd	%xmm14, %xmm5
	movsd	%xmm5, 112(%rsp)
	movapd	%xmm2, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm3, %xmm15
	divsd	24(%rsp), %xmm15
	movapd	%xmm4, %xmm5
	divsd	%xmm1, %xmm5
	movsd	%xmm5, 24(%rsp)
	movsd	8(%rsp), %xmm1
	divsd	32(%rsp), %xmm1
	movapd	%xmm6, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm7, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm8, %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm9, %xmm0
	divsd	64(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movapd	%xmm10, %xmm0
	divsd	72(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	movapd	%xmm11, %xmm0
	divsd	80(%rsp), %xmm0
	movsd	%xmm0, 72(%rsp)
	movapd	%xmm12, %xmm0
	divsd	88(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	movapd	%xmm13, %xmm0
	divsd	96(%rsp), %xmm0
	movsd	%xmm0, 88(%rsp)
	movsd	16(%rsp), %xmm0
	divsd	104(%rsp), %xmm0
	movsd	%xmm0, 96(%rsp)
	movsd	(%rsp), %xmm0
	divsd	112(%rsp), %xmm0
	movsd	%xmm0, 104(%rsp)
	movapd	%xmm2, %xmm5
	divsd	%xmm14, %xmm5
	movsd	%xmm5, 112(%rsp)
	movapd	%xmm3, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm4, %xmm15
	divsd	24(%rsp), %xmm15
	movsd	8(%rsp), %xmm5
	divsd	%xmm1, %xmm5
	movsd	%xmm5, 24(%rsp)
	movapd	%xmm6, %xmm1
	divsd	32(%rsp), %xmm1
	movapd	%xmm7, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm8, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm9, %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm10, %xmm0
	divsd	64(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movapd	%xmm11, %xmm0
	divsd	72(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	movapd	%xmm12, %xmm0
	divsd	80(%rsp), %xmm0
	movsd	%xmm0, 72(%rsp)
	movapd	%xmm13, %xmm0
	divsd	88(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	movsd	16(%rsp), %xmm0
	divsd	96(%rsp), %xmm0
	movsd	%xmm0, 88(%rsp)
	movsd	(%rsp), %xmm0
	divsd	104(%rsp), %xmm0
	movsd	%xmm0, 96(%rsp)
	movapd	%xmm2, %xmm0
	divsd	112(%rsp), %xmm0
	movsd	%xmm0, 104(%rsp)
	movapd	%xmm3, %xmm5
	divsd	%xmm14, %xmm5
	movsd	%xmm5, 112(%rsp)
	movapd	%xmm4, %xmm14
	divsd	%xmm15, %xmm14
	movsd	8(%rsp), %xmm15
	divsd	24(%rsp), %xmm15
	movapd	%xmm6, %xmm5
	divsd	%xmm1, %xmm5
	movsd	%xmm5, 24(%rsp)
	movapd	%xmm7, %xmm1
	divsd	32(%rsp), %xmm1
	movapd	%xmm8, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm9, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm10, %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm11, %xmm0
	divsd	64(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movapd	%xmm12, %xmm0
	divsd	72(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	movapd	%xmm13, %xmm0
	divsd	80(%rsp), %xmm0
	movsd	%xmm0, 72(%rsp)
	movsd	16(%rsp), %xmm0
	divsd	88(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	movsd	(%rsp), %xmm0
	divsd	96(%rsp), %xmm0
	movsd	%xmm0, 88(%rsp)
	movapd	%xmm2, %xmm0
	divsd	104(%rsp), %xmm0
	movsd	%xmm0, 96(%rsp)
	movapd	%xmm3, %xmm0
	divsd	112(%rsp), %xmm0
	movsd	%xmm0, 104(%rsp)
	movapd	%xmm4, %xmm5
	divsd	%xmm14, %xmm5
	movsd	%xmm5, 112(%rsp)
	movsd	8(%rsp), %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm6, %xmm15
	divsd	24(%rsp), %xmm15
	movapd	%xmm7, %xmm5
	divsd	%xmm1, %xmm5
	movsd	%xmm5, 24(%rsp)
	movapd	%xmm8, %xmm1
	divsd	32(%rsp), %xmm1
	movapd	%xmm9, %xmm0
	divsd	40(%rsp), %xmm0
	movsd	%xmm0, 32(%rsp)
	movapd	%xmm10, %xmm0
	divsd	48(%rsp), %xmm0
	movsd	%xmm0, 40(%rsp)
	movapd	%xmm11, %xmm0
	divsd	56(%rsp), %xmm0
	movsd	%xmm0, 48(%rsp)
	movapd	%xmm12, %xmm0
	divsd	64(%rsp), %xmm0
	movsd	%xmm0, 56(%rsp)
	movapd	%xmm13, %xmm0
	divsd	72(%rsp), %xmm0
	movsd	%xmm0, 64(%rsp)
	movsd	16(%rsp), %xmm0
	divsd	80(%rsp), %xmm0
	movsd	%xmm0, 72(%rsp)
	movsd	(%rsp), %xmm0
	divsd	88(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	movapd	%xmm2, %xmm0
	divsd	96(%rsp), %xmm0
	movsd	%xmm0, 88(%rsp)
	movapd	%xmm3, %xmm0
	divsd	104(%rsp), %xmm0
	movsd	%xmm0, 96(%rsp)
	movapd	%xmm4, %xmm0
	divsd	112(%rsp), %xmm0
	movsd	%xmm0, 104(%rsp)
	movsd	8(%rsp), %xmm5
	divsd	%xmm14, %xmm5
	movsd	%xmm5, 112(%rsp)
	movapd	%xmm6, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm7, %xmm15
	divsd	24(%rsp), %xmm15
	movapd	%xmm8, %xmm5
	divsd	%xmm1, %xmm5
	movsd	%xmm5, 120(%rsp)
	movapd	%xmm9, %xmm1
	divsd	32(%rsp), %xmm1
	movsd	%xmm1, 128(%rsp)
	movapd	%xmm10, %xmm1
	divsd	40(%rsp), %xmm1
	movsd	%xmm1, 136(%rsp)
	movapd	%xmm11, %xmm1
	divsd	48(%rsp), %xmm1
	movsd	%xmm1, 144(%rsp)
	movapd	%xmm12, %xmm1
	divsd	56(%rsp), %xmm1
	movsd	%xmm1, 152(%rsp)
	movapd	%xmm13, %xmm1
	divsd	64(%rsp), %xmm1
	movsd	%xmm1, 160(%rsp)
	movsd	16(%rsp), %xmm1
	divsd	72(%rsp), %xmm1
	movsd	(%rsp), %xmm0
	divsd	80(%rsp), %xmm0
	movsd	%xmm0, 24(%rsp)
	movapd	%xmm2, %xmm5
	divsd	88(%rsp), %xmm5
	movsd	%xmm5, 32(%rsp)
	movapd	%xmm3, %xmm5
	divsd	96(%rsp), %xmm5
	movsd	%xmm5, 40(%rsp)
	movapd	%xmm4, %xmm5
	divsd	104(%rsp), %xmm5
	movsd	%xmm5, 48(%rsp)
	movsd	8(%rsp), %xmm5
	divsd	112(%rsp), %xmm5
	movsd	%xmm5, 56(%rsp)
	movapd	%xmm6, %xmm0
	divsd	%xmm14, %xmm0
	movsd	%xmm0, 64(%rsp)
	movapd	%xmm7, %xmm0
	divsd	%xmm15, %xmm0
	movsd	%xmm0, 72(%rsp)
	movapd	%xmm8, %xmm0
	divsd	120(%rsp), %xmm0
	movsd	%xmm0, 80(%rsp)
	movapd	%xmm9, %xmm0
	divsd	128(%rsp), %xmm0
	movsd	%xmm0, 88(%rsp)
	movapd	%xmm10, %xmm0
	divsd	136(%rsp), %xmm0
	movsd	%xmm0, 96(%rsp)
	movapd	%xmm11, %xmm0
	divsd	144(%rsp), %xmm0
	movsd	%xmm0, 104(%rsp)
	movapd	%xmm12, %xmm14
	divsd	152(%rsp), %xmm14
	movapd	%xmm13, %xmm15
	divsd	160(%rsp), %xmm15
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1282
	movsd	%xmm14, (%rsp)
	movsd	%xmm15, 112(%rsp)
.L1281:
	cvttsd2sil	%xmm1, %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	24(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	176(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	184(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	192(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	200(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	208(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	216(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	224(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	232(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	240(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	248(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	256(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	168(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	264(%rsp), %edi
	call	use_int@PLT
	addq	$280, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE326:
	.size	double_div_13, .-double_div_13
	.globl	double_div_14
	.type	double_div_14, @function
double_div_14:
.LFB327:
	.cfi_startproc
	endbr64
	subq	$296, %rsp
	.cfi_def_cfa_offset 304
	movq	%rsi, %rax
	movsd	.LC9(%rip), %xmm1
	movapd	%xmm1, %xmm15
	mulsd	80(%rsi), %xmm15
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm2
	movsd	%xmm2, (%rsp)
	movapd	%xmm1, %xmm3
	mulsd	88(%rax), %xmm3
	movsd	%xmm3, 32(%rsp)
	pxor	%xmm2, %xmm2
	cvtsi2sdl	16(%rax), %xmm2
	mulsd	%xmm0, %xmm2
	movsd	%xmm2, 8(%rsp)
	movapd	%xmm1, %xmm4
	mulsd	96(%rax), %xmm4
	movsd	%xmm4, 40(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	movapd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm3
	movsd	%xmm3, 192(%rsp)
	movapd	%xmm1, %xmm5
	mulsd	104(%rax), %xmm5
	movsd	%xmm5, 48(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	movapd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm4
	movsd	%xmm4, 200(%rsp)
	movapd	%xmm1, %xmm6
	mulsd	112(%rax), %xmm6
	movsd	%xmm6, 56(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	movapd	%xmm2, %xmm5
	mulsd	%xmm0, %xmm5
	movsd	%xmm5, 208(%rsp)
	movapd	%xmm1, %xmm7
	mulsd	120(%rax), %xmm7
	movsd	%xmm7, 64(%rsp)
	movl	32(%rax), %esi
	leal	-4(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	movapd	%xmm2, %xmm6
	mulsd	%xmm0, %xmm6
	movsd	%xmm6, 216(%rsp)
	movapd	%xmm1, %xmm7
	mulsd	128(%rax), %xmm7
	movsd	%xmm7, 72(%rsp)
	movl	36(%rax), %esi
	leal	-5(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	movapd	%xmm2, %xmm7
	mulsd	%xmm0, %xmm7
	movsd	%xmm7, 224(%rsp)
	movapd	%xmm1, %xmm2
	mulsd	136(%rax), %xmm2
	movsd	%xmm2, 80(%rsp)
	movl	40(%rax), %esi
	leal	-6(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	mulsd	%xmm0, %xmm2
	movapd	%xmm2, %xmm8
	movsd	%xmm2, 232(%rsp)
	movapd	%xmm1, %xmm2
	mulsd	144(%rax), %xmm2
	movsd	%xmm2, 88(%rsp)
	movl	44(%rax), %esi
	leal	-7(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	mulsd	%xmm0, %xmm2
	movapd	%xmm2, %xmm9
	movsd	%xmm2, 240(%rsp)
	movapd	%xmm1, %xmm2
	mulsd	152(%rax), %xmm2
	movsd	%xmm2, 96(%rsp)
	movl	48(%rax), %esi
	leal	-8(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	mulsd	%xmm0, %xmm2
	movapd	%xmm2, %xmm10
	movsd	%xmm2, 248(%rsp)
	movapd	%xmm1, %xmm2
	mulsd	160(%rax), %xmm2
	movsd	%xmm2, 104(%rsp)
	movl	52(%rax), %esi
	leal	-9(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	mulsd	%xmm0, %xmm2
	movapd	%xmm2, %xmm11
	movsd	%xmm2, 256(%rsp)
	movapd	%xmm1, %xmm2
	mulsd	168(%rax), %xmm2
	movsd	%xmm2, 112(%rsp)
	movl	56(%rax), %esi
	leal	-10(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	mulsd	%xmm0, %xmm2
	movapd	%xmm2, %xmm14
	movsd	%xmm2, 264(%rsp)
	movapd	%xmm1, %xmm2
	mulsd	176(%rax), %xmm2
	movsd	%xmm2, 120(%rsp)
	movl	60(%rax), %esi
	leal	-11(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	mulsd	%xmm0, %xmm2
	movapd	%xmm2, %xmm13
	movsd	%xmm2, 272(%rsp)
	movapd	%xmm1, %xmm12
	mulsd	184(%rax), %xmm12
	movsd	%xmm12, 16(%rsp)
	movl	64(%rax), %ecx
	subl	$12, %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	mulsd	%xmm0, %xmm2
	movsd	%xmm2, 280(%rsp)
	mulsd	192(%rax), %xmm1
	movsd	%xmm1, 128(%rsp)
	movl	68(%rax), %eax
	subl	$13, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, 184(%rsp)
	testq	%rdi, %rdi
	je	.L1286
	leaq	-1(%rdi), %rax
	movapd	%xmm3, %xmm0
	movapd	%xmm4, %xmm1
	movsd	%xmm5, 16(%rsp)
	movapd	%xmm6, %xmm3
	movapd	%xmm7, %xmm4
	movapd	%xmm8, %xmm5
	movapd	%xmm9, %xmm6
	movapd	%xmm10, %xmm7
	movapd	%xmm11, %xmm8
	movapd	%xmm14, %xmm9
	movapd	%xmm13, %xmm10
	movapd	%xmm2, %xmm11
	movsd	184(%rsp), %xmm2
	movsd	%xmm2, 24(%rsp)
	movapd	%xmm12, %xmm13
	movsd	128(%rsp), %xmm14
.L1287:
	movsd	(%rsp), %xmm2
	divsd	%xmm15, %xmm2
	movsd	%xmm2, 128(%rsp)
	movsd	8(%rsp), %xmm12
	movapd	%xmm12, %xmm15
	divsd	32(%rsp), %xmm15
	movapd	%xmm0, %xmm2
	divsd	40(%rsp), %xmm2
	movsd	%xmm2, 32(%rsp)
	movapd	%xmm1, %xmm2
	divsd	48(%rsp), %xmm2
	movsd	%xmm2, 40(%rsp)
	movsd	16(%rsp), %xmm2
	divsd	56(%rsp), %xmm2
	movsd	%xmm2, 48(%rsp)
	movapd	%xmm3, %xmm2
	divsd	64(%rsp), %xmm2
	movsd	%xmm2, 56(%rsp)
	movapd	%xmm4, %xmm2
	divsd	72(%rsp), %xmm2
	movsd	%xmm2, 64(%rsp)
	movapd	%xmm5, %xmm2
	divsd	80(%rsp), %xmm2
	movsd	%xmm2, 72(%rsp)
	movapd	%xmm6, %xmm2
	divsd	88(%rsp), %xmm2
	movsd	%xmm2, 80(%rsp)
	movapd	%xmm7, %xmm2
	divsd	96(%rsp), %xmm2
	movsd	%xmm2, 88(%rsp)
	movapd	%xmm8, %xmm2
	divsd	104(%rsp), %xmm2
	movsd	%xmm2, 96(%rsp)
	movapd	%xmm9, %xmm2
	divsd	112(%rsp), %xmm2
	movsd	%xmm2, 104(%rsp)
	movapd	%xmm10, %xmm2
	divsd	120(%rsp), %xmm2
	movsd	%xmm2, 112(%rsp)
	movapd	%xmm11, %xmm2
	divsd	%xmm13, %xmm2
	movsd	%xmm2, 120(%rsp)
	movsd	24(%rsp), %xmm13
	divsd	%xmm14, %xmm13
	movsd	%xmm13, 136(%rsp)
	movsd	(%rsp), %xmm14
	divsd	128(%rsp), %xmm14
	movapd	%xmm12, %xmm2
	divsd	%xmm15, %xmm2
	movapd	%xmm0, %xmm12
	divsd	32(%rsp), %xmm12
	movapd	%xmm12, %xmm15
	movapd	%xmm1, %xmm12
	divsd	40(%rsp), %xmm12
	movsd	16(%rsp), %xmm13
	divsd	48(%rsp), %xmm13
	movsd	%xmm13, 32(%rsp)
	movapd	%xmm3, %xmm13
	divsd	56(%rsp), %xmm13
	movsd	%xmm13, 40(%rsp)
	movapd	%xmm4, %xmm13
	divsd	64(%rsp), %xmm13
	movsd	%xmm13, 48(%rsp)
	movapd	%xmm5, %xmm13
	divsd	72(%rsp), %xmm13
	movsd	%xmm13, 56(%rsp)
	movapd	%xmm6, %xmm13
	divsd	80(%rsp), %xmm13
	movsd	%xmm13, 64(%rsp)
	movapd	%xmm7, %xmm13
	divsd	88(%rsp), %xmm13
	movsd	%xmm13, 72(%rsp)
	movapd	%xmm8, %xmm13
	divsd	96(%rsp), %xmm13
	movsd	%xmm13, 80(%rsp)
	movapd	%xmm9, %xmm13
	divsd	104(%rsp), %xmm13
	movsd	%xmm13, 88(%rsp)
	movapd	%xmm10, %xmm13
	divsd	112(%rsp), %xmm13
	movsd	%xmm13, 96(%rsp)
	movapd	%xmm11, %xmm13
	divsd	120(%rsp), %xmm13
	movsd	%xmm13, 104(%rsp)
	movsd	24(%rsp), %xmm13
	divsd	136(%rsp), %xmm13
	movsd	%xmm13, 112(%rsp)
	movsd	(%rsp), %xmm13
	divsd	%xmm14, %xmm13
	movsd	8(%rsp), %xmm14
	divsd	%xmm2, %xmm14
	movapd	%xmm0, %xmm2
	divsd	%xmm15, %xmm2
	movsd	%xmm2, 120(%rsp)
	movapd	%xmm1, %xmm15
	divsd	%xmm12, %xmm15
	movsd	16(%rsp), %xmm2
	divsd	32(%rsp), %xmm2
	movsd	%xmm2, 32(%rsp)
	movapd	%xmm3, %xmm2
	divsd	40(%rsp), %xmm2
	movsd	%xmm2, 40(%rsp)
	movapd	%xmm4, %xmm2
	divsd	48(%rsp), %xmm2
	movsd	%xmm2, 48(%rsp)
	movapd	%xmm5, %xmm2
	divsd	56(%rsp), %xmm2
	movsd	%xmm2, 56(%rsp)
	movapd	%xmm6, %xmm2
	divsd	64(%rsp), %xmm2
	movsd	%xmm2, 64(%rsp)
	movapd	%xmm7, %xmm2
	divsd	72(%rsp), %xmm2
	movsd	%xmm2, 72(%rsp)
	movapd	%xmm8, %xmm2
	divsd	80(%rsp), %xmm2
	movsd	%xmm2, 80(%rsp)
	movapd	%xmm9, %xmm2
	divsd	88(%rsp), %xmm2
	movsd	%xmm2, 88(%rsp)
	movapd	%xmm10, %xmm2
	divsd	96(%rsp), %xmm2
	movsd	%xmm2, 96(%rsp)
	movapd	%xmm11, %xmm2
	divsd	104(%rsp), %xmm2
	movsd	%xmm2, 104(%rsp)
	movsd	24(%rsp), %xmm2
	divsd	112(%rsp), %xmm2
	movsd	%xmm2, 112(%rsp)
	movsd	(%rsp), %xmm2
	divsd	%xmm13, %xmm2
	movsd	%xmm2, 128(%rsp)
	movsd	8(%rsp), %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm0, %xmm14
	divsd	120(%rsp), %xmm14
	movapd	%xmm1, %xmm12
	divsd	%xmm15, %xmm12
	movsd	%xmm12, 120(%rsp)
	movsd	16(%rsp), %xmm12
	movapd	%xmm12, %xmm15
	divsd	32(%rsp), %xmm15
	movapd	%xmm3, %xmm2
	divsd	40(%rsp), %xmm2
	movsd	%xmm2, 32(%rsp)
	movapd	%xmm4, %xmm2
	divsd	48(%rsp), %xmm2
	movsd	%xmm2, 40(%rsp)
	movapd	%xmm5, %xmm2
	divsd	56(%rsp), %xmm2
	movsd	%xmm2, 48(%rsp)
	movapd	%xmm6, %xmm2
	divsd	64(%rsp), %xmm2
	movsd	%xmm2, 56(%rsp)
	movapd	%xmm7, %xmm2
	divsd	72(%rsp), %xmm2
	movsd	%xmm2, 64(%rsp)
	movapd	%xmm8, %xmm2
	divsd	80(%rsp), %xmm2
	movsd	%xmm2, 72(%rsp)
	movapd	%xmm9, %xmm2
	divsd	88(%rsp), %xmm2
	movsd	%xmm2, 80(%rsp)
	movapd	%xmm10, %xmm2
	divsd	96(%rsp), %xmm2
	movsd	%xmm2, 88(%rsp)
	movapd	%xmm11, %xmm2
	divsd	104(%rsp), %xmm2
	movsd	%xmm2, 96(%rsp)
	movsd	24(%rsp), %xmm2
	divsd	112(%rsp), %xmm2
	movsd	%xmm2, 104(%rsp)
	movsd	(%rsp), %xmm2
	divsd	128(%rsp), %xmm2
	movsd	%xmm2, 112(%rsp)
	movsd	8(%rsp), %xmm2
	divsd	%xmm13, %xmm2
	movsd	%xmm2, 128(%rsp)
	movapd	%xmm0, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm1, %xmm14
	divsd	120(%rsp), %xmm14
	divsd	%xmm15, %xmm12
	movsd	%xmm12, 120(%rsp)
	movapd	%xmm3, %xmm15
	divsd	32(%rsp), %xmm15
	movapd	%xmm4, %xmm2
	divsd	40(%rsp), %xmm2
	movsd	%xmm2, 32(%rsp)
	movapd	%xmm5, %xmm2
	divsd	48(%rsp), %xmm2
	movsd	%xmm2, 40(%rsp)
	movapd	%xmm6, %xmm2
	divsd	56(%rsp), %xmm2
	movsd	%xmm2, 48(%rsp)
	movapd	%xmm7, %xmm2
	divsd	64(%rsp), %xmm2
	movsd	%xmm2, 56(%rsp)
	movapd	%xmm8, %xmm2
	divsd	72(%rsp), %xmm2
	movsd	%xmm2, 64(%rsp)
	movapd	%xmm9, %xmm2
	divsd	80(%rsp), %xmm2
	movsd	%xmm2, 72(%rsp)
	movapd	%xmm10, %xmm2
	divsd	88(%rsp), %xmm2
	movsd	%xmm2, 80(%rsp)
	movapd	%xmm11, %xmm2
	divsd	96(%rsp), %xmm2
	movsd	%xmm2, 88(%rsp)
	movsd	24(%rsp), %xmm2
	divsd	104(%rsp), %xmm2
	movsd	%xmm2, 96(%rsp)
	movsd	(%rsp), %xmm2
	divsd	112(%rsp), %xmm2
	movsd	%xmm2, 104(%rsp)
	movsd	8(%rsp), %xmm2
	divsd	128(%rsp), %xmm2
	movsd	%xmm2, 112(%rsp)
	movapd	%xmm0, %xmm12
	divsd	%xmm13, %xmm12
	movsd	%xmm12, 128(%rsp)
	movapd	%xmm1, %xmm13
	divsd	%xmm14, %xmm13
	movsd	16(%rsp), %xmm14
	divsd	120(%rsp), %xmm14
	movapd	%xmm3, %xmm12
	divsd	%xmm15, %xmm12
	movsd	%xmm12, 120(%rsp)
	movapd	%xmm4, %xmm15
	divsd	32(%rsp), %xmm15
	movapd	%xmm5, %xmm2
	divsd	40(%rsp), %xmm2
	movsd	%xmm2, 32(%rsp)
	movapd	%xmm6, %xmm2
	divsd	48(%rsp), %xmm2
	movsd	%xmm2, 40(%rsp)
	movapd	%xmm7, %xmm2
	divsd	56(%rsp), %xmm2
	movsd	%xmm2, 48(%rsp)
	movapd	%xmm8, %xmm2
	divsd	64(%rsp), %xmm2
	movsd	%xmm2, 56(%rsp)
	movapd	%xmm9, %xmm2
	divsd	72(%rsp), %xmm2
	movsd	%xmm2, 64(%rsp)
	movapd	%xmm10, %xmm2
	divsd	80(%rsp), %xmm2
	movsd	%xmm2, 72(%rsp)
	movapd	%xmm11, %xmm2
	divsd	88(%rsp), %xmm2
	movsd	%xmm2, 80(%rsp)
	movsd	24(%rsp), %xmm2
	divsd	96(%rsp), %xmm2
	movsd	%xmm2, 88(%rsp)
	movsd	(%rsp), %xmm2
	divsd	104(%rsp), %xmm2
	movsd	%xmm2, 96(%rsp)
	movsd	8(%rsp), %xmm2
	divsd	112(%rsp), %xmm2
	movsd	%xmm2, 104(%rsp)
	movapd	%xmm0, %xmm2
	divsd	128(%rsp), %xmm2
	movsd	%xmm2, 112(%rsp)
	movapd	%xmm1, %xmm12
	divsd	%xmm13, %xmm12
	movsd	%xmm12, 128(%rsp)
	movsd	16(%rsp), %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm3, %xmm14
	divsd	120(%rsp), %xmm14
	movapd	%xmm4, %xmm12
	divsd	%xmm15, %xmm12
	movsd	%xmm12, 120(%rsp)
	movapd	%xmm5, %xmm15
	divsd	32(%rsp), %xmm15
	movapd	%xmm6, %xmm2
	divsd	40(%rsp), %xmm2
	movsd	%xmm2, 32(%rsp)
	movapd	%xmm7, %xmm2
	divsd	48(%rsp), %xmm2
	movsd	%xmm2, 40(%rsp)
	movapd	%xmm8, %xmm2
	divsd	56(%rsp), %xmm2
	movsd	%xmm2, 48(%rsp)
	movapd	%xmm9, %xmm2
	divsd	64(%rsp), %xmm2
	movsd	%xmm2, 56(%rsp)
	movapd	%xmm10, %xmm2
	divsd	72(%rsp), %xmm2
	movsd	%xmm2, 64(%rsp)
	movapd	%xmm11, %xmm2
	divsd	80(%rsp), %xmm2
	movsd	%xmm2, 72(%rsp)
	movsd	24(%rsp), %xmm2
	divsd	88(%rsp), %xmm2
	movsd	%xmm2, 80(%rsp)
	movsd	(%rsp), %xmm2
	divsd	96(%rsp), %xmm2
	movsd	%xmm2, 88(%rsp)
	movsd	8(%rsp), %xmm2
	divsd	104(%rsp), %xmm2
	movsd	%xmm2, 96(%rsp)
	movapd	%xmm0, %xmm2
	divsd	112(%rsp), %xmm2
	movsd	%xmm2, 104(%rsp)
	movapd	%xmm1, %xmm2
	divsd	128(%rsp), %xmm2
	movsd	%xmm2, 112(%rsp)
	movsd	16(%rsp), %xmm12
	divsd	%xmm13, %xmm12
	movsd	%xmm12, 128(%rsp)
	movapd	%xmm3, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm4, %xmm14
	divsd	120(%rsp), %xmm14
	movapd	%xmm5, %xmm12
	divsd	%xmm15, %xmm12
	movsd	%xmm12, 120(%rsp)
	movapd	%xmm6, %xmm15
	divsd	32(%rsp), %xmm15
	movapd	%xmm7, %xmm2
	divsd	40(%rsp), %xmm2
	movsd	%xmm2, 32(%rsp)
	movapd	%xmm8, %xmm2
	divsd	48(%rsp), %xmm2
	movsd	%xmm2, 40(%rsp)
	movapd	%xmm9, %xmm2
	divsd	56(%rsp), %xmm2
	movsd	%xmm2, 48(%rsp)
	movapd	%xmm10, %xmm2
	divsd	64(%rsp), %xmm2
	movsd	%xmm2, 56(%rsp)
	movapd	%xmm11, %xmm2
	divsd	72(%rsp), %xmm2
	movsd	%xmm2, 64(%rsp)
	movsd	24(%rsp), %xmm2
	divsd	80(%rsp), %xmm2
	movsd	%xmm2, 72(%rsp)
	movsd	(%rsp), %xmm2
	divsd	88(%rsp), %xmm2
	movsd	%xmm2, 80(%rsp)
	movsd	8(%rsp), %xmm2
	divsd	96(%rsp), %xmm2
	movsd	%xmm2, 88(%rsp)
	movapd	%xmm0, %xmm2
	divsd	104(%rsp), %xmm2
	movsd	%xmm2, 96(%rsp)
	movapd	%xmm1, %xmm2
	divsd	112(%rsp), %xmm2
	movsd	%xmm2, 104(%rsp)
	movsd	16(%rsp), %xmm2
	divsd	128(%rsp), %xmm2
	movsd	%xmm2, 112(%rsp)
	movapd	%xmm3, %xmm12
	divsd	%xmm13, %xmm12
	movsd	%xmm12, 128(%rsp)
	movapd	%xmm4, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm5, %xmm14
	divsd	120(%rsp), %xmm14
	movapd	%xmm6, %xmm12
	divsd	%xmm15, %xmm12
	movsd	%xmm12, 120(%rsp)
	movapd	%xmm7, %xmm15
	divsd	32(%rsp), %xmm15
	movsd	%xmm15, 136(%rsp)
	movapd	%xmm8, %xmm15
	divsd	40(%rsp), %xmm15
	movsd	%xmm15, 144(%rsp)
	movapd	%xmm9, %xmm15
	divsd	48(%rsp), %xmm15
	movsd	%xmm15, 152(%rsp)
	movapd	%xmm10, %xmm15
	divsd	56(%rsp), %xmm15
	movsd	%xmm15, 160(%rsp)
	movapd	%xmm11, %xmm15
	divsd	64(%rsp), %xmm15
	movsd	%xmm15, 168(%rsp)
	movsd	24(%rsp), %xmm12
	divsd	72(%rsp), %xmm12
	movsd	%xmm12, 176(%rsp)
	movsd	(%rsp), %xmm15
	divsd	80(%rsp), %xmm15
	movsd	8(%rsp), %xmm2
	divsd	88(%rsp), %xmm2
	movsd	%xmm2, 32(%rsp)
	movapd	%xmm0, %xmm12
	divsd	96(%rsp), %xmm12
	movsd	%xmm12, 40(%rsp)
	movapd	%xmm1, %xmm12
	divsd	104(%rsp), %xmm12
	movsd	%xmm12, 48(%rsp)
	movsd	16(%rsp), %xmm2
	divsd	112(%rsp), %xmm2
	movsd	%xmm2, 56(%rsp)
	movapd	%xmm3, %xmm2
	divsd	128(%rsp), %xmm2
	movsd	%xmm2, 64(%rsp)
	movapd	%xmm4, %xmm12
	divsd	%xmm13, %xmm12
	movsd	%xmm12, 72(%rsp)
	movapd	%xmm5, %xmm13
	divsd	%xmm14, %xmm13
	movsd	%xmm13, 80(%rsp)
	movapd	%xmm6, %xmm14
	divsd	120(%rsp), %xmm14
	movsd	%xmm14, 88(%rsp)
	movapd	%xmm7, %xmm14
	divsd	136(%rsp), %xmm14
	movsd	%xmm14, 96(%rsp)
	movapd	%xmm8, %xmm14
	divsd	144(%rsp), %xmm14
	movsd	%xmm14, 104(%rsp)
	movapd	%xmm9, %xmm14
	divsd	152(%rsp), %xmm14
	movsd	%xmm14, 112(%rsp)
	movapd	%xmm10, %xmm14
	divsd	160(%rsp), %xmm14
	movsd	%xmm14, 120(%rsp)
	movapd	%xmm11, %xmm13
	divsd	168(%rsp), %xmm13
	movsd	24(%rsp), %xmm14
	divsd	176(%rsp), %xmm14
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1287
	movsd	%xmm13, 16(%rsp)
	movsd	%xmm14, 128(%rsp)
.L1286:
	cvttsd2sil	%xmm15, %edi
	call	use_int@PLT
	cvttsd2sil	(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	192(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	200(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	56(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	208(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	216(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	224(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	232(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	240(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	248(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	256(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	264(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	272(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	280(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	128(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	184(%rsp), %edi
	call	use_int@PLT
	addq	$296, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE327:
	.size	double_div_14, .-double_div_14
	.globl	double_div_15
	.type	double_div_15, @function
double_div_15:
.LFB328:
	.cfi_startproc
	endbr64
	subq	$312, %rsp
	.cfi_def_cfa_offset 320
	movq	%rsi, %rax
	movsd	.LC9(%rip), %xmm1
	movapd	%xmm1, %xmm7
	mulsd	80(%rsi), %xmm7
	movl	12(%rsi), %esi
	leal	1(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm2
	movsd	%xmm2, (%rsp)
	movapd	%xmm1, %xmm3
	mulsd	88(%rax), %xmm3
	movsd	%xmm3, 64(%rsp)
	pxor	%xmm2, %xmm2
	cvtsi2sdl	16(%rax), %xmm2
	mulsd	%xmm0, %xmm2
	movsd	%xmm2, 32(%rsp)
	movapd	%xmm1, %xmm4
	mulsd	96(%rax), %xmm4
	movsd	%xmm4, 72(%rsp)
	movl	20(%rax), %esi
	leal	-1(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	mulsd	%xmm0, %xmm2
	movsd	%xmm2, 40(%rsp)
	movapd	%xmm1, %xmm5
	mulsd	104(%rax), %xmm5
	movsd	%xmm5, 80(%rsp)
	movl	24(%rax), %esi
	leal	-2(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	mulsd	%xmm0, %xmm2
	movsd	%xmm2, 48(%rsp)
	movapd	%xmm1, %xmm6
	mulsd	112(%rax), %xmm6
	movsd	%xmm6, 88(%rsp)
	movl	28(%rax), %esi
	leal	-3(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	movapd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm3
	movsd	%xmm3, 208(%rsp)
	movapd	%xmm1, %xmm4
	mulsd	120(%rax), %xmm4
	movsd	%xmm4, 96(%rsp)
	movl	32(%rax), %esi
	leal	-4(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	movapd	%xmm2, %xmm4
	mulsd	%xmm0, %xmm4
	movsd	%xmm4, 216(%rsp)
	movapd	%xmm1, %xmm5
	mulsd	128(%rax), %xmm5
	movsd	%xmm5, 104(%rsp)
	movl	36(%rax), %esi
	leal	-5(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	movapd	%xmm2, %xmm5
	mulsd	%xmm0, %xmm5
	movsd	%xmm5, 224(%rsp)
	movapd	%xmm1, %xmm6
	mulsd	136(%rax), %xmm6
	movsd	%xmm6, 112(%rsp)
	movl	40(%rax), %esi
	leal	-6(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	movapd	%xmm2, %xmm6
	mulsd	%xmm0, %xmm6
	movsd	%xmm6, 232(%rsp)
	movapd	%xmm1, %xmm2
	mulsd	144(%rax), %xmm2
	movsd	%xmm2, 120(%rsp)
	movl	44(%rax), %esi
	leal	-7(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	mulsd	%xmm0, %xmm2
	movapd	%xmm2, %xmm8
	movsd	%xmm2, 240(%rsp)
	movapd	%xmm1, %xmm2
	mulsd	152(%rax), %xmm2
	movsd	%xmm2, 128(%rsp)
	movl	48(%rax), %esi
	leal	-8(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	mulsd	%xmm0, %xmm2
	movapd	%xmm2, %xmm9
	movsd	%xmm2, 248(%rsp)
	movapd	%xmm1, %xmm2
	mulsd	160(%rax), %xmm2
	movsd	%xmm2, 136(%rsp)
	movl	52(%rax), %esi
	leal	-9(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	mulsd	%xmm0, %xmm2
	movapd	%xmm2, %xmm10
	movsd	%xmm2, 256(%rsp)
	movapd	%xmm1, %xmm2
	mulsd	168(%rax), %xmm2
	movsd	%xmm2, 144(%rsp)
	movl	56(%rax), %esi
	leal	-10(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	mulsd	%xmm0, %xmm2
	movapd	%xmm2, %xmm13
	movsd	%xmm2, 264(%rsp)
	movapd	%xmm1, %xmm2
	mulsd	176(%rax), %xmm2
	movsd	%xmm2, 152(%rsp)
	movl	60(%rax), %esi
	leal	-11(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	mulsd	%xmm0, %xmm2
	movapd	%xmm2, %xmm14
	movsd	%xmm2, 272(%rsp)
	movapd	%xmm1, %xmm15
	mulsd	184(%rax), %xmm15
	movsd	%xmm15, 8(%rsp)
	movl	64(%rax), %esi
	leal	-12(%rsi), %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	mulsd	%xmm0, %xmm2
	movapd	%xmm2, %xmm11
	movsd	%xmm2, 280(%rsp)
	movapd	%xmm1, %xmm12
	mulsd	192(%rax), %xmm12
	movsd	%xmm12, 16(%rsp)
	movl	68(%rax), %ecx
	subl	$13, %ecx
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%ecx, %xmm2
	mulsd	%xmm0, %xmm2
	movsd	%xmm2, 288(%rsp)
	mulsd	200(%rax), %xmm1
	movsd	%xmm1, 160(%rsp)
	movl	72(%rax), %eax
	subl	$14, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, 296(%rsp)
	testq	%rdi, %rdi
	je	.L1291
	leaq	-1(%rdi), %rax
	movapd	%xmm3, %xmm0
	movsd	%xmm4, 8(%rsp)
	movsd	%xmm5, 16(%rsp)
	movapd	%xmm6, %xmm3
	movapd	%xmm8, %xmm4
	movapd	%xmm9, %xmm5
	movapd	%xmm10, %xmm6
	movapd	%xmm13, %xmm8
	movapd	%xmm14, %xmm9
	movapd	%xmm11, %xmm10
	movsd	%xmm2, 24(%rsp)
	movsd	%xmm1, 56(%rsp)
	movapd	%xmm15, %xmm13
	movapd	%xmm12, %xmm14
	movsd	160(%rsp), %xmm15
.L1292:
	movsd	(%rsp), %xmm2
	movapd	%xmm2, %xmm1
	divsd	%xmm7, %xmm1
	movsd	%xmm1, 160(%rsp)
	movsd	32(%rsp), %xmm7
	divsd	64(%rsp), %xmm7
	movsd	40(%rsp), %xmm11
	divsd	72(%rsp), %xmm11
	movsd	%xmm11, 64(%rsp)
	movsd	48(%rsp), %xmm12
	movapd	%xmm12, %xmm11
	divsd	80(%rsp), %xmm11
	movsd	%xmm11, 48(%rsp)
	movapd	%xmm0, %xmm1
	divsd	88(%rsp), %xmm1
	movsd	%xmm1, 72(%rsp)
	movsd	8(%rsp), %xmm1
	divsd	96(%rsp), %xmm1
	movsd	%xmm1, 80(%rsp)
	movsd	16(%rsp), %xmm1
	divsd	104(%rsp), %xmm1
	movsd	%xmm1, 88(%rsp)
	movapd	%xmm3, %xmm1
	divsd	112(%rsp), %xmm1
	movsd	%xmm1, 96(%rsp)
	movapd	%xmm4, %xmm1
	divsd	120(%rsp), %xmm1
	movsd	%xmm1, 104(%rsp)
	movapd	%xmm5, %xmm1
	divsd	128(%rsp), %xmm1
	movsd	%xmm1, 112(%rsp)
	movapd	%xmm6, %xmm1
	divsd	136(%rsp), %xmm1
	movsd	%xmm1, 120(%rsp)
	movapd	%xmm8, %xmm1
	divsd	144(%rsp), %xmm1
	movsd	%xmm1, 128(%rsp)
	movapd	%xmm9, %xmm1
	divsd	152(%rsp), %xmm1
	movsd	%xmm1, 136(%rsp)
	movapd	%xmm10, %xmm1
	divsd	%xmm13, %xmm1
	movsd	%xmm1, 144(%rsp)
	movsd	24(%rsp), %xmm13
	divsd	%xmm14, %xmm13
	movsd	56(%rsp), %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm2, %xmm15
	divsd	160(%rsp), %xmm15
	movsd	32(%rsp), %xmm1
	movapd	%xmm1, %xmm11
	divsd	%xmm7, %xmm11
	movsd	%xmm11, 32(%rsp)
	movsd	40(%rsp), %xmm7
	divsd	64(%rsp), %xmm7
	movapd	%xmm12, %xmm2
	divsd	48(%rsp), %xmm12
	movsd	%xmm12, 64(%rsp)
	movapd	%xmm0, %xmm12
	divsd	72(%rsp), %xmm12
	movsd	%xmm12, 72(%rsp)
	movsd	8(%rsp), %xmm12
	divsd	80(%rsp), %xmm12
	movsd	%xmm12, 80(%rsp)
	movsd	16(%rsp), %xmm12
	divsd	88(%rsp), %xmm12
	movsd	%xmm12, 88(%rsp)
	movapd	%xmm3, %xmm12
	divsd	96(%rsp), %xmm12
	movsd	%xmm12, 96(%rsp)
	movapd	%xmm4, %xmm12
	divsd	104(%rsp), %xmm12
	movsd	%xmm12, 104(%rsp)
	movapd	%xmm5, %xmm12
	divsd	112(%rsp), %xmm12
	movsd	%xmm12, 112(%rsp)
	movapd	%xmm6, %xmm12
	divsd	120(%rsp), %xmm12
	movsd	%xmm12, 120(%rsp)
	movapd	%xmm8, %xmm12
	divsd	128(%rsp), %xmm12
	movsd	%xmm12, 128(%rsp)
	movapd	%xmm9, %xmm12
	divsd	136(%rsp), %xmm12
	movsd	%xmm12, 136(%rsp)
	movapd	%xmm10, %xmm12
	divsd	144(%rsp), %xmm12
	movsd	%xmm12, 144(%rsp)
	movsd	24(%rsp), %xmm11
	divsd	%xmm13, %xmm11
	movsd	%xmm11, 152(%rsp)
	movsd	56(%rsp), %xmm13
	divsd	%xmm14, %xmm13
	movsd	(%rsp), %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm1, %xmm15
	divsd	32(%rsp), %xmm15
	movsd	40(%rsp), %xmm11
	movapd	%xmm11, %xmm12
	divsd	%xmm7, %xmm12
	movsd	%xmm12, 40(%rsp)
	movsd	%xmm2, 48(%rsp)
	movapd	%xmm2, %xmm7
	divsd	64(%rsp), %xmm7
	movapd	%xmm0, %xmm2
	divsd	72(%rsp), %xmm2
	movsd	%xmm2, 64(%rsp)
	movsd	8(%rsp), %xmm2
	divsd	80(%rsp), %xmm2
	movsd	%xmm2, 72(%rsp)
	movsd	16(%rsp), %xmm2
	divsd	88(%rsp), %xmm2
	movsd	%xmm2, 80(%rsp)
	movapd	%xmm3, %xmm2
	divsd	96(%rsp), %xmm2
	movsd	%xmm2, 88(%rsp)
	movapd	%xmm4, %xmm2
	divsd	104(%rsp), %xmm2
	movsd	%xmm2, 96(%rsp)
	movapd	%xmm5, %xmm2
	divsd	112(%rsp), %xmm2
	movsd	%xmm2, 104(%rsp)
	movapd	%xmm6, %xmm2
	divsd	120(%rsp), %xmm2
	movsd	%xmm2, 112(%rsp)
	movapd	%xmm8, %xmm2
	divsd	128(%rsp), %xmm2
	movsd	%xmm2, 120(%rsp)
	movapd	%xmm9, %xmm2
	divsd	136(%rsp), %xmm2
	movsd	%xmm2, 128(%rsp)
	movapd	%xmm10, %xmm2
	divsd	144(%rsp), %xmm2
	movsd	%xmm2, 136(%rsp)
	movsd	24(%rsp), %xmm2
	divsd	152(%rsp), %xmm2
	movsd	%xmm2, 144(%rsp)
	movsd	56(%rsp), %xmm2
	movapd	%xmm2, %xmm12
	divsd	%xmm13, %xmm12
	movsd	%xmm12, 56(%rsp)
	movsd	(%rsp), %xmm13
	divsd	%xmm14, %xmm13
	movsd	%xmm1, 32(%rsp)
	movapd	%xmm1, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm11, %xmm15
	divsd	40(%rsp), %xmm15
	movsd	48(%rsp), %xmm12
	movapd	%xmm12, %xmm1
	divsd	%xmm7, %xmm1
	movsd	%xmm1, 48(%rsp)
	movapd	%xmm0, %xmm7
	divsd	64(%rsp), %xmm7
	movsd	8(%rsp), %xmm1
	divsd	72(%rsp), %xmm1
	movsd	%xmm1, 64(%rsp)
	movsd	16(%rsp), %xmm1
	divsd	80(%rsp), %xmm1
	movsd	%xmm1, 72(%rsp)
	movapd	%xmm3, %xmm1
	divsd	88(%rsp), %xmm1
	movsd	%xmm1, 80(%rsp)
	movapd	%xmm4, %xmm1
	divsd	96(%rsp), %xmm1
	movsd	%xmm1, 88(%rsp)
	movapd	%xmm5, %xmm1
	divsd	104(%rsp), %xmm1
	movsd	%xmm1, 96(%rsp)
	movapd	%xmm6, %xmm1
	divsd	112(%rsp), %xmm1
	movsd	%xmm1, 104(%rsp)
	movapd	%xmm8, %xmm1
	divsd	120(%rsp), %xmm1
	movsd	%xmm1, 112(%rsp)
	movapd	%xmm9, %xmm1
	divsd	128(%rsp), %xmm1
	movsd	%xmm1, 120(%rsp)
	movapd	%xmm10, %xmm1
	divsd	136(%rsp), %xmm1
	movsd	%xmm1, 128(%rsp)
	movsd	24(%rsp), %xmm1
	divsd	144(%rsp), %xmm1
	movsd	%xmm1, 136(%rsp)
	movapd	%xmm2, %xmm1
	divsd	56(%rsp), %xmm1
	movsd	%xmm1, 56(%rsp)
	movsd	(%rsp), %xmm1
	divsd	%xmm13, %xmm1
	movsd	%xmm1, 144(%rsp)
	movsd	32(%rsp), %xmm13
	divsd	%xmm14, %xmm13
	movsd	%xmm11, 40(%rsp)
	movapd	%xmm11, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm12, %xmm15
	divsd	48(%rsp), %xmm15
	movapd	%xmm0, %xmm11
	divsd	%xmm7, %xmm11
	movsd	%xmm11, 152(%rsp)
	movsd	8(%rsp), %xmm7
	divsd	64(%rsp), %xmm7
	movsd	16(%rsp), %xmm11
	divsd	72(%rsp), %xmm11
	movsd	%xmm11, 64(%rsp)
	movapd	%xmm3, %xmm11
	divsd	80(%rsp), %xmm11
	movsd	%xmm11, 72(%rsp)
	movapd	%xmm4, %xmm11
	divsd	88(%rsp), %xmm11
	movsd	%xmm11, 80(%rsp)
	movapd	%xmm5, %xmm11
	divsd	96(%rsp), %xmm11
	movsd	%xmm11, 88(%rsp)
	movapd	%xmm6, %xmm11
	divsd	104(%rsp), %xmm11
	movsd	%xmm11, 96(%rsp)
	movapd	%xmm8, %xmm11
	divsd	112(%rsp), %xmm11
	movsd	%xmm11, 104(%rsp)
	movapd	%xmm9, %xmm11
	divsd	120(%rsp), %xmm11
	movsd	%xmm11, 112(%rsp)
	movapd	%xmm10, %xmm1
	divsd	128(%rsp), %xmm1
	movsd	%xmm1, 120(%rsp)
	movsd	24(%rsp), %xmm1
	divsd	136(%rsp), %xmm1
	movsd	%xmm1, 128(%rsp)
	movapd	%xmm2, %xmm1
	divsd	56(%rsp), %xmm1
	movsd	%xmm1, 56(%rsp)
	movsd	(%rsp), %xmm1
	divsd	144(%rsp), %xmm1
	movsd	%xmm1, 136(%rsp)
	movsd	32(%rsp), %xmm11
	divsd	%xmm13, %xmm11
	movsd	%xmm11, 144(%rsp)
	movsd	40(%rsp), %xmm13
	divsd	%xmm14, %xmm13
	movsd	%xmm12, 48(%rsp)
	movapd	%xmm12, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm0, %xmm15
	divsd	152(%rsp), %xmm15
	movsd	8(%rsp), %xmm12
	divsd	%xmm7, %xmm12
	movsd	%xmm12, 152(%rsp)
	movsd	16(%rsp), %xmm12
	divsd	64(%rsp), %xmm12
	movapd	%xmm12, %xmm7
	movapd	%xmm3, %xmm12
	divsd	72(%rsp), %xmm12
	movsd	%xmm12, 64(%rsp)
	movapd	%xmm4, %xmm12
	divsd	80(%rsp), %xmm12
	movsd	%xmm12, 72(%rsp)
	movapd	%xmm5, %xmm12
	divsd	88(%rsp), %xmm12
	movsd	%xmm12, 80(%rsp)
	movapd	%xmm6, %xmm12
	divsd	96(%rsp), %xmm12
	movsd	%xmm12, 88(%rsp)
	movapd	%xmm8, %xmm11
	divsd	104(%rsp), %xmm11
	movsd	%xmm11, 96(%rsp)
	movapd	%xmm9, %xmm11
	divsd	112(%rsp), %xmm11
	movsd	%xmm11, 104(%rsp)
	movapd	%xmm10, %xmm11
	divsd	120(%rsp), %xmm11
	movsd	%xmm11, 112(%rsp)
	movsd	24(%rsp), %xmm11
	divsd	128(%rsp), %xmm11
	movsd	%xmm11, 120(%rsp)
	movapd	%xmm2, %xmm12
	divsd	56(%rsp), %xmm12
	movsd	%xmm12, 56(%rsp)
	movsd	(%rsp), %xmm1
	divsd	136(%rsp), %xmm1
	movsd	%xmm1, 128(%rsp)
	movsd	32(%rsp), %xmm1
	divsd	144(%rsp), %xmm1
	movsd	%xmm1, 136(%rsp)
	movsd	40(%rsp), %xmm12
	divsd	%xmm13, %xmm12
	movsd	%xmm12, 144(%rsp)
	movsd	48(%rsp), %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm0, %xmm14
	divsd	%xmm15, %xmm14
	movsd	8(%rsp), %xmm15
	divsd	152(%rsp), %xmm15
	movsd	16(%rsp), %xmm11
	divsd	%xmm7, %xmm11
	movsd	%xmm11, 152(%rsp)
	movapd	%xmm3, %xmm7
	divsd	64(%rsp), %xmm7
	movapd	%xmm4, %xmm1
	divsd	72(%rsp), %xmm1
	movsd	%xmm1, 64(%rsp)
	movapd	%xmm5, %xmm12
	divsd	80(%rsp), %xmm12
	movsd	%xmm12, 72(%rsp)
	movapd	%xmm6, %xmm12
	divsd	88(%rsp), %xmm12
	movsd	%xmm12, 80(%rsp)
	movapd	%xmm8, %xmm12
	divsd	96(%rsp), %xmm12
	movsd	%xmm12, 88(%rsp)
	movapd	%xmm9, %xmm11
	divsd	104(%rsp), %xmm11
	movsd	%xmm11, 96(%rsp)
	movapd	%xmm10, %xmm11
	divsd	112(%rsp), %xmm11
	movsd	%xmm11, 104(%rsp)
	movsd	24(%rsp), %xmm11
	divsd	120(%rsp), %xmm11
	movsd	%xmm11, 112(%rsp)
	movapd	%xmm2, %xmm11
	divsd	56(%rsp), %xmm11
	movsd	%xmm11, 120(%rsp)
	movsd	(%rsp), %xmm1
	divsd	128(%rsp), %xmm1
	movsd	%xmm1, 128(%rsp)
	movsd	32(%rsp), %xmm1
	divsd	136(%rsp), %xmm1
	movsd	%xmm1, 136(%rsp)
	movsd	40(%rsp), %xmm11
	divsd	144(%rsp), %xmm11
	movsd	%xmm11, 144(%rsp)
	movsd	48(%rsp), %xmm12
	divsd	%xmm13, %xmm12
	movsd	%xmm12, 160(%rsp)
	movapd	%xmm0, %xmm13
	divsd	%xmm14, %xmm13
	movsd	8(%rsp), %xmm14
	divsd	%xmm15, %xmm14
	movsd	16(%rsp), %xmm15
	divsd	152(%rsp), %xmm15
	movapd	%xmm3, %xmm12
	divsd	%xmm7, %xmm12
	movsd	%xmm12, 152(%rsp)
	movapd	%xmm4, %xmm7
	divsd	64(%rsp), %xmm7
	movapd	%xmm5, %xmm11
	divsd	72(%rsp), %xmm11
	movsd	%xmm11, 64(%rsp)
	movapd	%xmm6, %xmm12
	divsd	80(%rsp), %xmm12
	movsd	%xmm12, 72(%rsp)
	movapd	%xmm8, %xmm12
	divsd	88(%rsp), %xmm12
	movsd	%xmm12, 80(%rsp)
	movapd	%xmm9, %xmm12
	divsd	96(%rsp), %xmm12
	movsd	%xmm12, 88(%rsp)
	movapd	%xmm10, %xmm11
	divsd	104(%rsp), %xmm11
	movsd	%xmm11, 96(%rsp)
	movsd	24(%rsp), %xmm11
	divsd	112(%rsp), %xmm11
	movsd	%xmm11, 104(%rsp)
	movsd	%xmm2, 56(%rsp)
	movapd	%xmm2, %xmm12
	divsd	120(%rsp), %xmm12
	movsd	%xmm12, 112(%rsp)
	movsd	(%rsp), %xmm1
	divsd	128(%rsp), %xmm1
	movsd	%xmm1, 120(%rsp)
	movsd	32(%rsp), %xmm1
	divsd	136(%rsp), %xmm1
	movsd	%xmm1, 128(%rsp)
	movsd	40(%rsp), %xmm11
	divsd	144(%rsp), %xmm11
	movsd	%xmm11, 136(%rsp)
	movsd	48(%rsp), %xmm12
	movapd	%xmm12, %xmm11
	divsd	160(%rsp), %xmm11
	movsd	%xmm11, 144(%rsp)
	movapd	%xmm0, %xmm2
	divsd	%xmm13, %xmm2
	movsd	%xmm2, 160(%rsp)
	movsd	8(%rsp), %xmm13
	divsd	%xmm14, %xmm13
	movsd	16(%rsp), %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm3, %xmm15
	divsd	152(%rsp), %xmm15
	movapd	%xmm4, %xmm2
	divsd	%xmm7, %xmm2
	movapd	%xmm5, %xmm7
	divsd	64(%rsp), %xmm7
	movsd	%xmm7, 152(%rsp)
	movapd	%xmm6, %xmm7
	divsd	72(%rsp), %xmm7
	movsd	%xmm7, 168(%rsp)
	movapd	%xmm8, %xmm7
	divsd	80(%rsp), %xmm7
	movsd	%xmm7, 176(%rsp)
	movapd	%xmm9, %xmm7
	divsd	88(%rsp), %xmm7
	movsd	%xmm7, 184(%rsp)
	movapd	%xmm10, %xmm7
	divsd	96(%rsp), %xmm7
	movsd	%xmm7, 192(%rsp)
	movsd	24(%rsp), %xmm7
	divsd	104(%rsp), %xmm7
	movsd	%xmm7, 200(%rsp)
	movsd	56(%rsp), %xmm11
	divsd	112(%rsp), %xmm11
	movsd	(%rsp), %xmm7
	divsd	120(%rsp), %xmm7
	movsd	32(%rsp), %xmm1
	divsd	128(%rsp), %xmm1
	movsd	%xmm1, 64(%rsp)
	movsd	40(%rsp), %xmm1
	divsd	136(%rsp), %xmm1
	movsd	%xmm1, 72(%rsp)
	divsd	144(%rsp), %xmm12
	movsd	%xmm12, 80(%rsp)
	movapd	%xmm0, %xmm12
	divsd	160(%rsp), %xmm12
	movsd	%xmm12, 88(%rsp)
	movsd	8(%rsp), %xmm12
	divsd	%xmm13, %xmm12
	movsd	%xmm12, 96(%rsp)
	movsd	16(%rsp), %xmm13
	divsd	%xmm14, %xmm13
	movsd	%xmm13, 104(%rsp)
	movapd	%xmm3, %xmm14
	divsd	%xmm15, %xmm14
	movsd	%xmm14, 112(%rsp)
	movapd	%xmm4, %xmm15
	divsd	%xmm2, %xmm15
	movsd	%xmm15, 120(%rsp)
	movapd	%xmm5, %xmm2
	divsd	152(%rsp), %xmm2
	movsd	%xmm2, 128(%rsp)
	movapd	%xmm6, %xmm2
	divsd	168(%rsp), %xmm2
	movsd	%xmm2, 136(%rsp)
	movapd	%xmm8, %xmm2
	divsd	176(%rsp), %xmm2
	movsd	%xmm2, 144(%rsp)
	movapd	%xmm9, %xmm2
	divsd	184(%rsp), %xmm2
	movsd	%xmm2, 152(%rsp)
	movapd	%xmm10, %xmm13
	divsd	192(%rsp), %xmm13
	movsd	24(%rsp), %xmm14
	divsd	200(%rsp), %xmm14
	movsd	56(%rsp), %xmm15
	divsd	%xmm11, %xmm15
	subq	$1, %rax
	cmpq	$-1, %rax
	jne	.L1292
	movsd	%xmm13, 8(%rsp)
	movsd	%xmm14, 16(%rsp)
	movsd	%xmm15, 160(%rsp)
.L1291:
	cvttsd2sil	%xmm7, %edi
	call	use_int@PLT
	cvttsd2sil	(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	64(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	32(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	72(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	40(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	80(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	48(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	88(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	208(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	96(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	216(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	104(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	224(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	112(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	232(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	120(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	240(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	128(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	248(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	136(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	256(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	144(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	264(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	152(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	272(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	8(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	280(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	16(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	288(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	160(%rsp), %edi
	call	use_int@PLT
	cvttsd2sil	296(%rsp), %edi
	call	use_int@PLT
	addq	$312, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE328:
	.size	double_div_15, .-double_div_15
	.globl	max_parallelism
	.type	max_parallelism, @function
max_parallelism:
.LFB72:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$40, %rsp
	.cfi_def_cfa_offset 96
	movq	%rdi, %r14
	movl	%esi, %r13d
	movl	%edx, %r12d
	movq	%rcx, %rbp
	movl	$1, %ebx
	movsd	.LC0(%rip), %xmm5
	movsd	%xmm5, 8(%rsp)
	leaq	initialize(%rip), %r15
	jmp	.L1309
.L1313:
	call	usecs_spent@PLT
	movq	%rax, 16(%rsp)
	call	get_n@PLT
	movq	16(%rsp), %rdi
	testq	%rdi, %rdi
	js	.L1298
	pxor	%xmm1, %xmm1
	cvtsi2sdq	%rdi, %xmm1
.L1299:
	testq	%rax, %rax
	js	.L1300
	pxor	%xmm0, %xmm0
	cvtsi2sdq	%rax, %xmm0
.L1301:
	divsd	%xmm0, %xmm1
	movsd	%xmm1, 24(%rsp)
	jmp	.L1302
.L1298:
	movq	16(%rsp), %rdi
	movq	%rdi, %rcx
	shrq	%rcx
	andl	$1, %edi
	orq	%rdi, %rcx
	pxor	%xmm1, %xmm1
	cvtsi2sdq	%rcx, %xmm1
	addsd	%xmm1, %xmm1
	jmp	.L1299
.L1300:
	movq	%rax, %rdx
	shrq	%rdx
	andl	$1, %eax
	orq	%rax, %rdx
	pxor	%xmm0, %xmm0
	cvtsi2sdq	%rdx, %xmm0
	addsd	%xmm0, %xmm0
	jmp	.L1301
.L1303:
	movq	%rax, %rdx
	shrq	%rdx
	andl	$1, %eax
	orq	%rax, %rdx
	pxor	%xmm0, %xmm0
	cvtsi2sdq	%rdx, %xmm0
	addsd	%xmm0, %xmm0
	jmp	.L1304
.L1305:
	movq	%rax, %rdx
	shrq	%rdx
	andl	$1, %eax
	orq	%rax, %rdx
	pxor	%xmm0, %xmm0
	cvtsi2sdq	%rdx, %xmm0
	addsd	%xmm0, %xmm0
.L1306:
	mulsd	16(%rsp), %xmm0
	maxsd	8(%rsp), %xmm0
	movsd	%xmm0, 8(%rsp)
	cmpl	$15, %ebx
	jg	.L1295
.L1302:
	addq	$1, %rbx
.L1309:
	movq	-8(%r14,%rbx,8), %rsi
	pushq	%rbp
	.cfi_def_cfa_offset 104
	pushq	%r12
	.cfi_def_cfa_offset 112
	movl	%r13d, %r9d
	movl	$1, %r8d
	movl	$0, %ecx
	movl	$0, %edx
	movq	%r15, %rdi
	call	benchmp@PLT
	movl	$0, %eax
	call	save_minimum@PLT
	call	usecs_spent@PLT
	addq	$16, %rsp
	.cfi_def_cfa_offset 96
	testq	%rax, %rax
	je	.L1310
	cmpl	$1, %ebx
	je	.L1313
	call	usecs_spent@PLT
	testq	%rax, %rax
	js	.L1303
	pxor	%xmm0, %xmm0
	cvtsi2sdq	%rax, %xmm0
.L1304:
	movsd	24(%rsp), %xmm2
	divsd	%xmm0, %xmm2
	movsd	%xmm2, 16(%rsp)
	call	get_n@PLT
	imulq	%rbx, %rax
	testq	%rax, %rax
	js	.L1305
	pxor	%xmm0, %xmm0
	cvtsi2sdq	%rax, %xmm0
	jmp	.L1306
.L1310:
	movsd	.LC11(%rip), %xmm6
	movsd	%xmm6, 8(%rsp)
.L1295:
	movsd	8(%rsp), %xmm0
	addq	$40, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE72:
	.size	max_parallelism, .-max_parallelism
	.section	.rodata.str1.8,"aMS",@progbits,1
	.align 8
.LC12:
	.string	"[-W <warmup>] [-N <repetitions>]\n"
	.section	.rodata.str1.1,"aMS",@progbits,1
.LC13:
	.string	"W:N:"
	.section	.rodata.str1.8
	.align 8
.LC15:
	.string	"integer bit parallelism: %.2f\n"
	.align 8
.LC16:
	.string	"integer add parallelism: %.2f\n"
	.align 8
.LC17:
	.string	"integer mul parallelism: %.2f\n"
	.align 8
.LC18:
	.string	"integer div parallelism: %.2f\n"
	.align 8
.LC19:
	.string	"integer mod parallelism: %.2f\n"
	.section	.rodata.str1.1
.LC20:
	.string	"int64 bit parallelism: %.2f\n"
.LC21:
	.string	"int64 add parallelism: %.2f\n"
.LC22:
	.string	"int64 mul parallelism: %.2f\n"
.LC23:
	.string	"int64 div parallelism: %.2f\n"
.LC24:
	.string	"int64 mod parallelism: %.2f\n"
.LC25:
	.string	"float add parallelism: %.2f\n"
.LC26:
	.string	"float mul parallelism: %.2f\n"
.LC27:
	.string	"float div parallelism: %.2f\n"
.LC28:
	.string	"double add parallelism: %.2f\n"
.LC29:
	.string	"double mul parallelism: %.2f\n"
.LC30:
	.string	"double div parallelism: %.2f\n"
	.text
	.globl	main
	.type	main, @function
main:
.LFB330:
	.cfi_startproc
	endbr64
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$232, %rsp
	.cfi_def_cfa_offset 288
	movl	%edi, %r13d
	movq	%rsi, %r12
	movq	%fs:40, %rax
	movq	%rax, 216(%rsp)
	xorl	%eax, %eax
	movl	$0, %edi
	call	get_enough@PLT
	cmpl	$999999, %eax
	movl	$1, %ebx
	movl	$11, %eax
	cmovle	%eax, %ebx
	movl	$1, (%rsp)
	movl	$1000, 4(%rsp)
	movl	$-1023, 8(%rsp)
	movl	$0, %ebp
	leaq	.LC13(%rip), %r14
	leaq	.LC12(%rip), %r15
	jmp	.L1316
.L1317:
	movl	$10, %edx
	movl	$0, %esi
	movq	myoptarg(%rip), %rdi
	call	strtol@PLT
	movl	%eax, %ebx
.L1316:
	movq	%r14, %rdx
	movq	%r12, %rsi
	movl	%r13d, %edi
	call	mygetopt@PLT
	cmpl	$-1, %eax
	je	.L1372
	cmpl	$78, %eax
	je	.L1317
	cmpl	$87, %eax
	jne	.L1318
	movl	$10, %edx
	movl	$0, %esi
	movq	myoptarg(%rip), %rdi
	call	strtol@PLT
	movl	%eax, %ebp
	jmp	.L1316
.L1318:
	movq	%r15, %rdx
	movq	%r12, %rsi
	movl	%r13d, %edi
	call	lmbench_usage@PLT
	jmp	.L1316
.L1372:
	movq	%rsp, %rcx
	movl	%ebx, %edx
	movl	%ebp, %esi
	leaq	integer_bit_benchmarks(%rip), %rdi
	call	max_parallelism
	comisd	.LC14(%rip), %xmm0
	ja	.L1373
.L1321:
	movq	%rsp, %rcx
	movl	%ebx, %edx
	movl	%ebp, %esi
	leaq	integer_add_benchmarks(%rip), %rdi
	call	max_parallelism
	comisd	.LC14(%rip), %xmm0
	ja	.L1374
.L1323:
	movq	%rsp, %rcx
	movl	%ebx, %edx
	movl	%ebp, %esi
	leaq	integer_mul_benchmarks(%rip), %rdi
	call	max_parallelism
	comisd	.LC14(%rip), %xmm0
	ja	.L1375
.L1325:
	movq	%rsp, %rcx
	movl	%ebx, %edx
	movl	%ebp, %esi
	leaq	integer_div_benchmarks(%rip), %rdi
	call	max_parallelism
	comisd	.LC14(%rip), %xmm0
	ja	.L1376
.L1327:
	movq	%rsp, %rcx
	movl	%ebx, %edx
	movl	%ebp, %esi
	leaq	integer_mod_benchmarks(%rip), %rdi
	call	max_parallelism
	comisd	.LC14(%rip), %xmm0
	ja	.L1377
.L1329:
	movq	%rsp, %rcx
	movl	%ebx, %edx
	movl	%ebp, %esi
	leaq	int64_bit_benchmarks(%rip), %rdi
	call	max_parallelism
	comisd	.LC14(%rip), %xmm0
	ja	.L1378
.L1331:
	movq	%rsp, %rcx
	movl	%ebx, %edx
	movl	%ebp, %esi
	leaq	int64_add_benchmarks(%rip), %rdi
	call	max_parallelism
	comisd	.LC14(%rip), %xmm0
	ja	.L1379
.L1333:
	movq	%rsp, %rcx
	movl	%ebx, %edx
	movl	%ebp, %esi
	leaq	int64_mul_benchmarks(%rip), %rdi
	call	max_parallelism
	comisd	.LC14(%rip), %xmm0
	ja	.L1380
.L1335:
	movq	%rsp, %rcx
	movl	%ebx, %edx
	movl	%ebp, %esi
	leaq	int64_div_benchmarks(%rip), %rdi
	call	max_parallelism
	comisd	.LC14(%rip), %xmm0
	ja	.L1381
.L1337:
	movq	%rsp, %rcx
	movl	%ebx, %edx
	movl	%ebp, %esi
	leaq	int64_mod_benchmarks(%rip), %rdi
	call	max_parallelism
	comisd	.LC14(%rip), %xmm0
	ja	.L1382
.L1339:
	movq	%rsp, %rcx
	movl	%ebx, %edx
	movl	%ebp, %esi
	leaq	float_add_benchmarks(%rip), %rdi
	call	max_parallelism
	comisd	.LC14(%rip), %xmm0
	ja	.L1383
.L1341:
	movq	%rsp, %rcx
	movl	%ebx, %edx
	movl	%ebp, %esi
	leaq	float_mul_benchmarks(%rip), %rdi
	call	max_parallelism
	comisd	.LC14(%rip), %xmm0
	ja	.L1384
.L1343:
	movq	%rsp, %rcx
	movl	%ebx, %edx
	movl	%ebp, %esi
	leaq	float_div_benchmarks(%rip), %rdi
	call	max_parallelism
	comisd	.LC14(%rip), %xmm0
	ja	.L1385
.L1345:
	movq	%rsp, %rcx
	movl	%ebx, %edx
	movl	%ebp, %esi
	leaq	double_add_benchmarks(%rip), %rdi
	call	max_parallelism
	comisd	.LC14(%rip), %xmm0
	ja	.L1386
.L1347:
	movq	%rsp, %rcx
	movl	%ebx, %edx
	movl	%ebp, %esi
	leaq	double_mul_benchmarks(%rip), %rdi
	call	max_parallelism
	comisd	.LC14(%rip), %xmm0
	ja	.L1387
.L1349:
	movq	%rsp, %rcx
	movl	%ebx, %edx
	movl	%ebp, %esi
	leaq	double_div_benchmarks(%rip), %rdi
	call	max_parallelism
	comisd	.LC14(%rip), %xmm0
	ja	.L1388
.L1351:
	movq	216(%rsp), %rax
	subq	%fs:40, %rax
	jne	.L1389
	movl	$0, %eax
	addq	$232, %rsp
	.cfi_remember_state
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
.L1373:
	.cfi_restore_state
	leaq	.LC15(%rip), %rdx
	movl	$1, %esi
	movq	stderr(%rip), %rdi
	movl	$1, %eax
	call	__fprintf_chk@PLT
	jmp	.L1321
.L1374:
	leaq	.LC16(%rip), %rdx
	movl	$1, %esi
	movq	stderr(%rip), %rdi
	movl	$1, %eax
	call	__fprintf_chk@PLT
	jmp	.L1323
.L1375:
	leaq	.LC17(%rip), %rdx
	movl	$1, %esi
	movq	stderr(%rip), %rdi
	movl	$1, %eax
	call	__fprintf_chk@PLT
	jmp	.L1325
.L1376:
	leaq	.LC18(%rip), %rdx
	movl	$1, %esi
	movq	stderr(%rip), %rdi
	movl	$1, %eax
	call	__fprintf_chk@PLT
	jmp	.L1327
.L1377:
	leaq	.LC19(%rip), %rdx
	movl	$1, %esi
	movq	stderr(%rip), %rdi
	movl	$1, %eax
	call	__fprintf_chk@PLT
	jmp	.L1329
.L1378:
	leaq	.LC20(%rip), %rdx
	movl	$1, %esi
	movq	stderr(%rip), %rdi
	movl	$1, %eax
	call	__fprintf_chk@PLT
	jmp	.L1331
.L1379:
	leaq	.LC21(%rip), %rdx
	movl	$1, %esi
	movq	stderr(%rip), %rdi
	movl	$1, %eax
	call	__fprintf_chk@PLT
	jmp	.L1333
.L1380:
	leaq	.LC22(%rip), %rdx
	movl	$1, %esi
	movq	stderr(%rip), %rdi
	movl	$1, %eax
	call	__fprintf_chk@PLT
	jmp	.L1335
.L1381:
	leaq	.LC23(%rip), %rdx
	movl	$1, %esi
	movq	stderr(%rip), %rdi
	movl	$1, %eax
	call	__fprintf_chk@PLT
	jmp	.L1337
.L1382:
	leaq	.LC24(%rip), %rdx
	movl	$1, %esi
	movq	stderr(%rip), %rdi
	movl	$1, %eax
	call	__fprintf_chk@PLT
	jmp	.L1339
.L1383:
	leaq	.LC25(%rip), %rdx
	movl	$1, %esi
	movq	stderr(%rip), %rdi
	movl	$1, %eax
	call	__fprintf_chk@PLT
	jmp	.L1341
.L1384:
	leaq	.LC26(%rip), %rdx
	movl	$1, %esi
	movq	stderr(%rip), %rdi
	movl	$1, %eax
	call	__fprintf_chk@PLT
	jmp	.L1343
.L1385:
	leaq	.LC27(%rip), %rdx
	movl	$1, %esi
	movq	stderr(%rip), %rdi
	movl	$1, %eax
	call	__fprintf_chk@PLT
	jmp	.L1345
.L1386:
	leaq	.LC28(%rip), %rdx
	movl	$1, %esi
	movq	stderr(%rip), %rdi
	movl	$1, %eax
	call	__fprintf_chk@PLT
	jmp	.L1347
.L1387:
	leaq	.LC29(%rip), %rdx
	movl	$1, %esi
	movq	stderr(%rip), %rdi
	movl	$1, %eax
	call	__fprintf_chk@PLT
	jmp	.L1349
.L1388:
	leaq	.LC30(%rip), %rdx
	movl	$1, %esi
	movq	stderr(%rip), %rdi
	movl	$1, %eax
	call	__fprintf_chk@PLT
	jmp	.L1351
.L1389:
	call	__stack_chk_fail@PLT
	.cfi_endproc
.LFE330:
	.size	main, .-main
	.globl	double_div_benchmarks
	.section	.data.rel.local,"aw"
	.align 32
	.type	double_div_benchmarks, @object
	.size	double_div_benchmarks, 128
double_div_benchmarks:
	.quad	double_div_0
	.quad	double_div_1
	.quad	double_div_2
	.quad	double_div_3
	.quad	double_div_4
	.quad	double_div_5
	.quad	double_div_6
	.quad	double_div_7
	.quad	double_div_8
	.quad	double_div_9
	.quad	double_div_10
	.quad	double_div_11
	.quad	double_div_12
	.quad	double_div_13
	.quad	double_div_14
	.quad	double_div_15
	.globl	double_mul_benchmarks
	.align 32
	.type	double_mul_benchmarks, @object
	.size	double_mul_benchmarks, 128
double_mul_benchmarks:
	.quad	double_mul_0
	.quad	double_mul_1
	.quad	double_mul_2
	.quad	double_mul_3
	.quad	double_mul_4
	.quad	double_mul_5
	.quad	double_mul_6
	.quad	double_mul_7
	.quad	double_mul_8
	.quad	double_mul_9
	.quad	double_mul_10
	.quad	double_mul_11
	.quad	double_mul_12
	.quad	double_mul_13
	.quad	double_mul_14
	.quad	double_mul_15
	.globl	double_add_benchmarks
	.align 32
	.type	double_add_benchmarks, @object
	.size	double_add_benchmarks, 128
double_add_benchmarks:
	.quad	double_add_0
	.quad	double_add_1
	.quad	double_add_2
	.quad	double_add_3
	.quad	double_add_4
	.quad	double_add_5
	.quad	double_add_6
	.quad	double_add_7
	.quad	double_add_8
	.quad	double_add_9
	.quad	double_add_10
	.quad	double_add_11
	.quad	double_add_12
	.quad	double_add_13
	.quad	double_add_14
	.quad	double_add_15
	.globl	float_div_benchmarks
	.align 32
	.type	float_div_benchmarks, @object
	.size	float_div_benchmarks, 128
float_div_benchmarks:
	.quad	float_div_0
	.quad	float_div_1
	.quad	float_div_2
	.quad	float_div_3
	.quad	float_div_4
	.quad	float_div_5
	.quad	float_div_6
	.quad	float_div_7
	.quad	float_div_8
	.quad	float_div_9
	.quad	float_div_10
	.quad	float_div_11
	.quad	float_div_12
	.quad	float_div_13
	.quad	float_div_14
	.quad	float_div_15
	.globl	float_mul_benchmarks
	.align 32
	.type	float_mul_benchmarks, @object
	.size	float_mul_benchmarks, 128
float_mul_benchmarks:
	.quad	float_mul_0
	.quad	float_mul_1
	.quad	float_mul_2
	.quad	float_mul_3
	.quad	float_mul_4
	.quad	float_mul_5
	.quad	float_mul_6
	.quad	float_mul_7
	.quad	float_mul_8
	.quad	float_mul_9
	.quad	float_mul_10
	.quad	float_mul_11
	.quad	float_mul_12
	.quad	float_mul_13
	.quad	float_mul_14
	.quad	float_mul_15
	.globl	float_add_benchmarks
	.align 32
	.type	float_add_benchmarks, @object
	.size	float_add_benchmarks, 128
float_add_benchmarks:
	.quad	float_add_0
	.quad	float_add_1
	.quad	float_add_2
	.quad	float_add_3
	.quad	float_add_4
	.quad	float_add_5
	.quad	float_add_6
	.quad	float_add_7
	.quad	float_add_8
	.quad	float_add_9
	.quad	float_add_10
	.quad	float_add_11
	.quad	float_add_12
	.quad	float_add_13
	.quad	float_add_14
	.quad	float_add_15
	.globl	int64_mod_benchmarks
	.align 32
	.type	int64_mod_benchmarks, @object
	.size	int64_mod_benchmarks, 128
int64_mod_benchmarks:
	.quad	int64_mod_0
	.quad	int64_mod_1
	.quad	int64_mod_2
	.quad	int64_mod_3
	.quad	int64_mod_4
	.quad	int64_mod_5
	.quad	int64_mod_6
	.quad	int64_mod_7
	.quad	int64_mod_8
	.quad	int64_mod_9
	.quad	int64_mod_10
	.quad	int64_mod_11
	.quad	int64_mod_12
	.quad	int64_mod_13
	.quad	int64_mod_14
	.quad	int64_mod_15
	.globl	int64_div_benchmarks
	.align 32
	.type	int64_div_benchmarks, @object
	.size	int64_div_benchmarks, 128
int64_div_benchmarks:
	.quad	int64_div_0
	.quad	int64_div_1
	.quad	int64_div_2
	.quad	int64_div_3
	.quad	int64_div_4
	.quad	int64_div_5
	.quad	int64_div_6
	.quad	int64_div_7
	.quad	int64_div_8
	.quad	int64_div_9
	.quad	int64_div_10
	.quad	int64_div_11
	.quad	int64_div_12
	.quad	int64_div_13
	.quad	int64_div_14
	.quad	int64_div_15
	.globl	int64_mul_benchmarks
	.align 32
	.type	int64_mul_benchmarks, @object
	.size	int64_mul_benchmarks, 128
int64_mul_benchmarks:
	.quad	int64_mul_0
	.quad	int64_mul_1
	.quad	int64_mul_2
	.quad	int64_mul_3
	.quad	int64_mul_4
	.quad	int64_mul_5
	.quad	int64_mul_6
	.quad	int64_mul_7
	.quad	int64_mul_8
	.quad	int64_mul_9
	.quad	int64_mul_10
	.quad	int64_mul_11
	.quad	int64_mul_12
	.quad	int64_mul_13
	.quad	int64_mul_14
	.quad	int64_mul_15
	.globl	int64_add_benchmarks
	.align 32
	.type	int64_add_benchmarks, @object
	.size	int64_add_benchmarks, 128
int64_add_benchmarks:
	.quad	int64_add_0
	.quad	int64_add_1
	.quad	int64_add_2
	.quad	int64_add_3
	.quad	int64_add_4
	.quad	int64_add_5
	.quad	int64_add_6
	.quad	int64_add_7
	.quad	int64_add_8
	.quad	int64_add_9
	.quad	int64_add_10
	.quad	int64_add_11
	.quad	int64_add_12
	.quad	int64_add_13
	.quad	int64_add_14
	.quad	int64_add_15
	.globl	int64_bit_benchmarks
	.align 32
	.type	int64_bit_benchmarks, @object
	.size	int64_bit_benchmarks, 128
int64_bit_benchmarks:
	.quad	int64_bit_0
	.quad	int64_bit_1
	.quad	int64_bit_2
	.quad	int64_bit_3
	.quad	int64_bit_4
	.quad	int64_bit_5
	.quad	int64_bit_6
	.quad	int64_bit_7
	.quad	int64_bit_8
	.quad	int64_bit_9
	.quad	int64_bit_10
	.quad	int64_bit_11
	.quad	int64_bit_12
	.quad	int64_bit_13
	.quad	int64_bit_14
	.quad	int64_bit_15
	.globl	integer_mod_benchmarks
	.align 32
	.type	integer_mod_benchmarks, @object
	.size	integer_mod_benchmarks, 128
integer_mod_benchmarks:
	.quad	integer_mod_0
	.quad	integer_mod_1
	.quad	integer_mod_2
	.quad	integer_mod_3
	.quad	integer_mod_4
	.quad	integer_mod_5
	.quad	integer_mod_6
	.quad	integer_mod_7
	.quad	integer_mod_8
	.quad	integer_mod_9
	.quad	integer_mod_10
	.quad	integer_mod_11
	.quad	integer_mod_12
	.quad	integer_mod_13
	.quad	integer_mod_14
	.quad	integer_mod_15
	.globl	integer_div_benchmarks
	.align 32
	.type	integer_div_benchmarks, @object
	.size	integer_div_benchmarks, 128
integer_div_benchmarks:
	.quad	integer_div_0
	.quad	integer_div_1
	.quad	integer_div_2
	.quad	integer_div_3
	.quad	integer_div_4
	.quad	integer_div_5
	.quad	integer_div_6
	.quad	integer_div_7
	.quad	integer_div_8
	.quad	integer_div_9
	.quad	integer_div_10
	.quad	integer_div_11
	.quad	integer_div_12
	.quad	integer_div_13
	.quad	integer_div_14
	.quad	integer_div_15
	.globl	integer_mul_benchmarks
	.align 32
	.type	integer_mul_benchmarks, @object
	.size	integer_mul_benchmarks, 128
integer_mul_benchmarks:
	.quad	integer_mul_0
	.quad	integer_mul_1
	.quad	integer_mul_2
	.quad	integer_mul_3
	.quad	integer_mul_4
	.quad	integer_mul_5
	.quad	integer_mul_6
	.quad	integer_mul_7
	.quad	integer_mul_8
	.quad	integer_mul_9
	.quad	integer_mul_10
	.quad	integer_mul_11
	.quad	integer_mul_12
	.quad	integer_mul_13
	.quad	integer_mul_14
	.quad	integer_mul_15
	.globl	integer_add_benchmarks
	.align 32
	.type	integer_add_benchmarks, @object
	.size	integer_add_benchmarks, 128
integer_add_benchmarks:
	.quad	integer_add_0
	.quad	integer_add_1
	.quad	integer_add_2
	.quad	integer_add_3
	.quad	integer_add_4
	.quad	integer_add_5
	.quad	integer_add_6
	.quad	integer_add_7
	.quad	integer_add_8
	.quad	integer_add_9
	.quad	integer_add_10
	.quad	integer_add_11
	.quad	integer_add_12
	.quad	integer_add_13
	.quad	integer_add_14
	.quad	integer_add_15
	.globl	integer_bit_benchmarks
	.align 32
	.type	integer_bit_benchmarks, @object
	.size	integer_bit_benchmarks, 128
integer_bit_benchmarks:
	.quad	integer_bit_0
	.quad	integer_bit_1
	.quad	integer_bit_2
	.quad	integer_bit_3
	.quad	integer_bit_4
	.quad	integer_bit_5
	.quad	integer_bit_6
	.quad	integer_bit_7
	.quad	integer_bit_8
	.quad	integer_bit_9
	.quad	integer_bit_10
	.quad	integer_bit_11
	.quad	integer_bit_12
	.quad	integer_bit_13
	.quad	integer_bit_14
	.quad	integer_bit_15
	.globl	id
	.section	.rodata.str1.1
.LC31:
	.string	"$Id$\n"
	.section	.data.rel.local
	.align 8
	.type	id, @object
	.size	id, 8
id:
	.quad	.LC31
	.section	.rodata.cst8,"aM",@progbits,8
	.align 8
.LC0:
	.long	0
	.long	1072693248
	.section	.rodata.cst4,"aM",@progbits,4
	.align 4
.LC1:
	.long	1149222912
	.align 4
.LC2:
	.long	1090519040
	.section	.rodata.cst8
	.align 8
.LC3:
	.long	0
	.long	1069547520
	.align 8
.LC4:
	.long	0
	.long	1083129856
	.section	.rodata.cst4
	.align 4
.LC5:
	.long	1068827891
	.align 4
.LC6:
	.long	1078530011
	.section	.rodata.cst8
	.align 8
.LC7:
	.long	0
	.long	1083176960
	.align 8
.LC8:
	.long	0
	.long	1075838976
	.align 8
.LC9:
	.long	1708926943
	.long	1073127582
	.align 8
.LC10:
	.long	1405670641
	.long	1074340347
	.align 8
.LC11:
	.long	0
	.long	-1074790400
	.align 8
.LC14:
	.long	0
	.long	0
	.ident	"GCC: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0"
	.section	.note.GNU-stack,"",@progbits
	.section	.note.gnu.property,"a"
	.align 8
	.long	1f - 0f
	.long	4f - 1f
	.long	5
0:
	.string	"GNU"
1:
	.align 8
	.long	0xc0000002
	.long	3f - 2f
2:
	.long	0x3
3:
	.align 8
4:
