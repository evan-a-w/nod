	.file	"par_ops.c"
	.text
	.globl	id
	.section	.rodata
.LC0:
	.string	"$Id$\n"
	.section	.data.rel.local,"aw"
	.align 8
	.type	id, @object
	.size	id, 8
id:
	.quad	.LC0
	.text
	.globl	max_parallelism
	.type	max_parallelism, @function
max_parallelism:
.LFB8:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -56(%rbp)
	movl	%esi, -60(%rbp)
	movl	%edx, -64(%rbp)
	movq	%rcx, -72(%rbp)
	movsd	.LC1(%rip), %xmm0
	movsd	%xmm0, -32(%rbp)
	movl	$0, -44(%rbp)
	jmp	.L2
.L16:
	movl	-44(%rbp), %eax
	cltq
	leaq	0(,%rax,8), %rdx
	movq	-56(%rbp), %rax
	addq	%rdx, %rax
	movq	(%rax), %rax
	movl	-60(%rbp), %ecx
	pushq	-72(%rbp)
	movl	-64(%rbp), %edx
	pushq	%rdx
	movl	%ecx, %r9d
	movl	$1, %r8d
	movl	$0, %ecx
	movl	$0, %edx
	movq	%rax, %rsi
	leaq	initialize(%rip), %rax
	movq	%rax, %rdi
	call	benchmp@PLT
	addq	$16, %rsp
	movl	$0, %eax
	call	save_minimum@PLT
	call	usecs_spent@PLT
	testq	%rax, %rax
	jne	.L3
	movsd	.LC2(%rip), %xmm0
	jmp	.L4
.L3:
	cmpl	$0, -44(%rbp)
	jne	.L5
	call	usecs_spent@PLT
	testq	%rax, %rax
	js	.L6
	pxor	%xmm2, %xmm2
	cvtsi2sdq	%rax, %xmm2
	movsd	%xmm2, -80(%rbp)
	jmp	.L7
.L6:
	movq	%rax, %rdx
	shrq	%rdx
	andl	$1, %eax
	orq	%rax, %rdx
	pxor	%xmm0, %xmm0
	cvtsi2sdq	%rdx, %xmm0
	addsd	%xmm0, %xmm0
	movsd	%xmm0, -80(%rbp)
.L7:
	call	get_n@PLT
	testq	%rax, %rax
	js	.L8
	pxor	%xmm0, %xmm0
	cvtsi2sdq	%rax, %xmm0
	jmp	.L9
.L8:
	movq	%rax, %rdx
	shrq	%rdx
	andl	$1, %eax
	orq	%rax, %rdx
	pxor	%xmm0, %xmm0
	cvtsi2sdq	%rdx, %xmm0
	addsd	%xmm0, %xmm0
.L9:
	movsd	-80(%rbp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, -40(%rbp)
	jmp	.L10
.L5:
	movsd	-40(%rbp), %xmm0
	movsd	%xmm0, -24(%rbp)
	call	usecs_spent@PLT
	testq	%rax, %rax
	js	.L11
	pxor	%xmm0, %xmm0
	cvtsi2sdq	%rax, %xmm0
	jmp	.L12
.L11:
	movq	%rax, %rdx
	shrq	%rdx
	andl	$1, %eax
	orq	%rax, %rdx
	pxor	%xmm0, %xmm0
	cvtsi2sdq	%rdx, %xmm0
	addsd	%xmm0, %xmm0
.L12:
	movsd	-24(%rbp), %xmm1
	divsd	%xmm0, %xmm1
	movsd	%xmm1, -24(%rbp)
	movl	-44(%rbp), %eax
	addl	$1, %eax
	movslq	%eax, %rbx
	call	get_n@PLT
	imulq	%rbx, %rax
	testq	%rax, %rax
	js	.L13
	pxor	%xmm0, %xmm0
	cvtsi2sdq	%rax, %xmm0
	jmp	.L14
.L13:
	movq	%rax, %rdx
	shrq	%rdx
	andl	$1, %eax
	orq	%rax, %rdx
	pxor	%xmm0, %xmm0
	cvtsi2sdq	%rdx, %xmm0
	addsd	%xmm0, %xmm0
.L14:
	movsd	-24(%rbp), %xmm1
	mulsd	%xmm1, %xmm0
	movsd	%xmm0, -24(%rbp)
	movsd	-24(%rbp), %xmm0
	comisd	-32(%rbp), %xmm0
	jbe	.L10
	movsd	-24(%rbp), %xmm0
	movsd	%xmm0, -32(%rbp)
.L10:
	addl	$1, -44(%rbp)
.L2:
	cmpl	$15, -44(%rbp)
	jle	.L16
	movsd	-32(%rbp), %xmm0
.L4:
	movq	%xmm0, %rax
	movq	%rax, %xmm0
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE8:
	.size	max_parallelism, .-max_parallelism
	.globl	integer_bit_0
	.type	integer_bit_0, @function
integer_bit_0:
.LFB9:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 13, -24
	.cfi_offset 12, -32
	.cfi_offset 3, -40
	movq	%rdi, -56(%rbp)
	movq	%rsi, -64(%rbp)
	movq	-56(%rbp), %r13
	movq	-64(%rbp), %rax
	movq	%rax, -40(%rbp)
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	leal	1(%rax), %ebx
	leal	1(%rbx), %r12d
	jmp	.L19
.L20:
	xorl	%r12d, %ebx
	xorl	%ebx, %r12d
	orl	%r12d, %ebx
	xorl	%r12d, %ebx
	xorl	%ebx, %r12d
	orl	%r12d, %ebx
	xorl	%r12d, %ebx
	xorl	%ebx, %r12d
	orl	%r12d, %ebx
	xorl	%r12d, %ebx
	xorl	%ebx, %r12d
	orl	%r12d, %ebx
	xorl	%r12d, %ebx
	xorl	%ebx, %r12d
	orl	%r12d, %ebx
	xorl	%r12d, %ebx
	xorl	%ebx, %r12d
	orl	%r12d, %ebx
	xorl	%r12d, %ebx
	xorl	%ebx, %r12d
	orl	%r12d, %ebx
	xorl	%r12d, %ebx
	xorl	%ebx, %r12d
	orl	%r12d, %ebx
	xorl	%r12d, %ebx
	xorl	%ebx, %r12d
	orl	%r12d, %ebx
	xorl	%r12d, %ebx
	xorl	%ebx, %r12d
	orl	%r12d, %ebx
.L19:
	movq	%r13, %rax
	leaq	-1(%rax), %r13
	testq	%rax, %rax
	jne	.L20
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE9:
	.size	integer_bit_0, .-integer_bit_0
	.globl	integer_bit_1
	.type	integer_bit_1, @function
integer_bit_1:
.LFB10:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %r15
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	1(%rax), %r12d
	leal	1(%r12), %r13d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	1(%rax), %ebx
	leal	2(%rbx), %r14d
	jmp	.L22
.L23:
	xorl	%r13d, %r12d
	xorl	%r12d, %r13d
	orl	%r13d, %r12d
	xorl	%r14d, %ebx
	xorl	%ebx, %r14d
	orl	%r14d, %ebx
	xorl	%r13d, %r12d
	xorl	%r12d, %r13d
	orl	%r13d, %r12d
	xorl	%r14d, %ebx
	xorl	%ebx, %r14d
	orl	%r14d, %ebx
	xorl	%r13d, %r12d
	xorl	%r12d, %r13d
	orl	%r13d, %r12d
	xorl	%r14d, %ebx
	xorl	%ebx, %r14d
	orl	%r14d, %ebx
	xorl	%r13d, %r12d
	xorl	%r12d, %r13d
	orl	%r13d, %r12d
	xorl	%r14d, %ebx
	xorl	%ebx, %r14d
	orl	%r14d, %ebx
	xorl	%r13d, %r12d
	xorl	%r12d, %r13d
	orl	%r13d, %r12d
	xorl	%r14d, %ebx
	xorl	%ebx, %r14d
	orl	%r14d, %ebx
	xorl	%r13d, %r12d
	xorl	%r12d, %r13d
	orl	%r13d, %r12d
	xorl	%r14d, %ebx
	xorl	%ebx, %r14d
	orl	%r14d, %ebx
	xorl	%r13d, %r12d
	xorl	%r12d, %r13d
	orl	%r13d, %r12d
	xorl	%r14d, %ebx
	xorl	%ebx, %r14d
	orl	%r14d, %ebx
	xorl	%r13d, %r12d
	xorl	%r12d, %r13d
	orl	%r13d, %r12d
	xorl	%r14d, %ebx
	xorl	%ebx, %r14d
	orl	%r14d, %ebx
	xorl	%r13d, %r12d
	xorl	%r12d, %r13d
	orl	%r13d, %r12d
	xorl	%r14d, %ebx
	xorl	%ebx, %r14d
	orl	%r14d, %ebx
	xorl	%r13d, %r12d
	xorl	%r12d, %r13d
	orl	%r13d, %r12d
	xorl	%r14d, %ebx
	xorl	%ebx, %r14d
	orl	%r14d, %ebx
.L22:
	movq	%r15, %rax
	leaq	-1(%rax), %r15
	testq	%rax, %rax
	jne	.L23
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE10:
	.size	integer_bit_1, .-integer_bit_1
	.globl	integer_bit_2
	.type	integer_bit_2, @function
integer_bit_2:
.LFB11:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %rcx
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	1(%rax), %r13d
	leal	1(%r13), %r14d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	1(%rax), %r12d
	leal	2(%r12), %r15d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	1(%rax), %ebx
	leal	3(%rbx), %eax
	movl	%eax, %esi
	jmp	.L25
.L26:
	xorl	%r14d, %r13d
	xorl	%r13d, %r14d
	orl	%r14d, %r13d
	xorl	%r15d, %r12d
	xorl	%r12d, %r15d
	orl	%r15d, %r12d
	movl	%esi, %eax
	xorl	%eax, %ebx
	xorl	%ebx, %eax
	orl	%eax, %ebx
	xorl	%r14d, %r13d
	xorl	%r13d, %r14d
	orl	%r14d, %r13d
	xorl	%r15d, %r12d
	xorl	%r12d, %r15d
	orl	%r15d, %r12d
	xorl	%eax, %ebx
	xorl	%ebx, %eax
	orl	%eax, %ebx
	xorl	%r14d, %r13d
	xorl	%r13d, %r14d
	orl	%r14d, %r13d
	xorl	%r15d, %r12d
	xorl	%r12d, %r15d
	orl	%r15d, %r12d
	xorl	%eax, %ebx
	xorl	%ebx, %eax
	orl	%eax, %ebx
	xorl	%r14d, %r13d
	xorl	%r13d, %r14d
	orl	%r14d, %r13d
	xorl	%r15d, %r12d
	xorl	%r12d, %r15d
	orl	%r15d, %r12d
	xorl	%eax, %ebx
	xorl	%ebx, %eax
	orl	%eax, %ebx
	xorl	%r14d, %r13d
	xorl	%r13d, %r14d
	orl	%r14d, %r13d
	xorl	%r15d, %r12d
	xorl	%r12d, %r15d
	orl	%r15d, %r12d
	xorl	%eax, %ebx
	xorl	%ebx, %eax
	orl	%eax, %ebx
	xorl	%r14d, %r13d
	xorl	%r13d, %r14d
	orl	%r14d, %r13d
	xorl	%r15d, %r12d
	xorl	%r12d, %r15d
	orl	%r15d, %r12d
	xorl	%eax, %ebx
	xorl	%ebx, %eax
	orl	%eax, %ebx
	xorl	%r14d, %r13d
	xorl	%r13d, %r14d
	orl	%r14d, %r13d
	xorl	%r15d, %r12d
	xorl	%r12d, %r15d
	orl	%r15d, %r12d
	xorl	%eax, %ebx
	xorl	%ebx, %eax
	orl	%eax, %ebx
	xorl	%r14d, %r13d
	xorl	%r13d, %r14d
	orl	%r14d, %r13d
	xorl	%r15d, %r12d
	xorl	%r12d, %r15d
	orl	%r15d, %r12d
	xorl	%eax, %ebx
	xorl	%ebx, %eax
	orl	%eax, %ebx
	xorl	%r14d, %r13d
	xorl	%r13d, %r14d
	orl	%r14d, %r13d
	xorl	%r15d, %r12d
	xorl	%r12d, %r15d
	orl	%r15d, %r12d
	xorl	%eax, %ebx
	xorl	%ebx, %eax
	orl	%eax, %ebx
	xorl	%r14d, %r13d
	xorl	%r13d, %r14d
	orl	%r14d, %r13d
	xorl	%r15d, %r12d
	xorl	%r12d, %r15d
	orl	%r15d, %r12d
	xorl	%eax, %ebx
	xorl	%ebx, %eax
	movl	%eax, %esi
	orl	%eax, %ebx
.L25:
	movq	%rcx, %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, %rcx
	testq	%rax, %rax
	jne	.L26
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE11:
	.size	integer_bit_2, .-integer_bit_2
	.globl	integer_bit_3
	.type	integer_bit_3, @function
integer_bit_3:
.LFB12:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %rsi
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	1(%rax), %r14d
	leal	1(%r14), %r15d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	1(%rax), %r13d
	leal	2(%r13), %eax
	movl	%eax, %edi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	1(%rax), %r12d
	leal	3(%r12), %eax
	movl	%eax, %edx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	1(%rax), %ebx
	leal	4(%rbx), %eax
	movl	%eax, %ecx
	jmp	.L28
.L29:
	xorl	%r15d, %r14d
	xorl	%r14d, %r15d
	orl	%r15d, %r14d
	movl	%edi, %eax
	xorl	%eax, %r13d
	xorl	%r13d, %eax
	orl	%eax, %r13d
	xorl	%edx, %r12d
	xorl	%r12d, %edx
	orl	%edx, %r12d
	xorl	%ecx, %ebx
	xorl	%ebx, %ecx
	orl	%ecx, %ebx
	xorl	%r15d, %r14d
	xorl	%r14d, %r15d
	orl	%r15d, %r14d
	xorl	%eax, %r13d
	xorl	%r13d, %eax
	orl	%eax, %r13d
	xorl	%edx, %r12d
	xorl	%r12d, %edx
	orl	%edx, %r12d
	xorl	%ecx, %ebx
	xorl	%ebx, %ecx
	orl	%ecx, %ebx
	xorl	%r15d, %r14d
	xorl	%r14d, %r15d
	orl	%r15d, %r14d
	xorl	%eax, %r13d
	xorl	%r13d, %eax
	orl	%eax, %r13d
	xorl	%edx, %r12d
	xorl	%r12d, %edx
	orl	%edx, %r12d
	xorl	%ecx, %ebx
	xorl	%ebx, %ecx
	orl	%ecx, %ebx
	xorl	%r15d, %r14d
	xorl	%r14d, %r15d
	orl	%r15d, %r14d
	xorl	%eax, %r13d
	xorl	%r13d, %eax
	orl	%eax, %r13d
	xorl	%edx, %r12d
	xorl	%r12d, %edx
	orl	%edx, %r12d
	xorl	%ecx, %ebx
	xorl	%ebx, %ecx
	orl	%ecx, %ebx
	xorl	%r15d, %r14d
	xorl	%r14d, %r15d
	orl	%r15d, %r14d
	xorl	%eax, %r13d
	xorl	%r13d, %eax
	orl	%eax, %r13d
	xorl	%edx, %r12d
	xorl	%r12d, %edx
	orl	%edx, %r12d
	xorl	%ecx, %ebx
	xorl	%ebx, %ecx
	orl	%ecx, %ebx
	xorl	%r15d, %r14d
	xorl	%r14d, %r15d
	orl	%r15d, %r14d
	xorl	%eax, %r13d
	xorl	%r13d, %eax
	orl	%eax, %r13d
	xorl	%edx, %r12d
	xorl	%r12d, %edx
	orl	%edx, %r12d
	xorl	%ecx, %ebx
	xorl	%ebx, %ecx
	orl	%ecx, %ebx
	xorl	%r15d, %r14d
	xorl	%r14d, %r15d
	orl	%r15d, %r14d
	xorl	%eax, %r13d
	xorl	%r13d, %eax
	orl	%eax, %r13d
	xorl	%edx, %r12d
	xorl	%r12d, %edx
	orl	%edx, %r12d
	xorl	%ecx, %ebx
	xorl	%ebx, %ecx
	orl	%ecx, %ebx
	xorl	%r15d, %r14d
	xorl	%r14d, %r15d
	orl	%r15d, %r14d
	xorl	%eax, %r13d
	xorl	%r13d, %eax
	orl	%eax, %r13d
	xorl	%edx, %r12d
	xorl	%r12d, %edx
	orl	%edx, %r12d
	xorl	%ecx, %ebx
	xorl	%ebx, %ecx
	orl	%ecx, %ebx
	xorl	%r15d, %r14d
	xorl	%r14d, %r15d
	orl	%r15d, %r14d
	xorl	%eax, %r13d
	xorl	%r13d, %eax
	orl	%eax, %r13d
	xorl	%edx, %r12d
	xorl	%r12d, %edx
	orl	%edx, %r12d
	xorl	%ecx, %ebx
	xorl	%ebx, %ecx
	orl	%ecx, %ebx
	xorl	%r15d, %r14d
	xorl	%r14d, %r15d
	orl	%r15d, %r14d
	xorl	%eax, %r13d
	xorl	%r13d, %eax
	movl	%eax, %edi
	orl	%eax, %r13d
	xorl	%edx, %r12d
	xorl	%r12d, %edx
	orl	%edx, %r12d
	xorl	%ecx, %ebx
	xorl	%ebx, %ecx
	orl	%ecx, %ebx
.L28:
	movq	%rsi, %rax
	leaq	-1(%rax), %rsi
	testq	%rax, %rax
	jne	.L29
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE12:
	.size	integer_bit_3, .-integer_bit_3
	.globl	integer_bit_4
	.type	integer_bit_4, @function
integer_bit_4:
.LFB13:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r8
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	1(%rax), %r15d
	leal	1(%r15), %eax
	movl	%eax, %r9d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	1(%rax), %r14d
	leal	2(%r14), %eax
	movl	%eax, %r10d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	1(%rax), %r13d
	leal	3(%r13), %eax
	movl	%eax, %ecx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	1(%rax), %r12d
	leal	4(%r12), %eax
	movl	%eax, %esi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	1(%rax), %ebx
	leal	5(%rbx), %eax
	movl	%eax, %edi
	jmp	.L31
.L32:
	movl	%r9d, %eax
	xorl	%eax, %r15d
	xorl	%r15d, %eax
	orl	%eax, %r15d
	movl	%r10d, %edx
	xorl	%edx, %r14d
	xorl	%r14d, %edx
	orl	%edx, %r14d
	xorl	%ecx, %r13d
	xorl	%r13d, %ecx
	orl	%ecx, %r13d
	xorl	%esi, %r12d
	xorl	%r12d, %esi
	orl	%esi, %r12d
	xorl	%edi, %ebx
	xorl	%ebx, %edi
	orl	%edi, %ebx
	xorl	%eax, %r15d
	xorl	%r15d, %eax
	orl	%eax, %r15d
	xorl	%edx, %r14d
	xorl	%r14d, %edx
	orl	%edx, %r14d
	xorl	%ecx, %r13d
	xorl	%r13d, %ecx
	orl	%ecx, %r13d
	xorl	%esi, %r12d
	xorl	%r12d, %esi
	orl	%esi, %r12d
	xorl	%edi, %ebx
	xorl	%ebx, %edi
	orl	%edi, %ebx
	xorl	%eax, %r15d
	xorl	%r15d, %eax
	orl	%eax, %r15d
	xorl	%edx, %r14d
	xorl	%r14d, %edx
	orl	%edx, %r14d
	xorl	%ecx, %r13d
	xorl	%r13d, %ecx
	orl	%ecx, %r13d
	xorl	%esi, %r12d
	xorl	%r12d, %esi
	orl	%esi, %r12d
	xorl	%edi, %ebx
	xorl	%ebx, %edi
	orl	%edi, %ebx
	xorl	%eax, %r15d
	xorl	%r15d, %eax
	orl	%eax, %r15d
	xorl	%edx, %r14d
	xorl	%r14d, %edx
	orl	%edx, %r14d
	xorl	%ecx, %r13d
	xorl	%r13d, %ecx
	orl	%ecx, %r13d
	xorl	%esi, %r12d
	xorl	%r12d, %esi
	orl	%esi, %r12d
	xorl	%edi, %ebx
	xorl	%ebx, %edi
	orl	%edi, %ebx
	xorl	%eax, %r15d
	xorl	%r15d, %eax
	orl	%eax, %r15d
	xorl	%edx, %r14d
	xorl	%r14d, %edx
	orl	%edx, %r14d
	xorl	%ecx, %r13d
	xorl	%r13d, %ecx
	orl	%ecx, %r13d
	xorl	%esi, %r12d
	xorl	%r12d, %esi
	orl	%esi, %r12d
	xorl	%edi, %ebx
	xorl	%ebx, %edi
	orl	%edi, %ebx
	xorl	%eax, %r15d
	xorl	%r15d, %eax
	orl	%eax, %r15d
	xorl	%edx, %r14d
	xorl	%r14d, %edx
	orl	%edx, %r14d
	xorl	%ecx, %r13d
	xorl	%r13d, %ecx
	orl	%ecx, %r13d
	xorl	%esi, %r12d
	xorl	%r12d, %esi
	orl	%esi, %r12d
	xorl	%edi, %ebx
	xorl	%ebx, %edi
	orl	%edi, %ebx
	xorl	%eax, %r15d
	xorl	%r15d, %eax
	orl	%eax, %r15d
	xorl	%edx, %r14d
	xorl	%r14d, %edx
	orl	%edx, %r14d
	xorl	%ecx, %r13d
	xorl	%r13d, %ecx
	orl	%ecx, %r13d
	xorl	%esi, %r12d
	xorl	%r12d, %esi
	orl	%esi, %r12d
	xorl	%edi, %ebx
	xorl	%ebx, %edi
	orl	%edi, %ebx
	xorl	%eax, %r15d
	xorl	%r15d, %eax
	orl	%eax, %r15d
	xorl	%edx, %r14d
	xorl	%r14d, %edx
	orl	%edx, %r14d
	xorl	%ecx, %r13d
	xorl	%r13d, %ecx
	orl	%ecx, %r13d
	xorl	%esi, %r12d
	xorl	%r12d, %esi
	orl	%esi, %r12d
	xorl	%edi, %ebx
	xorl	%ebx, %edi
	orl	%edi, %ebx
	xorl	%eax, %r15d
	xorl	%r15d, %eax
	orl	%eax, %r15d
	xorl	%edx, %r14d
	xorl	%r14d, %edx
	orl	%edx, %r14d
	xorl	%ecx, %r13d
	xorl	%r13d, %ecx
	orl	%ecx, %r13d
	xorl	%esi, %r12d
	xorl	%r12d, %esi
	orl	%esi, %r12d
	xorl	%edi, %ebx
	xorl	%ebx, %edi
	orl	%edi, %ebx
	xorl	%eax, %r15d
	xorl	%r15d, %eax
	movl	%eax, %r9d
	orl	%eax, %r15d
	xorl	%edx, %r14d
	xorl	%r14d, %edx
	movl	%edx, %r10d
	orl	%edx, %r14d
	xorl	%ecx, %r13d
	xorl	%r13d, %ecx
	orl	%ecx, %r13d
	xorl	%esi, %r12d
	xorl	%r12d, %esi
	orl	%esi, %r12d
	xorl	%edi, %ebx
	xorl	%ebx, %edi
	orl	%edi, %ebx
.L31:
	movq	%r8, %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, %r8
	testq	%rax, %rax
	jne	.L32
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE13:
	.size	integer_bit_4, .-integer_bit_4
	.globl	integer_bit_5
	.type	integer_bit_5, @function
integer_bit_5:
.LFB14:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r10
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movl	%eax, %edx
	addl	$1, %eax
	movl	%eax, %r11d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	1(%rax), %r15d
	leal	2(%r15), %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	1(%rax), %r14d
	leal	3(%r14), %eax
	movl	%eax, %esi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	1(%rax), %r13d
	leal	4(%r13), %eax
	movl	%eax, %edi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	1(%rax), %r12d
	leal	5(%r12), %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	1(%rax), %ebx
	leal	6(%rbx), %eax
	movl	%eax, %r9d
	jmp	.L34
.L35:
	movl	%edx, %eax
	movl	%r11d, %edx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	-84(%rbp), %ecx
	xorl	%ecx, %r15d
	xorl	%r15d, %ecx
	orl	%ecx, %r15d
	xorl	%esi, %r14d
	xorl	%r14d, %esi
	orl	%esi, %r14d
	xorl	%edi, %r13d
	xorl	%r13d, %edi
	orl	%edi, %r13d
	xorl	%r8d, %r12d
	xorl	%r12d, %r8d
	orl	%r8d, %r12d
	xorl	%r9d, %ebx
	xorl	%ebx, %r9d
	orl	%r9d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%ecx, %r15d
	xorl	%r15d, %ecx
	orl	%ecx, %r15d
	xorl	%esi, %r14d
	xorl	%r14d, %esi
	orl	%esi, %r14d
	xorl	%edi, %r13d
	xorl	%r13d, %edi
	orl	%edi, %r13d
	xorl	%r8d, %r12d
	xorl	%r12d, %r8d
	orl	%r8d, %r12d
	xorl	%r9d, %ebx
	xorl	%ebx, %r9d
	orl	%r9d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%ecx, %r15d
	xorl	%r15d, %ecx
	orl	%ecx, %r15d
	xorl	%esi, %r14d
	xorl	%r14d, %esi
	orl	%esi, %r14d
	xorl	%edi, %r13d
	xorl	%r13d, %edi
	orl	%edi, %r13d
	xorl	%r8d, %r12d
	xorl	%r12d, %r8d
	orl	%r8d, %r12d
	xorl	%r9d, %ebx
	xorl	%ebx, %r9d
	orl	%r9d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%ecx, %r15d
	xorl	%r15d, %ecx
	orl	%ecx, %r15d
	xorl	%esi, %r14d
	xorl	%r14d, %esi
	orl	%esi, %r14d
	xorl	%edi, %r13d
	xorl	%r13d, %edi
	orl	%edi, %r13d
	xorl	%r8d, %r12d
	xorl	%r12d, %r8d
	orl	%r8d, %r12d
	xorl	%r9d, %ebx
	xorl	%ebx, %r9d
	orl	%r9d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%ecx, %r15d
	xorl	%r15d, %ecx
	orl	%ecx, %r15d
	xorl	%esi, %r14d
	xorl	%r14d, %esi
	orl	%esi, %r14d
	xorl	%edi, %r13d
	xorl	%r13d, %edi
	orl	%edi, %r13d
	xorl	%r8d, %r12d
	xorl	%r12d, %r8d
	orl	%r8d, %r12d
	xorl	%r9d, %ebx
	xorl	%ebx, %r9d
	orl	%r9d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%ecx, %r15d
	xorl	%r15d, %ecx
	orl	%ecx, %r15d
	xorl	%esi, %r14d
	xorl	%r14d, %esi
	orl	%esi, %r14d
	xorl	%edi, %r13d
	xorl	%r13d, %edi
	orl	%edi, %r13d
	xorl	%r8d, %r12d
	xorl	%r12d, %r8d
	orl	%r8d, %r12d
	xorl	%r9d, %ebx
	xorl	%ebx, %r9d
	orl	%r9d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%ecx, %r15d
	xorl	%r15d, %ecx
	orl	%ecx, %r15d
	xorl	%esi, %r14d
	xorl	%r14d, %esi
	orl	%esi, %r14d
	xorl	%edi, %r13d
	xorl	%r13d, %edi
	orl	%edi, %r13d
	xorl	%r8d, %r12d
	xorl	%r12d, %r8d
	orl	%r8d, %r12d
	xorl	%r9d, %ebx
	xorl	%ebx, %r9d
	orl	%r9d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%ecx, %r15d
	xorl	%r15d, %ecx
	orl	%ecx, %r15d
	xorl	%esi, %r14d
	xorl	%r14d, %esi
	orl	%esi, %r14d
	xorl	%edi, %r13d
	xorl	%r13d, %edi
	orl	%edi, %r13d
	xorl	%r8d, %r12d
	xorl	%r12d, %r8d
	orl	%r8d, %r12d
	xorl	%r9d, %ebx
	xorl	%ebx, %r9d
	orl	%r9d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%ecx, %r15d
	xorl	%r15d, %ecx
	orl	%ecx, %r15d
	xorl	%esi, %r14d
	xorl	%r14d, %esi
	orl	%esi, %r14d
	xorl	%edi, %r13d
	xorl	%r13d, %edi
	orl	%edi, %r13d
	xorl	%r8d, %r12d
	xorl	%r12d, %r8d
	orl	%r8d, %r12d
	xorl	%r9d, %ebx
	xorl	%ebx, %r9d
	orl	%r9d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	movl	%edx, %r11d
	orl	%edx, %eax
	movl	%eax, %edx
	xorl	%ecx, %r15d
	xorl	%r15d, %ecx
	movl	%ecx, -84(%rbp)
	orl	%ecx, %r15d
	xorl	%esi, %r14d
	xorl	%r14d, %esi
	orl	%esi, %r14d
	xorl	%edi, %r13d
	xorl	%r13d, %edi
	orl	%edi, %r13d
	xorl	%r8d, %r12d
	xorl	%r12d, %r8d
	orl	%r8d, %r12d
	xorl	%r9d, %ebx
	xorl	%ebx, %r9d
	orl	%r9d, %ebx
.L34:
	movq	%r10, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %r10
	testq	%rax, %rax
	jne	.L35
	movl	%edx, %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE14:
	.size	integer_bit_5, .-integer_bit_5
	.globl	integer_bit_6
	.type	integer_bit_6, @function
integer_bit_6:
.LFB15:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -96(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movl	%eax, %edx
	addl	$1, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	addl	$2, %eax
	movl	%eax, %esi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	1(%rax), %r15d
	leal	3(%r15), %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	1(%rax), %r14d
	leal	4(%r14), %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	1(%rax), %r13d
	leal	5(%r13), %eax
	movl	%eax, %r9d
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	1(%rax), %r12d
	leal	6(%r12), %eax
	movl	%eax, %r10d
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	1(%rax), %ebx
	leal	7(%rbx), %eax
	movl	%eax, %r11d
	jmp	.L37
.L38:
	movl	%edx, %eax
	movl	-88(%rbp), %edx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	-100(%rbp), %edi
	xorl	%edi, %r15d
	xorl	%r15d, %edi
	orl	%edi, %r15d
	xorl	%r8d, %r14d
	xorl	%r14d, %r8d
	orl	%r8d, %r14d
	xorl	%r9d, %r13d
	xorl	%r13d, %r9d
	orl	%r9d, %r13d
	xorl	%r10d, %r12d
	xorl	%r12d, %r10d
	orl	%r10d, %r12d
	xorl	%r11d, %ebx
	xorl	%ebx, %r11d
	orl	%r11d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%edi, %r15d
	xorl	%r15d, %edi
	orl	%edi, %r15d
	xorl	%r8d, %r14d
	xorl	%r14d, %r8d
	orl	%r8d, %r14d
	xorl	%r9d, %r13d
	xorl	%r13d, %r9d
	orl	%r9d, %r13d
	xorl	%r10d, %r12d
	xorl	%r12d, %r10d
	orl	%r10d, %r12d
	xorl	%r11d, %ebx
	xorl	%ebx, %r11d
	orl	%r11d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%edi, %r15d
	xorl	%r15d, %edi
	orl	%edi, %r15d
	xorl	%r8d, %r14d
	xorl	%r14d, %r8d
	orl	%r8d, %r14d
	xorl	%r9d, %r13d
	xorl	%r13d, %r9d
	orl	%r9d, %r13d
	xorl	%r10d, %r12d
	xorl	%r12d, %r10d
	orl	%r10d, %r12d
	xorl	%r11d, %ebx
	xorl	%ebx, %r11d
	orl	%r11d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%edi, %r15d
	xorl	%r15d, %edi
	orl	%edi, %r15d
	xorl	%r8d, %r14d
	xorl	%r14d, %r8d
	orl	%r8d, %r14d
	xorl	%r9d, %r13d
	xorl	%r13d, %r9d
	orl	%r9d, %r13d
	xorl	%r10d, %r12d
	xorl	%r12d, %r10d
	orl	%r10d, %r12d
	xorl	%r11d, %ebx
	xorl	%ebx, %r11d
	orl	%r11d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%edi, %r15d
	xorl	%r15d, %edi
	orl	%edi, %r15d
	xorl	%r8d, %r14d
	xorl	%r14d, %r8d
	orl	%r8d, %r14d
	xorl	%r9d, %r13d
	xorl	%r13d, %r9d
	orl	%r9d, %r13d
	xorl	%r10d, %r12d
	xorl	%r12d, %r10d
	orl	%r10d, %r12d
	xorl	%r11d, %ebx
	xorl	%ebx, %r11d
	orl	%r11d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%edi, %r15d
	xorl	%r15d, %edi
	orl	%edi, %r15d
	xorl	%r8d, %r14d
	xorl	%r14d, %r8d
	orl	%r8d, %r14d
	xorl	%r9d, %r13d
	xorl	%r13d, %r9d
	orl	%r9d, %r13d
	xorl	%r10d, %r12d
	xorl	%r12d, %r10d
	orl	%r10d, %r12d
	xorl	%r11d, %ebx
	xorl	%ebx, %r11d
	orl	%r11d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%edi, %r15d
	xorl	%r15d, %edi
	orl	%edi, %r15d
	xorl	%r8d, %r14d
	xorl	%r14d, %r8d
	orl	%r8d, %r14d
	xorl	%r9d, %r13d
	xorl	%r13d, %r9d
	orl	%r9d, %r13d
	xorl	%r10d, %r12d
	xorl	%r12d, %r10d
	orl	%r10d, %r12d
	xorl	%r11d, %ebx
	xorl	%ebx, %r11d
	orl	%r11d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%edi, %r15d
	xorl	%r15d, %edi
	orl	%edi, %r15d
	xorl	%r8d, %r14d
	xorl	%r14d, %r8d
	orl	%r8d, %r14d
	xorl	%r9d, %r13d
	xorl	%r13d, %r9d
	orl	%r9d, %r13d
	xorl	%r10d, %r12d
	xorl	%r12d, %r10d
	orl	%r10d, %r12d
	xorl	%r11d, %ebx
	xorl	%ebx, %r11d
	orl	%r11d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%edi, %r15d
	xorl	%r15d, %edi
	orl	%edi, %r15d
	xorl	%r8d, %r14d
	xorl	%r14d, %r8d
	orl	%r8d, %r14d
	xorl	%r9d, %r13d
	xorl	%r13d, %r9d
	orl	%r9d, %r13d
	xorl	%r10d, %r12d
	xorl	%r12d, %r10d
	orl	%r10d, %r12d
	xorl	%r11d, %ebx
	xorl	%ebx, %r11d
	orl	%r11d, %ebx
	xorl	%edx, %eax
	xorl	%eax, %edx
	movl	%edx, -88(%rbp)
	orl	%edx, %eax
	movl	%eax, %edx
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -84(%rbp)
	xorl	%edi, %r15d
	xorl	%r15d, %edi
	movl	%edi, -100(%rbp)
	orl	%edi, %r15d
	xorl	%r8d, %r14d
	xorl	%r14d, %r8d
	orl	%r8d, %r14d
	xorl	%r9d, %r13d
	xorl	%r13d, %r9d
	orl	%r9d, %r13d
	xorl	%r10d, %r12d
	xorl	%r12d, %r10d
	orl	%r10d, %r12d
	xorl	%r11d, %ebx
	xorl	%ebx, %r11d
	orl	%r11d, %ebx
.L37:
	movq	-96(%rbp), %rax
	leaq	-1(%rax), %rdi
	movq	%rdi, -96(%rbp)
	testq	%rax, %rax
	jne	.L38
	movl	%edx, %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE15:
	.size	integer_bit_6, .-integer_bit_6
	.globl	integer_bit_7
	.type	integer_bit_7, @function
integer_bit_7:
.LFB16:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -120(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movl	%eax, -88(%rbp)
	addl	$1, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$1, %eax
	movl	%eax, -96(%rbp)
	addl	$2, %eax
	movl	%eax, %esi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$1, %eax
	movl	%eax, -100(%rbp)
	addl	$3, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$1, %eax
	movl	%eax, -104(%rbp)
	addl	$4, %eax
	movl	%eax, %r10d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movl	%eax, -108(%rbp)
	addl	$5, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	leal	6(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	1(%rax), %r15d
	leal	7(%r15), %r12d
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	1(%rax), %r13d
	leal	8(%r13), %ebx
	jmp	.L40
.L41:
	movl	-112(%rbp), %edx
	xorl	%edx, -88(%rbp)
	movl	-88(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -88(%rbp)
	xorl	%esi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, -104(%rbp)
	movl	-104(%rbp), %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-92(%rbp), %eax
	xorl	%eax, -108(%rbp)
	movl	-108(%rbp), %r11d
	xorl	%r11d, %eax
	movl	%eax, -92(%rbp)
	orl	%eax, %r11d
	xorl	%r14d, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %r14d
	orl	%r14d, %eax
	movl	%eax, -84(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -88(%rbp)
	movl	-88(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -88(%rbp)
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-92(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -92(%rbp)
	orl	%eax, %r11d
	xorl	%r14d, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %r14d
	orl	%r14d, %eax
	movl	%eax, -84(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -88(%rbp)
	movl	-88(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -88(%rbp)
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-92(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -92(%rbp)
	orl	%eax, %r11d
	xorl	%r14d, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %r14d
	orl	%r14d, %eax
	movl	%eax, -84(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -88(%rbp)
	movl	-88(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -88(%rbp)
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-92(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -92(%rbp)
	orl	%eax, %r11d
	xorl	%r14d, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %r14d
	orl	%r14d, %eax
	movl	%eax, -84(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -88(%rbp)
	movl	-88(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -88(%rbp)
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-92(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -92(%rbp)
	orl	%eax, %r11d
	xorl	%r14d, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %r14d
	orl	%r14d, %eax
	movl	%eax, -84(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -88(%rbp)
	movl	-88(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -88(%rbp)
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-92(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -92(%rbp)
	orl	%eax, %r11d
	xorl	%r14d, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %r14d
	orl	%r14d, %eax
	movl	%eax, -84(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -88(%rbp)
	movl	-88(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -88(%rbp)
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-92(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -92(%rbp)
	orl	%eax, %r11d
	xorl	%r14d, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %r14d
	orl	%r14d, %eax
	movl	%eax, -84(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -88(%rbp)
	movl	-88(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -88(%rbp)
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-92(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -92(%rbp)
	orl	%eax, %r11d
	xorl	%r14d, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %r14d
	orl	%r14d, %eax
	movl	%eax, -84(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -88(%rbp)
	movl	-88(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-92(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -92(%rbp)
	orl	%ecx, %r11d
	xorl	%r14d, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -84(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, %eax
	xorl	%eax, %edx
	movl	%edx, -112(%rbp)
	orl	%edx, %eax
	movl	%eax, -88(%rbp)
	xorl	%esi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -100(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	%r9d, -104(%rbp)
	movl	-92(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -92(%rbp)
	orl	%eax, %r11d
	movl	%r11d, -108(%rbp)
	xorl	%r14d, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -84(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
.L40:
	movq	-120(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -120(%rbp)
	testq	%rax, %rax
	jne	.L41
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-104(%rbp), %edi
	call	use_int@PLT
	movl	-108(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE16:
	.size	integer_bit_7, .-integer_bit_7
	.globl	integer_bit_8
	.type	integer_bit_8, @function
integer_bit_8:
.LFB17:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -128(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movl	%eax, -96(%rbp)
	addl	$1, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$1, %eax
	movl	%eax, -92(%rbp)
	addl	$2, %eax
	movl	%eax, %esi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$1, %eax
	movl	%eax, -108(%rbp)
	addl	$3, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$1, %eax
	movl	%eax, -112(%rbp)
	addl	$4, %eax
	movl	%eax, %r10d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movl	%eax, -116(%rbp)
	addl	$5, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	addl	$6, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	movl	%eax, -88(%rbp)
	leal	7(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	1(%rax), %r15d
	leal	8(%r15), %r12d
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	1(%rax), %r13d
	leal	9(%r13), %ebx
	jmp	.L43
.L44:
	movl	-120(%rbp), %edx
	xorl	%edx, -96(%rbp)
	movl	-96(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -96(%rbp)
	xorl	%esi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r8d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, -112(%rbp)
	movl	-112(%rbp), %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-100(%rbp), %eax
	xorl	%eax, -116(%rbp)
	movl	-116(%rbp), %r11d
	xorl	%r11d, %eax
	movl	%eax, -100(%rbp)
	orl	%eax, %r11d
	movl	-104(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -104(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	xorl	%r14d, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -88(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -96(%rbp)
	movl	-96(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -96(%rbp)
	xorl	%esi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-100(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -100(%rbp)
	orl	%eax, %r11d
	movl	-104(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -104(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	xorl	%r14d, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -88(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -96(%rbp)
	movl	-96(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -96(%rbp)
	xorl	%esi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-100(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -100(%rbp)
	orl	%eax, %r11d
	movl	-104(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -104(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	xorl	%r14d, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -88(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -96(%rbp)
	movl	-96(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -96(%rbp)
	xorl	%esi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-100(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -100(%rbp)
	orl	%eax, %r11d
	movl	-104(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -104(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	xorl	%r14d, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -88(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -96(%rbp)
	movl	-96(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -96(%rbp)
	xorl	%esi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-100(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -100(%rbp)
	orl	%eax, %r11d
	movl	-104(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -104(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	xorl	%r14d, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -88(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -96(%rbp)
	movl	-96(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -96(%rbp)
	xorl	%esi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-100(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -100(%rbp)
	orl	%eax, %r11d
	movl	-104(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -104(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	xorl	%r14d, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -88(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -96(%rbp)
	movl	-96(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -96(%rbp)
	xorl	%esi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-100(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -100(%rbp)
	orl	%eax, %r11d
	movl	-104(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -104(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	xorl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -88(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -96(%rbp)
	movl	-96(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r8d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-100(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -100(%rbp)
	orl	%ecx, %r11d
	movl	-104(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -104(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	xorl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -88(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r8d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-100(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -100(%rbp)
	orl	%ecx, %r11d
	movl	-104(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -104(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	xorl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -88(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, %eax
	xorl	%eax, %edx
	movl	%edx, -120(%rbp)
	orl	%edx, %eax
	movl	%eax, -96(%rbp)
	xorl	%esi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r8d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	%r9d, -112(%rbp)
	movl	-100(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -100(%rbp)
	orl	%eax, %r11d
	movl	%r11d, -116(%rbp)
	movl	-104(%rbp), %eax
	xorl	%eax, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -104(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -84(%rbp)
	xorl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -88(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
.L43:
	movq	-128(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -128(%rbp)
	testq	%rax, %rax
	jne	.L44
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-108(%rbp), %edi
	call	use_int@PLT
	movl	-112(%rbp), %edi
	call	use_int@PLT
	movl	-116(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE17:
	.size	integer_bit_8, .-integer_bit_8
	.globl	integer_bit_9
	.type	integer_bit_9, @function
integer_bit_9:
.LFB18:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -136(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movl	%eax, -100(%rbp)
	addl	$1, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$1, %eax
	movl	%eax, -96(%rbp)
	addl	$2, %eax
	movl	%eax, %esi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$1, %eax
	movl	%eax, -116(%rbp)
	addl	$3, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$1, %eax
	movl	%eax, -120(%rbp)
	addl	$4, %eax
	movl	%eax, %r10d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movl	%eax, -124(%rbp)
	addl	$5, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	addl	$6, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	movl	%eax, -88(%rbp)
	addl	$7, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$1, %eax
	movl	%eax, -92(%rbp)
	leal	8(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	1(%rax), %r15d
	leal	9(%r15), %r12d
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	1(%rax), %r13d
	leal	10(%r13), %ebx
	jmp	.L46
.L47:
	movl	-128(%rbp), %edx
	xorl	%edx, -100(%rbp)
	movl	-100(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -100(%rbp)
	xorl	%esi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, -120(%rbp)
	movl	-120(%rbp), %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-104(%rbp), %eax
	xorl	%eax, -124(%rbp)
	movl	-124(%rbp), %r11d
	xorl	%r11d, %eax
	movl	%eax, -104(%rbp)
	orl	%eax, %r11d
	movl	-108(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -108(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-112(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -112(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -88(%rbp)
	xorl	%r14d, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -100(%rbp)
	movl	-100(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -100(%rbp)
	xorl	%esi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-104(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -104(%rbp)
	orl	%eax, %r11d
	movl	-108(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -108(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-112(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -112(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -88(%rbp)
	xorl	%r14d, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -100(%rbp)
	movl	-100(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -100(%rbp)
	xorl	%esi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-104(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -104(%rbp)
	orl	%eax, %r11d
	movl	-108(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -108(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-112(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -112(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -88(%rbp)
	xorl	%r14d, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -100(%rbp)
	movl	-100(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -100(%rbp)
	xorl	%esi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-104(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -104(%rbp)
	orl	%eax, %r11d
	movl	-108(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -108(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-112(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -112(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -88(%rbp)
	xorl	%r14d, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -100(%rbp)
	movl	-100(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -100(%rbp)
	xorl	%esi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-104(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -104(%rbp)
	orl	%eax, %r11d
	movl	-108(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -108(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-112(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -112(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -88(%rbp)
	xorl	%r14d, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -100(%rbp)
	movl	-100(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -100(%rbp)
	xorl	%esi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-104(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -104(%rbp)
	orl	%eax, %r11d
	movl	-108(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -108(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-112(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -112(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -88(%rbp)
	xorl	%r14d, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -100(%rbp)
	movl	-100(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -100(%rbp)
	xorl	%esi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-104(%rbp), %edi
	movl	%edi, %eax
	xorl	%eax, %r11d
	xorl	%r11d, %edi
	movl	%edi, -104(%rbp)
	movl	%edi, %eax
	orl	%eax, %r11d
	movl	-108(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -108(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-112(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -112(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	xorl	%r14d, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -92(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -100(%rbp)
	movl	-100(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-104(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -104(%rbp)
	orl	%ecx, %r11d
	movl	-108(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -108(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-112(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -112(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	xorl	%r14d, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -92(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-104(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -104(%rbp)
	orl	%ecx, %r11d
	movl	-108(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -108(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-112(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -112(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	xorl	%r14d, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -92(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, %eax
	xorl	%eax, %edx
	movl	%edx, -128(%rbp)
	orl	%edx, %eax
	movl	%eax, -100(%rbp)
	xorl	%esi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	%r9d, -120(%rbp)
	movl	-104(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -104(%rbp)
	orl	%eax, %r11d
	movl	%r11d, -124(%rbp)
	movl	-108(%rbp), %eax
	xorl	%eax, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -108(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -84(%rbp)
	movl	-112(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -112(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	xorl	%r14d, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -92(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
.L46:
	movq	-136(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -136(%rbp)
	testq	%rax, %rax
	jne	.L47
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-116(%rbp), %edi
	call	use_int@PLT
	movl	-120(%rbp), %edi
	call	use_int@PLT
	movl	-124(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE18:
	.size	integer_bit_9, .-integer_bit_9
	.globl	integer_bit_10
	.type	integer_bit_10, @function
integer_bit_10:
.LFB19:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -144(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movl	%eax, -104(%rbp)
	addl	$1, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$1, %eax
	movl	%eax, -100(%rbp)
	addl	$2, %eax
	movl	%eax, %esi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$1, %eax
	movl	%eax, -124(%rbp)
	addl	$3, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$1, %eax
	movl	%eax, -128(%rbp)
	addl	$4, %eax
	movl	%eax, %r10d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movl	%eax, -132(%rbp)
	addl	$5, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	addl	$6, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	movl	%eax, -88(%rbp)
	addl	$7, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$1, %eax
	movl	%eax, -92(%rbp)
	addl	$8, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$1, %eax
	movl	%eax, -96(%rbp)
	leal	9(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	1(%rax), %r15d
	leal	10(%r15), %r12d
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	1(%rax), %r13d
	leal	11(%r13), %ebx
	jmp	.L49
.L50:
	movl	-136(%rbp), %edx
	xorl	%edx, -104(%rbp)
	movl	-104(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -104(%rbp)
	xorl	%esi, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -100(%rbp)
	xorl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, -128(%rbp)
	movl	-128(%rbp), %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %eax
	xorl	%eax, -132(%rbp)
	movl	-132(%rbp), %r11d
	xorl	%r11d, %eax
	movl	%eax, -108(%rbp)
	orl	%eax, %r11d
	movl	-112(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -112(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -116(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -88(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r14d, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -104(%rbp)
	movl	-104(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -104(%rbp)
	xorl	%esi, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -100(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %eax
	xorl	%eax, %r11d
	movl	%eax, %ecx
	xorl	%r11d, %ecx
	movl	%ecx, %eax
	movl	%eax, -108(%rbp)
	orl	%eax, %r11d
	movl	-112(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -112(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -116(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -88(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r14d, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -104(%rbp)
	movl	-104(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -104(%rbp)
	xorl	%esi, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -100(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %eax
	xorl	%eax, %r11d
	movl	%eax, %ecx
	xorl	%r11d, %ecx
	movl	%ecx, %eax
	movl	%eax, -108(%rbp)
	orl	%eax, %r11d
	movl	-112(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -112(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -116(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -88(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r14d, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -104(%rbp)
	movl	-104(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -104(%rbp)
	xorl	%esi, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -100(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %eax
	xorl	%eax, %r11d
	movl	%eax, %ecx
	xorl	%r11d, %ecx
	movl	%ecx, %eax
	movl	%eax, -108(%rbp)
	orl	%eax, %r11d
	movl	-112(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -112(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -116(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -88(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r14d, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -104(%rbp)
	movl	-104(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -104(%rbp)
	xorl	%esi, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -100(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %eax
	xorl	%eax, %r11d
	movl	%eax, %ecx
	xorl	%r11d, %ecx
	movl	%ecx, %eax
	movl	%eax, -108(%rbp)
	orl	%eax, %r11d
	movl	-112(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -112(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -116(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -88(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r14d, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -104(%rbp)
	movl	-104(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -104(%rbp)
	xorl	%esi, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -100(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %eax
	xorl	%eax, %r11d
	movl	%eax, %ecx
	xorl	%r11d, %ecx
	movl	%ecx, %eax
	movl	%eax, -108(%rbp)
	orl	%eax, %r11d
	movl	-112(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -112(%rbp)
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -116(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -88(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -92(%rbp)
	xorl	%r14d, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -104(%rbp)
	movl	-104(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -104(%rbp)
	xorl	%esi, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -124(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %edi
	movl	%edi, %eax
	xorl	%eax, %r11d
	xorl	%r11d, %edi
	movl	%edi, -108(%rbp)
	movl	%edi, %eax
	orl	%eax, %r11d
	movl	-112(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -112(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -116(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	xorl	%r14d, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -96(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -104(%rbp)
	movl	-104(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -100(%rbp)
	xorl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -124(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -108(%rbp)
	orl	%ecx, %r11d
	movl	-112(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -112(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-116(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -116(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-120(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -120(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	xorl	%r14d, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -96(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -100(%rbp)
	xorl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -124(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %ecx
	xorl	%ecx, %r11d
	movl	%ecx, %edi
	xorl	%r11d, %edi
	movl	%edi, %ecx
	movl	%ecx, -108(%rbp)
	orl	%ecx, %r11d
	movl	-112(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -112(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-116(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -116(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-120(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -120(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	xorl	%r14d, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -96(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, %eax
	xorl	%eax, %edx
	movl	%edx, -136(%rbp)
	orl	%edx, %eax
	movl	%eax, -104(%rbp)
	xorl	%esi, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -100(%rbp)
	xorl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -124(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	%r9d, -128(%rbp)
	movl	-108(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -108(%rbp)
	orl	%eax, %r11d
	movl	%r11d, -132(%rbp)
	movl	-112(%rbp), %eax
	xorl	%eax, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -112(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -84(%rbp)
	movl	-116(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -116(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-120(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -120(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	xorl	%r14d, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -96(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
.L49:
	movq	-144(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -144(%rbp)
	testq	%rax, %rax
	jne	.L50
	movl	-104(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-124(%rbp), %edi
	call	use_int@PLT
	movl	-128(%rbp), %edi
	call	use_int@PLT
	movl	-132(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE19:
	.size	integer_bit_10, .-integer_bit_10
	.globl	integer_bit_11
	.type	integer_bit_11, @function
integer_bit_11:
.LFB20:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -152(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movl	%eax, -128(%rbp)
	addl	$1, %eax
	movl	%eax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$1, %eax
	movl	%eax, -132(%rbp)
	addl	$2, %eax
	movl	%eax, %esi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$1, %eax
	movl	%eax, -104(%rbp)
	addl	$3, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$1, %eax
	movl	%eax, -136(%rbp)
	addl	$4, %eax
	movl	%eax, %r10d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movl	%eax, -140(%rbp)
	addl	$5, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	addl	$6, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	movl	%eax, -88(%rbp)
	addl	$7, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$1, %eax
	movl	%eax, -92(%rbp)
	addl	$8, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$1, %eax
	movl	%eax, -96(%rbp)
	addl	$9, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$1, %eax
	movl	%eax, -100(%rbp)
	leal	10(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	1(%rax), %r15d
	leal	11(%r15), %r12d
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	1(%rax), %r13d
	leal	12(%r13), %ebx
	jmp	.L52
.L53:
	movl	-144(%rbp), %edx
	xorl	%edx, -128(%rbp)
	movl	-128(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -128(%rbp)
	xorl	%esi, -132(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r10d, -136(%rbp)
	movl	-136(%rbp), %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %eax
	xorl	%eax, -140(%rbp)
	movl	-140(%rbp), %r11d
	xorl	%r11d, %eax
	movl	%eax, -108(%rbp)
	orl	%eax, %r11d
	movl	-112(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -112(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -116(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-124(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -124(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	xorl	%r14d, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -100(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -128(%rbp)
	movl	-128(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -132(%rbp)
	xorl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -108(%rbp)
	orl	%ecx, %r11d
	movl	-112(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -112(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-116(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -116(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-120(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -120(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	movl	-124(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -124(%rbp)
	orl	%ecx, %edi
	movl	%edi, -96(%rbp)
	xorl	%r14d, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -100(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -128(%rbp)
	xorl	%esi, -132(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -108(%rbp)
	orl	%eax, %r11d
	movl	-112(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -112(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -116(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-124(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -124(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	xorl	%r14d, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -100(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -128(%rbp)
	movl	-128(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -132(%rbp)
	xorl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -108(%rbp)
	orl	%ecx, %r11d
	movl	-112(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -112(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-116(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -116(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-120(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -120(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	movl	-124(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -124(%rbp)
	orl	%ecx, %edi
	movl	%edi, -96(%rbp)
	xorl	%r14d, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -100(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -128(%rbp)
	xorl	%esi, -132(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -108(%rbp)
	orl	%eax, %r11d
	movl	-112(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -112(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -116(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-124(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -124(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	xorl	%r14d, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -100(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -128(%rbp)
	movl	-128(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -132(%rbp)
	xorl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -108(%rbp)
	orl	%ecx, %r11d
	movl	-112(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -112(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-116(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -116(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-120(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -120(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	movl	-124(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -124(%rbp)
	orl	%ecx, %edi
	movl	%edi, -96(%rbp)
	xorl	%r14d, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -100(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -128(%rbp)
	xorl	%esi, -132(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -108(%rbp)
	orl	%eax, %r11d
	movl	-112(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -112(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -116(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-124(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -124(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	xorl	%r14d, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -100(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -128(%rbp)
	movl	-128(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -128(%rbp)
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -132(%rbp)
	xorl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -108(%rbp)
	orl	%ecx, %r11d
	movl	-112(%rbp), %eax
	xorl	%eax, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -112(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -84(%rbp)
	movl	-116(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -116(%rbp)
	orl	%ecx, %eax
	movl	%eax, -88(%rbp)
	movl	-120(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -120(%rbp)
	orl	%ecx, %eax
	movl	%eax, -92(%rbp)
	movl	-124(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -124(%rbp)
	orl	%ecx, %eax
	movl	%eax, -96(%rbp)
	xorl	%r14d, -100(%rbp)
	movl	-100(%rbp), %eax
	xorl	%eax, %r14d
	orl	%r14d, %eax
	movl	%eax, -100(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -128(%rbp)
	movl	-128(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, -132(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -132(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-108(%rbp), %edi
	xorl	%edi, %r11d
	xorl	%r11d, %edi
	movl	%edi, -108(%rbp)
	orl	%edi, %r11d
	movl	-112(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -112(%rbp)
	orl	%ecx, %edi
	movl	%edi, -84(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -116(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-120(%rbp), %edi
	xorl	%edi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -120(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -92(%rbp)
	movl	-124(%rbp), %edi
	xorl	%edi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -124(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r14d, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -100(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, %eax
	xorl	%eax, %edx
	movl	%edx, -144(%rbp)
	orl	%edx, %eax
	movl	%eax, -128(%rbp)
	xorl	%esi, -132(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -132(%rbp)
	xorl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	%r9d, -136(%rbp)
	movl	-108(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -108(%rbp)
	orl	%eax, %r11d
	movl	%r11d, -140(%rbp)
	movl	-112(%rbp), %eax
	xorl	%eax, -84(%rbp)
	movl	-84(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -112(%rbp)
	orl	%eax, %edi
	movl	%edi, -84(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -116(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-120(%rbp), %edi
	xorl	%edi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -120(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -92(%rbp)
	movl	-124(%rbp), %edi
	xorl	%edi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -124(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -96(%rbp)
	xorl	%r14d, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -100(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
.L52:
	movq	-152(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -152(%rbp)
	testq	%rax, %rax
	jne	.L53
	movl	-128(%rbp), %edi
	call	use_int@PLT
	movl	-132(%rbp), %edi
	call	use_int@PLT
	movl	-104(%rbp), %edi
	call	use_int@PLT
	movl	-136(%rbp), %edi
	call	use_int@PLT
	movl	-140(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE20:
	.size	integer_bit_11, .-integer_bit_11
	.globl	integer_bit_12
	.type	integer_bit_12, @function
integer_bit_12:
.LFB21:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -160(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movl	%eax, -136(%rbp)
	addl	$1, %eax
	movl	%eax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$1, %eax
	movl	%eax, -140(%rbp)
	addl	$2, %eax
	movl	%eax, %esi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$1, %eax
	movl	%eax, -108(%rbp)
	addl	$3, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$1, %eax
	movl	%eax, -144(%rbp)
	addl	$4, %eax
	movl	%eax, %r10d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movl	%eax, -148(%rbp)
	addl	$5, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	addl	$6, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	movl	%eax, -88(%rbp)
	addl	$7, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$1, %eax
	movl	%eax, -92(%rbp)
	addl	$8, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$1, %eax
	movl	%eax, -96(%rbp)
	addl	$9, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$1, %eax
	movl	%eax, -100(%rbp)
	addl	$10, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$1, %eax
	movl	%eax, -104(%rbp)
	leal	11(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	1(%rax), %r15d
	leal	12(%r15), %r12d
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	leal	1(%rax), %r13d
	leal	13(%r13), %ebx
	jmp	.L55
.L56:
	movl	-152(%rbp), %edx
	xorl	%edx, -136(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -136(%rbp)
	xorl	%esi, -140(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r10d, -144(%rbp)
	movl	-144(%rbp), %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-112(%rbp), %eax
	xorl	%eax, -148(%rbp)
	movl	-148(%rbp), %r11d
	xorl	%r11d, %eax
	movl	%eax, -112(%rbp)
	orl	%eax, %r11d
	movl	-116(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -116(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-124(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -124(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-128(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -128(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	movl	-132(%rbp), %eax
	xorl	%eax, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -132(%rbp)
	orl	%eax, %edi
	movl	%edi, -100(%rbp)
	xorl	%r14d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -136(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -140(%rbp)
	xorl	%r8d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-112(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -112(%rbp)
	orl	%ecx, %r11d
	movl	-116(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -116(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-120(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -120(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-124(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -124(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	movl	-128(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -128(%rbp)
	orl	%ecx, %edi
	movl	%edi, -96(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -132(%rbp)
	orl	%ecx, %edi
	movl	%edi, -100(%rbp)
	xorl	%r14d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -136(%rbp)
	xorl	%esi, -140(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-112(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -112(%rbp)
	orl	%eax, %r11d
	movl	-116(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -116(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-124(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -124(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-128(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -128(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	movl	-132(%rbp), %eax
	xorl	%eax, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -132(%rbp)
	orl	%eax, %edi
	movl	%edi, -100(%rbp)
	xorl	%r14d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -136(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -140(%rbp)
	xorl	%r8d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-112(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -112(%rbp)
	orl	%ecx, %r11d
	movl	-116(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -116(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-120(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -120(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-124(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -124(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	movl	-128(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -128(%rbp)
	orl	%ecx, %edi
	movl	%edi, -96(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -132(%rbp)
	orl	%ecx, %edi
	movl	%edi, -100(%rbp)
	xorl	%r14d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -136(%rbp)
	xorl	%esi, -140(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-112(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -112(%rbp)
	orl	%eax, %r11d
	movl	-116(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -116(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-124(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -124(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-128(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -128(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	movl	-132(%rbp), %eax
	xorl	%eax, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -132(%rbp)
	orl	%eax, %edi
	movl	%edi, -100(%rbp)
	xorl	%r14d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -136(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -140(%rbp)
	xorl	%r8d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-112(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -112(%rbp)
	orl	%ecx, %r11d
	movl	-116(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -116(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-120(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -120(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-124(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -124(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	movl	-128(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -128(%rbp)
	orl	%ecx, %edi
	movl	%edi, -96(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -132(%rbp)
	orl	%ecx, %edi
	movl	%edi, -100(%rbp)
	xorl	%r14d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -136(%rbp)
	xorl	%esi, -140(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-112(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -112(%rbp)
	orl	%eax, %r11d
	movl	-116(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -116(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-124(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -124(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-128(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -128(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	movl	-132(%rbp), %eax
	xorl	%eax, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -132(%rbp)
	orl	%eax, %edi
	movl	%edi, -100(%rbp)
	xorl	%r14d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r14d
	orl	%r14d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -136(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -136(%rbp)
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -140(%rbp)
	xorl	%r8d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-112(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -112(%rbp)
	orl	%ecx, %r11d
	movl	-116(%rbp), %eax
	xorl	%eax, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -116(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -84(%rbp)
	movl	-120(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -120(%rbp)
	orl	%ecx, %eax
	movl	%eax, -88(%rbp)
	movl	-124(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -124(%rbp)
	orl	%ecx, %eax
	movl	%eax, -92(%rbp)
	movl	-128(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -128(%rbp)
	orl	%ecx, %eax
	movl	%eax, -96(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, -100(%rbp)
	movl	-100(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -132(%rbp)
	orl	%ecx, %eax
	movl	%eax, -100(%rbp)
	xorl	%r14d, -104(%rbp)
	movl	-104(%rbp), %eax
	xorl	%eax, %r14d
	orl	%r14d, %eax
	movl	%eax, -104(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, -136(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, -140(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -140(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-112(%rbp), %edi
	xorl	%edi, %r11d
	xorl	%r11d, %edi
	movl	%edi, -112(%rbp)
	orl	%edi, %r11d
	movl	-116(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -116(%rbp)
	orl	%ecx, %edi
	movl	%edi, -84(%rbp)
	movl	-120(%rbp), %edi
	xorl	%edi, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -120(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-124(%rbp), %edi
	xorl	%edi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -124(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -92(%rbp)
	movl	-128(%rbp), %edi
	xorl	%edi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -128(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -96(%rbp)
	movl	-132(%rbp), %edi
	xorl	%edi, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -132(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -100(%rbp)
	xorl	%r14d, -104(%rbp)
	movl	-104(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -104(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
	xorl	%edx, %eax
	xorl	%eax, %edx
	movl	%edx, -152(%rbp)
	orl	%edx, %eax
	movl	%eax, -136(%rbp)
	xorl	%esi, -140(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -140(%rbp)
	xorl	%r8d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	%r9d, -144(%rbp)
	movl	-112(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -112(%rbp)
	orl	%eax, %r11d
	movl	%r11d, -148(%rbp)
	movl	-116(%rbp), %eax
	xorl	%eax, -84(%rbp)
	movl	-84(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -116(%rbp)
	orl	%eax, %edi
	movl	%edi, -84(%rbp)
	movl	-120(%rbp), %edi
	xorl	%edi, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -120(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-124(%rbp), %edi
	xorl	%edi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -124(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -92(%rbp)
	movl	-128(%rbp), %edi
	xorl	%edi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -128(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -96(%rbp)
	movl	-132(%rbp), %edi
	xorl	%edi, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -132(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -100(%rbp)
	xorl	%r14d, -104(%rbp)
	movl	-104(%rbp), %ecx
	xorl	%ecx, %r14d
	orl	%r14d, %ecx
	movl	%ecx, -104(%rbp)
	xorl	%r12d, %r15d
	xorl	%r15d, %r12d
	orl	%r12d, %r15d
	xorl	%ebx, %r13d
	xorl	%r13d, %ebx
	orl	%ebx, %r13d
.L55:
	movq	-160(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -160(%rbp)
	testq	%rax, %rax
	jne	.L56
	movl	-136(%rbp), %edi
	call	use_int@PLT
	movl	-140(%rbp), %edi
	call	use_int@PLT
	movl	-108(%rbp), %edi
	call	use_int@PLT
	movl	-144(%rbp), %edi
	call	use_int@PLT
	movl	-148(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-104(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE21:
	.size	integer_bit_12, .-integer_bit_12
	.globl	integer_bit_13
	.type	integer_bit_13, @function
integer_bit_13:
.LFB22:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -168(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movl	%eax, -144(%rbp)
	addl	$1, %eax
	movl	%eax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$1, %eax
	movl	%eax, -148(%rbp)
	addl	$2, %eax
	movl	%eax, %esi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$1, %eax
	movl	%eax, -116(%rbp)
	addl	$3, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$1, %eax
	movl	%eax, -152(%rbp)
	addl	$4, %eax
	movl	%eax, %r10d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movl	%eax, -156(%rbp)
	addl	$5, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	addl	$6, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	movl	%eax, -88(%rbp)
	addl	$7, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$1, %eax
	movl	%eax, -92(%rbp)
	addl	$8, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$1, %eax
	movl	%eax, -96(%rbp)
	addl	$9, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$1, %eax
	movl	%eax, -100(%rbp)
	addl	$10, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$1, %eax
	movl	%eax, -104(%rbp)
	leal	11(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$1, %eax
	movl	%eax, -108(%rbp)
	leal	12(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$1, %eax
	movl	%eax, -112(%rbp)
	leal	13(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	leal	1(%rax), %r14d
	leal	14(%r14), %ebx
	jmp	.L58
.L59:
	movl	-160(%rbp), %edx
	xorl	%edx, -144(%rbp)
	movl	-144(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -144(%rbp)
	xorl	%esi, -148(%rbp)
	movl	-148(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r10d, -152(%rbp)
	movl	-152(%rbp), %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-120(%rbp), %eax
	xorl	%eax, -156(%rbp)
	movl	-156(%rbp), %r11d
	xorl	%r11d, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %r11d
	movl	-124(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -124(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-128(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -128(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-132(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -132(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -136(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	movl	-140(%rbp), %eax
	xorl	%eax, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -140(%rbp)
	orl	%eax, %edi
	movl	%edi, -100(%rbp)
	xorl	%r15d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r13d, -108(%rbp)
	movl	-108(%rbp), %eax
	xorl	%eax, %r13d
	orl	%r13d, %eax
	movl	%eax, -108(%rbp)
	xorl	%r12d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r12d
	orl	%r12d, %edi
	movl	%edi, -112(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, -144(%rbp)
	movl	-144(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -148(%rbp)
	xorl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-120(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -120(%rbp)
	orl	%ecx, %r11d
	movl	-124(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -124(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-128(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -128(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -132(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	movl	-136(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -136(%rbp)
	orl	%ecx, %edi
	movl	%edi, -96(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -140(%rbp)
	orl	%ecx, %edi
	movl	%edi, -100(%rbp)
	xorl	%r15d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r13d, -108(%rbp)
	movl	-108(%rbp), %ecx
	xorl	%ecx, %r13d
	orl	%r13d, %ecx
	movl	%ecx, -108(%rbp)
	xorl	%r12d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r12d
	orl	%r12d, %edi
	movl	%edi, -112(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -144(%rbp)
	xorl	%esi, -148(%rbp)
	movl	-148(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-120(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %r11d
	movl	-124(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -124(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-128(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -128(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-132(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -132(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -136(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	movl	-140(%rbp), %eax
	xorl	%eax, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -140(%rbp)
	orl	%eax, %edi
	movl	%edi, -100(%rbp)
	xorl	%r15d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r13d, -108(%rbp)
	movl	-108(%rbp), %eax
	xorl	%eax, %r13d
	orl	%r13d, %eax
	movl	%eax, -108(%rbp)
	xorl	%r12d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r12d
	orl	%r12d, %edi
	movl	%edi, -112(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, -144(%rbp)
	movl	-144(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -148(%rbp)
	xorl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-120(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -120(%rbp)
	orl	%ecx, %r11d
	movl	-124(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -124(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-128(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -128(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -132(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	movl	-136(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -136(%rbp)
	orl	%ecx, %edi
	movl	%edi, -96(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -140(%rbp)
	orl	%ecx, %edi
	movl	%edi, -100(%rbp)
	xorl	%r15d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r13d, -108(%rbp)
	movl	-108(%rbp), %ecx
	xorl	%ecx, %r13d
	orl	%r13d, %ecx
	movl	%ecx, -108(%rbp)
	xorl	%r12d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r12d
	orl	%r12d, %edi
	movl	%edi, -112(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -144(%rbp)
	xorl	%esi, -148(%rbp)
	movl	-148(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-120(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %r11d
	movl	-124(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -124(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-128(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -128(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-132(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -132(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -136(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	movl	-140(%rbp), %eax
	xorl	%eax, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -140(%rbp)
	orl	%eax, %edi
	movl	%edi, -100(%rbp)
	xorl	%r15d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r13d, -108(%rbp)
	movl	-108(%rbp), %eax
	xorl	%eax, %r13d
	orl	%r13d, %eax
	movl	%eax, -108(%rbp)
	xorl	%r12d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r12d
	orl	%r12d, %edi
	movl	%edi, -112(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, -144(%rbp)
	movl	-144(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -148(%rbp)
	xorl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-120(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -120(%rbp)
	orl	%ecx, %r11d
	movl	-124(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -124(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-128(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -128(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -132(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	movl	-136(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -136(%rbp)
	orl	%ecx, %edi
	movl	%edi, -96(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -140(%rbp)
	orl	%ecx, %edi
	movl	%edi, -100(%rbp)
	xorl	%r15d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r13d, -108(%rbp)
	movl	-108(%rbp), %ecx
	xorl	%ecx, %r13d
	orl	%r13d, %ecx
	movl	%ecx, -108(%rbp)
	xorl	%r12d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r12d
	orl	%r12d, %edi
	movl	%edi, -112(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -144(%rbp)
	xorl	%esi, -148(%rbp)
	movl	-148(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-120(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %r11d
	movl	-124(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -124(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-128(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -128(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-132(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -132(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -136(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	movl	-140(%rbp), %eax
	xorl	%eax, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -140(%rbp)
	orl	%eax, %edi
	movl	%edi, -100(%rbp)
	xorl	%r15d, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -104(%rbp)
	xorl	%r13d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r13d
	orl	%r13d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r12d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r12d
	orl	%r12d, %edi
	movl	%edi, -112(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, -144(%rbp)
	movl	-144(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -144(%rbp)
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -148(%rbp)
	xorl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-120(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -120(%rbp)
	orl	%ecx, %r11d
	movl	-124(%rbp), %eax
	xorl	%eax, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -124(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -84(%rbp)
	movl	-128(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -128(%rbp)
	orl	%ecx, %eax
	movl	%eax, -88(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -132(%rbp)
	orl	%ecx, %eax
	movl	%eax, -92(%rbp)
	movl	-136(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -136(%rbp)
	orl	%ecx, %eax
	movl	%eax, -96(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, -100(%rbp)
	movl	-100(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -140(%rbp)
	orl	%ecx, %eax
	movl	%eax, -100(%rbp)
	xorl	%r15d, -104(%rbp)
	movl	-104(%rbp), %eax
	xorl	%eax, %r15d
	orl	%r15d, %eax
	movl	%eax, -104(%rbp)
	xorl	%r13d, -108(%rbp)
	movl	-108(%rbp), %eax
	xorl	%eax, %r13d
	orl	%r13d, %eax
	movl	%eax, -108(%rbp)
	xorl	%r12d, -112(%rbp)
	movl	-112(%rbp), %eax
	xorl	%eax, %r12d
	orl	%r12d, %eax
	movl	%eax, -112(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, -144(%rbp)
	movl	-144(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, -148(%rbp)
	movl	-148(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -148(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-120(%rbp), %edi
	xorl	%edi, %r11d
	xorl	%r11d, %edi
	movl	%edi, -120(%rbp)
	orl	%edi, %r11d
	movl	-124(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -124(%rbp)
	orl	%ecx, %edi
	movl	%edi, -84(%rbp)
	movl	-128(%rbp), %edi
	xorl	%edi, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -128(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-132(%rbp), %edi
	xorl	%edi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -132(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -92(%rbp)
	movl	-136(%rbp), %edi
	xorl	%edi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -136(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -96(%rbp)
	movl	-140(%rbp), %edi
	xorl	%edi, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -140(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -100(%rbp)
	xorl	%r15d, -104(%rbp)
	movl	-104(%rbp), %ecx
	xorl	%ecx, %r15d
	orl	%r15d, %ecx
	movl	%ecx, -104(%rbp)
	xorl	%r13d, -108(%rbp)
	movl	-108(%rbp), %ecx
	xorl	%ecx, %r13d
	orl	%r13d, %ecx
	movl	%ecx, -108(%rbp)
	xorl	%r12d, -112(%rbp)
	movl	-112(%rbp), %ecx
	xorl	%ecx, %r12d
	orl	%r12d, %ecx
	movl	%ecx, -112(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, %eax
	xorl	%eax, %edx
	movl	%edx, -160(%rbp)
	orl	%edx, %eax
	movl	%eax, -144(%rbp)
	xorl	%esi, -148(%rbp)
	movl	-148(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -148(%rbp)
	xorl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	%r9d, -152(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -120(%rbp)
	orl	%eax, %r11d
	movl	%r11d, -156(%rbp)
	movl	-124(%rbp), %eax
	xorl	%eax, -84(%rbp)
	movl	-84(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -124(%rbp)
	orl	%eax, %edi
	movl	%edi, -84(%rbp)
	movl	-128(%rbp), %edi
	xorl	%edi, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -128(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-132(%rbp), %edi
	xorl	%edi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -132(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -92(%rbp)
	movl	-136(%rbp), %edi
	xorl	%edi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -136(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -96(%rbp)
	movl	-140(%rbp), %edi
	xorl	%edi, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -140(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -100(%rbp)
	xorl	%r15d, -104(%rbp)
	movl	-104(%rbp), %ecx
	xorl	%ecx, %r15d
	orl	%r15d, %ecx
	movl	%ecx, -104(%rbp)
	xorl	%r13d, -108(%rbp)
	movl	-108(%rbp), %ecx
	xorl	%ecx, %r13d
	orl	%r13d, %ecx
	movl	%ecx, -108(%rbp)
	xorl	%r12d, -112(%rbp)
	movl	-112(%rbp), %ecx
	xorl	%ecx, %r12d
	orl	%r12d, %ecx
	movl	%ecx, -112(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
.L58:
	movq	-168(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -168(%rbp)
	testq	%rax, %rax
	jne	.L59
	movl	-144(%rbp), %edi
	call	use_int@PLT
	movl	-148(%rbp), %edi
	call	use_int@PLT
	movl	-116(%rbp), %edi
	call	use_int@PLT
	movl	-152(%rbp), %edi
	call	use_int@PLT
	movl	-156(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-104(%rbp), %edi
	call	use_int@PLT
	movl	-108(%rbp), %edi
	call	use_int@PLT
	movl	-112(%rbp), %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE22:
	.size	integer_bit_13, .-integer_bit_13
	.globl	integer_bit_14
	.type	integer_bit_14, @function
integer_bit_14:
.LFB23:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -176(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movl	%eax, -152(%rbp)
	addl	$1, %eax
	movl	%eax, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$1, %eax
	movl	%eax, -156(%rbp)
	addl	$2, %eax
	movl	%eax, %esi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$1, %eax
	movl	%eax, -120(%rbp)
	addl	$3, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$1, %eax
	movl	%eax, -160(%rbp)
	addl	$4, %eax
	movl	%eax, %r10d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movl	%eax, -164(%rbp)
	addl	$5, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	addl	$6, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	movl	%eax, -88(%rbp)
	addl	$7, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$1, %eax
	movl	%eax, -92(%rbp)
	addl	$8, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$1, %eax
	movl	%eax, -96(%rbp)
	addl	$9, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$1, %eax
	movl	%eax, -100(%rbp)
	addl	$10, %eax
	movl	%eax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$1, %eax
	movl	%eax, -104(%rbp)
	addl	$11, %eax
	movl	%eax, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$1, %eax
	movl	%eax, -108(%rbp)
	leal	12(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$1, %eax
	movl	%eax, -112(%rbp)
	leal	13(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$1, %eax
	movl	%eax, -116(%rbp)
	leal	14(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	leal	1(%rax), %r14d
	leal	15(%r14), %ebx
	jmp	.L61
.L62:
	movl	-168(%rbp), %edx
	xorl	%edx, -152(%rbp)
	movl	-152(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -152(%rbp)
	xorl	%esi, -156(%rbp)
	movl	-156(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -120(%rbp)
	movl	-120(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -120(%rbp)
	xorl	%r10d, -160(%rbp)
	movl	-160(%rbp), %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-124(%rbp), %eax
	xorl	%eax, -164(%rbp)
	movl	-164(%rbp), %r11d
	xorl	%r11d, %eax
	movl	%eax, -124(%rbp)
	orl	%eax, %r11d
	movl	-128(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -128(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-132(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -132(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -136(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-140(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -140(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	movl	-144(%rbp), %eax
	xorl	%eax, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -144(%rbp)
	orl	%eax, %edi
	movl	%edi, -100(%rbp)
	movl	-148(%rbp), %eax
	xorl	%eax, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -148(%rbp)
	orl	%eax, %edi
	movl	%edi, -104(%rbp)
	xorl	%r15d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r13d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r13d
	orl	%r13d, %edi
	movl	%edi, -112(%rbp)
	xorl	%r12d, -116(%rbp)
	movl	-116(%rbp), %eax
	xorl	%eax, %r12d
	orl	%r12d, %eax
	movl	%eax, -116(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, -152(%rbp)
	movl	-152(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -156(%rbp)
	xorl	%r8d, -120(%rbp)
	movl	-120(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -120(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-124(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -124(%rbp)
	orl	%ecx, %r11d
	movl	-128(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -128(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -132(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-136(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -136(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -140(%rbp)
	orl	%ecx, %edi
	movl	%edi, -96(%rbp)
	movl	-144(%rbp), %ecx
	xorl	%ecx, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -144(%rbp)
	orl	%ecx, %edi
	movl	%edi, -100(%rbp)
	movl	-148(%rbp), %ecx
	xorl	%ecx, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -148(%rbp)
	orl	%ecx, %edi
	movl	%edi, -104(%rbp)
	xorl	%r15d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r13d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r13d
	orl	%r13d, %edi
	movl	%edi, -112(%rbp)
	xorl	%r12d, -116(%rbp)
	movl	-116(%rbp), %ecx
	xorl	%ecx, %r12d
	orl	%r12d, %ecx
	movl	%ecx, -116(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -152(%rbp)
	xorl	%esi, -156(%rbp)
	movl	-156(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -120(%rbp)
	movl	-120(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -120(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-124(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -124(%rbp)
	orl	%eax, %r11d
	movl	-128(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -128(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-132(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -132(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -136(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-140(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -140(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	movl	-144(%rbp), %eax
	xorl	%eax, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -144(%rbp)
	orl	%eax, %edi
	movl	%edi, -100(%rbp)
	movl	-148(%rbp), %eax
	xorl	%eax, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -148(%rbp)
	orl	%eax, %edi
	movl	%edi, -104(%rbp)
	xorl	%r15d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r13d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r13d
	orl	%r13d, %edi
	movl	%edi, -112(%rbp)
	xorl	%r12d, -116(%rbp)
	movl	-116(%rbp), %eax
	xorl	%eax, %r12d
	orl	%r12d, %eax
	movl	%eax, -116(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, -152(%rbp)
	movl	-152(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -156(%rbp)
	xorl	%r8d, -120(%rbp)
	movl	-120(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -120(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-124(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -124(%rbp)
	orl	%ecx, %r11d
	movl	-128(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -128(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -132(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-136(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -136(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -140(%rbp)
	orl	%ecx, %edi
	movl	%edi, -96(%rbp)
	movl	-144(%rbp), %ecx
	xorl	%ecx, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -144(%rbp)
	orl	%ecx, %edi
	movl	%edi, -100(%rbp)
	movl	-148(%rbp), %ecx
	xorl	%ecx, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -148(%rbp)
	orl	%ecx, %edi
	movl	%edi, -104(%rbp)
	xorl	%r15d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r13d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r13d
	orl	%r13d, %edi
	movl	%edi, -112(%rbp)
	xorl	%r12d, -116(%rbp)
	movl	-116(%rbp), %ecx
	xorl	%ecx, %r12d
	orl	%r12d, %ecx
	movl	%ecx, -116(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -152(%rbp)
	xorl	%esi, -156(%rbp)
	movl	-156(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -120(%rbp)
	movl	-120(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -120(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-124(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -124(%rbp)
	orl	%eax, %r11d
	movl	-128(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -128(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-132(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -132(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -136(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-140(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -140(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	movl	-144(%rbp), %eax
	xorl	%eax, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -144(%rbp)
	orl	%eax, %edi
	movl	%edi, -100(%rbp)
	movl	-148(%rbp), %eax
	xorl	%eax, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -148(%rbp)
	orl	%eax, %edi
	movl	%edi, -104(%rbp)
	xorl	%r15d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r13d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r13d
	orl	%r13d, %edi
	movl	%edi, -112(%rbp)
	xorl	%r12d, -116(%rbp)
	movl	-116(%rbp), %eax
	xorl	%eax, %r12d
	orl	%r12d, %eax
	movl	%eax, -116(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, -152(%rbp)
	movl	-152(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -156(%rbp)
	xorl	%r8d, -120(%rbp)
	movl	-120(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -120(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-124(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -124(%rbp)
	orl	%ecx, %r11d
	movl	-128(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -128(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -132(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-136(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -136(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -140(%rbp)
	orl	%ecx, %edi
	movl	%edi, -96(%rbp)
	movl	-144(%rbp), %ecx
	xorl	%ecx, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -144(%rbp)
	orl	%ecx, %edi
	movl	%edi, -100(%rbp)
	movl	-148(%rbp), %ecx
	xorl	%ecx, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -148(%rbp)
	orl	%ecx, %edi
	movl	%edi, -104(%rbp)
	xorl	%r15d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r13d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r13d
	orl	%r13d, %edi
	movl	%edi, -112(%rbp)
	xorl	%r12d, -116(%rbp)
	movl	-116(%rbp), %ecx
	xorl	%ecx, %r12d
	orl	%r12d, %ecx
	movl	%ecx, -116(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -152(%rbp)
	xorl	%esi, -156(%rbp)
	movl	-156(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -120(%rbp)
	movl	-120(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -120(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-124(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -124(%rbp)
	orl	%eax, %r11d
	movl	-128(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -128(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-132(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -132(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -136(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-140(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -140(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	movl	-144(%rbp), %eax
	xorl	%eax, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -144(%rbp)
	orl	%eax, %edi
	movl	%edi, -100(%rbp)
	movl	-148(%rbp), %eax
	xorl	%eax, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -148(%rbp)
	orl	%eax, %edi
	movl	%edi, -104(%rbp)
	xorl	%r15d, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -108(%rbp)
	xorl	%r13d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r13d
	orl	%r13d, %edi
	movl	%edi, -112(%rbp)
	xorl	%r12d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r12d
	orl	%r12d, %edi
	movl	%edi, -116(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, -152(%rbp)
	movl	-152(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -152(%rbp)
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -156(%rbp)
	xorl	%r8d, -120(%rbp)
	movl	-120(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-124(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -124(%rbp)
	orl	%ecx, %r11d
	movl	-128(%rbp), %eax
	xorl	%eax, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -128(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -84(%rbp)
	movl	-132(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -132(%rbp)
	orl	%ecx, %eax
	movl	%eax, -88(%rbp)
	movl	-136(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -136(%rbp)
	orl	%ecx, %eax
	movl	%eax, -92(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -140(%rbp)
	orl	%ecx, %eax
	movl	%eax, -96(%rbp)
	movl	-144(%rbp), %ecx
	xorl	%ecx, -100(%rbp)
	movl	-100(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -144(%rbp)
	orl	%ecx, %eax
	movl	%eax, -100(%rbp)
	movl	-148(%rbp), %ecx
	xorl	%ecx, -104(%rbp)
	movl	-104(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -148(%rbp)
	orl	%ecx, %eax
	movl	%eax, -104(%rbp)
	xorl	%r15d, -108(%rbp)
	movl	-108(%rbp), %eax
	xorl	%eax, %r15d
	orl	%r15d, %eax
	movl	%eax, -108(%rbp)
	xorl	%r13d, -112(%rbp)
	movl	-112(%rbp), %eax
	xorl	%eax, %r13d
	orl	%r13d, %eax
	movl	%eax, -112(%rbp)
	xorl	%r12d, -116(%rbp)
	movl	-116(%rbp), %eax
	xorl	%eax, %r12d
	orl	%r12d, %eax
	movl	%eax, -116(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, -152(%rbp)
	movl	-152(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, -156(%rbp)
	movl	-156(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -156(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -120(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-124(%rbp), %edi
	xorl	%edi, %r11d
	xorl	%r11d, %edi
	movl	%edi, -124(%rbp)
	orl	%edi, %r11d
	movl	-128(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -128(%rbp)
	orl	%ecx, %edi
	movl	%edi, -84(%rbp)
	movl	-132(%rbp), %edi
	xorl	%edi, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -132(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-136(%rbp), %edi
	xorl	%edi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -136(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -92(%rbp)
	movl	-140(%rbp), %edi
	xorl	%edi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -140(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -96(%rbp)
	movl	-144(%rbp), %edi
	xorl	%edi, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -144(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -100(%rbp)
	movl	-148(%rbp), %edi
	xorl	%edi, -104(%rbp)
	movl	-104(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -148(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -104(%rbp)
	xorl	%r15d, -108(%rbp)
	movl	-108(%rbp), %ecx
	xorl	%ecx, %r15d
	orl	%r15d, %ecx
	movl	%ecx, -108(%rbp)
	xorl	%r13d, -112(%rbp)
	movl	-112(%rbp), %ecx
	xorl	%ecx, %r13d
	orl	%r13d, %ecx
	movl	%ecx, -112(%rbp)
	xorl	%r12d, -116(%rbp)
	movl	-116(%rbp), %ecx
	xorl	%ecx, %r12d
	orl	%r12d, %ecx
	movl	%ecx, -116(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, %eax
	xorl	%eax, %edx
	movl	%edx, -168(%rbp)
	orl	%edx, %eax
	movl	%eax, -152(%rbp)
	xorl	%esi, -156(%rbp)
	movl	-156(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -156(%rbp)
	xorl	%r8d, -120(%rbp)
	movl	-120(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -120(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	%r9d, -160(%rbp)
	movl	-124(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -124(%rbp)
	orl	%eax, %r11d
	movl	%r11d, -164(%rbp)
	movl	-128(%rbp), %eax
	xorl	%eax, -84(%rbp)
	movl	-84(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -128(%rbp)
	orl	%eax, %edi
	movl	%edi, -84(%rbp)
	movl	-132(%rbp), %edi
	xorl	%edi, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -132(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-136(%rbp), %edi
	xorl	%edi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -136(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -92(%rbp)
	movl	-140(%rbp), %edi
	xorl	%edi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -140(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -96(%rbp)
	movl	-144(%rbp), %edi
	xorl	%edi, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -144(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -100(%rbp)
	movl	-148(%rbp), %edi
	xorl	%edi, -104(%rbp)
	movl	-104(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -148(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -104(%rbp)
	xorl	%r15d, -108(%rbp)
	movl	-108(%rbp), %ecx
	xorl	%ecx, %r15d
	orl	%r15d, %ecx
	movl	%ecx, -108(%rbp)
	xorl	%r13d, -112(%rbp)
	movl	-112(%rbp), %ecx
	xorl	%ecx, %r13d
	orl	%r13d, %ecx
	movl	%ecx, -112(%rbp)
	xorl	%r12d, -116(%rbp)
	movl	-116(%rbp), %ecx
	xorl	%ecx, %r12d
	orl	%r12d, %ecx
	movl	%ecx, -116(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
.L61:
	movq	-176(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -176(%rbp)
	testq	%rax, %rax
	jne	.L62
	movl	-152(%rbp), %edi
	call	use_int@PLT
	movl	-156(%rbp), %edi
	call	use_int@PLT
	movl	-120(%rbp), %edi
	call	use_int@PLT
	movl	-160(%rbp), %edi
	call	use_int@PLT
	movl	-164(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-104(%rbp), %edi
	call	use_int@PLT
	movl	-108(%rbp), %edi
	call	use_int@PLT
	movl	-112(%rbp), %edi
	call	use_int@PLT
	movl	-116(%rbp), %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE23:
	.size	integer_bit_14, .-integer_bit_14
	.globl	integer_bit_15
	.type	integer_bit_15, @function
integer_bit_15:
.LFB24:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -184(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movl	%eax, -160(%rbp)
	addl	$1, %eax
	movl	%eax, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$1, %eax
	movl	%eax, -164(%rbp)
	addl	$2, %eax
	movl	%eax, %esi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$1, %eax
	movl	%eax, -124(%rbp)
	addl	$3, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$1, %eax
	movl	%eax, -168(%rbp)
	addl	$4, %eax
	movl	%eax, %r10d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movl	%eax, -172(%rbp)
	addl	$5, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	addl	$6, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	movl	%eax, -88(%rbp)
	addl	$7, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$1, %eax
	movl	%eax, -92(%rbp)
	addl	$8, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$1, %eax
	movl	%eax, -96(%rbp)
	addl	$9, %eax
	movl	%eax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$1, %eax
	movl	%eax, -100(%rbp)
	addl	$10, %eax
	movl	%eax, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$1, %eax
	movl	%eax, -104(%rbp)
	addl	$11, %eax
	movl	%eax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$1, %eax
	movl	%eax, -108(%rbp)
	addl	$12, %eax
	movl	%eax, -156(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$1, %eax
	movl	%eax, -112(%rbp)
	leal	13(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$1, %eax
	movl	%eax, -116(%rbp)
	leal	14(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	addl	$1, %eax
	movl	%eax, -120(%rbp)
	leal	15(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	leal	1(%rax), %r14d
	leal	16(%r14), %ebx
	jmp	.L64
.L65:
	movl	-176(%rbp), %edx
	xorl	%edx, -160(%rbp)
	movl	-160(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -160(%rbp)
	xorl	%esi, -164(%rbp)
	movl	-164(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -124(%rbp)
	xorl	%r10d, -168(%rbp)
	movl	-168(%rbp), %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-128(%rbp), %eax
	xorl	%eax, -172(%rbp)
	movl	-172(%rbp), %r11d
	xorl	%r11d, %eax
	movl	%eax, -128(%rbp)
	orl	%eax, %r11d
	movl	-132(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -132(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -136(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-140(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -140(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-144(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -144(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	movl	-148(%rbp), %eax
	xorl	%eax, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -148(%rbp)
	orl	%eax, %edi
	movl	%edi, -100(%rbp)
	movl	-152(%rbp), %eax
	xorl	%eax, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -152(%rbp)
	orl	%eax, %edi
	movl	%edi, -104(%rbp)
	movl	-156(%rbp), %eax
	xorl	%eax, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -156(%rbp)
	orl	%eax, %edi
	movl	%edi, -108(%rbp)
	xorl	%r15d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -112(%rbp)
	xorl	%r13d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r13d
	orl	%r13d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r12d, -120(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, %r12d
	orl	%r12d, %eax
	movl	%eax, -120(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, -160(%rbp)
	movl	-160(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -164(%rbp)
	xorl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -124(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-128(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -128(%rbp)
	orl	%ecx, %r11d
	movl	-132(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -132(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-136(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -136(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -140(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	movl	-144(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -144(%rbp)
	orl	%ecx, %edi
	movl	%edi, -96(%rbp)
	movl	-148(%rbp), %ecx
	xorl	%ecx, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -148(%rbp)
	orl	%ecx, %edi
	movl	%edi, -100(%rbp)
	movl	-152(%rbp), %ecx
	xorl	%ecx, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -152(%rbp)
	orl	%ecx, %edi
	movl	%edi, -104(%rbp)
	movl	-156(%rbp), %ecx
	xorl	%ecx, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -156(%rbp)
	orl	%ecx, %edi
	movl	%edi, -108(%rbp)
	xorl	%r15d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -112(%rbp)
	xorl	%r13d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r13d
	orl	%r13d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r12d, -120(%rbp)
	movl	-120(%rbp), %ecx
	xorl	%ecx, %r12d
	orl	%r12d, %ecx
	movl	%ecx, -120(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -160(%rbp)
	xorl	%esi, -164(%rbp)
	movl	-164(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -124(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-128(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -128(%rbp)
	orl	%eax, %r11d
	movl	-132(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -132(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -136(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-140(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -140(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-144(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -144(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	movl	-148(%rbp), %eax
	xorl	%eax, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -148(%rbp)
	orl	%eax, %edi
	movl	%edi, -100(%rbp)
	movl	-152(%rbp), %eax
	xorl	%eax, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -152(%rbp)
	orl	%eax, %edi
	movl	%edi, -104(%rbp)
	movl	-156(%rbp), %eax
	xorl	%eax, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -156(%rbp)
	orl	%eax, %edi
	movl	%edi, -108(%rbp)
	xorl	%r15d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -112(%rbp)
	xorl	%r13d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r13d
	orl	%r13d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r12d, -120(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, %r12d
	orl	%r12d, %eax
	movl	%eax, -120(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, -160(%rbp)
	movl	-160(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -164(%rbp)
	xorl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -124(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-128(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -128(%rbp)
	orl	%ecx, %r11d
	movl	-132(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -132(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-136(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -136(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -140(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	movl	-144(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -144(%rbp)
	orl	%ecx, %edi
	movl	%edi, -96(%rbp)
	movl	-148(%rbp), %ecx
	xorl	%ecx, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -148(%rbp)
	orl	%ecx, %edi
	movl	%edi, -100(%rbp)
	movl	-152(%rbp), %ecx
	xorl	%ecx, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -152(%rbp)
	orl	%ecx, %edi
	movl	%edi, -104(%rbp)
	movl	-156(%rbp), %ecx
	xorl	%ecx, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -156(%rbp)
	orl	%ecx, %edi
	movl	%edi, -108(%rbp)
	xorl	%r15d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -112(%rbp)
	xorl	%r13d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r13d
	orl	%r13d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r12d, -120(%rbp)
	movl	-120(%rbp), %ecx
	xorl	%ecx, %r12d
	orl	%r12d, %ecx
	movl	%ecx, -120(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -160(%rbp)
	xorl	%esi, -164(%rbp)
	movl	-164(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -124(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-128(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -128(%rbp)
	orl	%eax, %r11d
	movl	-132(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -132(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -136(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-140(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -140(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-144(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -144(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	movl	-148(%rbp), %eax
	xorl	%eax, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -148(%rbp)
	orl	%eax, %edi
	movl	%edi, -100(%rbp)
	movl	-152(%rbp), %eax
	xorl	%eax, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -152(%rbp)
	orl	%eax, %edi
	movl	%edi, -104(%rbp)
	movl	-156(%rbp), %eax
	xorl	%eax, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -156(%rbp)
	orl	%eax, %edi
	movl	%edi, -108(%rbp)
	xorl	%r15d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -112(%rbp)
	xorl	%r13d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r13d
	orl	%r13d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r12d, -120(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, %r12d
	orl	%r12d, %eax
	movl	%eax, -120(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, -160(%rbp)
	movl	-160(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -164(%rbp)
	xorl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -124(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-128(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -128(%rbp)
	orl	%ecx, %r11d
	movl	-132(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -132(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -84(%rbp)
	movl	-136(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -136(%rbp)
	orl	%ecx, %edi
	movl	%edi, -88(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -140(%rbp)
	orl	%ecx, %edi
	movl	%edi, -92(%rbp)
	movl	-144(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -144(%rbp)
	orl	%ecx, %edi
	movl	%edi, -96(%rbp)
	movl	-148(%rbp), %ecx
	xorl	%ecx, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -148(%rbp)
	orl	%ecx, %edi
	movl	%edi, -100(%rbp)
	movl	-152(%rbp), %ecx
	xorl	%ecx, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -152(%rbp)
	orl	%ecx, %edi
	movl	%edi, -104(%rbp)
	movl	-156(%rbp), %ecx
	xorl	%ecx, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -156(%rbp)
	orl	%ecx, %edi
	movl	%edi, -108(%rbp)
	xorl	%r15d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -112(%rbp)
	xorl	%r13d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r13d
	orl	%r13d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r12d, -120(%rbp)
	movl	-120(%rbp), %ecx
	xorl	%ecx, %r12d
	orl	%r12d, %ecx
	movl	%ecx, -120(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -160(%rbp)
	xorl	%esi, -164(%rbp)
	movl	-164(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	xorl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -124(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-128(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -128(%rbp)
	orl	%eax, %r11d
	movl	-132(%rbp), %edi
	xorl	%edi, -84(%rbp)
	movl	-84(%rbp), %eax
	xorl	%eax, %edi
	movl	%edi, -132(%rbp)
	orl	%edi, %eax
	movl	%eax, -84(%rbp)
	movl	-136(%rbp), %eax
	xorl	%eax, -88(%rbp)
	movl	-88(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -136(%rbp)
	orl	%eax, %edi
	movl	%edi, -88(%rbp)
	movl	-140(%rbp), %eax
	xorl	%eax, -92(%rbp)
	movl	-92(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -140(%rbp)
	orl	%eax, %edi
	movl	%edi, -92(%rbp)
	movl	-144(%rbp), %eax
	xorl	%eax, -96(%rbp)
	movl	-96(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -144(%rbp)
	orl	%eax, %edi
	movl	%edi, -96(%rbp)
	movl	-148(%rbp), %eax
	xorl	%eax, -100(%rbp)
	movl	-100(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -148(%rbp)
	orl	%eax, %edi
	movl	%edi, -100(%rbp)
	movl	-152(%rbp), %eax
	xorl	%eax, -104(%rbp)
	movl	-104(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -152(%rbp)
	orl	%eax, %edi
	movl	%edi, -104(%rbp)
	movl	-156(%rbp), %eax
	xorl	%eax, -108(%rbp)
	movl	-108(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -156(%rbp)
	orl	%eax, %edi
	movl	%edi, -108(%rbp)
	xorl	%r15d, -112(%rbp)
	movl	-112(%rbp), %edi
	xorl	%edi, %r15d
	orl	%r15d, %edi
	movl	%edi, -112(%rbp)
	xorl	%r13d, -116(%rbp)
	movl	-116(%rbp), %edi
	xorl	%edi, %r13d
	orl	%r13d, %edi
	movl	%edi, -116(%rbp)
	xorl	%r12d, -120(%rbp)
	movl	-120(%rbp), %edi
	xorl	%edi, %r12d
	orl	%r12d, %edi
	movl	%edi, -120(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, -160(%rbp)
	movl	-160(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	movl	%eax, -160(%rbp)
	xorl	%esi, %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -164(%rbp)
	xorl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-128(%rbp), %ecx
	xorl	%ecx, %r11d
	xorl	%r11d, %ecx
	movl	%ecx, -128(%rbp)
	orl	%ecx, %r11d
	movl	-132(%rbp), %eax
	xorl	%eax, -84(%rbp)
	movl	-84(%rbp), %ecx
	xorl	%ecx, %eax
	movl	%eax, -132(%rbp)
	orl	%eax, %ecx
	movl	%ecx, -84(%rbp)
	movl	-136(%rbp), %ecx
	xorl	%ecx, -88(%rbp)
	movl	-88(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -136(%rbp)
	orl	%ecx, %eax
	movl	%eax, -88(%rbp)
	movl	-140(%rbp), %ecx
	xorl	%ecx, -92(%rbp)
	movl	-92(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -140(%rbp)
	orl	%ecx, %eax
	movl	%eax, -92(%rbp)
	movl	-144(%rbp), %ecx
	xorl	%ecx, -96(%rbp)
	movl	-96(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -144(%rbp)
	orl	%ecx, %eax
	movl	%eax, -96(%rbp)
	movl	-148(%rbp), %ecx
	xorl	%ecx, -100(%rbp)
	movl	-100(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -148(%rbp)
	orl	%ecx, %eax
	movl	%eax, -100(%rbp)
	movl	-152(%rbp), %ecx
	xorl	%ecx, -104(%rbp)
	movl	-104(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -152(%rbp)
	orl	%ecx, %eax
	movl	%eax, -104(%rbp)
	movl	-156(%rbp), %ecx
	xorl	%ecx, -108(%rbp)
	movl	-108(%rbp), %eax
	xorl	%eax, %ecx
	movl	%ecx, -156(%rbp)
	orl	%ecx, %eax
	movl	%eax, -108(%rbp)
	xorl	%r15d, -112(%rbp)
	movl	-112(%rbp), %eax
	xorl	%eax, %r15d
	orl	%r15d, %eax
	movl	%eax, -112(%rbp)
	xorl	%r13d, -116(%rbp)
	movl	-116(%rbp), %eax
	xorl	%eax, %r13d
	orl	%r13d, %eax
	movl	%eax, -116(%rbp)
	xorl	%r12d, -120(%rbp)
	movl	-120(%rbp), %eax
	xorl	%eax, %r12d
	orl	%r12d, %eax
	movl	%eax, -120(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, -160(%rbp)
	movl	-160(%rbp), %eax
	xorl	%eax, %edx
	orl	%edx, %eax
	xorl	%esi, -164(%rbp)
	movl	-164(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -164(%rbp)
	xorl	%r8d, %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -124(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	-128(%rbp), %edi
	xorl	%edi, %r11d
	xorl	%r11d, %edi
	movl	%edi, -128(%rbp)
	orl	%edi, %r11d
	movl	-132(%rbp), %ecx
	xorl	%ecx, -84(%rbp)
	movl	-84(%rbp), %edi
	xorl	%edi, %ecx
	movl	%ecx, -132(%rbp)
	orl	%ecx, %edi
	movl	%edi, -84(%rbp)
	movl	-136(%rbp), %edi
	xorl	%edi, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -136(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-140(%rbp), %edi
	xorl	%edi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -140(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -92(%rbp)
	movl	-144(%rbp), %edi
	xorl	%edi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -144(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -96(%rbp)
	movl	-148(%rbp), %edi
	xorl	%edi, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -148(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -100(%rbp)
	movl	-152(%rbp), %edi
	xorl	%edi, -104(%rbp)
	movl	-104(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -152(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -104(%rbp)
	movl	-156(%rbp), %edi
	xorl	%edi, -108(%rbp)
	movl	-108(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -156(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -108(%rbp)
	xorl	%r15d, -112(%rbp)
	movl	-112(%rbp), %ecx
	xorl	%ecx, %r15d
	orl	%r15d, %ecx
	movl	%ecx, -112(%rbp)
	xorl	%r13d, -116(%rbp)
	movl	-116(%rbp), %ecx
	xorl	%ecx, %r13d
	orl	%r13d, %ecx
	movl	%ecx, -116(%rbp)
	xorl	%r12d, -120(%rbp)
	movl	-120(%rbp), %ecx
	xorl	%ecx, %r12d
	orl	%r12d, %ecx
	movl	%ecx, -120(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
	xorl	%edx, %eax
	xorl	%eax, %edx
	movl	%edx, -176(%rbp)
	orl	%edx, %eax
	movl	%eax, -160(%rbp)
	xorl	%esi, -164(%rbp)
	movl	-164(%rbp), %ecx
	xorl	%ecx, %esi
	orl	%esi, %ecx
	movl	%ecx, -164(%rbp)
	xorl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edi
	xorl	%edi, %r8d
	orl	%r8d, %edi
	movl	%edi, -124(%rbp)
	xorl	%r10d, %r9d
	xorl	%r9d, %r10d
	orl	%r10d, %r9d
	movl	%r9d, -168(%rbp)
	movl	-128(%rbp), %eax
	xorl	%eax, %r11d
	xorl	%r11d, %eax
	movl	%eax, -128(%rbp)
	orl	%eax, %r11d
	movl	%r11d, -172(%rbp)
	movl	-132(%rbp), %eax
	xorl	%eax, -84(%rbp)
	movl	-84(%rbp), %edi
	xorl	%edi, %eax
	movl	%eax, -132(%rbp)
	orl	%eax, %edi
	movl	%edi, -84(%rbp)
	movl	-136(%rbp), %edi
	xorl	%edi, -88(%rbp)
	movl	-88(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -136(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-140(%rbp), %edi
	xorl	%edi, -92(%rbp)
	movl	-92(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -140(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -92(%rbp)
	movl	-144(%rbp), %edi
	xorl	%edi, -96(%rbp)
	movl	-96(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -144(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -96(%rbp)
	movl	-148(%rbp), %edi
	xorl	%edi, -100(%rbp)
	movl	-100(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -148(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -100(%rbp)
	movl	-152(%rbp), %edi
	xorl	%edi, -104(%rbp)
	movl	-104(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -152(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -104(%rbp)
	movl	-156(%rbp), %edi
	xorl	%edi, -108(%rbp)
	movl	-108(%rbp), %ecx
	xorl	%ecx, %edi
	movl	%edi, -156(%rbp)
	orl	%edi, %ecx
	movl	%ecx, -108(%rbp)
	xorl	%r15d, -112(%rbp)
	movl	-112(%rbp), %ecx
	xorl	%ecx, %r15d
	orl	%r15d, %ecx
	movl	%ecx, -112(%rbp)
	xorl	%r13d, -116(%rbp)
	movl	-116(%rbp), %ecx
	xorl	%ecx, %r13d
	orl	%r13d, %ecx
	movl	%ecx, -116(%rbp)
	xorl	%r12d, -120(%rbp)
	movl	-120(%rbp), %ecx
	xorl	%ecx, %r12d
	orl	%r12d, %ecx
	movl	%ecx, -120(%rbp)
	xorl	%ebx, %r14d
	xorl	%r14d, %ebx
	orl	%ebx, %r14d
.L64:
	movq	-184(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -184(%rbp)
	testq	%rax, %rax
	jne	.L65
	movl	-160(%rbp), %edi
	call	use_int@PLT
	movl	-164(%rbp), %edi
	call	use_int@PLT
	movl	-124(%rbp), %edi
	call	use_int@PLT
	movl	-168(%rbp), %edi
	call	use_int@PLT
	movl	-172(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-104(%rbp), %edi
	call	use_int@PLT
	movl	-108(%rbp), %edi
	call	use_int@PLT
	movl	-112(%rbp), %edi
	call	use_int@PLT
	movl	-116(%rbp), %edi
	call	use_int@PLT
	movl	-120(%rbp), %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	addq	$152, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE24:
	.size	integer_bit_15, .-integer_bit_15
	.globl	integer_bit_benchmarks
	.section	.data.rel.local
	.align 32
	.type	integer_bit_benchmarks, @object
	.size	integer_bit_benchmarks, 128
integer_bit_benchmarks:
	.quad	integer_bit_0
	.quad	integer_bit_1
	.quad	integer_bit_2
	.quad	integer_bit_3
	.quad	integer_bit_4
	.quad	integer_bit_5
	.quad	integer_bit_6
	.quad	integer_bit_7
	.quad	integer_bit_8
	.quad	integer_bit_9
	.quad	integer_bit_10
	.quad	integer_bit_11
	.quad	integer_bit_12
	.quad	integer_bit_13
	.quad	integer_bit_14
	.quad	integer_bit_15
	.text
	.globl	integer_add_0
	.type	integer_add_0, @function
integer_add_0:
.LFB25:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 13, -24
	.cfi_offset 12, -32
	.cfi_offset 3, -40
	movq	%rdi, -56(%rbp)
	movq	%rsi, -64(%rbp)
	movq	-56(%rbp), %r13
	movq	-64(%rbp), %rax
	movq	%rax, -40(%rbp)
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	leal	57(%rax), %r12d
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	leal	31(%rax), %ebx
	jmp	.L67
.L68:
	addl	%ebx, %r12d
	subl	%r12d, %ebx
	addl	%ebx, %r12d
	subl	%r12d, %ebx
	addl	%ebx, %r12d
	subl	%r12d, %ebx
	addl	%ebx, %r12d
	subl	%r12d, %ebx
	addl	%ebx, %r12d
	subl	%r12d, %ebx
	addl	%ebx, %r12d
	subl	%r12d, %ebx
	addl	%ebx, %r12d
	subl	%r12d, %ebx
	addl	%ebx, %r12d
	subl	%r12d, %ebx
	addl	%ebx, %r12d
	subl	%r12d, %ebx
	addl	%ebx, %r12d
	subl	%r12d, %ebx
.L67:
	movq	%r13, %rax
	leaq	-1(%rax), %r13
	testq	%rax, %rax
	jne	.L68
	leal	(%r12,%rbx), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE25:
	.size	integer_add_0, .-integer_add_0
	.globl	integer_add_1
	.type	integer_add_1, @function
integer_add_1:
.LFB26:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %r15
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	57(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	31(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	57(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	31(%rax), %ebx
	jmp	.L70
.L71:
	addl	%r12d, %r14d
	subl	%r14d, %r12d
	addl	%ebx, %r13d
	subl	%r13d, %ebx
	addl	%r12d, %r14d
	subl	%r14d, %r12d
	addl	%ebx, %r13d
	subl	%r13d, %ebx
	addl	%r12d, %r14d
	subl	%r14d, %r12d
	addl	%ebx, %r13d
	subl	%r13d, %ebx
	addl	%r12d, %r14d
	subl	%r14d, %r12d
	addl	%ebx, %r13d
	subl	%r13d, %ebx
	addl	%r12d, %r14d
	subl	%r14d, %r12d
	addl	%ebx, %r13d
	subl	%r13d, %ebx
	addl	%r12d, %r14d
	subl	%r14d, %r12d
	addl	%ebx, %r13d
	subl	%r13d, %ebx
	addl	%r12d, %r14d
	subl	%r14d, %r12d
	addl	%ebx, %r13d
	subl	%r13d, %ebx
	addl	%r12d, %r14d
	subl	%r14d, %r12d
	addl	%ebx, %r13d
	subl	%r13d, %ebx
	addl	%r12d, %r14d
	subl	%r14d, %r12d
	addl	%ebx, %r13d
	subl	%r13d, %ebx
	addl	%r12d, %r14d
	subl	%r14d, %r12d
	addl	%ebx, %r13d
	subl	%r13d, %ebx
.L70:
	movq	%r15, %rax
	leaq	-1(%rax), %r15
	testq	%rax, %rax
	jne	.L71
	leal	(%r14,%r12), %eax
	movl	%eax, %edi
	call	use_int@PLT
	leal	0(%r13,%rbx), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE26:
	.size	integer_add_1, .-integer_add_1
	.globl	integer_add_2
	.type	integer_add_2, @function
integer_add_2:
.LFB27:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %rsi
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$57, %eax
	movl	%eax, %ecx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	31(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	57(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	31(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	57(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	31(%rax), %ebx
	jmp	.L73
.L74:
	movl	%ecx, %eax
	addl	%r13d, %eax
	subl	%eax, %r13d
	addl	%r12d, %r15d
	subl	%r15d, %r12d
	addl	%ebx, %r14d
	subl	%r14d, %ebx
	addl	%r13d, %eax
	subl	%eax, %r13d
	addl	%r12d, %r15d
	subl	%r15d, %r12d
	addl	%ebx, %r14d
	subl	%r14d, %ebx
	addl	%r13d, %eax
	subl	%eax, %r13d
	addl	%r12d, %r15d
	subl	%r15d, %r12d
	addl	%ebx, %r14d
	subl	%r14d, %ebx
	addl	%r13d, %eax
	subl	%eax, %r13d
	addl	%r12d, %r15d
	subl	%r15d, %r12d
	addl	%ebx, %r14d
	subl	%r14d, %ebx
	addl	%r13d, %eax
	subl	%eax, %r13d
	addl	%r12d, %r15d
	subl	%r15d, %r12d
	addl	%ebx, %r14d
	subl	%r14d, %ebx
	addl	%r13d, %eax
	subl	%eax, %r13d
	addl	%r12d, %r15d
	subl	%r15d, %r12d
	addl	%ebx, %r14d
	subl	%r14d, %ebx
	addl	%r13d, %eax
	subl	%eax, %r13d
	addl	%r12d, %r15d
	subl	%r15d, %r12d
	addl	%ebx, %r14d
	subl	%r14d, %ebx
	addl	%r13d, %eax
	subl	%eax, %r13d
	addl	%r12d, %r15d
	subl	%r15d, %r12d
	addl	%ebx, %r14d
	subl	%r14d, %ebx
	addl	%r13d, %eax
	subl	%eax, %r13d
	addl	%r12d, %r15d
	subl	%r15d, %r12d
	addl	%ebx, %r14d
	subl	%r14d, %ebx
	addl	%r13d, %eax
	movl	%eax, %ecx
	subl	%eax, %r13d
	addl	%r12d, %r15d
	subl	%r15d, %r12d
	addl	%ebx, %r14d
	subl	%r14d, %ebx
.L73:
	movq	%rsi, %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, %rsi
	testq	%rax, %rax
	jne	.L74
	movl	%ecx, %eax
	addl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	leal	(%r15,%r12), %eax
	movl	%eax, %edi
	call	use_int@PLT
	leal	(%r14,%rbx), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE27:
	.size	integer_add_2, .-integer_add_2
	.globl	integer_add_3
	.type	integer_add_3, @function
integer_add_3:
.LFB28:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %rsi
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$57, %eax
	movl	%eax, %edi
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	31(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$57, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	31(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$57, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	31(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	57(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	31(%rax), %ebx
	jmp	.L76
.L77:
	movl	%edi, %eax
	addl	%r14d, %eax
	subl	%eax, %r14d
	addl	%r13d, -84(%rbp)
	movl	-84(%rbp), %edx
	subl	%edx, %r13d
	addl	%r12d, -88(%rbp)
	movl	-88(%rbp), %ecx
	subl	%ecx, %r12d
	addl	%ebx, %r15d
	subl	%r15d, %ebx
	addl	%r14d, %eax
	subl	%eax, %r14d
	addl	%r13d, %edx
	subl	%edx, %r13d
	addl	%r12d, %ecx
	subl	%ecx, %r12d
	addl	%ebx, %r15d
	subl	%r15d, %ebx
	addl	%r14d, %eax
	subl	%eax, %r14d
	addl	%r13d, %edx
	subl	%edx, %r13d
	addl	%r12d, %ecx
	subl	%ecx, %r12d
	addl	%ebx, %r15d
	subl	%r15d, %ebx
	addl	%r14d, %eax
	subl	%eax, %r14d
	addl	%r13d, %edx
	subl	%edx, %r13d
	addl	%r12d, %ecx
	subl	%ecx, %r12d
	addl	%ebx, %r15d
	subl	%r15d, %ebx
	addl	%r14d, %eax
	subl	%eax, %r14d
	addl	%r13d, %edx
	subl	%edx, %r13d
	addl	%r12d, %ecx
	subl	%ecx, %r12d
	addl	%ebx, %r15d
	subl	%r15d, %ebx
	addl	%r14d, %eax
	subl	%eax, %r14d
	addl	%r13d, %edx
	subl	%edx, %r13d
	addl	%r12d, %ecx
	subl	%ecx, %r12d
	addl	%ebx, %r15d
	subl	%r15d, %ebx
	addl	%r14d, %eax
	subl	%eax, %r14d
	addl	%r13d, %edx
	subl	%edx, %r13d
	addl	%r12d, %ecx
	subl	%ecx, %r12d
	addl	%ebx, %r15d
	subl	%r15d, %ebx
	addl	%r14d, %eax
	subl	%eax, %r14d
	addl	%r13d, %edx
	subl	%edx, %r13d
	addl	%r12d, %ecx
	subl	%ecx, %r12d
	addl	%ebx, %r15d
	subl	%r15d, %ebx
	addl	%r14d, %eax
	subl	%eax, %r14d
	addl	%r13d, %edx
	subl	%edx, %r13d
	addl	%r12d, %ecx
	subl	%ecx, %r12d
	addl	%ebx, %r15d
	subl	%r15d, %ebx
	addl	%r14d, %eax
	movl	%eax, %edi
	subl	%eax, %r14d
	addl	%r13d, %edx
	movl	%edx, -84(%rbp)
	subl	%edx, %r13d
	addl	%r12d, %ecx
	movl	%ecx, -88(%rbp)
	subl	%ecx, %r12d
	addl	%ebx, %r15d
	subl	%r15d, %ebx
.L76:
	movq	%rsi, %rax
	leaq	-1(%rax), %rsi
	testq	%rax, %rax
	jne	.L77
	movl	%edi, %eax
	addl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-84(%rbp), %eax
	addl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	addl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	leal	(%r15,%rbx), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE28:
	.size	integer_add_3, .-integer_add_3
	.globl	integer_add_4
	.type	integer_add_4, @function
integer_add_4:
.LFB29:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r9
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$57, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	31(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$57, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	31(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$57, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	31(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$57, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	31(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$57, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	31(%rax), %ebx
	jmp	.L79
.L80:
	movl	%r8d, %eax
	addl	%r15d, %eax
	subl	%eax, %r15d
	addl	%r14d, -84(%rbp)
	movl	-84(%rbp), %edx
	subl	%edx, %r14d
	addl	%r13d, -88(%rbp)
	movl	-88(%rbp), %ecx
	subl	%ecx, %r13d
	addl	%r12d, -92(%rbp)
	movl	-92(%rbp), %esi
	subl	%esi, %r12d
	addl	%ebx, -96(%rbp)
	movl	-96(%rbp), %edi
	subl	%edi, %ebx
	addl	%r15d, %eax
	subl	%eax, %r15d
	addl	%r14d, %edx
	subl	%edx, %r14d
	addl	%r13d, %ecx
	subl	%ecx, %r13d
	addl	%r12d, %esi
	subl	%esi, %r12d
	addl	%ebx, %edi
	subl	%edi, %ebx
	addl	%r15d, %eax
	subl	%eax, %r15d
	addl	%r14d, %edx
	subl	%edx, %r14d
	addl	%r13d, %ecx
	subl	%ecx, %r13d
	addl	%r12d, %esi
	subl	%esi, %r12d
	addl	%ebx, %edi
	subl	%edi, %ebx
	addl	%r15d, %eax
	subl	%eax, %r15d
	addl	%r14d, %edx
	subl	%edx, %r14d
	addl	%r13d, %ecx
	subl	%ecx, %r13d
	addl	%r12d, %esi
	subl	%esi, %r12d
	addl	%ebx, %edi
	subl	%edi, %ebx
	addl	%r15d, %eax
	subl	%eax, %r15d
	addl	%r14d, %edx
	subl	%edx, %r14d
	addl	%r13d, %ecx
	subl	%ecx, %r13d
	addl	%r12d, %esi
	subl	%esi, %r12d
	addl	%ebx, %edi
	subl	%edi, %ebx
	addl	%r15d, %eax
	subl	%eax, %r15d
	addl	%r14d, %edx
	subl	%edx, %r14d
	addl	%r13d, %ecx
	subl	%ecx, %r13d
	addl	%r12d, %esi
	subl	%esi, %r12d
	addl	%ebx, %edi
	subl	%edi, %ebx
	addl	%r15d, %eax
	subl	%eax, %r15d
	addl	%r14d, %edx
	subl	%edx, %r14d
	addl	%r13d, %ecx
	subl	%ecx, %r13d
	addl	%r12d, %esi
	subl	%esi, %r12d
	addl	%ebx, %edi
	subl	%edi, %ebx
	addl	%r15d, %eax
	subl	%eax, %r15d
	addl	%r14d, %edx
	subl	%edx, %r14d
	addl	%r13d, %ecx
	subl	%ecx, %r13d
	addl	%r12d, %esi
	subl	%esi, %r12d
	addl	%ebx, %edi
	subl	%edi, %ebx
	addl	%r15d, %eax
	subl	%eax, %r15d
	addl	%r14d, %edx
	subl	%edx, %r14d
	addl	%r13d, %ecx
	subl	%ecx, %r13d
	addl	%r12d, %esi
	subl	%esi, %r12d
	addl	%ebx, %edi
	subl	%edi, %ebx
	addl	%r15d, %eax
	movl	%eax, %r8d
	subl	%eax, %r15d
	addl	%r14d, %edx
	movl	%edx, -84(%rbp)
	subl	%edx, %r14d
	addl	%r13d, %ecx
	movl	%ecx, -88(%rbp)
	subl	%ecx, %r13d
	addl	%r12d, %esi
	movl	%esi, -92(%rbp)
	subl	%esi, %r12d
	addl	%ebx, %edi
	movl	%edi, -96(%rbp)
	subl	%edi, %ebx
.L79:
	movq	%r9, %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, %r9
	testq	%rax, %rax
	jne	.L80
	movl	%r8d, %eax
	addl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-84(%rbp), %eax
	addl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	addl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-92(%rbp), %eax
	addl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	addl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE29:
	.size	integer_add_4, .-integer_add_4
	.globl	integer_add_5
	.type	integer_add_5, @function
integer_add_5:
.LFB30:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r11
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$57, %eax
	movl	%eax, %r10d
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$31, %eax
	movl	%eax, %edx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$57, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	31(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$57, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	31(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$57, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	31(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$57, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	31(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$57, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	31(%rax), %ebx
	jmp	.L82
.L83:
	movl	%r10d, %eax
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %ecx
	subl	%ecx, %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %esi
	subl	%esi, %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edi
	subl	%edi, %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %r8d
	subl	%r8d, %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r9d
	subl	%r9d, %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%r15d, %ecx
	subl	%ecx, %r15d
	addl	%r14d, %esi
	subl	%esi, %r14d
	addl	%r13d, %edi
	subl	%edi, %r13d
	addl	%r12d, %r8d
	subl	%r8d, %r12d
	addl	%ebx, %r9d
	subl	%r9d, %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%r15d, %ecx
	subl	%ecx, %r15d
	addl	%r14d, %esi
	subl	%esi, %r14d
	addl	%r13d, %edi
	subl	%edi, %r13d
	addl	%r12d, %r8d
	subl	%r8d, %r12d
	addl	%ebx, %r9d
	subl	%r9d, %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%r15d, %ecx
	subl	%ecx, %r15d
	addl	%r14d, %esi
	subl	%esi, %r14d
	addl	%r13d, %edi
	subl	%edi, %r13d
	addl	%r12d, %r8d
	subl	%r8d, %r12d
	addl	%ebx, %r9d
	subl	%r9d, %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%r15d, %ecx
	subl	%ecx, %r15d
	addl	%r14d, %esi
	subl	%esi, %r14d
	addl	%r13d, %edi
	subl	%edi, %r13d
	addl	%r12d, %r8d
	subl	%r8d, %r12d
	addl	%ebx, %r9d
	subl	%r9d, %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%r15d, %ecx
	subl	%ecx, %r15d
	addl	%r14d, %esi
	subl	%esi, %r14d
	addl	%r13d, %edi
	subl	%edi, %r13d
	addl	%r12d, %r8d
	subl	%r8d, %r12d
	addl	%ebx, %r9d
	subl	%r9d, %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%r15d, %ecx
	subl	%ecx, %r15d
	addl	%r14d, %esi
	subl	%esi, %r14d
	addl	%r13d, %edi
	subl	%edi, %r13d
	addl	%r12d, %r8d
	subl	%r8d, %r12d
	addl	%ebx, %r9d
	subl	%r9d, %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%r15d, %ecx
	subl	%ecx, %r15d
	addl	%r14d, %esi
	subl	%esi, %r14d
	addl	%r13d, %edi
	subl	%edi, %r13d
	addl	%r12d, %r8d
	subl	%r8d, %r12d
	addl	%ebx, %r9d
	subl	%r9d, %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%r15d, %ecx
	subl	%ecx, %r15d
	addl	%r14d, %esi
	subl	%esi, %r14d
	addl	%r13d, %edi
	subl	%edi, %r13d
	addl	%r12d, %r8d
	subl	%r8d, %r12d
	addl	%ebx, %r9d
	subl	%r9d, %ebx
	addl	%edx, %eax
	movl	%eax, %r10d
	subl	%eax, %edx
	addl	%r15d, %ecx
	movl	%ecx, -84(%rbp)
	subl	%ecx, %r15d
	addl	%r14d, %esi
	movl	%esi, -88(%rbp)
	subl	%esi, %r14d
	addl	%r13d, %edi
	movl	%edi, -92(%rbp)
	subl	%edi, %r13d
	addl	%r12d, %r8d
	movl	%r8d, -96(%rbp)
	subl	%r8d, %r12d
	addl	%ebx, %r9d
	movl	%r9d, -100(%rbp)
	subl	%r9d, %ebx
.L82:
	movq	%r11, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %r11
	testq	%rax, %rax
	jne	.L83
	movl	%r10d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-84(%rbp), %eax
	addl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	addl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-92(%rbp), %eax
	addl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	addl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-100(%rbp), %eax
	addl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE30:
	.size	integer_add_5, .-integer_add_5
	.globl	integer_add_6
	.type	integer_add_6, @function
integer_add_6:
.LFB31:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -120(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$57, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$31, %eax
	movl	%eax, %edx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$57, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$31, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$57, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	31(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$57, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	31(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$57, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	31(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$57, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	31(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	31(%rax), %ebx
	jmp	.L85
.L86:
	addl	%edx, -84(%rbp)
	movl	-84(%rbp), %eax
	subl	%eax, %edx
	movl	-112(%rbp), %esi
	addl	%esi, -88(%rbp)
	movl	-88(%rbp), %ecx
	subl	%ecx, %esi
	addl	%r15d, -92(%rbp)
	movl	-92(%rbp), %edi
	subl	%edi, %r15d
	addl	%r14d, -96(%rbp)
	movl	-96(%rbp), %r8d
	subl	%r8d, %r14d
	addl	%r13d, -100(%rbp)
	movl	-100(%rbp), %r9d
	subl	%r9d, %r13d
	addl	%r12d, -104(%rbp)
	movl	-104(%rbp), %r10d
	subl	%r10d, %r12d
	addl	%ebx, -108(%rbp)
	movl	-108(%rbp), %r11d
	subl	%r11d, %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r15d, %edi
	subl	%edi, %r15d
	addl	%r14d, %r8d
	subl	%r8d, %r14d
	addl	%r13d, %r9d
	subl	%r9d, %r13d
	addl	%r12d, %r10d
	subl	%r10d, %r12d
	addl	%ebx, %r11d
	subl	%r11d, %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r15d, %edi
	subl	%edi, %r15d
	addl	%r14d, %r8d
	subl	%r8d, %r14d
	addl	%r13d, %r9d
	subl	%r9d, %r13d
	addl	%r12d, %r10d
	subl	%r10d, %r12d
	addl	%ebx, %r11d
	subl	%r11d, %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r15d, %edi
	subl	%edi, %r15d
	addl	%r14d, %r8d
	subl	%r8d, %r14d
	addl	%r13d, %r9d
	subl	%r9d, %r13d
	addl	%r12d, %r10d
	subl	%r10d, %r12d
	addl	%ebx, %r11d
	subl	%r11d, %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r15d, %edi
	subl	%edi, %r15d
	addl	%r14d, %r8d
	subl	%r8d, %r14d
	addl	%r13d, %r9d
	subl	%r9d, %r13d
	addl	%r12d, %r10d
	subl	%r10d, %r12d
	addl	%ebx, %r11d
	subl	%r11d, %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r15d, %edi
	subl	%edi, %r15d
	addl	%r14d, %r8d
	subl	%r8d, %r14d
	addl	%r13d, %r9d
	subl	%r9d, %r13d
	addl	%r12d, %r10d
	subl	%r10d, %r12d
	addl	%ebx, %r11d
	subl	%r11d, %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r15d, %edi
	subl	%edi, %r15d
	addl	%r14d, %r8d
	subl	%r8d, %r14d
	addl	%r13d, %r9d
	subl	%r9d, %r13d
	addl	%r12d, %r10d
	subl	%r10d, %r12d
	addl	%ebx, %r11d
	subl	%r11d, %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r15d, %edi
	subl	%edi, %r15d
	addl	%r14d, %r8d
	subl	%r8d, %r14d
	addl	%r13d, %r9d
	subl	%r9d, %r13d
	addl	%r12d, %r10d
	subl	%r10d, %r12d
	addl	%ebx, %r11d
	subl	%r11d, %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r15d, %edi
	subl	%edi, %r15d
	addl	%r14d, %r8d
	subl	%r8d, %r14d
	addl	%r13d, %r9d
	subl	%r9d, %r13d
	addl	%r12d, %r10d
	subl	%r10d, %r12d
	addl	%ebx, %r11d
	subl	%r11d, %ebx
	addl	%edx, %eax
	movl	%eax, -84(%rbp)
	subl	%eax, %edx
	addl	%esi, %ecx
	movl	%ecx, -88(%rbp)
	subl	%ecx, %esi
	movl	%esi, -112(%rbp)
	addl	%r15d, %edi
	movl	%edi, -92(%rbp)
	subl	%edi, %r15d
	addl	%r14d, %r8d
	movl	%r8d, -96(%rbp)
	subl	%r8d, %r14d
	addl	%r13d, %r9d
	movl	%r9d, -100(%rbp)
	subl	%r9d, %r13d
	addl	%r12d, %r10d
	movl	%r10d, -104(%rbp)
	subl	%r10d, %r12d
	addl	%ebx, %r11d
	movl	%r11d, -108(%rbp)
	subl	%r11d, %ebx
.L85:
	movq	-120(%rbp), %rax
	leaq	-1(%rax), %rdi
	movq	%rdi, -120(%rbp)
	testq	%rax, %rax
	jne	.L86
	movl	-84(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	addl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-92(%rbp), %eax
	addl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	addl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-100(%rbp), %eax
	addl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	addl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-108(%rbp), %eax
	addl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE31:
	.size	integer_add_6, .-integer_add_6
	.globl	integer_add_7
	.type	integer_add_7, @function
integer_add_7:
.LFB32:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -136(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$57, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$31, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$57, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$31, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$57, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$31, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$57, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	31(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$57, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	31(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$57, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	31(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	31(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$57, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	31(%rax), %ebx
	jmp	.L88
.L89:
	movl	-116(%rbp), %edx
	addl	%edx, -92(%rbp)
	movl	-92(%rbp), %eax
	subl	%eax, %edx
	movl	-120(%rbp), %esi
	addl	%esi, -96(%rbp)
	movl	-96(%rbp), %ecx
	subl	%ecx, %esi
	movl	-124(%rbp), %r8d
	addl	%r8d, -100(%rbp)
	movl	-100(%rbp), %edi
	subl	%edi, %r8d
	addl	%r15d, -104(%rbp)
	movl	-104(%rbp), %r9d
	subl	%r9d, %r15d
	addl	%r14d, -108(%rbp)
	movl	-108(%rbp), %r10d
	subl	%r10d, %r14d
	addl	%r13d, -112(%rbp)
	movl	-112(%rbp), %r11d
	subl	%r11d, %r13d
	addl	%r12d, -84(%rbp)
	subl	-84(%rbp), %r12d
	addl	%ebx, -88(%rbp)
	subl	-88(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r15d, %r9d
	subl	%r9d, %r15d
	addl	%r14d, %r10d
	subl	%r10d, %r14d
	addl	%r13d, %r11d
	subl	%r11d, %r13d
	addl	%r12d, -84(%rbp)
	subl	-84(%rbp), %r12d
	addl	%ebx, -88(%rbp)
	subl	-88(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r15d, %r9d
	subl	%r9d, %r15d
	addl	%r14d, %r10d
	subl	%r10d, %r14d
	addl	%r13d, %r11d
	subl	%r11d, %r13d
	addl	%r12d, -84(%rbp)
	subl	-84(%rbp), %r12d
	addl	%ebx, -88(%rbp)
	subl	-88(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r15d, %r9d
	subl	%r9d, %r15d
	addl	%r14d, %r10d
	subl	%r10d, %r14d
	addl	%r13d, %r11d
	subl	%r11d, %r13d
	addl	%r12d, -84(%rbp)
	subl	-84(%rbp), %r12d
	addl	%ebx, -88(%rbp)
	subl	-88(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r15d, %r9d
	subl	%r9d, %r15d
	addl	%r14d, %r10d
	subl	%r10d, %r14d
	addl	%r13d, %r11d
	subl	%r11d, %r13d
	addl	%r12d, -84(%rbp)
	subl	-84(%rbp), %r12d
	addl	%ebx, -88(%rbp)
	subl	-88(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r15d, %r9d
	subl	%r9d, %r15d
	addl	%r14d, %r10d
	subl	%r10d, %r14d
	addl	%r13d, %r11d
	subl	%r11d, %r13d
	addl	%r12d, -84(%rbp)
	subl	-84(%rbp), %r12d
	addl	%ebx, -88(%rbp)
	subl	-88(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r15d, %r9d
	subl	%r9d, %r15d
	addl	%r14d, %r10d
	subl	%r10d, %r14d
	addl	%r13d, %r11d
	subl	%r11d, %r13d
	addl	%r12d, -84(%rbp)
	subl	-84(%rbp), %r12d
	addl	%ebx, -88(%rbp)
	subl	-88(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r15d, %r9d
	subl	%r9d, %r15d
	addl	%r14d, %r10d
	subl	%r10d, %r14d
	addl	%r13d, %r11d
	subl	%r11d, %r13d
	addl	%r12d, -84(%rbp)
	subl	-84(%rbp), %r12d
	addl	%ebx, -88(%rbp)
	subl	-88(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r15d, %r9d
	subl	%r9d, %r15d
	addl	%r14d, %r10d
	subl	%r10d, %r14d
	addl	%r13d, %r11d
	subl	%r11d, %r13d
	addl	%r12d, -84(%rbp)
	subl	-84(%rbp), %r12d
	addl	%ebx, -88(%rbp)
	subl	-88(%rbp), %ebx
	addl	%edx, %eax
	movl	%eax, -92(%rbp)
	subl	%eax, %edx
	movl	%edx, -116(%rbp)
	addl	%esi, %ecx
	movl	%ecx, -96(%rbp)
	subl	%ecx, %esi
	movl	%esi, -120(%rbp)
	addl	%r8d, %edi
	movl	%edi, -100(%rbp)
	subl	%edi, %r8d
	movl	%r8d, -124(%rbp)
	addl	%r15d, %r9d
	movl	%r9d, -104(%rbp)
	subl	%r9d, %r15d
	addl	%r14d, %r10d
	movl	%r10d, -108(%rbp)
	subl	%r10d, %r14d
	addl	%r13d, %r11d
	movl	%r11d, -112(%rbp)
	subl	%r11d, %r13d
	addl	%r12d, -84(%rbp)
	movl	-84(%rbp), %eax
	movl	%eax, -84(%rbp)
	subl	%eax, %r12d
	addl	%ebx, -88(%rbp)
	movl	-88(%rbp), %eax
	movl	%eax, -88(%rbp)
	subl	%eax, %ebx
.L88:
	movq	-136(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -136(%rbp)
	testq	%rax, %rax
	jne	.L89
	movl	-92(%rbp), %eax
	addl	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	addl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-100(%rbp), %eax
	addl	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	addl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-108(%rbp), %eax
	addl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	addl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-84(%rbp), %eax
	addl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	addl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE32:
	.size	integer_add_7, .-integer_add_7
	.globl	integer_add_8
	.type	integer_add_8, @function
integer_add_8:
.LFB33:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -144(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$57, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$31, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$57, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$31, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$57, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$31, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$57, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$31, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$57, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	31(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$57, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	31(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	31(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$57, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	31(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$57, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	31(%rax), %ebx
	jmp	.L91
.L92:
	movl	-120(%rbp), %edx
	addl	%edx, -100(%rbp)
	movl	-100(%rbp), %eax
	subl	%eax, %edx
	movl	-124(%rbp), %esi
	addl	%esi, -104(%rbp)
	movl	-104(%rbp), %ecx
	subl	%ecx, %esi
	movl	-128(%rbp), %r8d
	addl	%r8d, -108(%rbp)
	movl	-108(%rbp), %edi
	subl	%edi, %r8d
	movl	-132(%rbp), %r10d
	addl	%r10d, -112(%rbp)
	movl	-112(%rbp), %r9d
	subl	%r9d, %r10d
	addl	%r15d, -116(%rbp)
	movl	-116(%rbp), %r11d
	subl	%r11d, %r15d
	addl	%r14d, -84(%rbp)
	subl	-84(%rbp), %r14d
	addl	%r13d, -88(%rbp)
	subl	-88(%rbp), %r13d
	addl	%r12d, -92(%rbp)
	subl	-92(%rbp), %r12d
	addl	%ebx, -96(%rbp)
	subl	-96(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	addl	%r15d, %r11d
	subl	%r11d, %r15d
	addl	%r14d, -84(%rbp)
	subl	-84(%rbp), %r14d
	addl	%r13d, -88(%rbp)
	subl	-88(%rbp), %r13d
	addl	%r12d, -92(%rbp)
	subl	-92(%rbp), %r12d
	addl	%ebx, -96(%rbp)
	subl	-96(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	addl	%r15d, %r11d
	subl	%r11d, %r15d
	addl	%r14d, -84(%rbp)
	subl	-84(%rbp), %r14d
	addl	%r13d, -88(%rbp)
	subl	-88(%rbp), %r13d
	addl	%r12d, -92(%rbp)
	subl	-92(%rbp), %r12d
	addl	%ebx, -96(%rbp)
	subl	-96(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	addl	%r15d, %r11d
	subl	%r11d, %r15d
	addl	%r14d, -84(%rbp)
	subl	-84(%rbp), %r14d
	addl	%r13d, -88(%rbp)
	subl	-88(%rbp), %r13d
	addl	%r12d, -92(%rbp)
	subl	-92(%rbp), %r12d
	addl	%ebx, -96(%rbp)
	subl	-96(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	addl	%r15d, %r11d
	subl	%r11d, %r15d
	addl	%r14d, -84(%rbp)
	subl	-84(%rbp), %r14d
	addl	%r13d, -88(%rbp)
	subl	-88(%rbp), %r13d
	addl	%r12d, -92(%rbp)
	subl	-92(%rbp), %r12d
	addl	%ebx, -96(%rbp)
	subl	-96(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	addl	%r15d, %r11d
	subl	%r11d, %r15d
	addl	%r14d, -84(%rbp)
	subl	-84(%rbp), %r14d
	addl	%r13d, -88(%rbp)
	subl	-88(%rbp), %r13d
	addl	%r12d, -92(%rbp)
	subl	-92(%rbp), %r12d
	addl	%ebx, -96(%rbp)
	subl	-96(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	addl	%r15d, %r11d
	subl	%r11d, %r15d
	addl	%r14d, -84(%rbp)
	subl	-84(%rbp), %r14d
	addl	%r13d, -88(%rbp)
	subl	-88(%rbp), %r13d
	addl	%r12d, -92(%rbp)
	subl	-92(%rbp), %r12d
	addl	%ebx, -96(%rbp)
	subl	-96(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	addl	%r15d, %r11d
	subl	%r11d, %r15d
	addl	%r14d, -84(%rbp)
	subl	-84(%rbp), %r14d
	addl	%r13d, -88(%rbp)
	subl	-88(%rbp), %r13d
	addl	%r12d, -92(%rbp)
	subl	-92(%rbp), %r12d
	addl	%ebx, -96(%rbp)
	subl	-96(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	addl	%r15d, %r11d
	subl	%r11d, %r15d
	addl	%r14d, -84(%rbp)
	subl	-84(%rbp), %r14d
	addl	%r13d, -88(%rbp)
	subl	-88(%rbp), %r13d
	addl	%r12d, -92(%rbp)
	subl	-92(%rbp), %r12d
	addl	%ebx, -96(%rbp)
	subl	-96(%rbp), %ebx
	addl	%edx, %eax
	movl	%eax, -100(%rbp)
	subl	%eax, %edx
	movl	%edx, -120(%rbp)
	addl	%esi, %ecx
	movl	%ecx, -104(%rbp)
	subl	%ecx, %esi
	movl	%esi, -124(%rbp)
	addl	%r8d, %edi
	movl	%edi, -108(%rbp)
	subl	%edi, %r8d
	movl	%r8d, -128(%rbp)
	addl	%r10d, %r9d
	movl	%r9d, -112(%rbp)
	subl	%r9d, %r10d
	movl	%r10d, -132(%rbp)
	addl	%r15d, %r11d
	movl	%r11d, -116(%rbp)
	subl	%r11d, %r15d
	addl	%r14d, -84(%rbp)
	movl	-84(%rbp), %eax
	movl	%eax, -84(%rbp)
	subl	%eax, %r14d
	addl	%r13d, -88(%rbp)
	movl	-88(%rbp), %eax
	movl	%eax, -88(%rbp)
	subl	%eax, %r13d
	addl	%r12d, -92(%rbp)
	movl	-92(%rbp), %eax
	movl	%eax, -92(%rbp)
	subl	%eax, %r12d
	addl	%ebx, -96(%rbp)
	movl	-96(%rbp), %eax
	movl	%eax, -96(%rbp)
	subl	%eax, %ebx
.L91:
	movq	-144(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -144(%rbp)
	testq	%rax, %rax
	jne	.L92
	movl	-100(%rbp), %eax
	addl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	addl	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-108(%rbp), %eax
	addl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	addl	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-116(%rbp), %eax
	addl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-84(%rbp), %eax
	addl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	addl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-92(%rbp), %eax
	addl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	addl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE33:
	.size	integer_add_8, .-integer_add_8
	.globl	integer_add_9
	.type	integer_add_9, @function
integer_add_9:
.LFB34:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -152(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$57, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$31, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$57, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$31, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$57, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$31, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$57, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$31, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$57, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$31, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$57, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	31(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	31(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$57, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	31(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$57, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	31(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$57, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	31(%rax), %ebx
	jmp	.L94
.L95:
	movl	-108(%rbp), %edx
	addl	%edx, -116(%rbp)
	movl	-116(%rbp), %eax
	subl	%eax, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %esi
	addl	%esi, -120(%rbp)
	movl	-120(%rbp), %ecx
	subl	%ecx, %esi
	movl	-136(%rbp), %r8d
	addl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edi
	subl	%edi, %r8d
	movl	-140(%rbp), %r10d
	addl	%r10d, -128(%rbp)
	movl	-128(%rbp), %r9d
	subl	%r9d, %r10d
	movl	-104(%rbp), %edx
	addl	%edx, -132(%rbp)
	movl	-132(%rbp), %r11d
	subl	%r11d, %edx
	movl	%edx, -104(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edx
	movl	%edx, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-108(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -112(%rbp)
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-104(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -104(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %esi
	movl	%esi, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %esi
	movl	%esi, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-104(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -104(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edx
	movl	%edx, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-108(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -112(%rbp)
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-104(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -104(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %esi
	movl	%esi, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %esi
	movl	%esi, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-104(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -104(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edx
	movl	%edx, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-108(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -112(%rbp)
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-104(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -104(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %esi
	movl	%esi, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %esi
	movl	%esi, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-104(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -104(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edx
	movl	%edx, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-108(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -112(%rbp)
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-104(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -104(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %esi
	movl	%esi, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %esi
	movl	%esi, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	addl	%r8d, %edi
	subl	%edi, %r8d
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-104(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -104(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edx
	movl	%edx, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-108(%rbp), %edx
	addl	%edx, %eax
	movl	%eax, -116(%rbp)
	subl	%eax, %edx
	movl	%edx, -108(%rbp)
	addl	%esi, %ecx
	movl	%ecx, -120(%rbp)
	subl	%ecx, %esi
	movl	%esi, -112(%rbp)
	addl	%r8d, %edi
	movl	%edi, -124(%rbp)
	subl	%edi, %r8d
	movl	%r8d, -136(%rbp)
	addl	%r10d, %r9d
	movl	%r9d, -128(%rbp)
	subl	%r9d, %r10d
	movl	%r10d, -140(%rbp)
	movl	-104(%rbp), %eax
	addl	%eax, %r11d
	movl	%r11d, -132(%rbp)
	subl	%r11d, %eax
	movl	%eax, -104(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	%edx, %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edx
	movl	%edx, -88(%rbp)
	subl	%edx, %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	%edx, %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	%edx, %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	%edx, %ebx
.L94:
	movq	-152(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -152(%rbp)
	testq	%rax, %rax
	jne	.L95
	movl	-116(%rbp), %eax
	addl	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %eax
	addl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-124(%rbp), %eax
	addl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %eax
	addl	-140(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-132(%rbp), %eax
	addl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-84(%rbp), %eax
	addl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	addl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-92(%rbp), %eax
	addl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	addl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-100(%rbp), %eax
	addl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE34:
	.size	integer_add_9, .-integer_add_9
	.globl	integer_add_10
	.type	integer_add_10, @function
integer_add_10:
.LFB35:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -160(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$57, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$31, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$57, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$31, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$57, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$31, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$57, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$31, %eax
	movl	%eax, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$57, %eax
	movl	%eax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$31, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$57, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$31, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	31(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$57, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	31(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$57, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	31(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$57, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	31(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$57, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	31(%rax), %ebx
	jmp	.L97
.L98:
	movl	-120(%rbp), %edx
	addl	%edx, -128(%rbp)
	movl	-128(%rbp), %eax
	subl	%eax, %edx
	movl	%edx, -120(%rbp)
	movl	-124(%rbp), %esi
	addl	%esi, -132(%rbp)
	movl	-132(%rbp), %ecx
	subl	%ecx, %esi
	movl	-108(%rbp), %r8d
	addl	%r8d, -136(%rbp)
	movl	-136(%rbp), %edi
	subl	%edi, %r8d
	movl	%r8d, -108(%rbp)
	movl	-148(%rbp), %r10d
	addl	%r10d, -140(%rbp)
	movl	-140(%rbp), %r9d
	subl	%r9d, %r10d
	movl	-112(%rbp), %edx
	addl	%edx, -144(%rbp)
	movl	-144(%rbp), %r11d
	subl	%r11d, %edx
	movl	%edx, -112(%rbp)
	movl	-116(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edx
	movl	%edx, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %r8d
	movl	%r8d, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-120(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -124(%rbp)
	movl	-108(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -108(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-112(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -112(%rbp)
	movl	-116(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %esi
	movl	%esi, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %r8d
	movl	%r8d, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %esi
	movl	%esi, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -120(%rbp)
	movl	-124(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-108(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -108(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-112(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -112(%rbp)
	movl	-116(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edx
	movl	%edx, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %r8d
	movl	%r8d, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-120(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -124(%rbp)
	movl	-108(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -108(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-112(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -112(%rbp)
	movl	-116(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %esi
	movl	%esi, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %r8d
	movl	%r8d, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %esi
	movl	%esi, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -120(%rbp)
	movl	-124(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-108(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -108(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-112(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -112(%rbp)
	movl	-116(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edx
	movl	%edx, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %r8d
	movl	%r8d, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-120(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -124(%rbp)
	movl	-108(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -108(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-112(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -112(%rbp)
	movl	-116(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %esi
	movl	%esi, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %r8d
	movl	%r8d, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %esi
	movl	%esi, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -120(%rbp)
	movl	-124(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-108(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -108(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-112(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -112(%rbp)
	movl	-116(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edx
	movl	%edx, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %r8d
	movl	%r8d, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-120(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -124(%rbp)
	movl	-108(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -108(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-112(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -112(%rbp)
	movl	-116(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %esi
	movl	%esi, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %r8d
	movl	%r8d, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %esi
	movl	%esi, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -120(%rbp)
	movl	-124(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-108(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -108(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-112(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -112(%rbp)
	movl	-116(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %r8d
	movl	%r8d, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-120(%rbp), %edx
	addl	%edx, %eax
	movl	%eax, -128(%rbp)
	subl	%eax, %edx
	movl	%edx, -120(%rbp)
	addl	%esi, %ecx
	movl	%ecx, -132(%rbp)
	subl	%ecx, %esi
	movl	%esi, -124(%rbp)
	movl	-108(%rbp), %r8d
	addl	%r8d, %edi
	movl	%edi, -136(%rbp)
	subl	%edi, %r8d
	movl	%r8d, -108(%rbp)
	addl	%r10d, %r9d
	movl	%r9d, -140(%rbp)
	subl	%r9d, %r10d
	movl	%r10d, -148(%rbp)
	movl	-112(%rbp), %eax
	addl	%eax, %r11d
	movl	%r11d, -144(%rbp)
	subl	%r11d, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	addl	%eax, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %eax
	movl	%eax, -116(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	subl	%r8d, %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	%r8d, %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	%edx, %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %r8d
	movl	%r8d, -96(%rbp)
	subl	%r8d, %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	%edx, %ebx
.L97:
	movq	-160(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -160(%rbp)
	testq	%rax, %rax
	jne	.L98
	movl	-128(%rbp), %eax
	addl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-132(%rbp), %eax
	addl	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %eax
	addl	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-140(%rbp), %eax
	addl	-148(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %eax
	addl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	addl	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-84(%rbp), %eax
	addl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	addl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-92(%rbp), %eax
	addl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	addl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-100(%rbp), %eax
	addl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE35:
	.size	integer_add_10, .-integer_add_10
	.globl	integer_add_11
	.type	integer_add_11, @function
integer_add_11:
.LFB36:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -168(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$57, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$31, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$57, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$31, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$57, %eax
	movl	%eax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$31, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$57, %eax
	movl	%eax, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$31, %eax
	movl	%eax, -156(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$57, %eax
	movl	%eax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$31, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$57, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$31, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$57, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	31(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$57, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	31(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$57, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	31(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$57, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	31(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$57, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	31(%rax), %ebx
	jmp	.L100
.L101:
	movl	-128(%rbp), %edx
	addl	%edx, -136(%rbp)
	movl	-136(%rbp), %eax
	subl	%eax, %edx
	movl	%edx, -128(%rbp)
	movl	-132(%rbp), %esi
	addl	%esi, -140(%rbp)
	movl	-140(%rbp), %ecx
	subl	%ecx, %esi
	movl	-112(%rbp), %r8d
	addl	%r8d, -144(%rbp)
	movl	-144(%rbp), %edi
	subl	%edi, %r8d
	movl	%r8d, -112(%rbp)
	movl	-156(%rbp), %r10d
	addl	%r10d, -148(%rbp)
	movl	-148(%rbp), %r9d
	subl	%r9d, %r10d
	movl	-116(%rbp), %edx
	addl	%edx, -152(%rbp)
	movl	-152(%rbp), %r11d
	subl	%r11d, %edx
	movl	%edx, -116(%rbp)
	movl	-120(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -120(%rbp)
	movl	-124(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -124(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-128(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -132(%rbp)
	movl	-112(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -112(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-116(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -116(%rbp)
	movl	-120(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -120(%rbp)
	movl	-124(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -124(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -128(%rbp)
	movl	-132(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-112(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -112(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-116(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -116(%rbp)
	movl	-120(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -120(%rbp)
	movl	-124(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -124(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-128(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -132(%rbp)
	movl	-112(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -112(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-116(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -116(%rbp)
	movl	-120(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -120(%rbp)
	movl	-124(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -124(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -128(%rbp)
	movl	-132(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-112(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -112(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-116(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -116(%rbp)
	movl	-120(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -120(%rbp)
	movl	-124(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -124(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-128(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -132(%rbp)
	movl	-112(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -112(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-116(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -116(%rbp)
	movl	-120(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -120(%rbp)
	movl	-124(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -124(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -128(%rbp)
	movl	-132(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-112(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -112(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-116(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -116(%rbp)
	movl	-120(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -120(%rbp)
	movl	-124(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -124(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-128(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -132(%rbp)
	movl	-112(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -112(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-116(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -116(%rbp)
	movl	-120(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -120(%rbp)
	movl	-124(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -124(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -128(%rbp)
	movl	-132(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-112(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -112(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-116(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -116(%rbp)
	movl	-120(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -120(%rbp)
	movl	-124(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -124(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-128(%rbp), %edx
	addl	%edx, %eax
	movl	%eax, -136(%rbp)
	subl	%eax, %edx
	movl	%edx, -128(%rbp)
	addl	%esi, %ecx
	movl	%ecx, -140(%rbp)
	subl	%ecx, %esi
	movl	%esi, -132(%rbp)
	movl	-112(%rbp), %r8d
	addl	%r8d, %edi
	movl	%edi, -144(%rbp)
	subl	%edi, %r8d
	movl	%r8d, -112(%rbp)
	addl	%r10d, %r9d
	movl	%r9d, -148(%rbp)
	subl	%r9d, %r10d
	movl	%r10d, -156(%rbp)
	movl	-116(%rbp), %eax
	addl	%eax, %r11d
	movl	%r11d, -152(%rbp)
	subl	%r11d, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	addl	%eax, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	-124(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -124(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	subl	%r8d, %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	%r8d, %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	%edx, %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	%edx, %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	%r8d, %ebx
.L100:
	movq	-168(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -168(%rbp)
	testq	%rax, %rax
	jne	.L101
	movl	-136(%rbp), %eax
	addl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-140(%rbp), %eax
	addl	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %eax
	addl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-148(%rbp), %eax
	addl	-156(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-152(%rbp), %eax
	addl	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	addl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-108(%rbp), %eax
	addl	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-84(%rbp), %eax
	addl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	addl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-92(%rbp), %eax
	addl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	addl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-100(%rbp), %eax
	addl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE36:
	.size	integer_add_11, .-integer_add_11
	.globl	integer_add_12
	.type	integer_add_12, @function
integer_add_12:
.LFB37:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -176(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$57, %eax
	movl	%eax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$31, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$57, %eax
	movl	%eax, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$31, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$57, %eax
	movl	%eax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$31, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$57, %eax
	movl	%eax, -156(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$31, %eax
	movl	%eax, -164(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$57, %eax
	movl	%eax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$31, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$57, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$31, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$57, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$31, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$57, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	31(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$57, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	31(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$57, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	31(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$57, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	31(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$57, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	leal	31(%rax), %ebx
	jmp	.L103
.L104:
	movl	-136(%rbp), %edx
	addl	%edx, -144(%rbp)
	movl	-144(%rbp), %eax
	subl	%eax, %edx
	movl	%edx, -136(%rbp)
	movl	-140(%rbp), %esi
	addl	%esi, -148(%rbp)
	movl	-148(%rbp), %ecx
	subl	%ecx, %esi
	movl	-116(%rbp), %r8d
	addl	%r8d, -152(%rbp)
	movl	-152(%rbp), %edi
	subl	%edi, %r8d
	movl	%r8d, -116(%rbp)
	movl	-164(%rbp), %r10d
	addl	%r10d, -156(%rbp)
	movl	-156(%rbp), %r9d
	subl	%r9d, %r10d
	movl	-120(%rbp), %edx
	addl	%edx, -160(%rbp)
	movl	-160(%rbp), %r11d
	subl	%r11d, %edx
	movl	%edx, -120(%rbp)
	movl	-124(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -124(%rbp)
	movl	-128(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -128(%rbp)
	movl	-132(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %edx
	movl	%edx, -112(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -132(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-136(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -140(%rbp)
	movl	-116(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-120(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -120(%rbp)
	movl	-124(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -124(%rbp)
	movl	-128(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -128(%rbp)
	movl	-132(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %esi
	movl	%esi, -112(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -132(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -136(%rbp)
	movl	-140(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-116(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-120(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -120(%rbp)
	movl	-124(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -124(%rbp)
	movl	-128(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -128(%rbp)
	movl	-132(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %edx
	movl	%edx, -112(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -132(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-136(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -140(%rbp)
	movl	-116(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-120(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -120(%rbp)
	movl	-124(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -124(%rbp)
	movl	-128(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -128(%rbp)
	movl	-132(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %esi
	movl	%esi, -112(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -132(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -136(%rbp)
	movl	-140(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-116(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-120(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -120(%rbp)
	movl	-124(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -124(%rbp)
	movl	-128(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -128(%rbp)
	movl	-132(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %edx
	movl	%edx, -112(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -132(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-136(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -140(%rbp)
	movl	-116(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-120(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -120(%rbp)
	movl	-124(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -124(%rbp)
	movl	-128(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -128(%rbp)
	movl	-132(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %esi
	movl	%esi, -112(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -132(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -136(%rbp)
	movl	-140(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-116(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-120(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -120(%rbp)
	movl	-124(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -124(%rbp)
	movl	-128(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -128(%rbp)
	movl	-132(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %edx
	movl	%edx, -112(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -132(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-136(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -140(%rbp)
	movl	-116(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-120(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -120(%rbp)
	movl	-124(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -124(%rbp)
	movl	-128(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -128(%rbp)
	movl	-132(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %esi
	movl	%esi, -112(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -132(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -136(%rbp)
	movl	-140(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-116(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-120(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -120(%rbp)
	movl	-124(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -124(%rbp)
	movl	-128(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -128(%rbp)
	movl	-132(%rbp), %edx
	addl	%edx, -112(%rbp)
	movl	-112(%rbp), %r8d
	movl	%r8d, -112(%rbp)
	subl	%r8d, %edx
	movl	%edx, -132(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edx
	movl	%edx, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %r8d
	movl	%r8d, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-136(%rbp), %edx
	addl	%edx, %eax
	movl	%eax, -144(%rbp)
	subl	%eax, %edx
	movl	%edx, -136(%rbp)
	addl	%esi, %ecx
	movl	%ecx, -148(%rbp)
	subl	%ecx, %esi
	movl	%esi, -140(%rbp)
	movl	-116(%rbp), %r8d
	addl	%r8d, %edi
	movl	%edi, -152(%rbp)
	subl	%edi, %r8d
	movl	%r8d, -116(%rbp)
	addl	%r10d, %r9d
	movl	%r9d, -156(%rbp)
	subl	%r9d, %r10d
	movl	%r10d, -164(%rbp)
	movl	-120(%rbp), %eax
	addl	%eax, %r11d
	movl	%r11d, -160(%rbp)
	subl	%r11d, %eax
	movl	%eax, -120(%rbp)
	movl	-124(%rbp), %eax
	addl	%eax, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %eax
	movl	%eax, -124(%rbp)
	movl	-128(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -128(%rbp)
	movl	-132(%rbp), %edx
	addl	%edx, -112(%rbp)
	movl	-112(%rbp), %r8d
	movl	%r8d, -112(%rbp)
	subl	%r8d, %edx
	movl	%edx, -132(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	subl	%r8d, %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edx
	movl	%edx, -88(%rbp)
	subl	%edx, %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %r8d
	movl	%r8d, -92(%rbp)
	subl	%r8d, %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	%edx, %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	%r8d, %ebx
.L103:
	movq	-176(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -176(%rbp)
	testq	%rax, %rax
	jne	.L104
	movl	-144(%rbp), %eax
	addl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-148(%rbp), %eax
	addl	-140(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-152(%rbp), %eax
	addl	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-156(%rbp), %eax
	addl	-164(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-160(%rbp), %eax
	addl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	addl	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-108(%rbp), %eax
	addl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	addl	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-84(%rbp), %eax
	addl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	addl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-92(%rbp), %eax
	addl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	addl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-100(%rbp), %eax
	addl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE37:
	.size	integer_add_12, .-integer_add_12
	.globl	integer_add_13
	.type	integer_add_13, @function
integer_add_13:
.LFB38:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -184(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$57, %eax
	movl	%eax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$31, %eax
	movl	%eax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$57, %eax
	movl	%eax, -156(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$31, %eax
	movl	%eax, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$57, %eax
	movl	%eax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$31, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$57, %eax
	movl	%eax, -164(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$31, %eax
	movl	%eax, -172(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$57, %eax
	movl	%eax, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$31, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$57, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$31, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$57, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$31, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$57, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$31, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$57, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	31(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$57, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	31(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$57, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	31(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$57, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	leal	31(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$57, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	leal	31(%rax), %ebx
	jmp	.L106
.L107:
	movl	-144(%rbp), %edx
	addl	%edx, -152(%rbp)
	movl	-152(%rbp), %eax
	subl	%eax, %edx
	movl	%edx, -144(%rbp)
	movl	-148(%rbp), %esi
	addl	%esi, -156(%rbp)
	movl	-156(%rbp), %ecx
	subl	%ecx, %esi
	movl	-120(%rbp), %r8d
	addl	%r8d, -160(%rbp)
	movl	-160(%rbp), %edi
	subl	%edi, %r8d
	movl	%r8d, -120(%rbp)
	movl	-172(%rbp), %r10d
	addl	%r10d, -164(%rbp)
	movl	-164(%rbp), %r9d
	subl	%r9d, %r10d
	movl	-124(%rbp), %edx
	addl	%edx, -168(%rbp)
	movl	-168(%rbp), %r11d
	subl	%r11d, %edx
	movl	%edx, -124(%rbp)
	movl	-128(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -128(%rbp)
	movl	-132(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -132(%rbp)
	movl	-136(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %edx
	movl	%edx, -112(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -136(%rbp)
	movl	-140(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edx
	movl	%edx, -116(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -140(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-144(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -148(%rbp)
	movl	-120(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -120(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-124(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -124(%rbp)
	movl	-128(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -128(%rbp)
	movl	-132(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -132(%rbp)
	movl	-136(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %esi
	movl	%esi, -112(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -136(%rbp)
	movl	-140(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %esi
	movl	%esi, -116(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -140(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -144(%rbp)
	movl	-148(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-120(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -120(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-124(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -124(%rbp)
	movl	-128(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -128(%rbp)
	movl	-132(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -132(%rbp)
	movl	-136(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %edx
	movl	%edx, -112(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -136(%rbp)
	movl	-140(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edx
	movl	%edx, -116(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -140(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-144(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -148(%rbp)
	movl	-120(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -120(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-124(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -124(%rbp)
	movl	-128(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -128(%rbp)
	movl	-132(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -132(%rbp)
	movl	-136(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %esi
	movl	%esi, -112(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -136(%rbp)
	movl	-140(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %esi
	movl	%esi, -116(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -140(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -144(%rbp)
	movl	-148(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-120(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -120(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-124(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -124(%rbp)
	movl	-128(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -128(%rbp)
	movl	-132(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -132(%rbp)
	movl	-136(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %edx
	movl	%edx, -112(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -136(%rbp)
	movl	-140(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edx
	movl	%edx, -116(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -140(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-144(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -148(%rbp)
	movl	-120(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -120(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-124(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -124(%rbp)
	movl	-128(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -128(%rbp)
	movl	-132(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -132(%rbp)
	movl	-136(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %esi
	movl	%esi, -112(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -136(%rbp)
	movl	-140(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %esi
	movl	%esi, -116(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -140(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -144(%rbp)
	movl	-148(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-120(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -120(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-124(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -124(%rbp)
	movl	-128(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -128(%rbp)
	movl	-132(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -132(%rbp)
	movl	-136(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %edx
	movl	%edx, -112(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -136(%rbp)
	movl	-140(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edx
	movl	%edx, -116(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -140(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edx
	movl	%edx, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %r8d
	movl	%r8d, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-144(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -148(%rbp)
	movl	-120(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -120(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-124(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -124(%rbp)
	movl	-128(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -128(%rbp)
	movl	-132(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -132(%rbp)
	movl	-136(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %esi
	movl	%esi, -112(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -136(%rbp)
	movl	-140(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %esi
	movl	%esi, -116(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -140(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %esi
	movl	%esi, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -144(%rbp)
	movl	-148(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-120(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -120(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-124(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -124(%rbp)
	movl	-128(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -128(%rbp)
	movl	-132(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -132(%rbp)
	movl	-136(%rbp), %edx
	addl	%edx, -112(%rbp)
	movl	-112(%rbp), %r8d
	movl	%r8d, -112(%rbp)
	subl	%r8d, %edx
	movl	%edx, -136(%rbp)
	movl	-140(%rbp), %edx
	addl	%edx, -116(%rbp)
	movl	-116(%rbp), %r8d
	movl	%r8d, -116(%rbp)
	subl	%r8d, %edx
	movl	%edx, -140(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edx
	movl	%edx, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %r8d
	movl	%r8d, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-144(%rbp), %edx
	addl	%edx, %eax
	movl	%eax, -152(%rbp)
	subl	%eax, %edx
	movl	%edx, -144(%rbp)
	addl	%esi, %ecx
	movl	%ecx, -156(%rbp)
	subl	%ecx, %esi
	movl	%esi, -148(%rbp)
	movl	-120(%rbp), %r8d
	addl	%r8d, %edi
	movl	%edi, -160(%rbp)
	subl	%edi, %r8d
	movl	%r8d, -120(%rbp)
	addl	%r10d, %r9d
	movl	%r9d, -164(%rbp)
	subl	%r9d, %r10d
	movl	%r10d, -172(%rbp)
	movl	-124(%rbp), %eax
	addl	%eax, %r11d
	movl	%r11d, -168(%rbp)
	subl	%r11d, %eax
	movl	%eax, -124(%rbp)
	movl	-128(%rbp), %eax
	addl	%eax, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %eax
	movl	%eax, -128(%rbp)
	movl	-132(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -132(%rbp)
	movl	-136(%rbp), %edx
	addl	%edx, -112(%rbp)
	movl	-112(%rbp), %r8d
	movl	%r8d, -112(%rbp)
	subl	%r8d, %edx
	movl	%edx, -136(%rbp)
	movl	-140(%rbp), %edx
	addl	%edx, -116(%rbp)
	movl	-116(%rbp), %r8d
	movl	%r8d, -116(%rbp)
	subl	%r8d, %edx
	movl	%edx, -140(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	subl	%r8d, %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %edx
	movl	%edx, -88(%rbp)
	subl	%edx, %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %r8d
	movl	%r8d, -92(%rbp)
	subl	%r8d, %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	%edx, %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	%r8d, %ebx
.L106:
	movq	-184(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -184(%rbp)
	testq	%rax, %rax
	jne	.L107
	movl	-152(%rbp), %eax
	addl	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-156(%rbp), %eax
	addl	-148(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-160(%rbp), %eax
	addl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-164(%rbp), %eax
	addl	-172(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-168(%rbp), %eax
	addl	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	addl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-108(%rbp), %eax
	addl	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	addl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-116(%rbp), %eax
	addl	-140(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-84(%rbp), %eax
	addl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	addl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-92(%rbp), %eax
	addl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	addl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-100(%rbp), %eax
	addl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$152, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE38:
	.size	integer_add_13, .-integer_add_13
	.globl	integer_add_14
	.type	integer_add_14, @function
integer_add_14:
.LFB39:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -192(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$57, %eax
	movl	%eax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$31, %eax
	movl	%eax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$57, %eax
	movl	%eax, -164(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$31, %eax
	movl	%eax, -156(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$57, %eax
	movl	%eax, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$31, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$57, %eax
	movl	%eax, -172(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$31, %eax
	movl	%eax, -180(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$57, %eax
	movl	%eax, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$31, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$57, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$31, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$57, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$31, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$57, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$31, %eax
	movl	%eax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$57, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$31, %eax
	movl	%eax, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$57, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	31(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$57, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	31(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$57, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	leal	31(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$57, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	leal	31(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	addl	$57, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	leal	31(%rax), %ebx
	jmp	.L109
.L110:
	movl	-152(%rbp), %edx
	addl	%edx, -160(%rbp)
	movl	-160(%rbp), %eax
	subl	%eax, %edx
	movl	%edx, -152(%rbp)
	movl	-156(%rbp), %esi
	addl	%esi, -164(%rbp)
	movl	-164(%rbp), %ecx
	subl	%ecx, %esi
	movl	-124(%rbp), %r8d
	addl	%r8d, -168(%rbp)
	movl	-168(%rbp), %edi
	subl	%edi, %r8d
	movl	%r8d, -124(%rbp)
	movl	-180(%rbp), %r10d
	addl	%r10d, -172(%rbp)
	movl	-172(%rbp), %r9d
	subl	%r9d, %r10d
	movl	-128(%rbp), %edx
	addl	%edx, -176(%rbp)
	movl	-176(%rbp), %r11d
	subl	%r11d, %edx
	movl	%edx, -128(%rbp)
	movl	-132(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -132(%rbp)
	movl	-136(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -136(%rbp)
	movl	-140(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %edx
	movl	%edx, -112(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -140(%rbp)
	movl	-144(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edx
	movl	%edx, -116(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -144(%rbp)
	movl	-148(%rbp), %r8d
	addl	%r8d, -120(%rbp)
	movl	-120(%rbp), %edx
	movl	%edx, -120(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -148(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-152(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -156(%rbp)
	movl	-124(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -124(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-128(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -128(%rbp)
	movl	-132(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -132(%rbp)
	movl	-136(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -136(%rbp)
	movl	-140(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %esi
	movl	%esi, -112(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -140(%rbp)
	movl	-144(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %esi
	movl	%esi, -116(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -144(%rbp)
	movl	-148(%rbp), %r8d
	addl	%r8d, -120(%rbp)
	movl	-120(%rbp), %esi
	movl	%esi, -120(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -148(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -152(%rbp)
	movl	-156(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-124(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -124(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-128(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -128(%rbp)
	movl	-132(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -132(%rbp)
	movl	-136(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -136(%rbp)
	movl	-140(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %edx
	movl	%edx, -112(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -140(%rbp)
	movl	-144(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edx
	movl	%edx, -116(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -144(%rbp)
	movl	-148(%rbp), %r8d
	addl	%r8d, -120(%rbp)
	movl	-120(%rbp), %edx
	movl	%edx, -120(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -148(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-152(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -156(%rbp)
	movl	-124(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -124(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-128(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -128(%rbp)
	movl	-132(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -132(%rbp)
	movl	-136(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -136(%rbp)
	movl	-140(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %esi
	movl	%esi, -112(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -140(%rbp)
	movl	-144(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %esi
	movl	%esi, -116(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -144(%rbp)
	movl	-148(%rbp), %r8d
	addl	%r8d, -120(%rbp)
	movl	-120(%rbp), %esi
	movl	%esi, -120(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -148(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -152(%rbp)
	movl	-156(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-124(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -124(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-128(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -128(%rbp)
	movl	-132(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -132(%rbp)
	movl	-136(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -136(%rbp)
	movl	-140(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %edx
	movl	%edx, -112(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -140(%rbp)
	movl	-144(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edx
	movl	%edx, -116(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -144(%rbp)
	movl	-148(%rbp), %r8d
	addl	%r8d, -120(%rbp)
	movl	-120(%rbp), %edx
	movl	%edx, -120(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -148(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-152(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -156(%rbp)
	movl	-124(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -124(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-128(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -128(%rbp)
	movl	-132(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -132(%rbp)
	movl	-136(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -136(%rbp)
	movl	-140(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %esi
	movl	%esi, -112(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -140(%rbp)
	movl	-144(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %esi
	movl	%esi, -116(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -144(%rbp)
	movl	-148(%rbp), %r8d
	addl	%r8d, -120(%rbp)
	movl	-120(%rbp), %esi
	movl	%esi, -120(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -148(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -152(%rbp)
	movl	-156(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-124(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -124(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-128(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -128(%rbp)
	movl	-132(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -132(%rbp)
	movl	-136(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -136(%rbp)
	movl	-140(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %edx
	movl	%edx, -112(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -140(%rbp)
	movl	-144(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edx
	movl	%edx, -116(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -144(%rbp)
	movl	-148(%rbp), %r8d
	addl	%r8d, -120(%rbp)
	movl	-120(%rbp), %edx
	movl	%edx, -120(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -148(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %r8d
	movl	%r8d, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-152(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -156(%rbp)
	movl	-124(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -124(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-128(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -128(%rbp)
	movl	-132(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -132(%rbp)
	movl	-136(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -136(%rbp)
	movl	-140(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %esi
	movl	%esi, -112(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -140(%rbp)
	movl	-144(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %esi
	movl	%esi, -116(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -144(%rbp)
	movl	-148(%rbp), %esi
	addl	%esi, -120(%rbp)
	movl	-120(%rbp), %r8d
	movl	%r8d, -120(%rbp)
	subl	%r8d, %esi
	movl	%esi, -148(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %esi
	movl	%esi, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %r8d
	movl	%r8d, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %esi
	movl	%esi, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -152(%rbp)
	movl	-156(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-124(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -124(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-128(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -128(%rbp)
	movl	-132(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -132(%rbp)
	movl	-136(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -136(%rbp)
	movl	-140(%rbp), %edx
	addl	%edx, -112(%rbp)
	movl	-112(%rbp), %r8d
	movl	%r8d, -112(%rbp)
	subl	%r8d, %edx
	movl	%edx, -140(%rbp)
	movl	-144(%rbp), %edx
	addl	%edx, -116(%rbp)
	movl	-116(%rbp), %r8d
	movl	%r8d, -116(%rbp)
	subl	%r8d, %edx
	movl	%edx, -144(%rbp)
	movl	-148(%rbp), %edx
	addl	%edx, -120(%rbp)
	movl	-120(%rbp), %r8d
	movl	%r8d, -120(%rbp)
	subl	%r8d, %edx
	movl	%edx, -148(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %r8d
	movl	%r8d, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-152(%rbp), %edx
	addl	%edx, %eax
	movl	%eax, -160(%rbp)
	subl	%eax, %edx
	movl	%edx, -152(%rbp)
	addl	%esi, %ecx
	movl	%ecx, -164(%rbp)
	subl	%ecx, %esi
	movl	%esi, -156(%rbp)
	movl	-124(%rbp), %r8d
	addl	%r8d, %edi
	movl	%edi, -168(%rbp)
	subl	%edi, %r8d
	movl	%r8d, -124(%rbp)
	addl	%r10d, %r9d
	movl	%r9d, -172(%rbp)
	subl	%r9d, %r10d
	movl	%r10d, -180(%rbp)
	movl	-128(%rbp), %eax
	addl	%eax, %r11d
	movl	%r11d, -176(%rbp)
	subl	%r11d, %eax
	movl	%eax, -128(%rbp)
	movl	-132(%rbp), %eax
	addl	%eax, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %eax
	movl	%eax, -132(%rbp)
	movl	-136(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -136(%rbp)
	movl	-140(%rbp), %edx
	addl	%edx, -112(%rbp)
	movl	-112(%rbp), %r8d
	movl	%r8d, -112(%rbp)
	subl	%r8d, %edx
	movl	%edx, -140(%rbp)
	movl	-144(%rbp), %edx
	addl	%edx, -116(%rbp)
	movl	-116(%rbp), %r8d
	movl	%r8d, -116(%rbp)
	subl	%r8d, %edx
	movl	%edx, -144(%rbp)
	movl	-148(%rbp), %edx
	addl	%edx, -120(%rbp)
	movl	-120(%rbp), %r8d
	movl	%r8d, -120(%rbp)
	subl	%r8d, %edx
	movl	%edx, -148(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	%edx, %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	%r8d, %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	%edx, %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %r8d
	movl	%r8d, -96(%rbp)
	subl	%r8d, %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	%edx, %ebx
.L109:
	movq	-192(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -192(%rbp)
	testq	%rax, %rax
	jne	.L110
	movl	-160(%rbp), %eax
	addl	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-164(%rbp), %eax
	addl	-156(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-168(%rbp), %eax
	addl	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-172(%rbp), %eax
	addl	-180(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-176(%rbp), %eax
	addl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	addl	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-108(%rbp), %eax
	addl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	addl	-140(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-116(%rbp), %eax
	addl	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %eax
	addl	-148(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-84(%rbp), %eax
	addl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	addl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-92(%rbp), %eax
	addl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	addl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-100(%rbp), %eax
	addl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$152, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE39:
	.size	integer_add_14, .-integer_add_14
	.globl	integer_add_15
	.type	integer_add_15, @function
integer_add_15:
.LFB40:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -200(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$57, %eax
	movl	%eax, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$31, %eax
	movl	%eax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$57, %eax
	movl	%eax, -172(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$31, %eax
	movl	%eax, -164(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$57, %eax
	movl	%eax, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$31, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$57, %eax
	movl	%eax, -180(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$31, %eax
	movl	%eax, -188(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$57, %eax
	movl	%eax, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$31, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$57, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$31, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$57, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$31, %eax
	movl	%eax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$57, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$31, %eax
	movl	%eax, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$57, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$31, %eax
	movl	%eax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$57, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$31, %eax
	movl	%eax, -156(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$57, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	31(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$57, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	leal	31(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$57, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	leal	31(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	addl	$57, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	leal	31(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	addl	$57, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	leal	31(%rax), %ebx
	jmp	.L112
.L113:
	movl	-160(%rbp), %edx
	addl	%edx, -168(%rbp)
	movl	-168(%rbp), %eax
	subl	%eax, %edx
	movl	%edx, -160(%rbp)
	movl	-164(%rbp), %esi
	addl	%esi, -172(%rbp)
	movl	-172(%rbp), %ecx
	subl	%ecx, %esi
	movl	-128(%rbp), %r8d
	addl	%r8d, -176(%rbp)
	movl	-176(%rbp), %edi
	subl	%edi, %r8d
	movl	%r8d, -128(%rbp)
	movl	-188(%rbp), %r10d
	addl	%r10d, -180(%rbp)
	movl	-180(%rbp), %r9d
	subl	%r9d, %r10d
	movl	-132(%rbp), %edx
	addl	%edx, -184(%rbp)
	movl	-184(%rbp), %r11d
	subl	%r11d, %edx
	movl	%edx, -132(%rbp)
	movl	-136(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -136(%rbp)
	movl	-140(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -140(%rbp)
	movl	-144(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %edx
	movl	%edx, -112(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -144(%rbp)
	movl	-148(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edx
	movl	%edx, -116(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -148(%rbp)
	movl	-152(%rbp), %r8d
	addl	%r8d, -120(%rbp)
	movl	-120(%rbp), %edx
	movl	%edx, -120(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -152(%rbp)
	movl	-156(%rbp), %r8d
	addl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edx
	movl	%edx, -124(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -156(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-160(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -164(%rbp)
	movl	-128(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -128(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-132(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -132(%rbp)
	movl	-136(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -136(%rbp)
	movl	-140(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -140(%rbp)
	movl	-144(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %esi
	movl	%esi, -112(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -144(%rbp)
	movl	-148(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %esi
	movl	%esi, -116(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -148(%rbp)
	movl	-152(%rbp), %r8d
	addl	%r8d, -120(%rbp)
	movl	-120(%rbp), %esi
	movl	%esi, -120(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -152(%rbp)
	movl	-156(%rbp), %r8d
	addl	%r8d, -124(%rbp)
	movl	-124(%rbp), %esi
	movl	%esi, -124(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -156(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -160(%rbp)
	movl	-164(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-128(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -128(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-132(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -132(%rbp)
	movl	-136(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -136(%rbp)
	movl	-140(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -140(%rbp)
	movl	-144(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %edx
	movl	%edx, -112(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -144(%rbp)
	movl	-148(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edx
	movl	%edx, -116(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -148(%rbp)
	movl	-152(%rbp), %r8d
	addl	%r8d, -120(%rbp)
	movl	-120(%rbp), %edx
	movl	%edx, -120(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -152(%rbp)
	movl	-156(%rbp), %r8d
	addl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edx
	movl	%edx, -124(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -156(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-160(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -164(%rbp)
	movl	-128(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -128(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-132(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -132(%rbp)
	movl	-136(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -136(%rbp)
	movl	-140(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -140(%rbp)
	movl	-144(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %esi
	movl	%esi, -112(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -144(%rbp)
	movl	-148(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %esi
	movl	%esi, -116(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -148(%rbp)
	movl	-152(%rbp), %r8d
	addl	%r8d, -120(%rbp)
	movl	-120(%rbp), %esi
	movl	%esi, -120(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -152(%rbp)
	movl	-156(%rbp), %r8d
	addl	%r8d, -124(%rbp)
	movl	-124(%rbp), %esi
	movl	%esi, -124(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -156(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -160(%rbp)
	movl	-164(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-128(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -128(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-132(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -132(%rbp)
	movl	-136(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -136(%rbp)
	movl	-140(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -140(%rbp)
	movl	-144(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %edx
	movl	%edx, -112(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -144(%rbp)
	movl	-148(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edx
	movl	%edx, -116(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -148(%rbp)
	movl	-152(%rbp), %r8d
	addl	%r8d, -120(%rbp)
	movl	-120(%rbp), %edx
	movl	%edx, -120(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -152(%rbp)
	movl	-156(%rbp), %r8d
	addl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edx
	movl	%edx, -124(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -156(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	%edx, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-160(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -164(%rbp)
	movl	-128(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -128(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-132(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -132(%rbp)
	movl	-136(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -136(%rbp)
	movl	-140(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -140(%rbp)
	movl	-144(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %esi
	movl	%esi, -112(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -144(%rbp)
	movl	-148(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %esi
	movl	%esi, -116(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -148(%rbp)
	movl	-152(%rbp), %r8d
	addl	%r8d, -120(%rbp)
	movl	-120(%rbp), %esi
	movl	%esi, -120(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -152(%rbp)
	movl	-156(%rbp), %r8d
	addl	%r8d, -124(%rbp)
	movl	-124(%rbp), %esi
	movl	%esi, -124(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -156(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	%esi, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %r8d
	movl	%r8d, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -160(%rbp)
	movl	-164(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-128(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -128(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-132(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -132(%rbp)
	movl	-136(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -136(%rbp)
	movl	-140(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -140(%rbp)
	movl	-144(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %edx
	movl	%edx, -112(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -144(%rbp)
	movl	-148(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edx
	movl	%edx, -116(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -148(%rbp)
	movl	-152(%rbp), %r8d
	addl	%r8d, -120(%rbp)
	movl	-120(%rbp), %edx
	movl	%edx, -120(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -152(%rbp)
	movl	-156(%rbp), %r8d
	addl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edx
	movl	%edx, -124(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -156(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %r8d
	movl	%r8d, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-160(%rbp), %edx
	addl	%edx, %eax
	subl	%eax, %edx
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	%esi, -164(%rbp)
	movl	-128(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -128(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-132(%rbp), %esi
	addl	%esi, %r11d
	subl	%r11d, %esi
	movl	%esi, -132(%rbp)
	movl	-136(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	%esi, -104(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -136(%rbp)
	movl	-140(%rbp), %esi
	addl	%esi, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %esi
	movl	%esi, -140(%rbp)
	movl	-144(%rbp), %r8d
	addl	%r8d, -112(%rbp)
	movl	-112(%rbp), %esi
	movl	%esi, -112(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -144(%rbp)
	movl	-148(%rbp), %r8d
	addl	%r8d, -116(%rbp)
	movl	-116(%rbp), %esi
	movl	%esi, -116(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -148(%rbp)
	movl	-152(%rbp), %esi
	addl	%esi, -120(%rbp)
	movl	-120(%rbp), %r8d
	movl	%r8d, -120(%rbp)
	subl	%r8d, %esi
	movl	%esi, -152(%rbp)
	movl	-156(%rbp), %r8d
	addl	%r8d, -124(%rbp)
	movl	-124(%rbp), %esi
	movl	%esi, -124(%rbp)
	subl	%esi, %r8d
	movl	%r8d, -156(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %esi
	movl	%esi, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %esi
	movl	%esi, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	%esi, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %r8d
	movl	%r8d, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %esi
	movl	%esi, -100(%rbp)
	subl	-100(%rbp), %ebx
	addl	%edx, %eax
	subl	%eax, %edx
	movl	%edx, -160(%rbp)
	movl	-164(%rbp), %esi
	addl	%esi, %ecx
	subl	%ecx, %esi
	movl	-128(%rbp), %r8d
	addl	%r8d, %edi
	subl	%edi, %r8d
	movl	%r8d, -128(%rbp)
	addl	%r10d, %r9d
	subl	%r9d, %r10d
	movl	-132(%rbp), %edx
	addl	%edx, %r11d
	subl	%r11d, %edx
	movl	%edx, -132(%rbp)
	movl	-136(%rbp), %r8d
	addl	%r8d, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -136(%rbp)
	movl	-140(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -140(%rbp)
	movl	-144(%rbp), %edx
	addl	%edx, -112(%rbp)
	movl	-112(%rbp), %r8d
	movl	%r8d, -112(%rbp)
	subl	%r8d, %edx
	movl	%edx, -144(%rbp)
	movl	-148(%rbp), %edx
	addl	%edx, -116(%rbp)
	movl	-116(%rbp), %r8d
	movl	%r8d, -116(%rbp)
	subl	%r8d, %edx
	movl	%edx, -148(%rbp)
	movl	-152(%rbp), %edx
	addl	%edx, -120(%rbp)
	movl	-120(%rbp), %r8d
	movl	%r8d, -120(%rbp)
	subl	%r8d, %edx
	movl	%edx, -152(%rbp)
	movl	-156(%rbp), %r8d
	addl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edx
	movl	%edx, -124(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -156(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	-84(%rbp), %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	-88(%rbp), %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	-92(%rbp), %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %r8d
	movl	%r8d, -96(%rbp)
	subl	-96(%rbp), %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	-100(%rbp), %ebx
	movl	-160(%rbp), %edx
	addl	%edx, %eax
	movl	%eax, -168(%rbp)
	subl	%eax, %edx
	movl	%edx, -160(%rbp)
	addl	%esi, %ecx
	movl	%ecx, -172(%rbp)
	subl	%ecx, %esi
	movl	%esi, -164(%rbp)
	movl	-128(%rbp), %r8d
	addl	%r8d, %edi
	movl	%edi, -176(%rbp)
	subl	%edi, %r8d
	movl	%r8d, -128(%rbp)
	addl	%r10d, %r9d
	movl	%r9d, -180(%rbp)
	subl	%r9d, %r10d
	movl	%r10d, -188(%rbp)
	movl	-132(%rbp), %eax
	addl	%eax, %r11d
	movl	%r11d, -184(%rbp)
	subl	%r11d, %eax
	movl	%eax, -132(%rbp)
	movl	-136(%rbp), %eax
	addl	%eax, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	%edx, -104(%rbp)
	subl	%edx, %eax
	movl	%eax, -136(%rbp)
	movl	-140(%rbp), %edx
	addl	%edx, -108(%rbp)
	movl	-108(%rbp), %r8d
	movl	%r8d, -108(%rbp)
	subl	%r8d, %edx
	movl	%edx, -140(%rbp)
	movl	-144(%rbp), %edx
	addl	%edx, -112(%rbp)
	movl	-112(%rbp), %r8d
	movl	%r8d, -112(%rbp)
	subl	%r8d, %edx
	movl	%edx, -144(%rbp)
	movl	-148(%rbp), %edx
	addl	%edx, -116(%rbp)
	movl	-116(%rbp), %r8d
	movl	%r8d, -116(%rbp)
	subl	%r8d, %edx
	movl	%edx, -148(%rbp)
	movl	-152(%rbp), %edx
	addl	%edx, -120(%rbp)
	movl	-120(%rbp), %r8d
	movl	%r8d, -120(%rbp)
	subl	%r8d, %edx
	movl	%edx, -152(%rbp)
	movl	-156(%rbp), %r8d
	addl	%r8d, -124(%rbp)
	movl	-124(%rbp), %edx
	movl	%edx, -124(%rbp)
	subl	%edx, %r8d
	movl	%r8d, -156(%rbp)
	addl	%r15d, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	%edx, -84(%rbp)
	subl	%edx, %r15d
	addl	%r14d, -88(%rbp)
	movl	-88(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	subl	%r8d, %r14d
	addl	%r13d, -92(%rbp)
	movl	-92(%rbp), %edx
	movl	%edx, -92(%rbp)
	subl	%edx, %r13d
	addl	%r12d, -96(%rbp)
	movl	-96(%rbp), %r8d
	movl	%r8d, -96(%rbp)
	subl	%r8d, %r12d
	addl	%ebx, -100(%rbp)
	movl	-100(%rbp), %edx
	movl	%edx, -100(%rbp)
	subl	%edx, %ebx
.L112:
	movq	-200(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -200(%rbp)
	testq	%rax, %rax
	jne	.L113
	movl	-168(%rbp), %eax
	addl	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-172(%rbp), %eax
	addl	-164(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-176(%rbp), %eax
	addl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-180(%rbp), %eax
	addl	-188(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-184(%rbp), %eax
	addl	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	addl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-108(%rbp), %eax
	addl	-140(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	addl	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-116(%rbp), %eax
	addl	-148(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %eax
	addl	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-124(%rbp), %eax
	addl	-156(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-84(%rbp), %eax
	addl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	addl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-92(%rbp), %eax
	addl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	addl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-100(%rbp), %eax
	addl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$168, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE40:
	.size	integer_add_15, .-integer_add_15
	.globl	integer_add_benchmarks
	.section	.data.rel.local
	.align 32
	.type	integer_add_benchmarks, @object
	.size	integer_add_benchmarks, 128
integer_add_benchmarks:
	.quad	integer_add_0
	.quad	integer_add_1
	.quad	integer_add_2
	.quad	integer_add_3
	.quad	integer_add_4
	.quad	integer_add_5
	.quad	integer_add_6
	.quad	integer_add_7
	.quad	integer_add_8
	.quad	integer_add_9
	.quad	integer_add_10
	.quad	integer_add_11
	.quad	integer_add_12
	.quad	integer_add_13
	.quad	integer_add_14
	.quad	integer_add_15
	.text
	.globl	integer_mul_0
	.type	integer_mul_0, @function
integer_mul_0:
.LFB41:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$32, %rsp
	.cfi_offset 14, -24
	.cfi_offset 13, -32
	.cfi_offset 12, -40
	.cfi_offset 3, -48
	movq	%rdi, -56(%rbp)
	movq	%rsi, -64(%rbp)
	movq	-56(%rbp), %r13
	movq	-64(%rbp), %rax
	movq	%rax, -40(%rbp)
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	leal	37432(%rax), %ebx
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	leal	5(%rax), %r12d
	movl	%ebx, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	subl	%ebx, %eax
	movl	%eax, %r14d
	addl	%r14d, %ebx
	jmp	.L115
.L116:
	subl	%r14d, %ebx
	imull	%r12d, %ebx
	imull	%r12d, %ebx
	imull	%r12d, %ebx
	imull	%r12d, %ebx
	imull	%r12d, %ebx
	imull	%r12d, %ebx
	imull	%r12d, %ebx
	imull	%r12d, %ebx
	imull	%r12d, %ebx
	imull	%r12d, %ebx
.L115:
	movq	%r13, %rax
	leaq	-1(%rax), %r13
	testq	%rax, %rax
	jne	.L116
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$32, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE41:
	.size	integer_mul_0, .-integer_mul_0
	.globl	integer_mul_1
	.type	integer_mul_1, @function
integer_mul_1:
.LFB42:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %r15
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37432(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	5(%rax), %r13d
	movl	%r12d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	subl	%r12d, %eax
	movl	%eax, %edx
	addl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	37431(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	4(%rax), %r14d
	movl	%ebx, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	subl	%ebx, %eax
	movl	%eax, %ecx
	addl	%eax, %ebx
	jmp	.L118
.L119:
	subl	%edx, %r12d
	subl	%ecx, %ebx
	imull	%r13d, %r12d
	imull	%r14d, %ebx
	imull	%r13d, %r12d
	imull	%r14d, %ebx
	imull	%r13d, %r12d
	imull	%r14d, %ebx
	imull	%r13d, %r12d
	imull	%r14d, %ebx
	imull	%r13d, %r12d
	imull	%r14d, %ebx
	imull	%r13d, %r12d
	imull	%r14d, %ebx
	imull	%r13d, %r12d
	imull	%r14d, %ebx
	imull	%r13d, %r12d
	imull	%r14d, %ebx
	imull	%r13d, %r12d
	imull	%r14d, %ebx
	imull	%r13d, %r12d
	imull	%r14d, %ebx
.L118:
	movq	%r15, %rax
	leaq	-1(%rax), %r15
	testq	%rax, %rax
	jne	.L119
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE42:
	.size	integer_mul_1, .-integer_mul_1
	.globl	integer_mul_2
	.type	integer_mul_2, @function
integer_mul_2:
.LFB43:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %rsi
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37432(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	5(%rax), %r14d
	movl	%r13d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	subl	%r13d, %eax
	movl	%eax, %edi
	addl	%eax, %r13d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	37431(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	4(%rax), %r15d
	movl	%r12d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	subl	%r12d, %eax
	movl	%eax, %r8d
	addl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	37430(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	3(%rax), %edx
	movl	%edx, %eax
	imull	%ebx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%ebx, %eax
	movl	%eax, %r9d
	addl	%eax, %ebx
	jmp	.L121
.L122:
	subl	%edi, %r13d
	subl	%r8d, %r12d
	subl	%r9d, %ebx
	imull	%r14d, %r13d
	imull	%r15d, %r12d
	movl	%edx, %eax
	imull	%eax, %ebx
	imull	%r14d, %r13d
	imull	%r15d, %r12d
	imull	%eax, %ebx
	imull	%r14d, %r13d
	imull	%r15d, %r12d
	imull	%eax, %ebx
	imull	%r14d, %r13d
	imull	%r15d, %r12d
	imull	%eax, %ebx
	imull	%r14d, %r13d
	imull	%r15d, %r12d
	imull	%eax, %ebx
	imull	%r14d, %r13d
	imull	%r15d, %r12d
	imull	%eax, %ebx
	imull	%r14d, %r13d
	imull	%r15d, %r12d
	imull	%eax, %ebx
	imull	%r14d, %r13d
	imull	%r15d, %r12d
	imull	%eax, %ebx
	imull	%r14d, %r13d
	imull	%r15d, %r12d
	imull	%eax, %ebx
	imull	%r14d, %r13d
	imull	%r15d, %r12d
	imull	%eax, %ebx
.L121:
	movq	%rsi, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %rsi
	testq	%rax, %rax
	jne	.L122
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE43:
	.size	integer_mul_2, .-integer_mul_2
	.globl	integer_mul_3
	.type	integer_mul_3, @function
integer_mul_3:
.LFB44:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r8
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37432(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	5(%rax), %r15d
	movl	%r14d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	subl	%r14d, %eax
	movl	%eax, %r9d
	addl	%eax, %r14d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	37431(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	4(%rax), %edx
	movl	%edx, %r10d
	movl	%edx, %eax
	imull	%r13d, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%r13d, %eax
	movl	%eax, %r11d
	addl	%eax, %r13d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	37430(%rax), %r12d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	3(%rax), %ecx
	movl	%ecx, %edi
	movl	%ecx, %eax
	imull	%r12d, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	subl	%r12d, %eax
	movl	%eax, -84(%rbp)
	addl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	37429(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	2(%rax), %esi
	movl	%esi, %eax
	imull	%ebx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	subl	%ebx, %eax
	movl	%eax, -88(%rbp)
	addl	%eax, %ebx
	jmp	.L124
.L125:
	subl	%r9d, %r14d
	subl	%r11d, %r13d
	subl	-84(%rbp), %r12d
	subl	-88(%rbp), %ebx
	imull	%r15d, %r14d
	movl	%r10d, %eax
	imull	%eax, %r13d
	imull	%edi, %r12d
	movl	%esi, %edx
	imull	%edx, %ebx
	imull	%r15d, %r14d
	imull	%eax, %r13d
	imull	%edi, %r12d
	imull	%edx, %ebx
	imull	%r15d, %r14d
	imull	%eax, %r13d
	imull	%edi, %r12d
	imull	%edx, %ebx
	imull	%r15d, %r14d
	imull	%eax, %r13d
	imull	%edi, %r12d
	imull	%edx, %ebx
	imull	%r15d, %r14d
	imull	%eax, %r13d
	imull	%edi, %r12d
	imull	%edx, %ebx
	imull	%r15d, %r14d
	imull	%eax, %r13d
	imull	%edi, %r12d
	imull	%edx, %ebx
	imull	%r15d, %r14d
	imull	%eax, %r13d
	imull	%edi, %r12d
	imull	%edx, %ebx
	imull	%r15d, %r14d
	imull	%eax, %r13d
	imull	%edi, %r12d
	imull	%edx, %ebx
	imull	%r15d, %r14d
	imull	%eax, %r13d
	imull	%edi, %r12d
	imull	%edx, %ebx
	imull	%r15d, %r14d
	imull	%eax, %r13d
	imull	%edi, %r12d
	imull	%edx, %ebx
.L124:
	movq	%r8, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %r8
	testq	%rax, %rax
	jne	.L125
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE44:
	.size	integer_mul_3, .-integer_mul_3
	.globl	integer_mul_4
	.type	integer_mul_4, @function
integer_mul_4:
.LFB45:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r10
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37432(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	5(%rax), %r12d
	movl	%ebx, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	subl	%ebx, %eax
	movl	%eax, %r11d
	addl	%eax, %ebx
	movl	%ebx, %r9d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	37431(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	4(%rax), %r13d
	movl	%ebx, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	subl	%ebx, %eax
	movl	%eax, -100(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	37430(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	3(%rax), %r14d
	movl	%ebx, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	subl	%ebx, %eax
	movl	%eax, -104(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	37429(%rax), %edx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	2(%rax), %r15d
	movl	%edx, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	subl	%edx, %eax
	movl	%eax, -108(%rbp)
	addl	%eax, %edx
	movl	%edx, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	37428(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	1(%rax), %ebx
	movl	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%ecx, %eax
	movl	%eax, %r8d
	addl	%eax, %ecx
	movl	%ecx, -96(%rbp)
	jmp	.L127
.L128:
	movl	%r9d, %eax
	movl	%r11d, %esi
	subl	%esi, %eax
	movl	-100(%rbp), %esi
	subl	%esi, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	-104(%rbp), %esi
	subl	%esi, -88(%rbp)
	movl	-88(%rbp), %ecx
	movl	-108(%rbp), %edi
	subl	%edi, -92(%rbp)
	movl	-92(%rbp), %esi
	subl	%r8d, -96(%rbp)
	movl	-96(%rbp), %edi
	imull	%r12d, %eax
	imull	%r13d, %edx
	imull	%r14d, %ecx
	imull	%r15d, %esi
	imull	%ebx, %edi
	imull	%r12d, %eax
	imull	%r13d, %edx
	imull	%r14d, %ecx
	imull	%r15d, %esi
	imull	%ebx, %edi
	imull	%r12d, %eax
	imull	%r13d, %edx
	imull	%r14d, %ecx
	imull	%r15d, %esi
	imull	%ebx, %edi
	imull	%r12d, %eax
	imull	%r13d, %edx
	imull	%r14d, %ecx
	imull	%r15d, %esi
	imull	%ebx, %edi
	imull	%r12d, %eax
	imull	%r13d, %edx
	imull	%r14d, %ecx
	imull	%r15d, %esi
	imull	%ebx, %edi
	imull	%r12d, %eax
	imull	%r13d, %edx
	imull	%r14d, %ecx
	imull	%r15d, %esi
	imull	%ebx, %edi
	imull	%r12d, %eax
	imull	%r13d, %edx
	imull	%r14d, %ecx
	imull	%r15d, %esi
	imull	%ebx, %edi
	imull	%r12d, %eax
	imull	%r13d, %edx
	imull	%r14d, %ecx
	imull	%r15d, %esi
	imull	%ebx, %edi
	imull	%r12d, %eax
	imull	%r13d, %edx
	imull	%r14d, %ecx
	imull	%r15d, %esi
	imull	%ebx, %edi
	imull	%r12d, %eax
	movl	%eax, %r9d
	imull	%r13d, %edx
	movl	%edx, -84(%rbp)
	imull	%r14d, %ecx
	movl	%ecx, -88(%rbp)
	imull	%r15d, %esi
	movl	%esi, -92(%rbp)
	imull	%ebx, %edi
	movl	%edi, -96(%rbp)
.L127:
	movq	%r10, %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, %r10
	testq	%rax, %rax
	jne	.L128
	movl	%r9d, %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE45:
	.size	integer_mul_4, .-integer_mul_4
	.globl	integer_mul_5
	.type	integer_mul_5, @function
integer_mul_5:
.LFB46:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -112(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37432(%rax), %edx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	5(%rax), %r13d
	movl	%edx, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	subl	%edx, %eax
	movl	%eax, -104(%rbp)
	addl	%eax, %edx
	movl	%edx, %r10d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	37431(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	4(%rax), %r14d
	movl	%ecx, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	subl	%ecx, %eax
	movl	%eax, -116(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	37430(%rax), %esi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	3(%rax), %r15d
	movl	%esi, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	subl	%esi, %eax
	movl	%eax, -120(%rbp)
	addl	%eax, %esi
	movl	%esi, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	37429(%rax), %edi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	2(%rax), %ebx
	movl	%ebx, %r9d
	movl	%edi, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%edi, %eax
	movl	%eax, -124(%rbp)
	addl	%eax, %edi
	movl	%edi, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	37428(%rax), %edx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	1(%rax), %r12d
	movl	%edx, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	subl	%edx, %eax
	movl	%eax, -128(%rbp)
	addl	%eax, %edx
	movl	%edx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	37427(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	32(%rax), %ebx
	movl	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%ecx, %eax
	movl	%eax, %r11d
	addl	%eax, %ecx
	movl	%ecx, -100(%rbp)
	jmp	.L130
.L131:
	movl	%r10d, %eax
	movl	-104(%rbp), %esi
	subl	%esi, %eax
	movl	-116(%rbp), %esi
	subl	%esi, -84(%rbp)
	movl	-84(%rbp), %edx
	movl	-120(%rbp), %esi
	subl	%esi, -88(%rbp)
	movl	-88(%rbp), %ecx
	movl	-124(%rbp), %edi
	subl	%edi, -92(%rbp)
	movl	-92(%rbp), %esi
	movl	-128(%rbp), %r10d
	subl	%r10d, -96(%rbp)
	movl	-96(%rbp), %edi
	subl	%r11d, -100(%rbp)
	movl	-100(%rbp), %r8d
	imull	%r13d, %eax
	imull	%r14d, %edx
	imull	%r15d, %ecx
	imull	%r9d, %esi
	imull	%r12d, %edi
	imull	%ebx, %r8d
	imull	%r13d, %eax
	imull	%r14d, %edx
	imull	%r15d, %ecx
	imull	%r9d, %esi
	imull	%r12d, %edi
	imull	%ebx, %r8d
	imull	%r13d, %eax
	imull	%r14d, %edx
	imull	%r15d, %ecx
	imull	%r9d, %esi
	imull	%r12d, %edi
	imull	%ebx, %r8d
	imull	%r13d, %eax
	imull	%r14d, %edx
	imull	%r15d, %ecx
	imull	%r9d, %esi
	imull	%r12d, %edi
	imull	%ebx, %r8d
	imull	%r13d, %eax
	imull	%r14d, %edx
	imull	%r15d, %ecx
	imull	%r9d, %esi
	imull	%r12d, %edi
	imull	%ebx, %r8d
	imull	%r13d, %eax
	imull	%r14d, %edx
	imull	%r15d, %ecx
	imull	%r9d, %esi
	imull	%r12d, %edi
	imull	%ebx, %r8d
	imull	%r13d, %eax
	imull	%r14d, %edx
	imull	%r15d, %ecx
	imull	%r9d, %esi
	imull	%r12d, %edi
	imull	%ebx, %r8d
	imull	%r13d, %eax
	imull	%r14d, %edx
	imull	%r15d, %ecx
	imull	%r9d, %esi
	imull	%r12d, %edi
	imull	%ebx, %r8d
	imull	%r13d, %eax
	imull	%r14d, %edx
	imull	%r15d, %ecx
	imull	%r9d, %esi
	imull	%r12d, %edi
	imull	%ebx, %r8d
	imull	%r13d, %eax
	movl	%eax, %r10d
	imull	%r14d, %edx
	movl	%edx, -84(%rbp)
	imull	%r15d, %ecx
	movl	%ecx, -88(%rbp)
	imull	%r9d, %esi
	movl	%esi, -92(%rbp)
	imull	%r12d, %edi
	movl	%edi, -96(%rbp)
	imull	%ebx, %r8d
	movl	%r8d, -100(%rbp)
.L130:
	movq	-112(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -112(%rbp)
	testq	%rax, %rax
	jne	.L131
	movl	%r10d, %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE46:
	.size	integer_mul_5, .-integer_mul_5
	.globl	integer_mul_6
	.type	integer_mul_6, @function
integer_mul_6:
.LFB47:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -120(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37432(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	5(%rax), %r14d
	movl	%ecx, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	subl	%ecx, %eax
	movl	%eax, -112(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	37431(%rax), %esi
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	4(%rax), %r15d
	movl	%esi, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	subl	%esi, %eax
	movl	%eax, -124(%rbp)
	addl	%eax, %esi
	movl	%esi, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	37430(%rax), %edi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	3(%rax), %ebx
	movl	%ebx, -128(%rbp)
	movl	%edi, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%edi, %eax
	movl	%eax, -132(%rbp)
	addl	%eax, %edi
	movl	%edi, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	37429(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	2(%rax), %edx
	movl	%edx, -136(%rbp)
	movl	%ecx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%ecx, %eax
	movl	%eax, -140(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	37428(%rax), %esi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	1(%rax), %r13d
	movl	%esi, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	subl	%esi, %eax
	movl	%eax, -144(%rbp)
	addl	%eax, %esi
	movl	%esi, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	37427(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	32(%rax), %r12d
	movl	%ebx, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	subl	%ebx, %eax
	movl	%eax, -148(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	37426(%rax), %edi
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	-1(%rax), %ebx
	movl	%edi, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%edi, %eax
	movl	%eax, -152(%rbp)
	addl	%eax, %edi
	movl	%edi, -108(%rbp)
	jmp	.L133
.L134:
	movl	-112(%rbp), %ecx
	subl	%ecx, -84(%rbp)
	movl	-84(%rbp), %eax
	movl	-124(%rbp), %esi
	subl	%esi, -88(%rbp)
	movl	-88(%rbp), %edx
	movl	-132(%rbp), %esi
	subl	%esi, -92(%rbp)
	movl	-92(%rbp), %ecx
	movl	-140(%rbp), %edi
	subl	%edi, -96(%rbp)
	movl	-96(%rbp), %esi
	movl	-144(%rbp), %r10d
	subl	%r10d, -100(%rbp)
	movl	-100(%rbp), %edi
	movl	-148(%rbp), %r11d
	subl	%r11d, -104(%rbp)
	movl	-104(%rbp), %r8d
	movl	-152(%rbp), %r10d
	subl	%r10d, -108(%rbp)
	movl	-108(%rbp), %r9d
	imull	%r14d, %eax
	imull	%r15d, %edx
	movl	-128(%rbp), %r10d
	imull	%r10d, %ecx
	movl	-136(%rbp), %r11d
	imull	%r11d, %esi
	imull	%r13d, %edi
	imull	%r12d, %r8d
	imull	%ebx, %r9d
	imull	%r14d, %eax
	imull	%r15d, %edx
	imull	%r10d, %ecx
	imull	%r11d, %esi
	imull	%r13d, %edi
	imull	%r12d, %r8d
	imull	%ebx, %r9d
	imull	%r14d, %eax
	imull	%r15d, %edx
	imull	%r10d, %ecx
	imull	%r11d, %esi
	imull	%r13d, %edi
	imull	%r12d, %r8d
	imull	%ebx, %r9d
	imull	%r14d, %eax
	imull	%r15d, %edx
	imull	%r10d, %ecx
	imull	%r11d, %esi
	imull	%r13d, %edi
	imull	%r12d, %r8d
	imull	%ebx, %r9d
	imull	%r14d, %eax
	imull	%r15d, %edx
	imull	%r10d, %ecx
	imull	%r11d, %esi
	imull	%r13d, %edi
	imull	%r12d, %r8d
	imull	%ebx, %r9d
	imull	%r14d, %eax
	imull	%r15d, %edx
	imull	%r10d, %ecx
	imull	%r11d, %esi
	imull	%r13d, %edi
	imull	%r12d, %r8d
	imull	%ebx, %r9d
	imull	%r14d, %eax
	imull	%r15d, %edx
	imull	%r10d, %ecx
	imull	%r11d, %esi
	imull	%r13d, %edi
	imull	%r12d, %r8d
	imull	%ebx, %r9d
	imull	%r14d, %eax
	imull	%r15d, %edx
	imull	%r10d, %ecx
	imull	%r11d, %esi
	imull	%r13d, %edi
	imull	%r12d, %r8d
	imull	%ebx, %r9d
	imull	%r14d, %eax
	imull	%r15d, %edx
	imull	%r10d, %ecx
	imull	%r11d, %esi
	imull	%r13d, %edi
	imull	%r12d, %r8d
	imull	%ebx, %r9d
	imull	%r14d, %eax
	movl	%eax, -84(%rbp)
	imull	%r15d, %edx
	movl	%edx, -88(%rbp)
	imull	%r10d, %ecx
	movl	%ecx, -92(%rbp)
	imull	%r11d, %esi
	movl	%esi, -96(%rbp)
	imull	%r13d, %edi
	movl	%edi, -100(%rbp)
	imull	%r12d, %r8d
	movl	%r8d, -104(%rbp)
	imull	%ebx, %r9d
	movl	%r9d, -108(%rbp)
.L133:
	movq	-120(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -120(%rbp)
	testq	%rax, %rax
	jne	.L134
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-104(%rbp), %edi
	call	use_int@PLT
	movl	-108(%rbp), %edi
	call	use_int@PLT
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE47:
	.size	integer_mul_6, .-integer_mul_6
	.globl	integer_mul_7
	.type	integer_mul_7, @function
integer_mul_7:
.LFB48:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -128(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37432(%rax), %esi
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	5(%rax), %r15d
	movl	%esi, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	subl	%esi, %eax
	movl	%eax, -132(%rbp)
	addl	%eax, %esi
	movl	%esi, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	37431(%rax), %edi
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	4(%rax), %ebx
	movl	%ebx, -136(%rbp)
	movl	%edi, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%edi, %eax
	movl	%eax, -140(%rbp)
	addl	%eax, %edi
	movl	%edi, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	37430(%rax), %esi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	3(%rax), %edx
	movl	%edx, -84(%rbp)
	movl	%esi, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%esi, %eax
	movl	%eax, -144(%rbp)
	addl	%eax, %esi
	movl	%esi, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	37429(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	2(%rax), %ecx
	movl	%ecx, -88(%rbp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	subl	%ebx, %eax
	movl	%eax, -148(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	37428(%rax), %edi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	1(%rax), %r14d
	movl	%edi, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	subl	%edi, %eax
	movl	%eax, -152(%rbp)
	addl	%eax, %edi
	movl	%edi, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	37427(%rax), %esi
	movq	-56(%rbp), %rax
	movl	32(%rax), %r13d
	movl	%esi, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	subl	%esi, %eax
	movl	%eax, -156(%rbp)
	addl	%eax, %esi
	movl	%esi, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	37426(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	-1(%rax), %r12d
	movl	%ebx, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	subl	%ebx, %eax
	movl	%eax, -160(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	37425(%rax), %edi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	-2(%rax), %ebx
	movl	%edi, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%edi, %eax
	movl	%eax, -164(%rbp)
	addl	%eax, %edi
	movl	%edi, -120(%rbp)
	jmp	.L136
.L137:
	movl	-132(%rbp), %esi
	subl	%esi, -92(%rbp)
	movl	-92(%rbp), %eax
	movl	-140(%rbp), %esi
	subl	%esi, -96(%rbp)
	movl	-96(%rbp), %edx
	movl	-144(%rbp), %esi
	subl	%esi, -100(%rbp)
	movl	-100(%rbp), %ecx
	movl	-148(%rbp), %edi
	subl	%edi, -104(%rbp)
	movl	-104(%rbp), %esi
	movl	-152(%rbp), %r11d
	subl	%r11d, -108(%rbp)
	movl	-108(%rbp), %edi
	movl	-156(%rbp), %r11d
	subl	%r11d, -112(%rbp)
	movl	-112(%rbp), %r8d
	movl	-160(%rbp), %r11d
	subl	%r11d, -116(%rbp)
	movl	-116(%rbp), %r9d
	movl	-164(%rbp), %r11d
	subl	%r11d, -120(%rbp)
	movl	-120(%rbp), %r10d
	imull	%r15d, %eax
	movl	-136(%rbp), %r11d
	imull	%r11d, %edx
	imull	-84(%rbp), %ecx
	imull	-88(%rbp), %esi
	imull	%r14d, %edi
	imull	%r13d, %r8d
	imull	%r12d, %r9d
	imull	%ebx, %r10d
	imull	%r15d, %eax
	imull	%r11d, %edx
	imull	-84(%rbp), %ecx
	imull	-88(%rbp), %esi
	imull	%r14d, %edi
	imull	%r13d, %r8d
	imull	%r12d, %r9d
	imull	%ebx, %r10d
	imull	%r15d, %eax
	imull	%r11d, %edx
	imull	-84(%rbp), %ecx
	imull	-88(%rbp), %esi
	imull	%r14d, %edi
	imull	%r13d, %r8d
	imull	%r12d, %r9d
	imull	%ebx, %r10d
	imull	%r15d, %eax
	imull	%r11d, %edx
	imull	-84(%rbp), %ecx
	imull	-88(%rbp), %esi
	imull	%r14d, %edi
	imull	%r13d, %r8d
	imull	%r12d, %r9d
	imull	%ebx, %r10d
	imull	%r15d, %eax
	imull	%r11d, %edx
	imull	-84(%rbp), %ecx
	imull	-88(%rbp), %esi
	imull	%r14d, %edi
	imull	%r13d, %r8d
	imull	%r12d, %r9d
	imull	%ebx, %r10d
	imull	%r15d, %eax
	imull	%r11d, %edx
	imull	-84(%rbp), %ecx
	imull	-88(%rbp), %esi
	imull	%r14d, %edi
	imull	%r13d, %r8d
	imull	%r12d, %r9d
	imull	%ebx, %r10d
	imull	%r15d, %eax
	imull	%r11d, %edx
	imull	-84(%rbp), %ecx
	imull	-88(%rbp), %esi
	imull	%r14d, %edi
	imull	%r13d, %r8d
	imull	%r12d, %r9d
	imull	%ebx, %r10d
	imull	%r15d, %eax
	imull	%r11d, %edx
	imull	-84(%rbp), %ecx
	imull	-88(%rbp), %esi
	imull	%r14d, %edi
	imull	%r13d, %r8d
	imull	%r12d, %r9d
	imull	%ebx, %r10d
	imull	%r15d, %eax
	imull	%r11d, %edx
	imull	-84(%rbp), %ecx
	imull	-88(%rbp), %esi
	imull	%r14d, %edi
	imull	%r13d, %r8d
	imull	%r12d, %r9d
	imull	%ebx, %r10d
	imull	%r15d, %eax
	movl	%eax, -92(%rbp)
	imull	%r11d, %edx
	movl	%edx, -96(%rbp)
	imull	-84(%rbp), %ecx
	movl	%ecx, -100(%rbp)
	imull	-88(%rbp), %esi
	movl	%esi, -104(%rbp)
	imull	%r14d, %edi
	movl	%edi, -108(%rbp)
	imull	%r13d, %r8d
	movl	%r8d, -112(%rbp)
	imull	%r12d, %r9d
	movl	%r9d, -116(%rbp)
	imull	%ebx, %r10d
	movl	%r10d, -120(%rbp)
.L136:
	movq	-128(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -128(%rbp)
	testq	%rax, %rax
	jne	.L137
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-104(%rbp), %edi
	call	use_int@PLT
	movl	-108(%rbp), %edi
	call	use_int@PLT
	movl	-112(%rbp), %edi
	call	use_int@PLT
	movl	-116(%rbp), %edi
	call	use_int@PLT
	movl	-120(%rbp), %edi
	call	use_int@PLT
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE48:
	.size	integer_mul_7, .-integer_mul_7
	.globl	integer_mul_8
	.type	integer_mul_8, @function
integer_mul_8:
.LFB49:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -144(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37432(%rax), %edi
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	5(%rax), %ebx
	movl	%ebx, -84(%rbp)
	movl	%edi, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%edi, %eax
	movl	%eax, -136(%rbp)
	addl	%eax, %edi
	movl	%edi, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	37431(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	4(%rax), %edx
	movl	%edx, -88(%rbp)
	movl	%ebx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%ebx, %eax
	movl	%eax, -148(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	37430(%rax), %edi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	3(%rax), %ecx
	movl	%ecx, -92(%rbp)
	movl	%edi, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	subl	%edi, %eax
	movl	%eax, -152(%rbp)
	addl	%eax, %edi
	movl	%edi, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	37429(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	2(%rax), %esi
	movl	%esi, -156(%rbp)
	movl	%ebx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	subl	%ebx, %eax
	movl	%eax, -160(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	37428(%rax), %edi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	1(%rax), %r15d
	movl	%edi, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	subl	%edi, %eax
	movl	%eax, -164(%rbp)
	addl	%eax, %edi
	movl	%edi, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	37427(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	32(%rax), %r14d
	movl	%ebx, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	subl	%ebx, %eax
	movl	%eax, -168(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	37426(%rax), %edi
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	-1(%rax), %r13d
	movl	%edi, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	subl	%edi, %eax
	movl	%eax, -172(%rbp)
	addl	%eax, %edi
	movl	%edi, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	37425(%rax), %edx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	-2(%rax), %r12d
	movl	%edx, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	subl	%edx, %eax
	movl	%eax, -176(%rbp)
	addl	%eax, %edx
	movl	%edx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	37424(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	-3(%rax), %esi
	movl	%esi, -96(%rbp)
	movl	%ecx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	subl	%ecx, %eax
	movl	%eax, -180(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -132(%rbp)
	jmp	.L139
.L140:
	movl	-136(%rbp), %esi
	subl	%esi, -100(%rbp)
	movl	-100(%rbp), %eax
	movl	-148(%rbp), %esi
	subl	%esi, -104(%rbp)
	movl	-104(%rbp), %edx
	movl	-152(%rbp), %esi
	subl	%esi, -108(%rbp)
	movl	-108(%rbp), %ecx
	movl	-160(%rbp), %edi
	subl	%edi, -112(%rbp)
	movl	-112(%rbp), %esi
	movl	-164(%rbp), %r8d
	subl	%r8d, -116(%rbp)
	movl	-116(%rbp), %edi
	movl	-168(%rbp), %r9d
	subl	%r9d, -120(%rbp)
	movl	-120(%rbp), %r8d
	movl	-172(%rbp), %r10d
	subl	%r10d, -124(%rbp)
	movl	-124(%rbp), %r9d
	movl	-176(%rbp), %r11d
	subl	%r11d, -128(%rbp)
	movl	-128(%rbp), %r10d
	movl	-180(%rbp), %ebx
	subl	%ebx, -132(%rbp)
	movl	-132(%rbp), %r11d
	imull	-84(%rbp), %eax
	imull	-88(%rbp), %edx
	imull	-92(%rbp), %ecx
	movl	-156(%rbp), %ebx
	imull	%ebx, %esi
	imull	%r15d, %edi
	imull	%r14d, %r8d
	imull	%r13d, %r9d
	imull	%r12d, %r10d
	imull	-96(%rbp), %r11d
	imull	-84(%rbp), %eax
	imull	-88(%rbp), %edx
	imull	-92(%rbp), %ecx
	imull	%ebx, %esi
	imull	%r15d, %edi
	imull	%r14d, %r8d
	imull	%r13d, %r9d
	imull	%r12d, %r10d
	imull	-96(%rbp), %r11d
	imull	-84(%rbp), %eax
	imull	-88(%rbp), %edx
	imull	-92(%rbp), %ecx
	imull	%ebx, %esi
	imull	%r15d, %edi
	imull	%r14d, %r8d
	imull	%r13d, %r9d
	imull	%r12d, %r10d
	imull	-96(%rbp), %r11d
	imull	-84(%rbp), %eax
	imull	-88(%rbp), %edx
	imull	-92(%rbp), %ecx
	imull	%ebx, %esi
	imull	%r15d, %edi
	imull	%r14d, %r8d
	imull	%r13d, %r9d
	imull	%r12d, %r10d
	imull	-96(%rbp), %r11d
	imull	-84(%rbp), %eax
	imull	-88(%rbp), %edx
	imull	-92(%rbp), %ecx
	imull	%ebx, %esi
	imull	%r15d, %edi
	imull	%r14d, %r8d
	imull	%r13d, %r9d
	imull	%r12d, %r10d
	imull	-96(%rbp), %r11d
	imull	-84(%rbp), %eax
	imull	-88(%rbp), %edx
	imull	-92(%rbp), %ecx
	imull	%ebx, %esi
	imull	%r15d, %edi
	imull	%r14d, %r8d
	imull	%r13d, %r9d
	imull	%r12d, %r10d
	imull	-96(%rbp), %r11d
	imull	-84(%rbp), %eax
	imull	-88(%rbp), %edx
	imull	-92(%rbp), %ecx
	imull	%ebx, %esi
	imull	%r15d, %edi
	imull	%r14d, %r8d
	imull	%r13d, %r9d
	imull	%r12d, %r10d
	imull	-96(%rbp), %r11d
	imull	-84(%rbp), %eax
	imull	-88(%rbp), %edx
	imull	-92(%rbp), %ecx
	imull	%ebx, %esi
	imull	%r15d, %edi
	imull	%r14d, %r8d
	imull	%r13d, %r9d
	imull	%r12d, %r10d
	imull	-96(%rbp), %r11d
	imull	-84(%rbp), %eax
	imull	-88(%rbp), %edx
	imull	-92(%rbp), %ecx
	imull	%ebx, %esi
	imull	%r15d, %edi
	imull	%r14d, %r8d
	imull	%r13d, %r9d
	imull	%r12d, %r10d
	imull	-96(%rbp), %r11d
	imull	-84(%rbp), %eax
	movl	%eax, -100(%rbp)
	imull	-88(%rbp), %edx
	movl	%edx, -104(%rbp)
	imull	-92(%rbp), %ecx
	movl	%ecx, -108(%rbp)
	movl	%ebx, %eax
	imull	%eax, %esi
	movl	%esi, -112(%rbp)
	imull	%r15d, %edi
	movl	%edi, -116(%rbp)
	imull	%r14d, %r8d
	movl	%r8d, -120(%rbp)
	imull	%r13d, %r9d
	movl	%r9d, -124(%rbp)
	imull	%r12d, %r10d
	movl	%r10d, -128(%rbp)
	imull	-96(%rbp), %r11d
	movl	%r11d, -132(%rbp)
.L139:
	movq	-144(%rbp), %rax
	leaq	-1(%rax), %rbx
	movq	%rbx, -144(%rbp)
	testq	%rax, %rax
	jne	.L140
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-104(%rbp), %edi
	call	use_int@PLT
	movl	-108(%rbp), %edi
	call	use_int@PLT
	movl	-112(%rbp), %edi
	call	use_int@PLT
	movl	-116(%rbp), %edi
	call	use_int@PLT
	movl	-120(%rbp), %edi
	call	use_int@PLT
	movl	-124(%rbp), %edi
	call	use_int@PLT
	movl	-128(%rbp), %edi
	call	use_int@PLT
	movl	-132(%rbp), %edi
	call	use_int@PLT
	addq	$152, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE49:
	.size	integer_mul_8, .-integer_mul_8
	.globl	integer_mul_9
	.type	integer_mul_9, @function
integer_mul_9:
.LFB50:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -152(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37432(%rax), %edx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	5(%rax), %ebx
	movl	%ebx, -88(%rbp)
	movl	%edx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%edx, %eax
	movl	%eax, -156(%rbp)
	addl	%eax, %edx
	movl	%edx, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	37431(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	4(%rax), %edx
	movl	%edx, -92(%rbp)
	movl	%ebx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%ebx, %eax
	movl	%eax, -160(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	37430(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	3(%rax), %ecx
	movl	%ecx, -96(%rbp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	subl	%ebx, %eax
	movl	%eax, -164(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	37429(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	2(%rax), %esi
	movl	%esi, -100(%rbp)
	movl	%ebx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	subl	%ebx, %eax
	movl	%eax, -168(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	37428(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	1(%rax), %edi
	movl	%edi, -104(%rbp)
	movl	%ecx, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	subl	%ecx, %eax
	movl	%eax, -172(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	37427(%rax), %esi
	movq	-56(%rbp), %rax
	movl	32(%rax), %r15d
	movl	%esi, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	subl	%esi, %eax
	movl	%eax, -176(%rbp)
	addl	%eax, %esi
	movl	%esi, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	37426(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	-1(%rax), %r14d
	movl	%ebx, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	subl	%ebx, %eax
	movl	%eax, -180(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	37425(%rax), %edi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	-2(%rax), %r13d
	movl	%edi, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	subl	%edi, %eax
	movl	%eax, -184(%rbp)
	addl	%eax, %edi
	movl	%edi, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	37424(%rax), %edx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	-3(%rax), %esi
	movl	%esi, -108(%rbp)
	movl	%edx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	subl	%edx, %eax
	movl	%eax, -188(%rbp)
	addl	%eax, %edx
	movl	%edx, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	37423(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	-4(%rax), %ebx
	movl	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%ecx, %eax
	movl	%eax, -192(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -84(%rbp)
	jmp	.L142
.L143:
	movl	-156(%rbp), %esi
	subl	%esi, -112(%rbp)
	movl	-112(%rbp), %eax
	movl	-160(%rbp), %esi
	subl	%esi, -116(%rbp)
	movl	-116(%rbp), %edx
	movl	-164(%rbp), %esi
	subl	%esi, -120(%rbp)
	movl	-120(%rbp), %ecx
	movl	-168(%rbp), %edi
	subl	%edi, -124(%rbp)
	movl	-124(%rbp), %esi
	movl	-172(%rbp), %r8d
	subl	%r8d, -128(%rbp)
	movl	-128(%rbp), %edi
	movl	-176(%rbp), %r9d
	subl	%r9d, -132(%rbp)
	movl	-132(%rbp), %r8d
	movl	-180(%rbp), %r10d
	subl	%r10d, -136(%rbp)
	movl	-136(%rbp), %r9d
	movl	-184(%rbp), %r11d
	subl	%r11d, -140(%rbp)
	movl	-140(%rbp), %r10d
	movl	-188(%rbp), %r12d
	subl	%r12d, -144(%rbp)
	movl	-144(%rbp), %r11d
	movl	-192(%rbp), %r12d
	subl	%r12d, -84(%rbp)
	imull	-88(%rbp), %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	%r15d, %r8d
	imull	%r14d, %r9d
	imull	%r13d, %r10d
	imull	-108(%rbp), %r11d
	movl	-84(%rbp), %r12d
	imull	%ebx, %r12d
	movl	%r12d, -84(%rbp)
	imull	-88(%rbp), %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	%r15d, %r8d
	imull	%r14d, %r9d
	imull	%r13d, %r10d
	imull	-108(%rbp), %r11d
	movl	-84(%rbp), %r12d
	imull	%ebx, %r12d
	movl	%r12d, -84(%rbp)
	imull	-88(%rbp), %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	%r15d, %r8d
	imull	%r14d, %r9d
	imull	%r13d, %r10d
	imull	-108(%rbp), %r11d
	movl	-84(%rbp), %r12d
	imull	%ebx, %r12d
	movl	%r12d, -84(%rbp)
	imull	-88(%rbp), %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	%r15d, %r8d
	imull	%r14d, %r9d
	imull	%r13d, %r10d
	imull	-108(%rbp), %r11d
	movl	-84(%rbp), %r12d
	imull	%ebx, %r12d
	movl	%r12d, -84(%rbp)
	imull	-88(%rbp), %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	%r15d, %r8d
	imull	%r14d, %r9d
	imull	%r13d, %r10d
	imull	-108(%rbp), %r11d
	movl	-84(%rbp), %r12d
	imull	%ebx, %r12d
	movl	%r12d, -84(%rbp)
	imull	-88(%rbp), %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	%r15d, %r8d
	imull	%r14d, %r9d
	imull	%r13d, %r10d
	imull	-108(%rbp), %r11d
	movl	-84(%rbp), %r12d
	imull	%ebx, %r12d
	movl	%r12d, -84(%rbp)
	imull	-88(%rbp), %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	%r15d, %r8d
	imull	%r14d, %r9d
	imull	%r13d, %r10d
	imull	-108(%rbp), %r11d
	movl	-84(%rbp), %r12d
	imull	%ebx, %r12d
	movl	%r12d, -84(%rbp)
	imull	-88(%rbp), %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	%r15d, %r8d
	imull	%r14d, %r9d
	imull	%r13d, %r10d
	imull	-108(%rbp), %r11d
	movl	-84(%rbp), %r12d
	imull	%ebx, %r12d
	movl	%r12d, -84(%rbp)
	imull	-88(%rbp), %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	%r15d, %r8d
	imull	%r14d, %r9d
	imull	%r13d, %r10d
	imull	-108(%rbp), %r11d
	movl	-84(%rbp), %r12d
	imull	%ebx, %r12d
	movl	%r12d, -84(%rbp)
	imull	-88(%rbp), %eax
	movl	%eax, -112(%rbp)
	imull	-92(%rbp), %edx
	movl	%edx, -116(%rbp)
	imull	-96(%rbp), %ecx
	movl	%ecx, -120(%rbp)
	imull	-100(%rbp), %esi
	movl	%esi, -124(%rbp)
	imull	-104(%rbp), %edi
	movl	%edi, -128(%rbp)
	imull	%r15d, %r8d
	movl	%r8d, -132(%rbp)
	imull	%r14d, %r9d
	movl	%r9d, -136(%rbp)
	imull	%r13d, %r10d
	movl	%r10d, -140(%rbp)
	imull	-108(%rbp), %r11d
	movl	%r11d, -144(%rbp)
	movl	-84(%rbp), %r12d
	imull	%ebx, %r12d
	movl	%r12d, -84(%rbp)
.L142:
	movq	-152(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -152(%rbp)
	testq	%rax, %rax
	jne	.L143
	movl	-112(%rbp), %edi
	call	use_int@PLT
	movl	-116(%rbp), %edi
	call	use_int@PLT
	movl	-120(%rbp), %edi
	call	use_int@PLT
	movl	-124(%rbp), %edi
	call	use_int@PLT
	movl	-128(%rbp), %edi
	call	use_int@PLT
	movl	-132(%rbp), %edi
	call	use_int@PLT
	movl	-136(%rbp), %edi
	call	use_int@PLT
	movl	-140(%rbp), %edi
	call	use_int@PLT
	movl	-144(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	addq	$152, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE50:
	.size	integer_mul_9, .-integer_mul_9
	.globl	integer_mul_10
	.type	integer_mul_10, @function
integer_mul_10:
.LFB51:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -160(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37432(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	5(%rax), %edx
	movl	%edx, -152(%rbp)
	movl	%ecx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%ecx, %eax
	movl	%eax, -164(%rbp)
	addl	%eax, %ecx
	movl	%ecx, %r12d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	37431(%rax), %esi
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	4(%rax), %ecx
	movl	%ecx, -92(%rbp)
	movl	%esi, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	subl	%esi, %eax
	movl	%eax, -168(%rbp)
	addl	%eax, %esi
	movl	%esi, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	37430(%rax), %edi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	3(%rax), %esi
	movl	%esi, -96(%rbp)
	movl	%edi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	subl	%edi, %eax
	movl	%eax, -172(%rbp)
	addl	%eax, %edi
	movl	%edi, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	37429(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	2(%rax), %ebx
	movl	%ebx, -100(%rbp)
	movl	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%ecx, %eax
	movl	%eax, -176(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	37428(%rax), %esi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	1(%rax), %edi
	movl	%edi, -104(%rbp)
	movl	%esi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	subl	%esi, %eax
	movl	%eax, -180(%rbp)
	addl	%eax, %esi
	movl	%esi, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	37427(%rax), %edi
	movq	-56(%rbp), %rax
	movl	32(%rax), %edx
	movl	%edx, -108(%rbp)
	movl	%edi, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%edi, %eax
	movl	%eax, -184(%rbp)
	addl	%eax, %edi
	movl	%edi, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	37426(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	-1(%rax), %r15d
	movl	%ecx, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	subl	%ecx, %eax
	movl	%eax, -188(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	37425(%rax), %esi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	-2(%rax), %r14d
	movl	%esi, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	subl	%esi, %eax
	movl	%eax, -192(%rbp)
	addl	%eax, %esi
	movl	%esi, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	37424(%rax), %edi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	-3(%rax), %r13d
	movl	%edi, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	subl	%edi, %eax
	movl	%eax, -196(%rbp)
	addl	%eax, %edi
	movl	%edi, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	37423(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	-4(%rax), %edx
	movl	%edx, -112(%rbp)
	movl	%ecx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%ecx, %eax
	movl	%eax, -200(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	37422(%rax), %esi
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	-5(%rax), %ebx
	movl	%ebx, -116(%rbp)
	movl	%esi, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%esi, %eax
	movl	%eax, -204(%rbp)
	addl	%eax, %esi
	movl	%esi, -88(%rbp)
	jmp	.L145
.L146:
	movl	%r12d, %eax
	movl	-164(%rbp), %edi
	subl	%edi, %eax
	movl	-168(%rbp), %ecx
	subl	%ecx, -120(%rbp)
	movl	-120(%rbp), %edx
	movl	-172(%rbp), %esi
	subl	%esi, -124(%rbp)
	movl	-124(%rbp), %ecx
	movl	-176(%rbp), %edi
	subl	%edi, -128(%rbp)
	movl	-128(%rbp), %esi
	movl	-180(%rbp), %ebx
	subl	%ebx, -132(%rbp)
	movl	-132(%rbp), %edi
	movl	-184(%rbp), %ebx
	subl	%ebx, -136(%rbp)
	movl	-136(%rbp), %r8d
	movl	-188(%rbp), %ebx
	subl	%ebx, -140(%rbp)
	movl	-140(%rbp), %r9d
	movl	-192(%rbp), %ebx
	subl	%ebx, -144(%rbp)
	movl	-144(%rbp), %r10d
	movl	-196(%rbp), %ebx
	subl	%ebx, -148(%rbp)
	movl	-148(%rbp), %r11d
	movl	-200(%rbp), %ebx
	subl	%ebx, -84(%rbp)
	movl	-204(%rbp), %r12d
	subl	%r12d, -88(%rbp)
	movl	-152(%rbp), %r12d
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	-108(%rbp), %r8d
	imull	%r15d, %r9d
	imull	%r14d, %r10d
	imull	%r13d, %r11d
	movl	-84(%rbp), %ebx
	imull	-112(%rbp), %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-116(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	-108(%rbp), %r8d
	imull	%r15d, %r9d
	imull	%r14d, %r10d
	imull	%r13d, %r11d
	movl	-84(%rbp), %ebx
	imull	-112(%rbp), %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-116(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	-108(%rbp), %r8d
	imull	%r15d, %r9d
	imull	%r14d, %r10d
	imull	%r13d, %r11d
	movl	-84(%rbp), %ebx
	imull	-112(%rbp), %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-116(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	-108(%rbp), %r8d
	imull	%r15d, %r9d
	imull	%r14d, %r10d
	imull	%r13d, %r11d
	movl	-84(%rbp), %ebx
	imull	-112(%rbp), %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-116(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	-108(%rbp), %r8d
	imull	%r15d, %r9d
	imull	%r14d, %r10d
	imull	%r13d, %r11d
	movl	-84(%rbp), %ebx
	imull	-112(%rbp), %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-116(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	-108(%rbp), %r8d
	imull	%r15d, %r9d
	imull	%r14d, %r10d
	imull	%r13d, %r11d
	movl	-84(%rbp), %ebx
	imull	-112(%rbp), %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-116(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	-108(%rbp), %r8d
	imull	%r15d, %r9d
	imull	%r14d, %r10d
	imull	%r13d, %r11d
	movl	-84(%rbp), %ebx
	imull	-112(%rbp), %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-116(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	-108(%rbp), %r8d
	imull	%r15d, %r9d
	imull	%r14d, %r10d
	imull	%r13d, %r11d
	movl	-84(%rbp), %ebx
	imull	-112(%rbp), %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-116(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-92(%rbp), %edx
	imull	-96(%rbp), %ecx
	imull	-100(%rbp), %esi
	imull	-104(%rbp), %edi
	imull	-108(%rbp), %r8d
	imull	%r15d, %r9d
	imull	%r14d, %r10d
	imull	%r13d, %r11d
	movl	-84(%rbp), %ebx
	imull	-112(%rbp), %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-116(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	movl	%eax, %r12d
	imull	-92(%rbp), %edx
	movl	%edx, -120(%rbp)
	imull	-96(%rbp), %ecx
	movl	%ecx, -124(%rbp)
	imull	-100(%rbp), %esi
	movl	%esi, -128(%rbp)
	imull	-104(%rbp), %edi
	movl	%edi, -132(%rbp)
	imull	-108(%rbp), %r8d
	movl	%r8d, -136(%rbp)
	imull	%r15d, %r9d
	movl	%r9d, -140(%rbp)
	imull	%r14d, %r10d
	movl	%r10d, -144(%rbp)
	imull	%r13d, %r11d
	movl	%r11d, -148(%rbp)
	movl	-84(%rbp), %ebx
	imull	-112(%rbp), %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-116(%rbp), %ebx
	movl	%ebx, -88(%rbp)
.L145:
	movq	-160(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -160(%rbp)
	testq	%rax, %rax
	jne	.L146
	movl	%r12d, %edi
	call	use_int@PLT
	movl	-120(%rbp), %edi
	call	use_int@PLT
	movl	-124(%rbp), %edi
	call	use_int@PLT
	movl	-128(%rbp), %edi
	call	use_int@PLT
	movl	-132(%rbp), %edi
	call	use_int@PLT
	movl	-136(%rbp), %edi
	call	use_int@PLT
	movl	-140(%rbp), %edi
	call	use_int@PLT
	movl	-144(%rbp), %edi
	call	use_int@PLT
	movl	-148(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	addq	$168, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE51:
	.size	integer_mul_10, .-integer_mul_10
	.globl	integer_mul_11
	.type	integer_mul_11, @function
integer_mul_11:
.LFB52:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$184, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -168(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37432(%rax), %esi
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	5(%rax), %edx
	movl	%edx, -160(%rbp)
	movl	%esi, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%esi, %eax
	movl	%eax, -172(%rbp)
	addl	%eax, %esi
	movl	%esi, %r12d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	37431(%rax), %edi
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	4(%rax), %ecx
	movl	%ecx, -96(%rbp)
	movl	%edi, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	subl	%edi, %eax
	movl	%eax, -176(%rbp)
	addl	%eax, %edi
	movl	%edi, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	37430(%rax), %edi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	3(%rax), %esi
	movl	%esi, -100(%rbp)
	movl	%edi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	subl	%edi, %eax
	movl	%eax, -180(%rbp)
	addl	%eax, %edi
	movl	%edi, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	37429(%rax), %esi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	2(%rax), %edi
	movl	%edi, -104(%rbp)
	movl	%esi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	subl	%esi, %eax
	movl	%eax, -184(%rbp)
	addl	%eax, %esi
	movl	%esi, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	37428(%rax), %esi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	1(%rax), %ebx
	movl	%ebx, -108(%rbp)
	movl	%esi, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%esi, %eax
	movl	%eax, -188(%rbp)
	addl	%eax, %esi
	movl	%esi, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	37427(%rax), %edi
	movq	-56(%rbp), %rax
	movl	32(%rax), %edx
	movl	%edx, -112(%rbp)
	movl	%edi, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%edi, %eax
	movl	%eax, -192(%rbp)
	addl	%eax, %edi
	movl	%edi, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	37426(%rax), %esi
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	-1(%rax), %ecx
	movl	%ecx, -116(%rbp)
	movl	%esi, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	subl	%esi, %eax
	movl	%eax, -196(%rbp)
	addl	%eax, %esi
	movl	%esi, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	37425(%rax), %edx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	-2(%rax), %r15d
	movl	%edx, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	subl	%edx, %eax
	movl	%eax, -200(%rbp)
	addl	%eax, %edx
	movl	%edx, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	37424(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	-3(%rax), %r14d
	movl	%ecx, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	subl	%ecx, %eax
	movl	%eax, -204(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -156(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	37423(%rax), %edi
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	-4(%rax), %r13d
	movl	%edi, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	subl	%edi, %eax
	movl	%eax, -208(%rbp)
	addl	%eax, %edi
	movl	%edi, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	37422(%rax), %esi
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	-5(%rax), %ecx
	movl	%ecx, -120(%rbp)
	movl	%esi, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	subl	%esi, %eax
	movl	%eax, -212(%rbp)
	addl	%eax, %esi
	movl	%esi, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	37421(%rax), %edx
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	-6(%rax), %esi
	movl	%esi, -124(%rbp)
	movl	%edx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	subl	%edx, %eax
	movl	%eax, -216(%rbp)
	addl	%eax, %edx
	movl	%edx, -92(%rbp)
	jmp	.L148
.L149:
	movl	%r12d, %eax
	movl	-172(%rbp), %ecx
	subl	%ecx, %eax
	movl	-176(%rbp), %edi
	subl	%edi, -128(%rbp)
	movl	-128(%rbp), %edx
	movl	-180(%rbp), %esi
	subl	%esi, -132(%rbp)
	movl	-132(%rbp), %ecx
	movl	-184(%rbp), %edi
	subl	%edi, -136(%rbp)
	movl	-136(%rbp), %esi
	movl	-188(%rbp), %ebx
	subl	%ebx, -140(%rbp)
	movl	-140(%rbp), %edi
	movl	-192(%rbp), %ebx
	subl	%ebx, -144(%rbp)
	movl	-144(%rbp), %r8d
	movl	-196(%rbp), %ebx
	subl	%ebx, -148(%rbp)
	movl	-148(%rbp), %r9d
	movl	-200(%rbp), %ebx
	subl	%ebx, -152(%rbp)
	movl	-152(%rbp), %r10d
	movl	-204(%rbp), %ebx
	subl	%ebx, -156(%rbp)
	movl	-156(%rbp), %r11d
	movl	-208(%rbp), %ebx
	subl	%ebx, -84(%rbp)
	movl	-212(%rbp), %r12d
	subl	%r12d, -88(%rbp)
	movl	-216(%rbp), %r12d
	subl	%r12d, -92(%rbp)
	movl	-160(%rbp), %r12d
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-96(%rbp), %edx
	imull	-100(%rbp), %ecx
	imull	-104(%rbp), %esi
	imull	-108(%rbp), %edi
	imull	-112(%rbp), %r8d
	imull	-116(%rbp), %r9d
	imull	%r15d, %r10d
	imull	%r14d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-120(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-124(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-96(%rbp), %edx
	imull	-100(%rbp), %ecx
	imull	-104(%rbp), %esi
	imull	-108(%rbp), %edi
	imull	-112(%rbp), %r8d
	imull	-116(%rbp), %r9d
	imull	%r15d, %r10d
	imull	%r14d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-120(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-124(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-96(%rbp), %edx
	imull	-100(%rbp), %ecx
	imull	-104(%rbp), %esi
	imull	-108(%rbp), %edi
	imull	-112(%rbp), %r8d
	imull	-116(%rbp), %r9d
	imull	%r15d, %r10d
	imull	%r14d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-120(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-124(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-96(%rbp), %edx
	imull	-100(%rbp), %ecx
	imull	-104(%rbp), %esi
	imull	-108(%rbp), %edi
	imull	-112(%rbp), %r8d
	imull	-116(%rbp), %r9d
	imull	%r15d, %r10d
	imull	%r14d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-120(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-124(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-96(%rbp), %edx
	imull	-100(%rbp), %ecx
	imull	-104(%rbp), %esi
	imull	-108(%rbp), %edi
	imull	-112(%rbp), %r8d
	imull	-116(%rbp), %r9d
	imull	%r15d, %r10d
	imull	%r14d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-120(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-124(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-96(%rbp), %edx
	imull	-100(%rbp), %ecx
	imull	-104(%rbp), %esi
	imull	-108(%rbp), %edi
	imull	-112(%rbp), %r8d
	imull	-116(%rbp), %r9d
	imull	%r15d, %r10d
	imull	%r14d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-120(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-124(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-96(%rbp), %edx
	imull	-100(%rbp), %ecx
	imull	-104(%rbp), %esi
	imull	-108(%rbp), %edi
	imull	-112(%rbp), %r8d
	imull	-116(%rbp), %r9d
	imull	%r15d, %r10d
	imull	%r14d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-120(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-124(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-96(%rbp), %edx
	imull	-100(%rbp), %ecx
	imull	-104(%rbp), %esi
	imull	-108(%rbp), %edi
	imull	-112(%rbp), %r8d
	imull	-116(%rbp), %r9d
	imull	%r15d, %r10d
	imull	%r14d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-120(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-124(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-96(%rbp), %edx
	imull	-100(%rbp), %ecx
	imull	-104(%rbp), %esi
	imull	-108(%rbp), %edi
	imull	-112(%rbp), %r8d
	imull	-116(%rbp), %r9d
	imull	%r15d, %r10d
	imull	%r14d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-120(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-124(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	movl	%eax, %r12d
	imull	-96(%rbp), %edx
	movl	%edx, -128(%rbp)
	imull	-100(%rbp), %ecx
	movl	%ecx, -132(%rbp)
	imull	-104(%rbp), %esi
	movl	%esi, -136(%rbp)
	imull	-108(%rbp), %edi
	movl	%edi, -140(%rbp)
	imull	-112(%rbp), %r8d
	movl	%r8d, -144(%rbp)
	imull	-116(%rbp), %r9d
	movl	%r9d, -148(%rbp)
	imull	%r15d, %r10d
	movl	%r10d, -152(%rbp)
	imull	%r14d, %r11d
	movl	%r11d, -156(%rbp)
	movl	-84(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	-120(%rbp), %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-124(%rbp), %ebx
	movl	%ebx, -92(%rbp)
.L148:
	movq	-168(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -168(%rbp)
	testq	%rax, %rax
	jne	.L149
	movl	%r12d, %edi
	call	use_int@PLT
	movl	-128(%rbp), %edi
	call	use_int@PLT
	movl	-132(%rbp), %edi
	call	use_int@PLT
	movl	-136(%rbp), %edi
	call	use_int@PLT
	movl	-140(%rbp), %edi
	call	use_int@PLT
	movl	-144(%rbp), %edi
	call	use_int@PLT
	movl	-148(%rbp), %edi
	call	use_int@PLT
	movl	-152(%rbp), %edi
	call	use_int@PLT
	movl	-156(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	addq	$184, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE52:
	.size	integer_mul_11, .-integer_mul_11
	.globl	integer_mul_12
	.type	integer_mul_12, @function
integer_mul_12:
.LFB53:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$200, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -176(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37432(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	5(%rax), %edx
	movl	%edx, -168(%rbp)
	movl	%ecx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%ecx, %eax
	movl	%eax, -180(%rbp)
	addl	%eax, %ecx
	movl	%ecx, %r12d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	37431(%rax), %edx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	4(%rax), %ecx
	movl	%ecx, -100(%rbp)
	movl	%edx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	subl	%edx, %eax
	movl	%eax, -184(%rbp)
	addl	%eax, %edx
	movl	%edx, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	37430(%rax), %edx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	3(%rax), %esi
	movl	%esi, -104(%rbp)
	movl	%edx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	subl	%edx, %eax
	movl	%eax, -188(%rbp)
	addl	%eax, %edx
	movl	%edx, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	37429(%rax), %esi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	2(%rax), %edi
	movl	%edi, -108(%rbp)
	movl	%esi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	subl	%esi, %eax
	movl	%eax, -192(%rbp)
	addl	%eax, %esi
	movl	%esi, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	37428(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	1(%rax), %edx
	movl	%edx, -112(%rbp)
	movl	%ecx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%ecx, %eax
	movl	%eax, -196(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	37427(%rax), %esi
	movq	-56(%rbp), %rax
	movl	32(%rax), %ecx
	movl	%ecx, -116(%rbp)
	movl	%esi, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	subl	%esi, %eax
	movl	%eax, -200(%rbp)
	addl	%eax, %esi
	movl	%esi, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	37426(%rax), %edi
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	-1(%rax), %esi
	movl	%esi, -120(%rbp)
	movl	%edi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	subl	%edi, %eax
	movl	%eax, -204(%rbp)
	addl	%eax, %edi
	movl	%edi, -156(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	37425(%rax), %edx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	-2(%rax), %edi
	movl	%edi, -124(%rbp)
	movl	%edx, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	subl	%edx, %eax
	movl	%eax, -208(%rbp)
	addl	%eax, %edx
	movl	%edx, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	37424(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	-3(%rax), %r15d
	movl	%ecx, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	subl	%ecx, %eax
	movl	%eax, -212(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -164(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	37423(%rax), %esi
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	-4(%rax), %r14d
	movl	%esi, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	subl	%esi, %eax
	movl	%eax, -216(%rbp)
	addl	%eax, %esi
	movl	%esi, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	37422(%rax), %edi
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	-5(%rax), %r13d
	movl	%edi, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	subl	%edi, %eax
	movl	%eax, -220(%rbp)
	addl	%eax, %edi
	movl	%edi, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	37421(%rax), %edx
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	-6(%rax), %edi
	movl	%edi, -128(%rbp)
	movl	%edx, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	subl	%edx, %eax
	movl	%eax, -224(%rbp)
	addl	%eax, %edx
	movl	%edx, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	leal	37420(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	leal	-7(%rax), %ebx
	movl	%ebx, -132(%rbp)
	movl	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%ecx, %eax
	movl	%eax, -228(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -96(%rbp)
	jmp	.L151
.L152:
	movl	%r12d, %eax
	movl	-180(%rbp), %esi
	subl	%esi, %eax
	movl	-184(%rbp), %edi
	subl	%edi, -136(%rbp)
	movl	-136(%rbp), %edx
	movl	-188(%rbp), %esi
	subl	%esi, -140(%rbp)
	movl	-140(%rbp), %ecx
	movl	-192(%rbp), %edi
	subl	%edi, -144(%rbp)
	movl	-144(%rbp), %esi
	movl	-196(%rbp), %ebx
	subl	%ebx, -148(%rbp)
	movl	-148(%rbp), %edi
	movl	-200(%rbp), %ebx
	subl	%ebx, -152(%rbp)
	movl	-152(%rbp), %r8d
	movl	-204(%rbp), %ebx
	subl	%ebx, -156(%rbp)
	movl	-156(%rbp), %r9d
	movl	-208(%rbp), %ebx
	subl	%ebx, -160(%rbp)
	movl	-160(%rbp), %r10d
	movl	-212(%rbp), %ebx
	subl	%ebx, -164(%rbp)
	movl	-164(%rbp), %r11d
	movl	-216(%rbp), %ebx
	subl	%ebx, -84(%rbp)
	movl	-220(%rbp), %r12d
	subl	%r12d, -88(%rbp)
	movl	-224(%rbp), %r12d
	subl	%r12d, -92(%rbp)
	movl	-228(%rbp), %r12d
	subl	%r12d, -96(%rbp)
	movl	-168(%rbp), %r12d
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-100(%rbp), %edx
	imull	-104(%rbp), %ecx
	imull	-108(%rbp), %esi
	imull	-112(%rbp), %edi
	imull	-116(%rbp), %r8d
	imull	-120(%rbp), %r9d
	imull	-124(%rbp), %r10d
	imull	%r15d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-128(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-132(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-100(%rbp), %edx
	imull	-104(%rbp), %ecx
	imull	-108(%rbp), %esi
	imull	-112(%rbp), %edi
	imull	-116(%rbp), %r8d
	imull	-120(%rbp), %r9d
	imull	-124(%rbp), %r10d
	imull	%r15d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-128(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-132(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-100(%rbp), %edx
	imull	-104(%rbp), %ecx
	imull	-108(%rbp), %esi
	imull	-112(%rbp), %edi
	imull	-116(%rbp), %r8d
	imull	-120(%rbp), %r9d
	imull	-124(%rbp), %r10d
	imull	%r15d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-128(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-132(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-100(%rbp), %edx
	imull	-104(%rbp), %ecx
	imull	-108(%rbp), %esi
	imull	-112(%rbp), %edi
	imull	-116(%rbp), %r8d
	imull	-120(%rbp), %r9d
	imull	-124(%rbp), %r10d
	imull	%r15d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-128(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-132(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-100(%rbp), %edx
	imull	-104(%rbp), %ecx
	imull	-108(%rbp), %esi
	imull	-112(%rbp), %edi
	imull	-116(%rbp), %r8d
	imull	-120(%rbp), %r9d
	imull	-124(%rbp), %r10d
	imull	%r15d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-128(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-132(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-100(%rbp), %edx
	imull	-104(%rbp), %ecx
	imull	-108(%rbp), %esi
	imull	-112(%rbp), %edi
	imull	-116(%rbp), %r8d
	imull	-120(%rbp), %r9d
	imull	-124(%rbp), %r10d
	imull	%r15d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-128(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-132(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-100(%rbp), %edx
	imull	-104(%rbp), %ecx
	imull	-108(%rbp), %esi
	imull	-112(%rbp), %edi
	imull	-116(%rbp), %r8d
	imull	-120(%rbp), %r9d
	imull	-124(%rbp), %r10d
	imull	%r15d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-128(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-132(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-100(%rbp), %edx
	imull	-104(%rbp), %ecx
	imull	-108(%rbp), %esi
	imull	-112(%rbp), %edi
	imull	-116(%rbp), %r8d
	imull	-120(%rbp), %r9d
	imull	-124(%rbp), %r10d
	imull	%r15d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-128(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-132(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-100(%rbp), %edx
	imull	-104(%rbp), %ecx
	imull	-108(%rbp), %esi
	imull	-112(%rbp), %edi
	imull	-116(%rbp), %r8d
	imull	-120(%rbp), %r9d
	imull	-124(%rbp), %r10d
	imull	%r15d, %r11d
	movl	-84(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-128(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-132(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	movl	%eax, %r12d
	imull	-100(%rbp), %edx
	movl	%edx, -136(%rbp)
	imull	-104(%rbp), %ecx
	movl	%ecx, -140(%rbp)
	imull	-108(%rbp), %esi
	movl	%esi, -144(%rbp)
	imull	-112(%rbp), %edi
	movl	%edi, -148(%rbp)
	imull	-116(%rbp), %r8d
	movl	%r8d, -152(%rbp)
	imull	-120(%rbp), %r9d
	movl	%r9d, -156(%rbp)
	imull	-124(%rbp), %r10d
	movl	%r10d, -160(%rbp)
	imull	%r15d, %r11d
	movl	%r11d, -164(%rbp)
	movl	-84(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	-128(%rbp), %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-132(%rbp), %ebx
	movl	%ebx, -96(%rbp)
.L151:
	movq	-176(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -176(%rbp)
	testq	%rax, %rax
	jne	.L152
	movl	%r12d, %edi
	call	use_int@PLT
	movl	-136(%rbp), %edi
	call	use_int@PLT
	movl	-140(%rbp), %edi
	call	use_int@PLT
	movl	-144(%rbp), %edi
	call	use_int@PLT
	movl	-148(%rbp), %edi
	call	use_int@PLT
	movl	-152(%rbp), %edi
	call	use_int@PLT
	movl	-156(%rbp), %edi
	call	use_int@PLT
	movl	-160(%rbp), %edi
	call	use_int@PLT
	movl	-164(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	addq	$200, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE53:
	.size	integer_mul_12, .-integer_mul_12
	.globl	integer_mul_13
	.type	integer_mul_13, @function
integer_mul_13:
.LFB54:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$200, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -184(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37432(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	5(%rax), %edx
	movl	%edx, -176(%rbp)
	movl	%ecx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%ecx, %eax
	movl	%eax, -188(%rbp)
	addl	%eax, %ecx
	movl	%ecx, %r12d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	37431(%rax), %esi
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	4(%rax), %ecx
	movl	%ecx, -104(%rbp)
	movl	%esi, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	subl	%esi, %eax
	movl	%eax, -192(%rbp)
	addl	%eax, %esi
	movl	%esi, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	37430(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	3(%rax), %esi
	movl	%esi, -108(%rbp)
	movl	%ecx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	subl	%ecx, %eax
	movl	%eax, -196(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	37429(%rax), %esi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	2(%rax), %edi
	movl	%edi, -112(%rbp)
	movl	%esi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	subl	%esi, %eax
	movl	%eax, -200(%rbp)
	addl	%eax, %esi
	movl	%esi, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	37428(%rax), %edi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	1(%rax), %edx
	movl	%edx, -116(%rbp)
	movl	%edi, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%edi, %eax
	movl	%eax, -204(%rbp)
	addl	%eax, %edi
	movl	%edi, -156(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	37427(%rax), %esi
	movq	-56(%rbp), %rax
	movl	32(%rax), %ecx
	movl	%ecx, -120(%rbp)
	movl	%esi, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	subl	%esi, %eax
	movl	%eax, -208(%rbp)
	addl	%eax, %esi
	movl	%esi, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	37426(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	-1(%rax), %esi
	movl	%esi, -124(%rbp)
	movl	%ecx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	subl	%ecx, %eax
	movl	%eax, -212(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -164(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	37425(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	-2(%rax), %edi
	movl	%edi, -128(%rbp)
	movl	%ecx, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	subl	%ecx, %eax
	movl	%eax, -216(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	37424(%rax), %edi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	-3(%rax), %edx
	movl	%edx, -132(%rbp)
	movl	%edi, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%edi, %eax
	movl	%eax, -220(%rbp)
	addl	%eax, %edi
	movl	%edi, -172(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	37423(%rax), %esi
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	-4(%rax), %r15d
	movl	%esi, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	subl	%esi, %eax
	movl	%eax, -224(%rbp)
	addl	%eax, %esi
	movl	%esi, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	37422(%rax), %edi
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	-5(%rax), %r14d
	movl	%edi, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	subl	%edi, %eax
	movl	%eax, -228(%rbp)
	addl	%eax, %edi
	movl	%edi, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	37421(%rax), %edx
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	-6(%rax), %r13d
	movl	%edx, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	subl	%edx, %eax
	movl	%eax, -232(%rbp)
	addl	%eax, %edx
	movl	%edx, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	leal	37420(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	leal	-7(%rax), %edi
	movl	%edi, -136(%rbp)
	movl	%ecx, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	subl	%ecx, %eax
	movl	%eax, -236(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	leal	37419(%rax), %esi
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	leal	-8(%rax), %ebx
	movl	%ebx, -140(%rbp)
	movl	%esi, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%esi, %eax
	movl	%eax, -240(%rbp)
	addl	%eax, %esi
	movl	%esi, -100(%rbp)
	jmp	.L154
.L155:
	movl	%r12d, %eax
	movl	-188(%rbp), %edi
	subl	%edi, %eax
	movl	-192(%rbp), %edi
	subl	%edi, -144(%rbp)
	movl	-144(%rbp), %edx
	movl	-196(%rbp), %edi
	subl	%edi, -148(%rbp)
	movl	-148(%rbp), %ecx
	movl	-200(%rbp), %edi
	subl	%edi, -152(%rbp)
	movl	-152(%rbp), %esi
	movl	-204(%rbp), %ebx
	subl	%ebx, -156(%rbp)
	movl	-156(%rbp), %edi
	movl	-208(%rbp), %ebx
	subl	%ebx, -160(%rbp)
	movl	-160(%rbp), %r8d
	movl	-212(%rbp), %ebx
	subl	%ebx, -164(%rbp)
	movl	-164(%rbp), %r9d
	movl	-216(%rbp), %ebx
	subl	%ebx, -168(%rbp)
	movl	-168(%rbp), %r10d
	movl	-220(%rbp), %ebx
	subl	%ebx, -172(%rbp)
	movl	-172(%rbp), %r11d
	movl	-224(%rbp), %ebx
	subl	%ebx, -84(%rbp)
	movl	-228(%rbp), %r12d
	subl	%r12d, -88(%rbp)
	movl	-232(%rbp), %r12d
	subl	%r12d, -92(%rbp)
	movl	-236(%rbp), %r12d
	subl	%r12d, -96(%rbp)
	movl	-240(%rbp), %r12d
	subl	%r12d, -100(%rbp)
	movl	-176(%rbp), %r12d
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-104(%rbp), %edx
	imull	-108(%rbp), %ecx
	imull	-112(%rbp), %esi
	imull	-116(%rbp), %edi
	imull	-120(%rbp), %r8d
	imull	-124(%rbp), %r9d
	imull	-128(%rbp), %r10d
	imull	-132(%rbp), %r11d
	movl	-84(%rbp), %ebx
	imull	%r15d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-136(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	-100(%rbp), %ebx
	imull	-140(%rbp), %ebx
	movl	%ebx, -100(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-104(%rbp), %edx
	imull	-108(%rbp), %ecx
	imull	-112(%rbp), %esi
	imull	-116(%rbp), %edi
	imull	-120(%rbp), %r8d
	imull	-124(%rbp), %r9d
	imull	-128(%rbp), %r10d
	imull	-132(%rbp), %r11d
	movl	-84(%rbp), %ebx
	imull	%r15d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-136(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	-100(%rbp), %ebx
	imull	-140(%rbp), %ebx
	movl	%ebx, -100(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-104(%rbp), %edx
	imull	-108(%rbp), %ecx
	imull	-112(%rbp), %esi
	imull	-116(%rbp), %edi
	imull	-120(%rbp), %r8d
	imull	-124(%rbp), %r9d
	imull	-128(%rbp), %r10d
	imull	-132(%rbp), %r11d
	movl	-84(%rbp), %ebx
	imull	%r15d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-136(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	-100(%rbp), %ebx
	imull	-140(%rbp), %ebx
	movl	%ebx, -100(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-104(%rbp), %edx
	imull	-108(%rbp), %ecx
	imull	-112(%rbp), %esi
	imull	-116(%rbp), %edi
	imull	-120(%rbp), %r8d
	imull	-124(%rbp), %r9d
	imull	-128(%rbp), %r10d
	imull	-132(%rbp), %r11d
	movl	-84(%rbp), %ebx
	imull	%r15d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-136(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	-100(%rbp), %ebx
	imull	-140(%rbp), %ebx
	movl	%ebx, -100(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-104(%rbp), %edx
	imull	-108(%rbp), %ecx
	imull	-112(%rbp), %esi
	imull	-116(%rbp), %edi
	imull	-120(%rbp), %r8d
	imull	-124(%rbp), %r9d
	imull	-128(%rbp), %r10d
	imull	-132(%rbp), %r11d
	movl	-84(%rbp), %ebx
	imull	%r15d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-136(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	-100(%rbp), %ebx
	imull	-140(%rbp), %ebx
	movl	%ebx, -100(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-104(%rbp), %edx
	imull	-108(%rbp), %ecx
	imull	-112(%rbp), %esi
	imull	-116(%rbp), %edi
	imull	-120(%rbp), %r8d
	imull	-124(%rbp), %r9d
	imull	-128(%rbp), %r10d
	imull	-132(%rbp), %r11d
	movl	-84(%rbp), %ebx
	imull	%r15d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-136(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	-100(%rbp), %ebx
	imull	-140(%rbp), %ebx
	movl	%ebx, -100(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-104(%rbp), %edx
	imull	-108(%rbp), %ecx
	imull	-112(%rbp), %esi
	imull	-116(%rbp), %edi
	imull	-120(%rbp), %r8d
	imull	-124(%rbp), %r9d
	imull	-128(%rbp), %r10d
	imull	-132(%rbp), %r11d
	movl	-84(%rbp), %ebx
	imull	%r15d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-136(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	-100(%rbp), %ebx
	imull	-140(%rbp), %ebx
	movl	%ebx, -100(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-104(%rbp), %edx
	imull	-108(%rbp), %ecx
	imull	-112(%rbp), %esi
	imull	-116(%rbp), %edi
	imull	-120(%rbp), %r8d
	imull	-124(%rbp), %r9d
	imull	-128(%rbp), %r10d
	imull	-132(%rbp), %r11d
	movl	-84(%rbp), %ebx
	imull	%r15d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-136(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	-100(%rbp), %ebx
	imull	-140(%rbp), %ebx
	movl	%ebx, -100(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	imull	-104(%rbp), %edx
	imull	-108(%rbp), %ecx
	imull	-112(%rbp), %esi
	imull	-116(%rbp), %edi
	imull	-120(%rbp), %r8d
	imull	-124(%rbp), %r9d
	imull	-128(%rbp), %r10d
	imull	-132(%rbp), %r11d
	movl	-84(%rbp), %ebx
	imull	%r15d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-136(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	-100(%rbp), %ebx
	imull	-140(%rbp), %ebx
	movl	%ebx, -100(%rbp)
	movl	%r12d, %ebx
	imull	%ebx, %eax
	movl	%eax, %r12d
	imull	-104(%rbp), %edx
	movl	%edx, -144(%rbp)
	imull	-108(%rbp), %ecx
	movl	%ecx, -148(%rbp)
	imull	-112(%rbp), %esi
	movl	%esi, -152(%rbp)
	imull	-116(%rbp), %edi
	movl	%edi, -156(%rbp)
	imull	-120(%rbp), %r8d
	movl	%r8d, -160(%rbp)
	imull	-124(%rbp), %r9d
	movl	%r9d, -164(%rbp)
	imull	-128(%rbp), %r10d
	movl	%r10d, -168(%rbp)
	imull	-132(%rbp), %r11d
	movl	%r11d, -172(%rbp)
	movl	-84(%rbp), %ebx
	imull	%r15d, %ebx
	movl	%ebx, -84(%rbp)
	movl	-88(%rbp), %ebx
	imull	%r14d, %ebx
	movl	%ebx, -88(%rbp)
	movl	-92(%rbp), %ebx
	imull	%r13d, %ebx
	movl	%ebx, -92(%rbp)
	movl	-96(%rbp), %ebx
	imull	-136(%rbp), %ebx
	movl	%ebx, -96(%rbp)
	movl	-100(%rbp), %ebx
	imull	-140(%rbp), %ebx
	movl	%ebx, -100(%rbp)
.L154:
	movq	-184(%rbp), %rax
	leaq	-1(%rax), %rdi
	movq	%rdi, -184(%rbp)
	testq	%rax, %rax
	jne	.L155
	movl	%r12d, %edi
	call	use_int@PLT
	movl	-144(%rbp), %edi
	call	use_int@PLT
	movl	-148(%rbp), %edi
	call	use_int@PLT
	movl	-152(%rbp), %edi
	call	use_int@PLT
	movl	-156(%rbp), %edi
	call	use_int@PLT
	movl	-160(%rbp), %edi
	call	use_int@PLT
	movl	-164(%rbp), %edi
	call	use_int@PLT
	movl	-168(%rbp), %edi
	call	use_int@PLT
	movl	-172(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	addq	$200, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE54:
	.size	integer_mul_13, .-integer_mul_13
	.globl	integer_mul_14
	.type	integer_mul_14, @function
integer_mul_14:
.LFB55:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$216, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -192(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37432(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	5(%rax), %ebx
	movl	%ebx, -112(%rbp)
	movl	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%ecx, %eax
	movl	%eax, -184(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	37431(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	4(%rax), %ecx
	movl	%ecx, -116(%rbp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	subl	%ebx, %eax
	movl	%eax, -196(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	37430(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	3(%rax), %esi
	movl	%esi, -120(%rbp)
	movl	%ebx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	subl	%ebx, %eax
	movl	%eax, -200(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -156(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	37429(%rax), %esi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	2(%rax), %ebx
	movl	%ebx, -124(%rbp)
	movl	%esi, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%esi, %eax
	movl	%eax, -204(%rbp)
	addl	%eax, %esi
	movl	%esi, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	37428(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	1(%rax), %edi
	movl	%edi, -128(%rbp)
	movl	%ecx, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	subl	%ecx, %eax
	movl	%eax, -208(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -164(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	37427(%rax), %esi
	movq	-56(%rbp), %rax
	movl	32(%rax), %edx
	movl	%edx, -132(%rbp)
	movl	%esi, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%esi, %eax
	movl	%eax, -212(%rbp)
	addl	%eax, %esi
	movl	%esi, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	37426(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	-1(%rax), %ecx
	movl	%ecx, -136(%rbp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	subl	%ebx, %eax
	movl	%eax, -216(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -172(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	37425(%rax), %edi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	-2(%rax), %esi
	movl	%esi, -140(%rbp)
	movl	%edi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	subl	%edi, %eax
	movl	%eax, -220(%rbp)
	addl	%eax, %edi
	movl	%edi, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	37424(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	-3(%rax), %ebx
	movl	%ebx, -144(%rbp)
	movl	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%ecx, %eax
	movl	%eax, -224(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -180(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	37423(%rax), %esi
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	-4(%rax), %edi
	movl	%edi, -148(%rbp)
	movl	%esi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	subl	%esi, %eax
	movl	%eax, -228(%rbp)
	addl	%eax, %esi
	movl	%esi, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	37422(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	-5(%rax), %r15d
	movl	%ebx, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	subl	%ebx, %eax
	movl	%eax, -232(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	37421(%rax), %edx
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	-6(%rax), %r14d
	movl	%edx, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	subl	%edx, %eax
	movl	%eax, -236(%rbp)
	addl	%eax, %edx
	movl	%edx, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	leal	37420(%rax), %edi
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	leal	-7(%rax), %r13d
	movl	%edi, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	subl	%edi, %eax
	movl	%eax, -240(%rbp)
	addl	%eax, %edi
	movl	%edi, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	leal	37419(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	leal	-8(%rax), %r12d
	movl	%ecx, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	subl	%ecx, %eax
	movl	%eax, -244(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	leal	37418(%rax), %esi
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	leal	-9(%rax), %ebx
	movl	%esi, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%esi, %eax
	movl	%eax, -248(%rbp)
	addl	%eax, %esi
	movl	%esi, -104(%rbp)
	jmp	.L157
.L158:
	movl	-184(%rbp), %edx
	subl	%edx, -152(%rbp)
	movl	-152(%rbp), %eax
	movl	-196(%rbp), %edi
	subl	%edi, -108(%rbp)
	movl	-108(%rbp), %edx
	movl	-200(%rbp), %edi
	subl	%edi, -156(%rbp)
	movl	-204(%rbp), %edi
	subl	%edi, -160(%rbp)
	movl	-160(%rbp), %esi
	movl	-208(%rbp), %r10d
	subl	%r10d, -164(%rbp)
	movl	-164(%rbp), %edi
	movl	-212(%rbp), %r11d
	subl	%r11d, -168(%rbp)
	movl	-168(%rbp), %r8d
	movl	-216(%rbp), %r10d
	subl	%r10d, -172(%rbp)
	movl	-172(%rbp), %r9d
	movl	-220(%rbp), %r11d
	subl	%r11d, -176(%rbp)
	movl	-176(%rbp), %r10d
	movl	-224(%rbp), %ecx
	subl	%ecx, -180(%rbp)
	movl	-180(%rbp), %r11d
	movl	-228(%rbp), %ecx
	subl	%ecx, -84(%rbp)
	movl	-232(%rbp), %ecx
	subl	%ecx, -88(%rbp)
	movl	-236(%rbp), %ecx
	subl	%ecx, -92(%rbp)
	movl	-240(%rbp), %ecx
	subl	%ecx, -96(%rbp)
	movl	-244(%rbp), %ecx
	subl	%ecx, -100(%rbp)
	movl	-248(%rbp), %ecx
	subl	%ecx, -104(%rbp)
	imull	-112(%rbp), %eax
	imull	-116(%rbp), %edx
	movl	%edx, -108(%rbp)
	movl	-156(%rbp), %ecx
	imull	-120(%rbp), %ecx
	imull	-124(%rbp), %esi
	imull	-128(%rbp), %edi
	imull	-132(%rbp), %r8d
	imull	-136(%rbp), %r9d
	imull	-140(%rbp), %r10d
	imull	-144(%rbp), %r11d
	movl	-84(%rbp), %edx
	imull	-148(%rbp), %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %edx
	imull	%r15d, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %edx
	imull	%r14d, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %edx
	imull	%r13d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %edx
	imull	%r12d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %edx
	imull	%ebx, %edx
	movl	%edx, -104(%rbp)
	imull	-112(%rbp), %eax
	movl	%eax, -152(%rbp)
	movl	-108(%rbp), %edx
	imull	-116(%rbp), %edx
	imull	-120(%rbp), %ecx
	imull	-124(%rbp), %esi
	imull	-128(%rbp), %edi
	imull	-132(%rbp), %r8d
	imull	-136(%rbp), %r9d
	imull	-140(%rbp), %r10d
	imull	-144(%rbp), %r11d
	movl	-84(%rbp), %eax
	imull	-148(%rbp), %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	imull	%r15d, %eax
	movl	%eax, -88(%rbp)
	movl	-92(%rbp), %eax
	imull	%r14d, %eax
	movl	%eax, -92(%rbp)
	movl	-96(%rbp), %eax
	imull	%r13d, %eax
	movl	%eax, -96(%rbp)
	movl	-100(%rbp), %eax
	imull	%r12d, %eax
	movl	%eax, -100(%rbp)
	movl	-104(%rbp), %eax
	imull	%ebx, %eax
	movl	%eax, -104(%rbp)
	movl	-152(%rbp), %eax
	imull	-112(%rbp), %eax
	imull	-116(%rbp), %edx
	movl	%edx, -108(%rbp)
	imull	-120(%rbp), %ecx
	imull	-124(%rbp), %esi
	imull	-128(%rbp), %edi
	imull	-132(%rbp), %r8d
	imull	-136(%rbp), %r9d
	imull	-140(%rbp), %r10d
	imull	-144(%rbp), %r11d
	movl	-84(%rbp), %edx
	imull	-148(%rbp), %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %edx
	imull	%r15d, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %edx
	imull	%r14d, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %edx
	imull	%r13d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %edx
	imull	%r12d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %edx
	imull	%ebx, %edx
	movl	%edx, -104(%rbp)
	imull	-112(%rbp), %eax
	movl	-108(%rbp), %edx
	imull	-116(%rbp), %edx
	imull	-120(%rbp), %ecx
	movl	%ecx, -156(%rbp)
	imull	-124(%rbp), %esi
	imull	-128(%rbp), %edi
	imull	-132(%rbp), %r8d
	imull	-136(%rbp), %r9d
	imull	-140(%rbp), %r10d
	imull	-144(%rbp), %r11d
	movl	-84(%rbp), %ecx
	imull	-148(%rbp), %ecx
	movl	%ecx, -84(%rbp)
	movl	-88(%rbp), %ecx
	imull	%r15d, %ecx
	movl	%ecx, -88(%rbp)
	movl	-92(%rbp), %ecx
	imull	%r14d, %ecx
	movl	%ecx, -92(%rbp)
	movl	-96(%rbp), %ecx
	imull	%r13d, %ecx
	movl	%ecx, -96(%rbp)
	movl	-100(%rbp), %ecx
	imull	%r12d, %ecx
	movl	%ecx, -100(%rbp)
	movl	-104(%rbp), %ecx
	imull	%ebx, %ecx
	movl	%ecx, -104(%rbp)
	imull	-112(%rbp), %eax
	imull	-116(%rbp), %edx
	movl	-156(%rbp), %ecx
	imull	-120(%rbp), %ecx
	imull	-124(%rbp), %esi
	movl	%esi, -160(%rbp)
	imull	-128(%rbp), %edi
	imull	-132(%rbp), %r8d
	imull	-136(%rbp), %r9d
	imull	-140(%rbp), %r10d
	imull	-144(%rbp), %r11d
	movl	-84(%rbp), %esi
	imull	-148(%rbp), %esi
	movl	%esi, -84(%rbp)
	movl	-88(%rbp), %esi
	imull	%r15d, %esi
	movl	%esi, -88(%rbp)
	movl	-92(%rbp), %esi
	imull	%r14d, %esi
	movl	%esi, -92(%rbp)
	movl	-96(%rbp), %esi
	imull	%r13d, %esi
	movl	%esi, -96(%rbp)
	movl	-100(%rbp), %esi
	imull	%r12d, %esi
	movl	%esi, -100(%rbp)
	movl	-104(%rbp), %esi
	imull	%ebx, %esi
	movl	%esi, -104(%rbp)
	imull	-112(%rbp), %eax
	imull	-116(%rbp), %edx
	imull	-120(%rbp), %ecx
	movl	-160(%rbp), %esi
	imull	-124(%rbp), %esi
	imull	-128(%rbp), %edi
	movl	%edi, -164(%rbp)
	imull	-132(%rbp), %r8d
	imull	-136(%rbp), %r9d
	imull	-140(%rbp), %r10d
	imull	-144(%rbp), %r11d
	movl	-84(%rbp), %edi
	imull	-148(%rbp), %edi
	movl	%edi, -84(%rbp)
	movl	-88(%rbp), %edi
	imull	%r15d, %edi
	movl	%edi, -88(%rbp)
	movl	-92(%rbp), %edi
	imull	%r14d, %edi
	movl	%edi, -92(%rbp)
	movl	-96(%rbp), %edi
	imull	%r13d, %edi
	movl	%edi, -96(%rbp)
	movl	-100(%rbp), %edi
	imull	%r12d, %edi
	movl	%edi, -100(%rbp)
	movl	-104(%rbp), %edi
	imull	%ebx, %edi
	movl	%edi, -104(%rbp)
	imull	-112(%rbp), %eax
	imull	-116(%rbp), %edx
	imull	-120(%rbp), %ecx
	imull	-124(%rbp), %esi
	movl	-164(%rbp), %edi
	imull	-128(%rbp), %edi
	imull	-132(%rbp), %r8d
	movl	%r8d, -168(%rbp)
	imull	-136(%rbp), %r9d
	imull	-140(%rbp), %r10d
	imull	-144(%rbp), %r11d
	movl	-84(%rbp), %r8d
	imull	-148(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	movl	-88(%rbp), %r8d
	imull	%r15d, %r8d
	movl	%r8d, -88(%rbp)
	movl	-92(%rbp), %r8d
	imull	%r14d, %r8d
	movl	%r8d, -92(%rbp)
	movl	-96(%rbp), %r8d
	imull	%r13d, %r8d
	movl	%r8d, -96(%rbp)
	movl	-100(%rbp), %r8d
	imull	%r12d, %r8d
	movl	%r8d, -100(%rbp)
	movl	-104(%rbp), %r8d
	imull	%ebx, %r8d
	movl	%r8d, -104(%rbp)
	imull	-112(%rbp), %eax
	imull	-116(%rbp), %edx
	imull	-120(%rbp), %ecx
	imull	-124(%rbp), %esi
	imull	-128(%rbp), %edi
	movl	-168(%rbp), %r8d
	imull	-132(%rbp), %r8d
	imull	-136(%rbp), %r9d
	movl	%r9d, -172(%rbp)
	imull	-140(%rbp), %r10d
	imull	-144(%rbp), %r11d
	movl	-84(%rbp), %r9d
	imull	-148(%rbp), %r9d
	movl	%r9d, -84(%rbp)
	movl	-88(%rbp), %r9d
	imull	%r15d, %r9d
	movl	%r9d, -88(%rbp)
	movl	-92(%rbp), %r9d
	imull	%r14d, %r9d
	movl	%r9d, -92(%rbp)
	movl	-96(%rbp), %r9d
	imull	%r13d, %r9d
	movl	%r9d, -96(%rbp)
	movl	-100(%rbp), %r9d
	imull	%r12d, %r9d
	movl	%r9d, -100(%rbp)
	movl	-104(%rbp), %r9d
	imull	%ebx, %r9d
	movl	%r9d, -104(%rbp)
	imull	-112(%rbp), %eax
	imull	-116(%rbp), %edx
	movl	%edx, -108(%rbp)
	imull	-120(%rbp), %ecx
	imull	-124(%rbp), %esi
	imull	-128(%rbp), %edi
	imull	-132(%rbp), %r8d
	movl	-172(%rbp), %r9d
	imull	-136(%rbp), %r9d
	imull	-140(%rbp), %r10d
	imull	-144(%rbp), %r11d
	movl	-84(%rbp), %edx
	imull	-148(%rbp), %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %edx
	imull	%r15d, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %edx
	imull	%r14d, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %edx
	imull	%r13d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %edx
	imull	%r12d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %edx
	imull	%ebx, %edx
	movl	%edx, -104(%rbp)
	imull	-112(%rbp), %eax
	movl	%eax, -152(%rbp)
	movl	-108(%rbp), %edx
	imull	-116(%rbp), %edx
	movl	%edx, -108(%rbp)
	imull	-120(%rbp), %ecx
	movl	%ecx, -156(%rbp)
	imull	-124(%rbp), %esi
	movl	%esi, -160(%rbp)
	imull	-128(%rbp), %edi
	movl	%edi, -164(%rbp)
	imull	-132(%rbp), %r8d
	movl	%r8d, -168(%rbp)
	imull	-136(%rbp), %r9d
	movl	%r9d, -172(%rbp)
	imull	-140(%rbp), %r10d
	movl	%r10d, -176(%rbp)
	imull	-144(%rbp), %r11d
	movl	%r11d, -180(%rbp)
	movl	-84(%rbp), %edx
	imull	-148(%rbp), %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %edx
	imull	%r15d, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %edx
	imull	%r14d, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %edx
	imull	%r13d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %edx
	imull	%r12d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %edx
	imull	%ebx, %edx
	movl	%edx, -104(%rbp)
.L157:
	movq	-192(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -192(%rbp)
	testq	%rax, %rax
	jne	.L158
	movl	-152(%rbp), %edi
	call	use_int@PLT
	movl	-108(%rbp), %edi
	call	use_int@PLT
	movl	-156(%rbp), %edi
	call	use_int@PLT
	movl	-160(%rbp), %edi
	call	use_int@PLT
	movl	-164(%rbp), %edi
	call	use_int@PLT
	movl	-168(%rbp), %edi
	call	use_int@PLT
	movl	-172(%rbp), %edi
	call	use_int@PLT
	movl	-176(%rbp), %edi
	call	use_int@PLT
	movl	-180(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-104(%rbp), %edi
	call	use_int@PLT
	addq	$216, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE55:
	.size	integer_mul_14, .-integer_mul_14
	.globl	integer_mul_15
	.type	integer_mul_15, @function
integer_mul_15:
.LFB56:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$232, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -200(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37432(%rax), %esi
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	5(%rax), %ebx
	movl	%ebx, -116(%rbp)
	movl	%esi, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%esi, %eax
	movl	%eax, -192(%rbp)
	addl	%eax, %esi
	movl	%esi, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	37431(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	4(%rax), %ecx
	movl	%ecx, -120(%rbp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	subl	%ebx, %eax
	movl	%eax, -204(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	37430(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	3(%rax), %esi
	movl	%esi, -124(%rbp)
	movl	%ebx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	subl	%ebx, %eax
	movl	%eax, -208(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -164(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	37429(%rax), %esi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	2(%rax), %ebx
	movl	%ebx, -128(%rbp)
	movl	%esi, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%esi, %eax
	movl	%eax, -212(%rbp)
	addl	%eax, %esi
	movl	%esi, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	37428(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	1(%rax), %edi
	movl	%edi, -132(%rbp)
	movl	%ebx, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	subl	%ebx, %eax
	movl	%eax, -216(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -172(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	37427(%rax), %edi
	movq	-56(%rbp), %rax
	movl	32(%rax), %ecx
	movl	%ecx, -136(%rbp)
	movl	%edi, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	subl	%edi, %eax
	movl	%eax, -220(%rbp)
	addl	%eax, %edi
	movl	%edi, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	37426(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	-1(%rax), %esi
	movl	%esi, -140(%rbp)
	movl	%ebx, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	imull	%esi, %eax
	subl	%ebx, %eax
	movl	%eax, -224(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -180(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	37425(%rax), %esi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	-2(%rax), %ebx
	movl	%ebx, -144(%rbp)
	movl	%esi, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%esi, %eax
	movl	%eax, -228(%rbp)
	addl	%eax, %esi
	movl	%esi, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	37424(%rax), %edi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	-3(%rax), %edx
	movl	%edx, -148(%rbp)
	movl	%edi, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	imull	%edx, %eax
	subl	%edi, %eax
	movl	%eax, -232(%rbp)
	addl	%eax, %edi
	movl	%edi, -188(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	37423(%rax), %edx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	-4(%rax), %edi
	movl	%edi, -152(%rbp)
	movl	%edx, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	imull	%edi, %eax
	subl	%edx, %eax
	movl	%eax, -236(%rbp)
	addl	%eax, %edx
	movl	%edx, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	37422(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	-5(%rax), %ecx
	movl	%ecx, -156(%rbp)
	movl	%ebx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	imull	%ecx, %eax
	subl	%ebx, %eax
	movl	%eax, -240(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	37421(%rax), %esi
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	-6(%rax), %r15d
	movl	%esi, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	imull	%r15d, %eax
	subl	%esi, %eax
	movl	%eax, -244(%rbp)
	addl	%eax, %esi
	movl	%esi, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	leal	37420(%rax), %edi
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	leal	-7(%rax), %r14d
	movl	%edi, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	imull	%r14d, %eax
	subl	%edi, %eax
	movl	%eax, -248(%rbp)
	addl	%eax, %edi
	movl	%edi, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	leal	37419(%rax), %edx
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	leal	-8(%rax), %r13d
	movl	%edx, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	imull	%r13d, %eax
	subl	%edx, %eax
	movl	%eax, -252(%rbp)
	addl	%eax, %edx
	movl	%edx, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	leal	37418(%rax), %ebx
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	leal	-9(%rax), %r12d
	movl	%ebx, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	imull	%r12d, %eax
	subl	%ebx, %eax
	movl	%eax, -256(%rbp)
	addl	%eax, %ebx
	movl	%ebx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	leal	37417(%rax), %ecx
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	leal	-10(%rax), %ebx
	movl	%ecx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	imull	%ebx, %eax
	subl	%ecx, %eax
	movl	%eax, -260(%rbp)
	addl	%eax, %ecx
	movl	%ecx, -108(%rbp)
	jmp	.L160
.L161:
	movl	-192(%rbp), %esi
	subl	%esi, -160(%rbp)
	movl	-160(%rbp), %eax
	movl	-204(%rbp), %esi
	subl	%esi, -112(%rbp)
	movl	-112(%rbp), %edx
	movl	-208(%rbp), %esi
	subl	%esi, -164(%rbp)
	movl	-212(%rbp), %edi
	subl	%edi, -168(%rbp)
	movl	-168(%rbp), %esi
	movl	-216(%rbp), %r10d
	subl	%r10d, -172(%rbp)
	movl	-172(%rbp), %edi
	movl	-220(%rbp), %r11d
	subl	%r11d, -176(%rbp)
	movl	-176(%rbp), %r8d
	movl	-224(%rbp), %r10d
	subl	%r10d, -180(%rbp)
	movl	-180(%rbp), %r9d
	movl	-228(%rbp), %r11d
	subl	%r11d, -184(%rbp)
	movl	-184(%rbp), %r10d
	movl	-232(%rbp), %ecx
	subl	%ecx, -188(%rbp)
	movl	-188(%rbp), %r11d
	movl	-236(%rbp), %ecx
	subl	%ecx, -84(%rbp)
	movl	-240(%rbp), %ecx
	subl	%ecx, -88(%rbp)
	movl	-244(%rbp), %ecx
	subl	%ecx, -92(%rbp)
	movl	-248(%rbp), %ecx
	subl	%ecx, -96(%rbp)
	movl	-252(%rbp), %ecx
	subl	%ecx, -100(%rbp)
	movl	-256(%rbp), %ecx
	subl	%ecx, -104(%rbp)
	movl	-260(%rbp), %ecx
	subl	%ecx, -108(%rbp)
	imull	-116(%rbp), %eax
	imull	-120(%rbp), %edx
	movl	%edx, -112(%rbp)
	movl	-164(%rbp), %ecx
	imull	-124(%rbp), %ecx
	imull	-128(%rbp), %esi
	imull	-132(%rbp), %edi
	imull	-136(%rbp), %r8d
	imull	-140(%rbp), %r9d
	imull	-144(%rbp), %r10d
	imull	-148(%rbp), %r11d
	movl	-84(%rbp), %edx
	imull	-152(%rbp), %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %edx
	imull	-156(%rbp), %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %edx
	imull	%r15d, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %edx
	imull	%r14d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %edx
	imull	%r13d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %edx
	imull	%r12d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %edx
	imull	%ebx, %edx
	movl	%edx, -108(%rbp)
	imull	-116(%rbp), %eax
	movl	%eax, -160(%rbp)
	movl	-112(%rbp), %edx
	imull	-120(%rbp), %edx
	imull	-124(%rbp), %ecx
	imull	-128(%rbp), %esi
	imull	-132(%rbp), %edi
	imull	-136(%rbp), %r8d
	imull	-140(%rbp), %r9d
	imull	-144(%rbp), %r10d
	imull	-148(%rbp), %r11d
	movl	-84(%rbp), %eax
	imull	-152(%rbp), %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	imull	-156(%rbp), %eax
	movl	%eax, -88(%rbp)
	movl	-92(%rbp), %eax
	imull	%r15d, %eax
	movl	%eax, -92(%rbp)
	movl	-96(%rbp), %eax
	imull	%r14d, %eax
	movl	%eax, -96(%rbp)
	movl	-100(%rbp), %eax
	imull	%r13d, %eax
	movl	%eax, -100(%rbp)
	movl	-104(%rbp), %eax
	imull	%r12d, %eax
	movl	%eax, -104(%rbp)
	movl	-108(%rbp), %eax
	imull	%ebx, %eax
	movl	%eax, -108(%rbp)
	movl	-160(%rbp), %eax
	imull	-116(%rbp), %eax
	imull	-120(%rbp), %edx
	movl	%edx, -112(%rbp)
	imull	-124(%rbp), %ecx
	imull	-128(%rbp), %esi
	imull	-132(%rbp), %edi
	imull	-136(%rbp), %r8d
	imull	-140(%rbp), %r9d
	imull	-144(%rbp), %r10d
	imull	-148(%rbp), %r11d
	movl	-84(%rbp), %edx
	imull	-152(%rbp), %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %edx
	imull	-156(%rbp), %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %edx
	imull	%r15d, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %edx
	imull	%r14d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %edx
	imull	%r13d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %edx
	imull	%r12d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %edx
	imull	%ebx, %edx
	movl	%edx, -108(%rbp)
	imull	-116(%rbp), %eax
	movl	-112(%rbp), %edx
	imull	-120(%rbp), %edx
	imull	-124(%rbp), %ecx
	movl	%ecx, -164(%rbp)
	imull	-128(%rbp), %esi
	imull	-132(%rbp), %edi
	imull	-136(%rbp), %r8d
	imull	-140(%rbp), %r9d
	imull	-144(%rbp), %r10d
	imull	-148(%rbp), %r11d
	movl	-84(%rbp), %ecx
	imull	-152(%rbp), %ecx
	movl	%ecx, -84(%rbp)
	movl	-88(%rbp), %ecx
	imull	-156(%rbp), %ecx
	movl	%ecx, -88(%rbp)
	movl	-92(%rbp), %ecx
	imull	%r15d, %ecx
	movl	%ecx, -92(%rbp)
	movl	-96(%rbp), %ecx
	imull	%r14d, %ecx
	movl	%ecx, -96(%rbp)
	movl	-100(%rbp), %ecx
	imull	%r13d, %ecx
	movl	%ecx, -100(%rbp)
	movl	-104(%rbp), %ecx
	imull	%r12d, %ecx
	movl	%ecx, -104(%rbp)
	movl	-108(%rbp), %ecx
	imull	%ebx, %ecx
	movl	%ecx, -108(%rbp)
	imull	-116(%rbp), %eax
	imull	-120(%rbp), %edx
	movl	-164(%rbp), %ecx
	imull	-124(%rbp), %ecx
	imull	-128(%rbp), %esi
	movl	%esi, -168(%rbp)
	imull	-132(%rbp), %edi
	imull	-136(%rbp), %r8d
	imull	-140(%rbp), %r9d
	imull	-144(%rbp), %r10d
	imull	-148(%rbp), %r11d
	movl	-84(%rbp), %esi
	imull	-152(%rbp), %esi
	movl	%esi, -84(%rbp)
	movl	-88(%rbp), %esi
	imull	-156(%rbp), %esi
	movl	%esi, -88(%rbp)
	movl	-92(%rbp), %esi
	imull	%r15d, %esi
	movl	%esi, -92(%rbp)
	movl	-96(%rbp), %esi
	imull	%r14d, %esi
	movl	%esi, -96(%rbp)
	movl	-100(%rbp), %esi
	imull	%r13d, %esi
	movl	%esi, -100(%rbp)
	movl	-104(%rbp), %esi
	imull	%r12d, %esi
	movl	%esi, -104(%rbp)
	movl	-108(%rbp), %esi
	imull	%ebx, %esi
	movl	%esi, -108(%rbp)
	imull	-116(%rbp), %eax
	imull	-120(%rbp), %edx
	imull	-124(%rbp), %ecx
	movl	-168(%rbp), %esi
	imull	-128(%rbp), %esi
	imull	-132(%rbp), %edi
	movl	%edi, -172(%rbp)
	imull	-136(%rbp), %r8d
	imull	-140(%rbp), %r9d
	imull	-144(%rbp), %r10d
	imull	-148(%rbp), %r11d
	movl	-84(%rbp), %edi
	imull	-152(%rbp), %edi
	movl	%edi, -84(%rbp)
	movl	-88(%rbp), %edi
	imull	-156(%rbp), %edi
	movl	%edi, -88(%rbp)
	movl	-92(%rbp), %edi
	imull	%r15d, %edi
	movl	%edi, -92(%rbp)
	movl	-96(%rbp), %edi
	imull	%r14d, %edi
	movl	%edi, -96(%rbp)
	movl	-100(%rbp), %edi
	imull	%r13d, %edi
	movl	%edi, -100(%rbp)
	movl	-104(%rbp), %edi
	imull	%r12d, %edi
	movl	%edi, -104(%rbp)
	movl	-108(%rbp), %edi
	imull	%ebx, %edi
	movl	%edi, -108(%rbp)
	imull	-116(%rbp), %eax
	imull	-120(%rbp), %edx
	imull	-124(%rbp), %ecx
	imull	-128(%rbp), %esi
	movl	-172(%rbp), %edi
	imull	-132(%rbp), %edi
	imull	-136(%rbp), %r8d
	movl	%r8d, -176(%rbp)
	imull	-140(%rbp), %r9d
	imull	-144(%rbp), %r10d
	imull	-148(%rbp), %r11d
	movl	-84(%rbp), %r8d
	imull	-152(%rbp), %r8d
	movl	%r8d, -84(%rbp)
	movl	-88(%rbp), %r8d
	imull	-156(%rbp), %r8d
	movl	%r8d, -88(%rbp)
	movl	-92(%rbp), %r8d
	imull	%r15d, %r8d
	movl	%r8d, -92(%rbp)
	movl	-96(%rbp), %r8d
	imull	%r14d, %r8d
	movl	%r8d, -96(%rbp)
	movl	-100(%rbp), %r8d
	imull	%r13d, %r8d
	movl	%r8d, -100(%rbp)
	movl	-104(%rbp), %r8d
	imull	%r12d, %r8d
	movl	%r8d, -104(%rbp)
	movl	-108(%rbp), %r8d
	imull	%ebx, %r8d
	movl	%r8d, -108(%rbp)
	imull	-116(%rbp), %eax
	imull	-120(%rbp), %edx
	imull	-124(%rbp), %ecx
	imull	-128(%rbp), %esi
	imull	-132(%rbp), %edi
	movl	-176(%rbp), %r8d
	imull	-136(%rbp), %r8d
	imull	-140(%rbp), %r9d
	movl	%r9d, -180(%rbp)
	imull	-144(%rbp), %r10d
	imull	-148(%rbp), %r11d
	movl	-84(%rbp), %r9d
	imull	-152(%rbp), %r9d
	movl	%r9d, -84(%rbp)
	movl	-88(%rbp), %r9d
	imull	-156(%rbp), %r9d
	movl	%r9d, -88(%rbp)
	movl	-92(%rbp), %r9d
	imull	%r15d, %r9d
	movl	%r9d, -92(%rbp)
	movl	-96(%rbp), %r9d
	imull	%r14d, %r9d
	movl	%r9d, -96(%rbp)
	movl	-100(%rbp), %r9d
	imull	%r13d, %r9d
	movl	%r9d, -100(%rbp)
	movl	-104(%rbp), %r9d
	imull	%r12d, %r9d
	movl	%r9d, -104(%rbp)
	movl	-108(%rbp), %r9d
	imull	%ebx, %r9d
	movl	%r9d, -108(%rbp)
	imull	-116(%rbp), %eax
	imull	-120(%rbp), %edx
	movl	%edx, -112(%rbp)
	imull	-124(%rbp), %ecx
	imull	-128(%rbp), %esi
	imull	-132(%rbp), %edi
	imull	-136(%rbp), %r8d
	movl	-180(%rbp), %r9d
	imull	-140(%rbp), %r9d
	imull	-144(%rbp), %r10d
	imull	-148(%rbp), %r11d
	movl	-84(%rbp), %edx
	imull	-152(%rbp), %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %edx
	imull	-156(%rbp), %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %edx
	imull	%r15d, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %edx
	imull	%r14d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %edx
	imull	%r13d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %edx
	imull	%r12d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %edx
	imull	%ebx, %edx
	movl	%edx, -108(%rbp)
	imull	-116(%rbp), %eax
	movl	%eax, -160(%rbp)
	movl	-112(%rbp), %edx
	imull	-120(%rbp), %edx
	movl	%edx, -112(%rbp)
	imull	-124(%rbp), %ecx
	movl	%ecx, -164(%rbp)
	imull	-128(%rbp), %esi
	movl	%esi, -168(%rbp)
	imull	-132(%rbp), %edi
	movl	%edi, -172(%rbp)
	imull	-136(%rbp), %r8d
	movl	%r8d, -176(%rbp)
	imull	-140(%rbp), %r9d
	movl	%r9d, -180(%rbp)
	imull	-144(%rbp), %r10d
	movl	%r10d, -184(%rbp)
	imull	-148(%rbp), %r11d
	movl	%r11d, -188(%rbp)
	movl	-84(%rbp), %edx
	imull	-152(%rbp), %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %edx
	imull	-156(%rbp), %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %edx
	imull	%r15d, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %edx
	imull	%r14d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %edx
	imull	%r13d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %edx
	imull	%r12d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %edx
	imull	%ebx, %edx
	movl	%edx, -108(%rbp)
.L160:
	movq	-200(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -200(%rbp)
	testq	%rax, %rax
	jne	.L161
	movl	-160(%rbp), %edi
	call	use_int@PLT
	movl	-112(%rbp), %edi
	call	use_int@PLT
	movl	-164(%rbp), %edi
	call	use_int@PLT
	movl	-168(%rbp), %edi
	call	use_int@PLT
	movl	-172(%rbp), %edi
	call	use_int@PLT
	movl	-176(%rbp), %edi
	call	use_int@PLT
	movl	-180(%rbp), %edi
	call	use_int@PLT
	movl	-184(%rbp), %edi
	call	use_int@PLT
	movl	-188(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-104(%rbp), %edi
	call	use_int@PLT
	movl	-108(%rbp), %edi
	call	use_int@PLT
	addq	$232, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE56:
	.size	integer_mul_15, .-integer_mul_15
	.globl	integer_mul_benchmarks
	.section	.data.rel.local
	.align 32
	.type	integer_mul_benchmarks, @object
	.size	integer_mul_benchmarks, 128
integer_mul_benchmarks:
	.quad	integer_mul_0
	.quad	integer_mul_1
	.quad	integer_mul_2
	.quad	integer_mul_3
	.quad	integer_mul_4
	.quad	integer_mul_5
	.quad	integer_mul_6
	.quad	integer_mul_7
	.quad	integer_mul_8
	.quad	integer_mul_9
	.quad	integer_mul_10
	.quad	integer_mul_11
	.quad	integer_mul_12
	.quad	integer_mul_13
	.quad	integer_mul_14
	.quad	integer_mul_15
	.text
	.globl	integer_div_0
	.type	integer_div_0, @function
integer_div_0:
.LFB57:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 13, -24
	.cfi_offset 12, -32
	.cfi_offset 3, -40
	movq	%rdi, -56(%rbp)
	movq	%rsi, -64(%rbp)
	movq	-56(%rbp), %r13
	movq	-64(%rbp), %rax
	movq	%rax, -40(%rbp)
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	leal	37(%rax), %ebx
	leal	1(%rbx), %eax
	sall	$20, %eax
	movl	%eax, %r12d
	jmp	.L163
.L164:
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
.L163:
	movq	%r13, %rax
	leaq	-1(%rax), %r13
	testq	%rax, %rax
	jne	.L164
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE57:
	.size	integer_div_0, .-integer_div_0
	.globl	integer_div_1
	.type	integer_div_1, @function
integer_div_1:
.LFB58:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %r15
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37(%rax), %r12d
	leal	1(%r12), %eax
	sall	$20, %eax
	movl	%eax, %r13d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	36(%rax), %ebx
	leal	1(%rbx), %eax
	sall	$20, %eax
	movl	%eax, %r14d
	jmp	.L166
.L167:
	movl	%r13d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r14d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r14d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r14d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r14d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r14d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r14d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r14d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r14d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r14d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	%r14d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
.L166:
	movq	%r15, %rax
	leaq	-1(%rax), %r15
	testq	%rax, %rax
	jne	.L167
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE58:
	.size	integer_div_1, .-integer_div_1
	.globl	integer_div_2
	.type	integer_div_2, @function
integer_div_2:
.LFB59:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %rsi
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37(%rax), %r13d
	leal	1(%r13), %eax
	sall	$20, %eax
	movl	%eax, %r14d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	36(%rax), %r12d
	leal	1(%r12), %eax
	sall	$20, %eax
	movl	%eax, %r15d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	35(%rax), %ebx
	leal	1(%rbx), %eax
	sall	$20, %eax
	movl	%eax, -84(%rbp)
	jmp	.L169
.L170:
	movl	%r14d, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%r15d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-84(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%r15d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-84(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%r15d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-84(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%r15d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-84(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%r15d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-84(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%r15d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-84(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%r15d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-84(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%r15d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-84(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%r15d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-84(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	%r15d, %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-84(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
.L169:
	movq	%rsi, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %rsi
	testq	%rax, %rax
	jne	.L170
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE59:
	.size	integer_div_2, .-integer_div_2
	.globl	integer_div_3
	.type	integer_div_3, @function
integer_div_3:
.LFB60:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %rsi
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37(%rax), %r14d
	leal	1(%r14), %eax
	sall	$20, %eax
	movl	%eax, %r15d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	36(%rax), %r13d
	leal	1(%r13), %eax
	sall	$20, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	35(%rax), %r12d
	leal	1(%r12), %eax
	sall	$20, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	34(%rax), %ebx
	leal	1(%rbx), %eax
	sall	$20, %eax
	movl	%eax, -92(%rbp)
	jmp	.L172
.L173:
	movl	%r15d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-84(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-84(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-84(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-84(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-84(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-84(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-84(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-84(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-84(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-84(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
.L172:
	movq	%rsi, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %rsi
	testq	%rax, %rax
	jne	.L173
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE60:
	.size	integer_div_3, .-integer_div_3
	.globl	integer_div_4
	.type	integer_div_4, @function
integer_div_4:
.LFB61:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %rsi
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	37(%rax), %r15d
	leal	1(%r15), %eax
	sall	$20, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	36(%rax), %r14d
	leal	1(%r14), %eax
	sall	$20, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	35(%rax), %r13d
	leal	1(%r13), %eax
	sall	$20, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	34(%rax), %r12d
	leal	1(%r12), %eax
	sall	$20, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	33(%rax), %ebx
	leal	1(%rbx), %eax
	sall	$20, %eax
	movl	%eax, -100(%rbp)
	jmp	.L175
.L176:
	movl	-84(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-88(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
.L175:
	movq	%rsi, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %rsi
	testq	%rax, %rax
	jne	.L176
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE61:
	.size	integer_div_4, .-integer_div_4
	.globl	integer_div_5
	.type	integer_div_5, @function
integer_div_5:
.LFB62:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r8
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movl	%eax, %edi
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	36(%rax), %r15d
	leal	1(%r15), %eax
	sall	$20, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	35(%rax), %r14d
	leal	1(%r14), %eax
	sall	$20, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	34(%rax), %r13d
	leal	1(%r13), %eax
	sall	$20, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	33(%rax), %r12d
	leal	1(%r12), %eax
	sall	$20, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	32(%rax), %ebx
	leal	1(%rbx), %eax
	sall	$20, %eax
	movl	%eax, -104(%rbp)
	jmp	.L178
.L179:
	movl	-84(%rbp), %eax
	movl	%edi, %ecx
	cltd
	idivl	%ecx
	movl	%eax, %esi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-104(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %ecx
	movl	-88(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-104(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %esi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-104(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %ecx
	movl	-88(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-104(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-104(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %esi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-104(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %ecx
	movl	-88(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-104(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-104(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %esi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-104(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-104(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
.L178:
	movq	%r8, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %r8
	testq	%rax, %rax
	jne	.L179
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE62:
	.size	integer_div_5, .-integer_div_5
	.globl	integer_div_6
	.type	integer_div_6, @function
integer_div_6:
.LFB63:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r8
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movl	%eax, %edi
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movl	%eax, -112(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	leal	35(%rax), %r15d
	leal	1(%r15), %eax
	sall	$20, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	34(%rax), %r14d
	leal	1(%r14), %eax
	sall	$20, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	33(%rax), %r13d
	leal	1(%r13), %eax
	sall	$20, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	32(%rax), %r12d
	leal	1(%r12), %eax
	sall	$20, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	31(%rax), %ebx
	leal	1(%rbx), %eax
	sall	$20, %eax
	movl	%eax, -108(%rbp)
	jmp	.L181
.L182:
	movl	-84(%rbp), %eax
	movl	%edi, %ecx
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-88(%rbp), %eax
	cltd
	idivl	-112(%rbp)
	movl	%eax, %esi
	movl	-92(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-108(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-92(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-108(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %ecx
	movl	-92(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-108(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %esi
	movl	-88(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %edi
	movl	-92(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-108(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %ecx
	movl	-88(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %esi
	movl	-92(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-108(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %ecx
	movl	-92(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-108(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %esi
	movl	-88(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %edi
	movl	-92(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-108(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %ecx
	movl	-88(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %esi
	movl	-92(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-108(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %ecx
	movl	-92(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-108(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, -112(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-108(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
.L181:
	movq	%r8, %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, %r8
	testq	%rax, %rax
	jne	.L182
	call	use_int@PLT
	movl	-112(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE63:
	.size	integer_div_6, .-integer_div_6
	.globl	integer_div_7
	.type	integer_div_7, @function
integer_div_7:
.LFB64:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r8
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movl	%eax, %edi
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movl	%eax, -116(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movl	%eax, -120(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	leal	34(%rax), %r15d
	leal	1(%r15), %eax
	sall	$20, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	33(%rax), %r14d
	leal	1(%r14), %eax
	sall	$20, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	32(%rax), %r13d
	leal	1(%r13), %eax
	sall	$20, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	31(%rax), %r12d
	leal	1(%r12), %eax
	sall	$20, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	30(%rax), %ebx
	leal	1(%rbx), %eax
	sall	$20, %eax
	movl	%eax, -112(%rbp)
	jmp	.L184
.L185:
	movl	-84(%rbp), %eax
	movl	%edi, %ecx
	cltd
	idivl	%ecx
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	-116(%rbp)
	movl	%eax, %esi
	movl	-92(%rbp), %eax
	cltd
	idivl	-120(%rbp)
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-112(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-112(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-112(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-112(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-112(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-112(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-112(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-112(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-112(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, -116(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, -120(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-112(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
.L184:
	movq	%r8, %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, %r8
	testq	%rax, %rax
	jne	.L185
	call	use_int@PLT
	movl	-116(%rbp), %edi
	call	use_int@PLT
	movl	-120(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE64:
	.size	integer_div_7, .-integer_div_7
	.globl	integer_div_8
	.type	integer_div_8, @function
integer_div_8:
.LFB65:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -136(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movl	%eax, %edi
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movl	%eax, -120(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movl	%eax, -124(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movl	%eax, -128(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	leal	33(%rax), %r15d
	leal	1(%r15), %eax
	sall	$20, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	32(%rax), %r14d
	leal	1(%r14), %eax
	sall	$20, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	31(%rax), %r13d
	leal	1(%r13), %eax
	sall	$20, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	30(%rax), %r12d
	leal	1(%r12), %eax
	sall	$20, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	29(%rax), %ebx
	leal	1(%rbx), %eax
	sall	$20, %eax
	movl	%eax, -116(%rbp)
	jmp	.L187
.L188:
	movl	-84(%rbp), %eax
	movl	%edi, %ecx
	cltd
	idivl	%ecx
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	-120(%rbp)
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-116(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r9d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-116(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r10d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-116(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r11d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-116(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-116(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r9d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-116(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r10d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-116(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r11d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-116(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-116(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, -120(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, -124(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, -128(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-116(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
.L187:
	movq	-136(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -136(%rbp)
	testq	%rax, %rax
	jne	.L188
	call	use_int@PLT
	movl	-120(%rbp), %edi
	call	use_int@PLT
	movl	-124(%rbp), %edi
	call	use_int@PLT
	movl	-128(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE65:
	.size	integer_div_8, .-integer_div_8
	.globl	integer_div_9
	.type	integer_div_9, @function
integer_div_9:
.LFB66:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -144(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movl	%eax, %edi
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movl	%eax, -124(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movl	%eax, -128(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movl	%eax, -132(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movl	%eax, -136(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	leal	32(%rax), %r15d
	leal	1(%r15), %eax
	sall	$20, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	31(%rax), %r14d
	leal	1(%r14), %eax
	sall	$20, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	30(%rax), %r13d
	leal	1(%r13), %eax
	sall	$20, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	29(%rax), %r12d
	leal	1(%r12), %eax
	sall	$20, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	28(%rax), %ebx
	leal	1(%rbx), %eax
	sall	$20, %eax
	movl	%eax, -120(%rbp)
	jmp	.L190
.L191:
	movl	-84(%rbp), %eax
	movl	%edi, %ecx
	cltd
	idivl	%ecx
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%eax, %r9d
	movl	-96(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	-136(%rbp)
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-120(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r10d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r11d
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-120(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r9d
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-120(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r10d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r11d
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-120(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r9d
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-120(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r10d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r11d
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-120(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r9d
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-120(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r10d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r11d
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-120(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r9d
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-120(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, -124(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, -128(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, -132(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, -136(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-120(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
.L190:
	movq	-144(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -144(%rbp)
	testq	%rax, %rax
	jne	.L191
	call	use_int@PLT
	movl	-124(%rbp), %edi
	call	use_int@PLT
	movl	-128(%rbp), %edi
	call	use_int@PLT
	movl	-132(%rbp), %edi
	call	use_int@PLT
	movl	-136(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE66:
	.size	integer_div_9, .-integer_div_9
	.globl	integer_div_10
	.type	integer_div_10, @function
integer_div_10:
.LFB67:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -152(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movl	%eax, %edi
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movl	%eax, -128(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movl	%eax, -132(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movl	%eax, -136(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movl	%eax, -140(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$32, %eax
	movl	%eax, -144(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	leal	31(%rax), %r15d
	leal	1(%r15), %eax
	sall	$20, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	30(%rax), %r14d
	leal	1(%r14), %eax
	sall	$20, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	29(%rax), %r13d
	leal	1(%r13), %eax
	sall	$20, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	28(%rax), %r12d
	leal	1(%r12), %eax
	sall	$20, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	27(%rax), %ebx
	leal	1(%rbx), %eax
	sall	$20, %eax
	movl	%eax, -124(%rbp)
	jmp	.L193
.L194:
	movl	-84(%rbp), %eax
	movl	%edi, %ecx
	cltd
	idivl	%ecx
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%eax, %r9d
	movl	-96(%rbp), %eax
	cltd
	idivl	-136(%rbp)
	movl	%eax, %r10d
	movl	-100(%rbp), %eax
	cltd
	idivl	-140(%rbp)
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	-144(%rbp)
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-124(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r11d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r8d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r9d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-124(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r10d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r11d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r8d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-124(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r9d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r10d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r11d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-124(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r9d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r10d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-124(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r11d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r8d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r9d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-124(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r10d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r11d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r8d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-124(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r9d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r10d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r11d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-124(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r9d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r10d
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-124(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, -128(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, -132(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, -136(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, -140(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, -144(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-124(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
.L193:
	movq	-152(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -152(%rbp)
	testq	%rax, %rax
	jne	.L194
	call	use_int@PLT
	movl	-128(%rbp), %edi
	call	use_int@PLT
	movl	-132(%rbp), %edi
	call	use_int@PLT
	movl	-136(%rbp), %edi
	call	use_int@PLT
	movl	-140(%rbp), %edi
	call	use_int@PLT
	movl	-144(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE67:
	.size	integer_div_10, .-integer_div_10
	.globl	integer_div_11
	.type	integer_div_11, @function
integer_div_11:
.LFB68:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -160(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movl	%eax, %edi
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movl	%eax, -132(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movl	%eax, -136(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movl	%eax, -140(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movl	%eax, -144(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$32, %eax
	movl	%eax, -148(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movl	%eax, -152(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	leal	30(%rax), %r15d
	leal	1(%r15), %eax
	sall	$20, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	29(%rax), %r14d
	leal	1(%r14), %eax
	sall	$20, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	28(%rax), %r13d
	leal	1(%r13), %eax
	sall	$20, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	27(%rax), %r12d
	leal	1(%r12), %eax
	sall	$20, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	26(%rax), %ebx
	leal	1(%rbx), %eax
	sall	$20, %eax
	movl	%eax, -128(%rbp)
	jmp	.L196
.L197:
	movl	-84(%rbp), %eax
	movl	%edi, %ecx
	cltd
	idivl	%ecx
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	-136(%rbp)
	movl	%eax, %r9d
	movl	-96(%rbp), %eax
	cltd
	idivl	-140(%rbp)
	movl	%eax, %r10d
	movl	-100(%rbp), %eax
	cltd
	idivl	-144(%rbp)
	movl	%eax, %r11d
	movl	-104(%rbp), %eax
	cltd
	idivl	-148(%rbp)
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	-152(%rbp)
	movl	%eax, %ecx
	movl	-112(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-128(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-112(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-128(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-112(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-128(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-112(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-128(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-112(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-128(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-112(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-128(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-112(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-128(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-112(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-128(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-92(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-108(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-112(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-128(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-88(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, -132(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, -136(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, -140(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, -144(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, -148(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, -152(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-128(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
.L196:
	movq	-160(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -160(%rbp)
	testq	%rax, %rax
	jne	.L197
	call	use_int@PLT
	movl	-132(%rbp), %edi
	call	use_int@PLT
	movl	-136(%rbp), %edi
	call	use_int@PLT
	movl	-140(%rbp), %edi
	call	use_int@PLT
	movl	-144(%rbp), %edi
	call	use_int@PLT
	movl	-148(%rbp), %edi
	call	use_int@PLT
	movl	-152(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE68:
	.size	integer_div_11, .-integer_div_11
	.globl	integer_div_12
	.type	integer_div_12, @function
integer_div_12:
.LFB69:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -168(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movl	%eax, %edi
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movl	%eax, -140(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movl	%eax, -144(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movl	%eax, -148(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movl	%eax, -152(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$32, %eax
	movl	%eax, -84(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movl	%eax, -156(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$30, %eax
	movl	%eax, -160(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	leal	29(%rax), %r15d
	leal	1(%r15), %eax
	sall	$20, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	28(%rax), %r14d
	leal	1(%r14), %eax
	sall	$20, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	27(%rax), %r13d
	leal	1(%r13), %eax
	sall	$20, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	26(%rax), %r12d
	leal	1(%r12), %eax
	sall	$20, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	leal	25(%rax), %ebx
	leal	1(%rbx), %eax
	sall	$20, %eax
	movl	%eax, -136(%rbp)
	jmp	.L199
.L200:
	movl	-88(%rbp), %eax
	movl	%edi, %ecx
	cltd
	idivl	%ecx
	movl	%eax, %edi
	movl	-92(%rbp), %eax
	cltd
	idivl	-140(%rbp)
	movl	%eax, %r8d
	movl	-96(%rbp), %eax
	cltd
	idivl	-144(%rbp)
	movl	%eax, %r9d
	movl	-100(%rbp), %eax
	cltd
	idivl	-148(%rbp)
	movl	%eax, %r10d
	movl	-104(%rbp), %eax
	cltd
	idivl	-152(%rbp)
	movl	%eax, %r11d
	movl	-108(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-156(%rbp)
	movl	%eax, %esi
	movl	-116(%rbp), %eax
	cltd
	idivl	-160(%rbp)
	movl	%eax, %ecx
	movl	-120(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-128(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-136(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-88(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-92(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-108(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-116(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-120(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-128(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-136(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-88(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-92(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-108(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-116(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-120(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-128(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-136(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-88(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-92(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-108(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-116(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-120(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-128(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-136(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-88(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-92(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-108(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-116(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-120(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-128(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-136(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-88(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-92(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-108(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-116(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-120(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-128(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-136(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-88(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-92(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-108(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-116(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-120(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-128(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-136(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-88(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-92(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-108(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-116(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-120(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-128(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-136(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-88(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-92(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-96(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-108(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-116(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-120(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-128(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-136(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-88(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-92(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, -140(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, -144(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, -148(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, -152(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, -156(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, -160(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-124(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-128(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-136(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
.L199:
	movq	-168(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -168(%rbp)
	testq	%rax, %rax
	jne	.L200
	call	use_int@PLT
	movl	-140(%rbp), %edi
	call	use_int@PLT
	movl	-144(%rbp), %edi
	call	use_int@PLT
	movl	-148(%rbp), %edi
	call	use_int@PLT
	movl	-152(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-156(%rbp), %edi
	call	use_int@PLT
	movl	-160(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE69:
	.size	integer_div_12, .-integer_div_12
	.globl	integer_div_13
	.type	integer_div_13, @function
integer_div_13:
.LFB70:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -176(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movl	%eax, %edi
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movl	%eax, -148(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movl	%eax, -152(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movl	%eax, -156(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movl	%eax, -160(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$32, %eax
	movl	%eax, -84(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movl	%eax, -88(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$30, %eax
	movl	%eax, -164(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$29, %eax
	movl	%eax, -168(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	leal	28(%rax), %r15d
	leal	1(%r15), %eax
	sall	$20, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	27(%rax), %r14d
	leal	1(%r14), %eax
	sall	$20, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	26(%rax), %r13d
	leal	1(%r13), %eax
	sall	$20, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	leal	25(%rax), %r12d
	leal	1(%r12), %eax
	sall	$20, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	leal	24(%rax), %ebx
	leal	1(%rbx), %eax
	sall	$20, %eax
	movl	%eax, -144(%rbp)
	jmp	.L202
.L203:
	movl	-92(%rbp), %eax
	movl	%edi, %ecx
	cltd
	idivl	%ecx
	movl	%eax, %edi
	movl	-96(%rbp), %eax
	cltd
	idivl	-148(%rbp)
	movl	%eax, %r8d
	movl	-100(%rbp), %eax
	cltd
	idivl	-152(%rbp)
	movl	%eax, %r9d
	movl	-104(%rbp), %eax
	cltd
	idivl	-156(%rbp)
	movl	%eax, %r10d
	movl	-108(%rbp), %eax
	cltd
	idivl	-160(%rbp)
	movl	%eax, %r11d
	movl	-112(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-164(%rbp)
	movl	%eax, %esi
	movl	-124(%rbp), %eax
	cltd
	idivl	-168(%rbp)
	movl	%eax, %ecx
	movl	-128(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-136(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-144(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-112(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-124(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-128(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-136(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-144(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-112(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-124(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-128(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-136(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-144(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-112(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-124(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-128(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-136(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-144(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-112(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-124(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-128(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-136(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-144(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-112(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-124(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-128(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-136(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-144(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-112(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-124(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-128(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-136(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-144(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-112(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-124(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-128(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-136(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-144(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-112(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-124(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-128(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-136(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-144(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, -148(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, -152(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, -156(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, -160(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, -164(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, -168(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-132(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-136(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-144(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
.L202:
	movq	-176(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -176(%rbp)
	testq	%rax, %rax
	jne	.L203
	call	use_int@PLT
	movl	-148(%rbp), %edi
	call	use_int@PLT
	movl	-152(%rbp), %edi
	call	use_int@PLT
	movl	-156(%rbp), %edi
	call	use_int@PLT
	movl	-160(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-164(%rbp), %edi
	call	use_int@PLT
	movl	-168(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE70:
	.size	integer_div_13, .-integer_div_13
	.globl	integer_div_14
	.type	integer_div_14, @function
integer_div_14:
.LFB71:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -192(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movl	%eax, -156(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movl	%eax, -160(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movl	%eax, -164(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movl	%eax, -168(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movl	%eax, -172(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$32, %eax
	movl	%eax, -84(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movl	%eax, -88(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$30, %eax
	movl	%eax, -92(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$29, %eax
	movl	%eax, -176(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$28, %eax
	movl	%eax, -180(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	leal	27(%rax), %r15d
	leal	1(%r15), %eax
	sall	$20, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	26(%rax), %r14d
	leal	1(%r14), %eax
	sall	$20, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	leal	25(%rax), %r13d
	leal	1(%r13), %eax
	sall	$20, %eax
	movl	%eax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	leal	24(%rax), %r12d
	leal	1(%r12), %eax
	sall	$20, %eax
	movl	%eax, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	leal	23(%rax), %ebx
	leal	1(%rbx), %eax
	sall	$20, %eax
	movl	%eax, -152(%rbp)
	jmp	.L205
.L206:
	movl	-96(%rbp), %eax
	cltd
	idivl	-156(%rbp)
	movl	%eax, %edi
	movl	-100(%rbp), %eax
	cltd
	idivl	-160(%rbp)
	movl	%eax, %r8d
	movl	-104(%rbp), %eax
	cltd
	idivl	-164(%rbp)
	movl	%eax, %r9d
	movl	-108(%rbp), %eax
	cltd
	idivl	-168(%rbp)
	movl	%eax, %r10d
	movl	-112(%rbp), %eax
	cltd
	idivl	-172(%rbp)
	movl	%eax, %r11d
	movl	-116(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	-176(%rbp)
	movl	%eax, %esi
	movl	-132(%rbp), %eax
	cltd
	idivl	-180(%rbp)
	movl	%eax, %ecx
	movl	-136(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-144(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-148(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-152(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-96(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-100(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-116(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-132(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-136(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-144(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-148(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-152(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-96(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-100(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-116(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-132(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-136(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-144(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-148(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-152(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-96(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-100(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-116(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-132(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-136(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-144(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-148(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-152(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-96(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-100(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-116(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-132(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-136(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-144(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-148(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-152(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-96(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-100(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-116(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-132(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-136(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-144(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-148(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-152(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-96(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-100(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-116(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-132(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-136(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-144(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-148(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-152(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-96(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-100(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-116(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-132(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-136(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-144(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-148(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-152(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-96(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, -156(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-104(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-108(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-116(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, %edi
	movl	-128(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-132(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-136(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-144(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-148(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-152(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-96(%rbp), %eax
	cltd
	idivl	-156(%rbp)
	movl	%eax, -156(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, -160(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, -164(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, -168(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, -172(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, -92(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, -176(%rbp)
	movl	-132(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, -180(%rbp)
	movl	-136(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-140(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-144(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-148(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-152(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
.L205:
	movq	-192(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -192(%rbp)
	testq	%rax, %rax
	jne	.L206
	movl	-156(%rbp), %edi
	call	use_int@PLT
	movl	-160(%rbp), %edi
	call	use_int@PLT
	movl	-164(%rbp), %edi
	call	use_int@PLT
	movl	-168(%rbp), %edi
	call	use_int@PLT
	movl	-172(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-176(%rbp), %edi
	call	use_int@PLT
	movl	-180(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$152, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE71:
	.size	integer_div_14, .-integer_div_14
	.globl	integer_div_15
	.type	integer_div_15, @function
integer_div_15:
.LFB72:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -200(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movl	%eax, -96(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movl	%eax, -168(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movl	%eax, -172(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movl	%eax, -176(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movl	%eax, -180(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$32, %eax
	movl	%eax, -84(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movl	%eax, -88(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$30, %eax
	movl	%eax, -92(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$29, %eax
	movl	%eax, -100(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$28, %eax
	movl	%eax, -184(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$27, %eax
	movl	%eax, -188(%rbp)
	addl	$1, %eax
	sall	$20, %eax
	movl	%eax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	leal	26(%rax), %r15d
	leal	1(%r15), %eax
	sall	$20, %eax
	movl	%eax, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	leal	25(%rax), %r14d
	leal	1(%r14), %eax
	sall	$20, %eax
	movl	%eax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	leal	24(%rax), %r13d
	leal	1(%r13), %eax
	sall	$20, %eax
	movl	%eax, -156(%rbp)
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	leal	23(%rax), %r12d
	leal	1(%r12), %eax
	sall	$20, %eax
	movl	%eax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	leal	22(%rax), %ebx
	leal	1(%rbx), %eax
	sall	$20, %eax
	movl	%eax, -164(%rbp)
	jmp	.L208
.L209:
	movl	-104(%rbp), %eax
	cltd
	idivl	-96(%rbp)
	movl	%eax, %edi
	movl	-108(%rbp), %eax
	cltd
	idivl	-168(%rbp)
	movl	%eax, %r8d
	movl	-112(%rbp), %eax
	cltd
	idivl	-172(%rbp)
	movl	%eax, %r9d
	movl	-116(%rbp), %eax
	cltd
	idivl	-176(%rbp)
	movl	%eax, %r10d
	movl	-120(%rbp), %eax
	cltd
	idivl	-180(%rbp)
	movl	%eax, %r11d
	movl	-124(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-132(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-136(%rbp), %eax
	cltd
	idivl	-100(%rbp)
	movl	%eax, -100(%rbp)
	movl	-140(%rbp), %eax
	cltd
	idivl	-184(%rbp)
	movl	%eax, %esi
	movl	-144(%rbp), %eax
	cltd
	idivl	-188(%rbp)
	movl	%eax, %ecx
	movl	-148(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-152(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-156(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-160(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-164(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-104(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-108(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-124(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-132(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-136(%rbp), %eax
	cltd
	idivl	-100(%rbp)
	movl	%eax, -100(%rbp)
	movl	-140(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-144(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-148(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-152(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-156(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-160(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-164(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-104(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-108(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-124(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-132(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-136(%rbp), %eax
	cltd
	idivl	-100(%rbp)
	movl	%eax, -100(%rbp)
	movl	-140(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-144(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-148(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-152(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-156(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-160(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-164(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-104(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-108(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-124(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-132(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-136(%rbp), %eax
	cltd
	idivl	-100(%rbp)
	movl	%eax, -100(%rbp)
	movl	-140(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-144(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-148(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-152(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-156(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-160(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-164(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-104(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, -96(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-124(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-132(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-136(%rbp), %eax
	cltd
	idivl	-100(%rbp)
	movl	%eax, %edi
	movl	-140(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-144(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-148(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-152(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-156(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-160(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-164(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-104(%rbp), %eax
	cltd
	idivl	-96(%rbp)
	movl	%eax, -96(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-124(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-132(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-136(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-140(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-144(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-148(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-152(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-156(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-160(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-164(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-104(%rbp), %eax
	cltd
	idivl	-96(%rbp)
	movl	%eax, -96(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-124(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-132(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-136(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-140(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-144(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-148(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-152(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-156(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-160(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-164(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-104(%rbp), %eax
	cltd
	idivl	-96(%rbp)
	movl	%eax, -96(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-124(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-132(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-136(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-140(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-144(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-148(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-152(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-156(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-160(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-164(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-104(%rbp), %eax
	cltd
	idivl	-96(%rbp)
	movl	%eax, -96(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, %r8d
	movl	-112(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, %r9d
	movl	-116(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, %r10d
	movl	-120(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, %r11d
	movl	-124(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-132(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-136(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, %edi
	movl	-140(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, %esi
	movl	-144(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, %ecx
	movl	-148(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-152(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-156(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-160(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-164(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
	movl	-104(%rbp), %eax
	cltd
	idivl	-96(%rbp)
	movl	%eax, -96(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%eax, -168(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%eax, -172(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%eax, -176(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%eax, -180(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-84(%rbp)
	movl	%eax, -84(%rbp)
	movl	-128(%rbp), %eax
	cltd
	idivl	-88(%rbp)
	movl	%eax, -88(%rbp)
	movl	-132(%rbp), %eax
	cltd
	idivl	-92(%rbp)
	movl	%eax, -92(%rbp)
	movl	-136(%rbp), %eax
	cltd
	idivl	%edi
	movl	%eax, -100(%rbp)
	movl	-140(%rbp), %eax
	cltd
	idivl	%esi
	movl	%eax, -184(%rbp)
	movl	-144(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%eax, -188(%rbp)
	movl	-148(%rbp), %eax
	cltd
	idivl	%r15d
	movl	%eax, %r15d
	movl	-152(%rbp), %eax
	cltd
	idivl	%r14d
	movl	%eax, %r14d
	movl	-156(%rbp), %eax
	cltd
	idivl	%r13d
	movl	%eax, %r13d
	movl	-160(%rbp), %eax
	cltd
	idivl	%r12d
	movl	%eax, %r12d
	movl	-164(%rbp), %eax
	cltd
	idivl	%ebx
	movl	%eax, %ebx
.L208:
	movq	-200(%rbp), %rax
	leaq	-1(%rax), %rdi
	movq	%rdi, -200(%rbp)
	testq	%rax, %rax
	jne	.L209
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-168(%rbp), %edi
	call	use_int@PLT
	movl	-172(%rbp), %edi
	call	use_int@PLT
	movl	-176(%rbp), %edi
	call	use_int@PLT
	movl	-180(%rbp), %edi
	call	use_int@PLT
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-184(%rbp), %edi
	call	use_int@PLT
	movl	-188(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$168, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE72:
	.size	integer_div_15, .-integer_div_15
	.globl	integer_div_benchmarks
	.section	.data.rel.local
	.align 32
	.type	integer_div_benchmarks, @object
	.size	integer_div_benchmarks, 128
integer_div_benchmarks:
	.quad	integer_div_0
	.quad	integer_div_1
	.quad	integer_div_2
	.quad	integer_div_3
	.quad	integer_div_4
	.quad	integer_div_5
	.quad	integer_div_6
	.quad	integer_div_7
	.quad	integer_div_8
	.quad	integer_div_9
	.quad	integer_div_10
	.quad	integer_div_11
	.quad	integer_div_12
	.quad	integer_div_13
	.quad	integer_div_14
	.quad	integer_div_15
	.text
	.globl	integer_mod_0
	.type	integer_mod_0, @function
integer_mod_0:
.LFB73:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 13, -24
	.cfi_offset 12, -32
	.cfi_offset 3, -40
	movq	%rdi, -56(%rbp)
	movq	%rsi, -64(%rbp)
	movq	-56(%rbp), %r13
	movq	-64(%rbp), %rax
	movq	%rax, -40(%rbp)
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	movl	%eax, %edx
	movq	-56(%rbp), %rax
	addl	%edx, %eax
	addl	$1, %eax
	movl	%eax, %ebx
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	leal	63(%rax), %r12d
	jmp	.L211
.L212:
	movl	%ebx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebx
	orl	%r12d, %ebx
	movl	%ebx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebx
	orl	%r12d, %ebx
	movl	%ebx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebx
	orl	%r12d, %ebx
	movl	%ebx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebx
	orl	%r12d, %ebx
	movl	%ebx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebx
	orl	%r12d, %ebx
	movl	%ebx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebx
	orl	%r12d, %ebx
	movl	%ebx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebx
	orl	%r12d, %ebx
	movl	%ebx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebx
	orl	%r12d, %ebx
	movl	%ebx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebx
	orl	%r12d, %ebx
	movl	%ebx, %eax
	cltd
	idivl	%r12d
	movl	%edx, %ebx
	orl	%r12d, %ebx
.L211:
	movq	%r13, %rax
	leaq	-1(%rax), %r13
	testq	%rax, %rax
	jne	.L212
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE73:
	.size	integer_mod_0, .-integer_mod_0
	.globl	integer_mod_1
	.type	integer_mod_1, @function
integer_mod_1:
.LFB74:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %r15
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	addl	$1, %eax
	movl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	63(%rax), %r13d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	movl	%eax, %ebx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	62(%rax), %r14d
	jmp	.L214
.L215:
	movl	%r12d, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r12d
	orl	%r13d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ebx
	orl	%r14d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r12d
	orl	%r13d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ebx
	orl	%r14d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r12d
	orl	%r13d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ebx
	orl	%r14d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r12d
	orl	%r13d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ebx
	orl	%r14d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r12d
	orl	%r13d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ebx
	orl	%r14d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r12d
	orl	%r13d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ebx
	orl	%r14d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r12d
	orl	%r13d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ebx
	orl	%r14d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r12d
	orl	%r13d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ebx
	orl	%r14d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r12d
	orl	%r13d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ebx
	orl	%r14d, %ebx
	movl	%r12d, %eax
	cltd
	idivl	%r13d
	movl	%edx, %r12d
	orl	%r13d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r14d
	movl	%edx, %ebx
	orl	%r14d, %ebx
.L214:
	movq	%r15, %rax
	leaq	-1(%rax), %r15
	testq	%rax, %rax
	jne	.L215
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE74:
	.size	integer_mod_1, .-integer_mod_1
	.globl	integer_mod_2
	.type	integer_mod_2, @function
integer_mod_2:
.LFB75:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %rsi
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	addl	$1, %eax
	movl	%eax, %r13d
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	63(%rax), %r14d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	movl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	leal	62(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$1, %eax
	movl	%eax, %ebx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$61, %eax
	movl	%eax, %edi
	jmp	.L217
.L218:
	movl	%r13d, %eax
	cltd
	idivl	%r14d
	movl	%edx, %r13d
	orl	%r14d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r12d
	orl	%r15d, %r12d
	movl	%ebx, %eax
	movl	%edi, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r14d
	movl	%edx, %r13d
	orl	%r14d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r12d
	orl	%r15d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r14d
	movl	%edx, %r13d
	orl	%r14d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r12d
	orl	%r15d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r14d
	movl	%edx, %r13d
	orl	%r14d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r12d
	orl	%r15d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r14d
	movl	%edx, %r13d
	orl	%r14d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r12d
	orl	%r15d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r14d
	movl	%edx, %r13d
	orl	%r14d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r12d
	orl	%r15d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r14d
	movl	%edx, %r13d
	orl	%r14d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r12d
	orl	%r15d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r14d
	movl	%edx, %r13d
	orl	%r14d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r12d
	orl	%r15d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r14d
	movl	%edx, %r13d
	orl	%r14d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r12d
	orl	%r15d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
	movl	%r13d, %eax
	cltd
	idivl	%r14d
	movl	%edx, %r13d
	orl	%r14d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r12d
	orl	%r15d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
.L217:
	movq	%rsi, %rax
	leaq	-1(%rax), %rsi
	testq	%rax, %rax
	jne	.L218
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE75:
	.size	integer_mod_2, .-integer_mod_2
	.globl	integer_mod_3
	.type	integer_mod_3, @function
integer_mod_3:
.LFB76:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r8
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	addl	$1, %eax
	movl	%eax, %r14d
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	leal	63(%rax), %r15d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	movl	%eax, %r13d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$62, %eax
	movl	%eax, %r9d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$1, %eax
	movl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$61, %eax
	movl	%eax, %esi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$2, %eax
	movl	%eax, %ebx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$60, %eax
	movl	%eax, %edi
	jmp	.L220
.L221:
	movl	%r14d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r14d
	orl	%r15d, %r14d
	movl	%r13d, %eax
	movl	%r9d, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	orl	%ecx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	orl	%esi, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%edi
	movl	%edx, %ebx
	orl	%edi, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r14d
	orl	%r15d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	orl	%ecx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	orl	%esi, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%edi
	movl	%edx, %ebx
	orl	%edi, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r14d
	orl	%r15d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	orl	%ecx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	orl	%esi, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%edi
	movl	%edx, %ebx
	orl	%edi, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r14d
	orl	%r15d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	orl	%ecx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	orl	%esi, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%edi
	movl	%edx, %ebx
	orl	%edi, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r14d
	orl	%r15d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	orl	%ecx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	orl	%esi, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%edi
	movl	%edx, %ebx
	orl	%edi, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r14d
	orl	%r15d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	orl	%ecx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	orl	%esi, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%edi
	movl	%edx, %ebx
	orl	%edi, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r14d
	orl	%r15d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	orl	%ecx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	orl	%esi, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%edi
	movl	%edx, %ebx
	orl	%edi, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r14d
	orl	%r15d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	orl	%ecx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	orl	%esi, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%edi
	movl	%edx, %ebx
	orl	%edi, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r14d
	orl	%r15d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	orl	%ecx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	orl	%esi, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%edi
	movl	%edx, %ebx
	orl	%edi, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%r15d
	movl	%edx, %r14d
	orl	%r15d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	orl	%ecx, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	orl	%esi, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%edi
	movl	%edx, %ebx
	orl	%edi, %ebx
.L220:
	movq	%r8, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %r8
	testq	%rax, %rax
	jne	.L221
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE76:
	.size	integer_mod_3, .-integer_mod_3
	.globl	integer_mod_4
	.type	integer_mod_4, @function
integer_mod_4:
.LFB77:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r10
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	addl	$1, %eax
	movl	%eax, %r15d
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$63, %eax
	movl	%eax, %r11d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	movl	%eax, %r14d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$62, %eax
	movl	%eax, %esi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$1, %eax
	movl	%eax, %r13d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$61, %eax
	movl	%eax, %edi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$2, %eax
	movl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$60, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$3, %eax
	movl	%eax, %ebx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$59, %eax
	movl	%eax, %r9d
	jmp	.L223
.L224:
	movl	%r15d, %eax
	movl	%r11d, %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	orl	%ecx, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r14d
	orl	%esi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r13d
	orl	%edi, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r12d
	orl	%r8d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ebx
	orl	%r9d, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	orl	%ecx, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r14d
	orl	%esi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r13d
	orl	%edi, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r12d
	orl	%r8d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ebx
	orl	%r9d, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	orl	%ecx, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r14d
	orl	%esi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r13d
	orl	%edi, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r12d
	orl	%r8d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ebx
	orl	%r9d, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	orl	%ecx, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r14d
	orl	%esi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r13d
	orl	%edi, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r12d
	orl	%r8d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ebx
	orl	%r9d, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	orl	%ecx, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r14d
	orl	%esi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r13d
	orl	%edi, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r12d
	orl	%r8d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ebx
	orl	%r9d, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	orl	%ecx, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r14d
	orl	%esi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r13d
	orl	%edi, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r12d
	orl	%r8d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ebx
	orl	%r9d, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	orl	%ecx, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r14d
	orl	%esi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r13d
	orl	%edi, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r12d
	orl	%r8d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ebx
	orl	%r9d, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	orl	%ecx, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r14d
	orl	%esi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r13d
	orl	%edi, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r12d
	orl	%r8d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ebx
	orl	%r9d, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	orl	%ecx, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r14d
	orl	%esi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r13d
	orl	%edi, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r12d
	orl	%r8d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ebx
	orl	%r9d, %ebx
	movl	%r15d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	orl	%ecx, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r14d
	orl	%esi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r13d
	orl	%edi, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r12d
	orl	%r8d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r9d
	movl	%edx, %ebx
	orl	%r9d, %ebx
.L223:
	movq	%r10, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %r10
	testq	%rax, %rax
	jne	.L224
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE77:
	.size	integer_mod_4, .-integer_mod_4
	.globl	integer_mod_5
	.type	integer_mod_5, @function
integer_mod_5:
.LFB78:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -88(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	addl	$1, %eax
	movl	%eax, %ecx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$63, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	movl	%eax, %r15d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$62, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$1, %eax
	movl	%eax, %r14d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$61, %eax
	movl	%eax, %edi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$2, %eax
	movl	%eax, %r13d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$60, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$3, %eax
	movl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$59, %eax
	movl	%eax, %r9d
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$4, %eax
	movl	%eax, %ebx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$58, %eax
	movl	%eax, %r10d
	jmp	.L226
.L227:
	movl	%ecx, %eax
	movl	-92(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	orl	%ecx, %r11d
	movl	%r15d, %eax
	movl	-96(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %r15d
	orl	%esi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	orl	%edi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r13d
	orl	%r8d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r12d
	orl	%r9d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	orl	%r10d, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	orl	%ecx, %r11d
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r15d
	orl	%esi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	orl	%edi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r13d
	orl	%r8d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r12d
	orl	%r9d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	orl	%r10d, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	orl	%ecx, %r11d
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r15d
	orl	%esi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	orl	%edi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r13d
	orl	%r8d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r12d
	orl	%r9d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	orl	%r10d, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	orl	%ecx, %r11d
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r15d
	orl	%esi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	orl	%edi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r13d
	orl	%r8d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r12d
	orl	%r9d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	orl	%r10d, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	orl	%ecx, %r11d
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r15d
	orl	%esi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	orl	%edi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r13d
	orl	%r8d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r12d
	orl	%r9d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	orl	%r10d, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	orl	%ecx, %r11d
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r15d
	orl	%esi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	orl	%edi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r13d
	orl	%r8d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r12d
	orl	%r9d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	orl	%r10d, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	orl	%ecx, %r11d
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r15d
	orl	%esi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	orl	%edi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r13d
	orl	%r8d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r12d
	orl	%r9d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	orl	%r10d, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	orl	%ecx, %r11d
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r15d
	orl	%esi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	orl	%edi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r13d
	orl	%r8d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r12d
	orl	%r9d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	orl	%r10d, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	orl	%ecx, %r11d
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r15d
	orl	%esi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	orl	%edi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r13d
	orl	%r8d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r12d
	orl	%r9d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	orl	%r10d, %ebx
	movl	%r11d, %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, %ecx
	movl	%r15d, %eax
	cltd
	idivl	%esi
	movl	%edx, %r15d
	orl	%esi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r14d
	orl	%edi, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r13d
	orl	%r8d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r12d
	orl	%r9d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r10d
	movl	%edx, %ebx
	orl	%r10d, %ebx
.L226:
	movq	-88(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -88(%rbp)
	testq	%rax, %rax
	jne	.L227
	movl	%ecx, %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE78:
	.size	integer_mod_5, .-integer_mod_5
	.globl	integer_mod_6
	.type	integer_mod_6, @function
integer_mod_6:
.LFB79:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -96(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$63, %eax
	movl	%eax, %ecx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$62, %eax
	movl	%eax, %esi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$1, %eax
	movl	%eax, %r15d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$61, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$2, %eax
	movl	%eax, %r14d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$60, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$3, %eax
	movl	%eax, %r13d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$59, %eax
	movl	%eax, %r9d
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$4, %eax
	movl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$58, %eax
	movl	%eax, %r10d
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$5, %eax
	movl	%eax, %ebx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, %r11d
	jmp	.L229
.L230:
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %edi
	orl	%esi, %edi
	movl	%edi, -88(%rbp)
	movl	%r15d, %eax
	movl	-100(%rbp), %edi
	cltd
	idivl	%edi
	movl	%edx, %r15d
	orl	%edi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r14d
	orl	%r8d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r13d
	orl	%r9d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r12d
	orl	%r10d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebx
	orl	%r11d, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r15d
	orl	%edi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r14d
	orl	%r8d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r13d
	orl	%r9d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r12d
	orl	%r10d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebx
	orl	%r11d, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r15d
	orl	%edi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r14d
	orl	%r8d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r13d
	orl	%r9d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r12d
	orl	%r10d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebx
	orl	%r11d, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r15d
	orl	%edi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r14d
	orl	%r8d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r13d
	orl	%r9d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r12d
	orl	%r10d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebx
	orl	%r11d, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r15d
	orl	%edi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r14d
	orl	%r8d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r13d
	orl	%r9d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r12d
	orl	%r10d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebx
	orl	%r11d, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r15d
	orl	%edi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r14d
	orl	%r8d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r13d
	orl	%r9d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r12d
	orl	%r10d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebx
	orl	%r11d, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r15d
	orl	%edi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r14d
	orl	%r8d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r13d
	orl	%r9d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r12d
	orl	%r10d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebx
	orl	%r11d, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r15d
	orl	%edi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r14d
	orl	%r8d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r13d
	orl	%r9d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r12d
	orl	%r10d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebx
	orl	%r11d, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r15d
	orl	%edi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r14d
	orl	%r8d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r13d
	orl	%r9d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r12d
	orl	%r10d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebx
	orl	%r11d, %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%edi
	movl	%edx, %r15d
	orl	%edi, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r14d
	orl	%r8d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r13d
	orl	%r9d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r12d
	orl	%r10d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	%r11d
	movl	%edx, %ebx
	orl	%r11d, %ebx
.L229:
	movq	-96(%rbp), %rax
	leaq	-1(%rax), %rdi
	movq	%rdi, -96(%rbp)
	testq	%rax, %rax
	jne	.L230
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE79:
	.size	integer_mod_6, .-integer_mod_6
	.globl	integer_mod_7
	.type	integer_mod_7, @function
integer_mod_7:
.LFB80:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -104(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$63, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$62, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$1, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$61, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$2, %eax
	movl	%eax, %r15d
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$60, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$3, %eax
	movl	%eax, %r14d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$59, %eax
	movl	%eax, %r9d
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$4, %eax
	movl	%eax, %r13d
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$58, %eax
	movl	%eax, %r10d
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$5, %eax
	movl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, %r11d
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$6, %eax
	movl	%eax, %ebx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$56, %eax
	movl	%eax, -96(%rbp)
	jmp	.L232
.L233:
	movl	-84(%rbp), %eax
	movl	-108(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	movl	-112(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %edi
	orl	%esi, %edi
	movl	%edi, -88(%rbp)
	movl	-92(%rbp), %eax
	movl	-116(%rbp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r15d
	orl	%r8d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r14d
	orl	%r9d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r13d
	orl	%r10d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r12d
	orl	%r11d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	-96(%rbp)
	movl	%edx, %ebx
	orl	-96(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r15d
	orl	%r8d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r14d
	orl	%r9d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r13d
	orl	%r10d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r12d
	orl	%r11d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	-96(%rbp)
	movl	%edx, %ebx
	orl	-96(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r15d
	orl	%r8d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r14d
	orl	%r9d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r13d
	orl	%r10d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r12d
	orl	%r11d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	-96(%rbp)
	movl	%edx, %ebx
	orl	-96(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r15d
	orl	%r8d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r14d
	orl	%r9d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r13d
	orl	%r10d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r12d
	orl	%r11d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	-96(%rbp)
	movl	%edx, %ebx
	orl	-96(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r15d
	orl	%r8d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r14d
	orl	%r9d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r13d
	orl	%r10d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r12d
	orl	%r11d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	-96(%rbp)
	movl	%edx, %ebx
	orl	-96(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r15d
	orl	%r8d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r14d
	orl	%r9d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r13d
	orl	%r10d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r12d
	orl	%r11d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	-96(%rbp)
	movl	%edx, %ebx
	orl	-96(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r15d
	orl	%r8d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r14d
	orl	%r9d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r13d
	orl	%r10d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r12d
	orl	%r11d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	-96(%rbp)
	movl	%edx, %ebx
	orl	-96(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r15d
	orl	%r8d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r14d
	orl	%r9d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r13d
	orl	%r10d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r12d
	orl	%r11d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	-96(%rbp)
	movl	%edx, %ebx
	orl	-96(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r15d
	orl	%r8d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r14d
	orl	%r9d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r13d
	orl	%r10d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r12d
	orl	%r11d, %r12d
	movl	%ebx, %eax
	cltd
	idivl	-96(%rbp)
	movl	%edx, %ebx
	orl	-96(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	movl	%edx, %eax
	orl	%edi, %eax
	movl	%eax, -92(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r8d
	movl	%edx, %r15d
	orl	%r8d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r14d
	orl	%r9d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r13d
	orl	%r10d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r12d
	orl	%r11d, %r12d
	movl	%ebx, %eax
	movl	-96(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
.L232:
	movq	-104(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -104(%rbp)
	testq	%rax, %rax
	jne	.L233
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE80:
	.size	integer_mod_7, .-integer_mod_7
	.globl	integer_mod_8
	.type	integer_mod_8, @function
integer_mod_8:
.LFB81:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -112(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$63, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$62, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$1, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$61, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$2, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$60, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$3, %eax
	movl	%eax, %r15d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$59, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$4, %eax
	movl	%eax, %r14d
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$58, %eax
	movl	%eax, %r10d
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$5, %eax
	movl	%eax, %r13d
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, %r11d
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$6, %eax
	movl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$56, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$7, %eax
	movl	%eax, %ebx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$55, %eax
	movl	%eax, -104(%rbp)
	jmp	.L235
.L236:
	movl	-84(%rbp), %eax
	movl	-116(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	movl	-120(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %edi
	orl	%esi, %edi
	movl	%edi, -88(%rbp)
	movl	-92(%rbp), %eax
	movl	-124(%rbp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %r9d
	orl	%r8d, %r9d
	movl	%r9d, -96(%rbp)
	movl	%r15d, %eax
	movl	-128(%rbp), %r9d
	cltd
	idivl	%r9d
	movl	%edx, %r15d
	orl	%r9d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r14d
	orl	%r10d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r13d
	orl	%r11d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	-100(%rbp)
	movl	%edx, %r12d
	orl	-100(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %ebx
	orl	-104(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %eax
	orl	%r8d, %eax
	movl	%eax, -96(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r15d
	orl	%r9d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r14d
	orl	%r10d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r13d
	orl	%r11d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	-100(%rbp)
	movl	%edx, %r12d
	orl	-100(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %ebx
	orl	-104(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %eax
	orl	%r8d, %eax
	movl	%eax, -96(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r15d
	orl	%r9d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r14d
	orl	%r10d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r13d
	orl	%r11d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	-100(%rbp)
	movl	%edx, %r12d
	orl	-100(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %ebx
	orl	-104(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %eax
	orl	%r8d, %eax
	movl	%eax, -96(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r15d
	orl	%r9d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r14d
	orl	%r10d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r13d
	orl	%r11d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	-100(%rbp)
	movl	%edx, %r12d
	orl	-100(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %ebx
	orl	-104(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %eax
	orl	%r8d, %eax
	movl	%eax, -96(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r15d
	orl	%r9d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r14d
	orl	%r10d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r13d
	orl	%r11d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	-100(%rbp)
	movl	%edx, %r12d
	orl	-100(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %ebx
	orl	-104(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %eax
	orl	%r8d, %eax
	movl	%eax, -96(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r15d
	orl	%r9d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r14d
	orl	%r10d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r13d
	orl	%r11d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	-100(%rbp)
	movl	%edx, %r12d
	orl	-100(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %ebx
	orl	-104(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %eax
	orl	%r8d, %eax
	movl	%eax, -96(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r15d
	orl	%r9d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r14d
	orl	%r10d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r13d
	orl	%r11d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	-100(%rbp)
	movl	%edx, %r12d
	orl	-100(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %ebx
	orl	-104(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %eax
	orl	%r8d, %eax
	movl	%eax, -96(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r15d
	orl	%r9d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r14d
	orl	%r10d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r13d
	orl	%r11d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	-100(%rbp)
	movl	%edx, %r12d
	orl	-100(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %ebx
	orl	-104(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %eax
	orl	%r8d, %eax
	movl	%eax, -96(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r15d
	orl	%r9d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r14d
	orl	%r10d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r13d
	orl	%r11d, %r13d
	movl	%r12d, %eax
	cltd
	idivl	-100(%rbp)
	movl	%edx, %r12d
	orl	-100(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %ebx
	orl	-104(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %ecx
	orl	%esi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %eax
	orl	%r8d, %eax
	movl	%eax, -96(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r9d
	movl	%edx, %r15d
	orl	%r9d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r14d
	orl	%r10d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r13d
	orl	%r11d, %r13d
	movl	%r12d, %eax
	movl	-100(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r12d
	orl	%ecx, %r12d
	movl	%ebx, %eax
	movl	-104(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %ebx
	orl	%esi, %ebx
.L235:
	movq	-112(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -112(%rbp)
	testq	%rax, %rax
	jne	.L236
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE81:
	.size	integer_mod_8, .-integer_mod_8
	.globl	integer_mod_9
	.type	integer_mod_9, @function
integer_mod_9:
.LFB82:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -120(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$63, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$62, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$1, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$61, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$2, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$60, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$3, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$59, %eax
	movl	%eax, %r9d
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$4, %eax
	movl	%eax, %r15d
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$58, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$5, %eax
	movl	%eax, %r14d
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$6, %eax
	movl	%eax, %r13d
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$56, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$7, %eax
	movl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$55, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$8, %eax
	movl	%eax, %ebx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$54, %eax
	movl	%eax, -112(%rbp)
	jmp	.L238
.L239:
	movl	-84(%rbp), %eax
	movl	-124(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	movl	-128(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %edi
	orl	%esi, %edi
	movl	%edi, -88(%rbp)
	movl	-92(%rbp), %eax
	movl	-132(%rbp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %r10d
	orl	%r8d, %r10d
	movl	%r10d, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %r11d
	orl	%r9d, %r11d
	movl	%r11d, -100(%rbp)
	movl	%r15d, %eax
	movl	-136(%rbp), %r10d
	cltd
	idivl	%r10d
	movl	%edx, %r15d
	orl	%r10d, %r15d
	movl	%r14d, %eax
	movl	-140(%rbp), %r11d
	cltd
	idivl	%r11d
	movl	%edx, %r14d
	orl	%r11d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %r13d
	orl	-104(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r12d
	orl	-108(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %ebx
	orl	-112(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	orl	%r9d, %eax
	movl	%eax, -100(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r15d
	orl	%r10d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r14d
	orl	%r11d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %r13d
	orl	-104(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r12d
	orl	-108(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %ebx
	orl	-112(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	orl	%r9d, %eax
	movl	%eax, -100(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r15d
	orl	%r10d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r14d
	orl	%r11d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %r13d
	orl	-104(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r12d
	orl	-108(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %ebx
	orl	-112(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	orl	%r9d, %eax
	movl	%eax, -100(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r15d
	orl	%r10d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r14d
	orl	%r11d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %r13d
	orl	-104(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r12d
	orl	-108(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %ebx
	orl	-112(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	orl	%r9d, %eax
	movl	%eax, -100(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r15d
	orl	%r10d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r14d
	orl	%r11d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %r13d
	orl	-104(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r12d
	orl	-108(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %ebx
	orl	-112(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	orl	%r9d, %eax
	movl	%eax, -100(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r15d
	orl	%r10d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r14d
	orl	%r11d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %r13d
	orl	-104(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r12d
	orl	-108(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %ebx
	orl	-112(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	orl	%r9d, %eax
	movl	%eax, -100(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r15d
	orl	%r10d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r14d
	orl	%r11d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %r13d
	orl	-104(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r12d
	orl	-108(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %ebx
	orl	-112(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	orl	%r9d, %eax
	movl	%eax, -100(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r15d
	orl	%r10d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r14d
	orl	%r11d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %r13d
	orl	-104(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r12d
	orl	-108(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %ebx
	orl	-112(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	orl	%r9d, %eax
	movl	%eax, -100(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r15d
	orl	%r10d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r14d
	orl	%r11d, %r14d
	movl	%r13d, %eax
	cltd
	idivl	-104(%rbp)
	movl	%edx, %r13d
	orl	-104(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r12d
	orl	-108(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %ebx
	orl	-112(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %ecx
	orl	%esi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	orl	%edi, %esi
	movl	%esi, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	%edi, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	orl	%r9d, %eax
	movl	%eax, -100(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r10d
	movl	%edx, %r15d
	orl	%r10d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r14d
	orl	%r11d, %r14d
	movl	%r13d, %eax
	movl	-104(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	orl	%ecx, %r13d
	movl	%r12d, %eax
	movl	-108(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %r12d
	orl	%esi, %r12d
	movl	%ebx, %eax
	movl	-112(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
.L238:
	movq	-120(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -120(%rbp)
	testq	%rax, %rax
	jne	.L239
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE82:
	.size	integer_mod_9, .-integer_mod_9
	.globl	integer_mod_10
	.type	integer_mod_10, @function
integer_mod_10:
.LFB83:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -128(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$63, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$62, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$1, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$61, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$2, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$60, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$3, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$59, %eax
	movl	%eax, %r9d
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$4, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$58, %eax
	movl	%eax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$5, %eax
	movl	%eax, %r15d
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$6, %eax
	movl	%eax, %r14d
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$56, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$7, %eax
	movl	%eax, %r13d
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$55, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$8, %eax
	movl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$54, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$9, %eax
	movl	%eax, %ebx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$53, %eax
	movl	%eax, -120(%rbp)
	jmp	.L241
.L242:
	movl	-84(%rbp), %eax
	movl	-132(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	movl	-136(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %edi
	orl	%esi, %edi
	movl	%edi, -88(%rbp)
	movl	-92(%rbp), %eax
	movl	-140(%rbp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %r11d
	orl	%r8d, %r11d
	movl	%r11d, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %r10d
	orl	%r9d, %r10d
	movl	%r10d, -100(%rbp)
	movl	-104(%rbp), %eax
	movl	-144(%rbp), %r10d
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	%r15d, %eax
	movl	-148(%rbp), %r11d
	cltd
	idivl	%r11d
	movl	%edx, %r15d
	orl	%r11d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r14d
	orl	-108(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r13d
	orl	-112(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r12d
	orl	-116(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %ebx
	orl	-120(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	orl	%r9d, %eax
	movl	%eax, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r15d
	orl	%r11d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r14d
	orl	-108(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r13d
	orl	-112(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r12d
	orl	-116(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %ebx
	orl	-120(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	orl	%r9d, %eax
	movl	%eax, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r15d
	orl	%r11d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r14d
	orl	-108(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r13d
	orl	-112(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r12d
	orl	-116(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %ebx
	orl	-120(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	orl	%r9d, %eax
	movl	%eax, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r15d
	orl	%r11d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r14d
	orl	-108(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r13d
	orl	-112(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r12d
	orl	-116(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %ebx
	orl	-120(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	orl	%r9d, %eax
	movl	%eax, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r15d
	orl	%r11d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r14d
	orl	-108(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r13d
	orl	-112(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r12d
	orl	-116(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %ebx
	orl	-120(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	orl	%r9d, %eax
	movl	%eax, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r15d
	orl	%r11d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r14d
	orl	-108(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r13d
	orl	-112(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r12d
	orl	-116(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %ebx
	orl	-120(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	orl	%r9d, %eax
	movl	%eax, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r15d
	orl	%r11d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r14d
	orl	-108(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r13d
	orl	-112(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r12d
	orl	-116(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %ebx
	orl	-120(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	orl	%r9d, %eax
	movl	%eax, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r15d
	orl	%r11d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r14d
	orl	-108(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r13d
	orl	-112(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r12d
	orl	-116(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %ebx
	orl	-120(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %eax
	orl	%r9d, %eax
	movl	%eax, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r15d
	orl	%r11d, %r15d
	movl	%r14d, %eax
	cltd
	idivl	-108(%rbp)
	movl	%edx, %r14d
	orl	-108(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r13d
	orl	-112(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r12d
	orl	-116(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %ebx
	orl	-120(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %ecx
	orl	%esi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	orl	%edi, %esi
	movl	%esi, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	%edi, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	orl	%r9d, %edi
	movl	%edi, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%edx, %eax
	orl	%r10d, %eax
	movl	%eax, -104(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	%r11d
	movl	%edx, %r15d
	orl	%r11d, %r15d
	movl	%r14d, %eax
	movl	-108(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r14d
	orl	%ecx, %r14d
	movl	%r13d, %eax
	movl	-112(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %r13d
	orl	%esi, %r13d
	movl	%r12d, %eax
	movl	-116(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r12d
	orl	%ecx, %r12d
	movl	%ebx, %eax
	movl	-120(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %ebx
	orl	%esi, %ebx
.L241:
	movq	-128(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -128(%rbp)
	testq	%rax, %rax
	jne	.L242
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-104(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE83:
	.size	integer_mod_10, .-integer_mod_10
	.globl	integer_mod_11
	.type	integer_mod_11, @function
integer_mod_11:
.LFB84:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -136(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$63, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$62, %eax
	movl	%eax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$1, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$61, %eax
	movl	%eax, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$2, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$60, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$3, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$59, %eax
	movl	%eax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$4, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$58, %eax
	movl	%eax, -156(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$5, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$6, %eax
	movl	%eax, %r15d
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$56, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$7, %eax
	movl	%eax, %r14d
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$55, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$8, %eax
	movl	%eax, %r13d
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$54, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$9, %eax
	movl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$53, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$10, %eax
	movl	%eax, %ebx
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$52, %eax
	movl	%eax, -128(%rbp)
	jmp	.L244
.L245:
	movl	-84(%rbp), %eax
	movl	-140(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	movl	-144(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %edi
	orl	%esi, %edi
	movl	%edi, -88(%rbp)
	movl	-92(%rbp), %eax
	movl	-148(%rbp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %r9d
	orl	%r8d, %r9d
	movl	%r9d, -96(%rbp)
	movl	-100(%rbp), %eax
	movl	-152(%rbp), %r9d
	cltd
	idivl	%r9d
	movl	%edx, %r10d
	orl	%r9d, %r10d
	movl	%r10d, -100(%rbp)
	movl	-104(%rbp), %eax
	movl	-156(%rbp), %r10d
	cltd
	idivl	%r10d
	movl	%edx, %r11d
	orl	%r10d, %r11d
	movl	%r11d, -104(%rbp)
	movl	-108(%rbp), %eax
	movl	-160(%rbp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r15d
	orl	-112(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r14d
	orl	-116(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r13d
	orl	-120(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r12d
	orl	-124(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %ebx
	orl	-128(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%edx, %eax
	orl	%r10d, %eax
	movl	%eax, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r15d
	orl	-112(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r14d
	orl	-116(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r13d
	orl	-120(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r12d
	orl	-124(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %ebx
	orl	-128(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%edx, %eax
	orl	%r10d, %eax
	movl	%eax, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r15d
	orl	-112(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r14d
	orl	-116(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r13d
	orl	-120(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r12d
	orl	-124(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %ebx
	orl	-128(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%edx, %eax
	orl	%r10d, %eax
	movl	%eax, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r15d
	orl	-112(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r14d
	orl	-116(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r13d
	orl	-120(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r12d
	orl	-124(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %ebx
	orl	-128(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%edx, %eax
	orl	%r10d, %eax
	movl	%eax, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r15d
	orl	-112(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r14d
	orl	-116(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r13d
	orl	-120(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r12d
	orl	-124(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %ebx
	orl	-128(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%edx, %eax
	orl	%r10d, %eax
	movl	%eax, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r15d
	orl	-112(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r14d
	orl	-116(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r13d
	orl	-120(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r12d
	orl	-124(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %ebx
	orl	-128(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%edx, %eax
	orl	%r10d, %eax
	movl	%eax, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r15d
	orl	-112(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r14d
	orl	-116(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r13d
	orl	-120(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r12d
	orl	-124(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %ebx
	orl	-128(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%edx, %eax
	orl	%r10d, %eax
	movl	%eax, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r15d
	orl	-112(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r14d
	orl	-116(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r13d
	orl	-120(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r12d
	orl	-124(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %ebx
	orl	-128(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%edx, %eax
	orl	%r10d, %eax
	movl	%eax, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-112(%rbp)
	movl	%edx, %r15d
	orl	-112(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %r14d
	orl	-116(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r13d
	orl	-120(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r12d
	orl	-124(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %ebx
	orl	-128(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %ecx
	orl	%esi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	orl	%edi, %esi
	movl	%esi, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	%edi, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	orl	%r9d, %edi
	movl	%edi, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	orl	%r10d, %edi
	movl	%edi, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%edx, %eax
	orl	%r11d, %eax
	movl	%eax, -108(%rbp)
	movl	%r15d, %eax
	movl	-112(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	orl	%ecx, %r15d
	movl	%r14d, %eax
	movl	-116(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %r14d
	orl	%esi, %r14d
	movl	%r13d, %eax
	movl	-120(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	orl	%ecx, %r13d
	movl	%r12d, %eax
	movl	-124(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %r12d
	orl	%esi, %r12d
	movl	%ebx, %eax
	movl	-128(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
.L244:
	movq	-136(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -136(%rbp)
	testq	%rax, %rax
	jne	.L245
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-104(%rbp), %edi
	call	use_int@PLT
	movl	-108(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE84:
	.size	integer_mod_11, .-integer_mod_11
	.globl	integer_mod_12
	.type	integer_mod_12, @function
integer_mod_12:
.LFB85:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -144(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$63, %eax
	movl	%eax, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$62, %eax
	movl	%eax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$1, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$61, %eax
	movl	%eax, -156(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$2, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$60, %eax
	movl	%eax, %r8d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$3, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$59, %eax
	movl	%eax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$4, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$58, %eax
	movl	%eax, -164(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$5, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$6, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$56, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$7, %eax
	movl	%eax, %r15d
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$55, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$8, %eax
	movl	%eax, %r14d
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$54, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$9, %eax
	movl	%eax, %r13d
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$53, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$10, %eax
	movl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$52, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$11, %eax
	movl	%eax, %ebx
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$51, %eax
	movl	%eax, -136(%rbp)
	jmp	.L247
.L248:
	movl	-84(%rbp), %eax
	movl	-148(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	movl	-152(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %edi
	orl	%esi, %edi
	movl	%edi, -88(%rbp)
	movl	-92(%rbp), %eax
	movl	-156(%rbp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %r9d
	orl	%r8d, %r9d
	movl	%r9d, -96(%rbp)
	movl	-100(%rbp), %eax
	movl	-160(%rbp), %r9d
	cltd
	idivl	%r9d
	movl	%edx, %r10d
	orl	%r9d, %r10d
	movl	%r10d, -100(%rbp)
	movl	-104(%rbp), %eax
	movl	-164(%rbp), %r10d
	cltd
	idivl	%r10d
	movl	%edx, %r11d
	orl	%r10d, %r11d
	movl	%r11d, -104(%rbp)
	movl	-108(%rbp), %eax
	movl	-168(%rbp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %eax
	movl	-116(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r15d
	orl	-120(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r14d
	orl	-124(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r13d
	orl	-128(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r12d
	orl	-132(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %ebx
	orl	-136(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %eax
	movl	-116(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r15d
	orl	-120(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r14d
	orl	-124(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r13d
	orl	-128(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r12d
	orl	-132(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %ebx
	orl	-136(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %eax
	movl	-116(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r15d
	orl	-120(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r14d
	orl	-124(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r13d
	orl	-128(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r12d
	orl	-132(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %ebx
	orl	-136(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %eax
	movl	-116(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r15d
	orl	-120(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r14d
	orl	-124(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r13d
	orl	-128(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r12d
	orl	-132(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %ebx
	orl	-136(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %eax
	movl	-116(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r15d
	orl	-120(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r14d
	orl	-124(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r13d
	orl	-128(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r12d
	orl	-132(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %ebx
	orl	-136(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %eax
	movl	-116(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r15d
	orl	-120(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r14d
	orl	-124(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r13d
	orl	-128(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r12d
	orl	-132(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %ebx
	orl	-136(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %eax
	movl	-116(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r15d
	orl	-120(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r14d
	orl	-124(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r13d
	orl	-128(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r12d
	orl	-132(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %ebx
	orl	-136(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %eax
	movl	-116(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r15d
	orl	-120(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r14d
	orl	-124(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r13d
	orl	-128(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r12d
	orl	-132(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %ebx
	orl	-136(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-116(%rbp)
	movl	%edx, %eax
	movl	-116(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %r15d
	orl	-120(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %r14d
	orl	-124(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r13d
	orl	-128(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r12d
	orl	-132(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %ebx
	orl	-136(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %ecx
	orl	%esi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	orl	%edi, %esi
	movl	%esi, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	%edi, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %edi
	orl	%r9d, %edi
	movl	%edi, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%edx, %edi
	orl	%r10d, %edi
	movl	%edi, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%edx, %edi
	orl	%r11d, %edi
	movl	%edi, -108(%rbp)
	movl	-112(%rbp), %eax
	movl	-116(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -112(%rbp)
	movl	%r15d, %eax
	movl	-120(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %r15d
	orl	%esi, %r15d
	movl	%r14d, %eax
	movl	-124(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %r14d
	orl	%esi, %r14d
	movl	%r13d, %eax
	movl	-128(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	orl	%ecx, %r13d
	movl	%r12d, %eax
	movl	-132(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %r12d
	orl	%esi, %r12d
	movl	%ebx, %eax
	movl	-136(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %ebx
	orl	%ecx, %ebx
.L247:
	movq	-144(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -144(%rbp)
	testq	%rax, %rax
	jne	.L248
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-104(%rbp), %edi
	call	use_int@PLT
	movl	-108(%rbp), %edi
	call	use_int@PLT
	movl	-112(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE85:
	.size	integer_mod_12, .-integer_mod_12
	.globl	integer_mod_13
	.type	integer_mod_13, @function
integer_mod_13:
.LFB86:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -152(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$63, %eax
	movl	%eax, -156(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$62, %eax
	movl	%eax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$1, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$61, %eax
	movl	%eax, -164(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$2, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$60, %eax
	movl	%eax, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$3, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$59, %eax
	movl	%eax, -172(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$4, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$58, %eax
	movl	%eax, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$5, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, -180(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$6, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$56, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$7, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$55, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$8, %eax
	movl	%eax, %r15d
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$54, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$9, %eax
	movl	%eax, %r14d
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$53, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$10, %eax
	movl	%eax, %r13d
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$52, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$11, %eax
	movl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$51, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$12, %eax
	movl	%eax, %ebx
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$50, %eax
	movl	%eax, -144(%rbp)
	jmp	.L250
.L251:
	movl	-84(%rbp), %eax
	movl	-156(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	movl	-160(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %edi
	orl	%esi, %edi
	movl	%edi, -88(%rbp)
	movl	-92(%rbp), %eax
	movl	-164(%rbp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	movl	-168(%rbp), %r8d
	cltd
	idivl	%r8d
	movl	%edx, %r9d
	orl	%r8d, %r9d
	movl	%r9d, -96(%rbp)
	movl	-100(%rbp), %eax
	movl	-172(%rbp), %r9d
	cltd
	idivl	%r9d
	movl	%edx, %r10d
	orl	%r9d, %r10d
	movl	%r10d, -100(%rbp)
	movl	-104(%rbp), %eax
	movl	-176(%rbp), %r10d
	cltd
	idivl	%r10d
	movl	%edx, %r11d
	orl	%r10d, %r11d
	movl	%r11d, -104(%rbp)
	movl	-108(%rbp), %eax
	movl	-180(%rbp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %eax
	movl	-120(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r15d
	orl	-128(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r14d
	orl	-132(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r13d
	orl	-136(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r12d
	orl	-140(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %ebx
	orl	-144(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %eax
	movl	-120(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r15d
	orl	-128(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r14d
	orl	-132(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r13d
	orl	-136(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r12d
	orl	-140(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %ebx
	orl	-144(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %eax
	movl	-120(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r15d
	orl	-128(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r14d
	orl	-132(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r13d
	orl	-136(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r12d
	orl	-140(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %ebx
	orl	-144(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %eax
	movl	-120(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r15d
	orl	-128(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r14d
	orl	-132(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r13d
	orl	-136(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r12d
	orl	-140(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %ebx
	orl	-144(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %eax
	movl	-120(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r15d
	orl	-128(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r14d
	orl	-132(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r13d
	orl	-136(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r12d
	orl	-140(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %ebx
	orl	-144(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %eax
	movl	-120(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r15d
	orl	-128(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r14d
	orl	-132(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r13d
	orl	-136(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r12d
	orl	-140(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %ebx
	orl	-144(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %eax
	movl	-120(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r15d
	orl	-128(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r14d
	orl	-132(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r13d
	orl	-136(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r12d
	orl	-140(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %ebx
	orl	-144(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %eax
	movl	-120(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r15d
	orl	-128(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r14d
	orl	-132(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r13d
	orl	-136(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r12d
	orl	-140(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %ebx
	orl	-144(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-120(%rbp)
	movl	%edx, %eax
	movl	-120(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %r15d
	orl	-128(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %r14d
	orl	-132(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r13d
	orl	-136(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r12d
	orl	-140(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %ebx
	orl	-144(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %ecx
	orl	%esi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	orl	%edi, %esi
	movl	%esi, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	%edi, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %r8d
	orl	%r9d, %r8d
	movl	%r8d, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%edx, %r8d
	orl	%r10d, %r8d
	movl	%r8d, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%edx, %r9d
	orl	%r11d, %r9d
	movl	%r9d, -108(%rbp)
	movl	-112(%rbp), %eax
	movl	-120(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	orl	%ecx, %r10d
	movl	%r10d, -112(%rbp)
	movl	-116(%rbp), %eax
	movl	-124(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -116(%rbp)
	movl	%r15d, %eax
	movl	-128(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	orl	%ecx, %r15d
	movl	%r14d, %eax
	movl	-132(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %r14d
	orl	%esi, %r14d
	movl	%r13d, %eax
	movl	-136(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	orl	%ecx, %r13d
	movl	%r12d, %eax
	movl	-140(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %r12d
	orl	%esi, %r12d
	movl	%ebx, %eax
	movl	-144(%rbp), %edi
	cltd
	idivl	%edi
	movl	%edx, %ebx
	orl	%edi, %ebx
.L250:
	movq	-152(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -152(%rbp)
	testq	%rax, %rax
	jne	.L251
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-104(%rbp), %edi
	call	use_int@PLT
	movl	-108(%rbp), %edi
	call	use_int@PLT
	movl	-112(%rbp), %edi
	call	use_int@PLT
	movl	-116(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$152, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE86:
	.size	integer_mod_13, .-integer_mod_13
	.globl	integer_mod_14
	.type	integer_mod_14, @function
integer_mod_14:
.LFB87:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -160(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$63, %eax
	movl	%eax, -164(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$62, %eax
	movl	%eax, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$1, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$61, %eax
	movl	%eax, -172(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$2, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$60, %eax
	movl	%eax, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$3, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$59, %eax
	movl	%eax, -180(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$4, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$58, %eax
	movl	%eax, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$5, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, -188(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$6, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$56, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$7, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$55, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$8, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$54, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$9, %eax
	movl	%eax, %r15d
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$53, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$10, %eax
	movl	%eax, %r14d
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$52, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$11, %eax
	movl	%eax, %r13d
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$51, %eax
	movl	%eax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$12, %eax
	movl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$50, %eax
	movl	%eax, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$13, %eax
	movl	%eax, %ebx
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	addl	$49, %eax
	movl	%eax, -152(%rbp)
	jmp	.L253
.L254:
	movl	-84(%rbp), %eax
	movl	-164(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	movl	-168(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %edi
	orl	%esi, %edi
	movl	%edi, -88(%rbp)
	movl	-92(%rbp), %eax
	movl	-172(%rbp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	movl	-176(%rbp), %r8d
	cltd
	idivl	%r8d
	movl	%edx, %r9d
	orl	%r8d, %r9d
	movl	%r9d, -96(%rbp)
	movl	-100(%rbp), %eax
	movl	-180(%rbp), %r9d
	cltd
	idivl	%r9d
	movl	%edx, %r10d
	orl	%r9d, %r10d
	movl	%r10d, -100(%rbp)
	movl	-104(%rbp), %eax
	movl	-184(%rbp), %r10d
	cltd
	idivl	%r10d
	movl	%edx, %r11d
	orl	%r10d, %r11d
	movl	%r11d, -104(%rbp)
	movl	-108(%rbp), %eax
	movl	-188(%rbp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r15d
	orl	-136(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r14d
	orl	-140(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r13d
	orl	-144(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r12d
	orl	-148(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %ebx
	orl	-152(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r15d
	orl	-136(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r14d
	orl	-140(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r13d
	orl	-144(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r12d
	orl	-148(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %ebx
	orl	-152(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r15d
	orl	-136(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r14d
	orl	-140(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r13d
	orl	-144(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r12d
	orl	-148(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %ebx
	orl	-152(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r15d
	orl	-136(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r14d
	orl	-140(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r13d
	orl	-144(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r12d
	orl	-148(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %ebx
	orl	-152(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r15d
	orl	-136(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r14d
	orl	-140(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r13d
	orl	-144(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r12d
	orl	-148(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %ebx
	orl	-152(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r15d
	orl	-136(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r14d
	orl	-140(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r13d
	orl	-144(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r12d
	orl	-148(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %ebx
	orl	-152(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r15d
	orl	-136(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r14d
	orl	-140(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r13d
	orl	-144(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r12d
	orl	-148(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %ebx
	orl	-152(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r15d
	orl	-136(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r14d
	orl	-140(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r13d
	orl	-144(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r12d
	orl	-148(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %ebx
	orl	-152(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-124(%rbp)
	movl	%edx, %eax
	movl	-124(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %r15d
	orl	-136(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %r14d
	orl	-140(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r13d
	orl	-144(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r12d
	orl	-148(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %ebx
	orl	-152(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %ecx
	orl	%esi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	movl	%edx, %ecx
	orl	%edi, %ecx
	movl	%ecx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	%edi, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %r8d
	orl	%r9d, %r8d
	movl	%r8d, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%edx, %r8d
	orl	%r10d, %r8d
	movl	%r8d, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%edx, %r9d
	orl	%r11d, %r9d
	movl	%r9d, -108(%rbp)
	movl	-112(%rbp), %eax
	movl	-124(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	orl	%ecx, %r10d
	movl	%r10d, -112(%rbp)
	movl	-116(%rbp), %eax
	movl	-128(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %r11d
	orl	%esi, %r11d
	movl	%r11d, -116(%rbp)
	movl	-120(%rbp), %eax
	movl	-132(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -120(%rbp)
	movl	%r15d, %eax
	movl	-136(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %r15d
	orl	%esi, %r15d
	movl	%r14d, %eax
	movl	-140(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %r14d
	orl	%esi, %r14d
	movl	%r13d, %eax
	movl	-144(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	orl	%ecx, %r13d
	movl	%r12d, %eax
	movl	-148(%rbp), %edi
	cltd
	idivl	%edi
	movl	%edx, %r12d
	orl	%edi, %r12d
	movl	%ebx, %eax
	movl	-152(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %ebx
	orl	%esi, %ebx
.L253:
	movq	-160(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -160(%rbp)
	testq	%rax, %rax
	jne	.L254
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-104(%rbp), %edi
	call	use_int@PLT
	movl	-108(%rbp), %edi
	call	use_int@PLT
	movl	-112(%rbp), %edi
	call	use_int@PLT
	movl	-116(%rbp), %edi
	call	use_int@PLT
	movl	-120(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$152, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE87:
	.size	integer_mod_14, .-integer_mod_14
	.globl	integer_mod_15
	.type	integer_mod_15, @function
integer_mod_15:
.LFB88:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -168(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	addl	$1, %eax
	movl	%eax, -84(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$63, %eax
	movl	%eax, -172(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	movl	%eax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$62, %eax
	movl	%eax, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$1, %eax
	movl	%eax, -92(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$61, %eax
	movl	%eax, -180(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$2, %eax
	movl	%eax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$60, %eax
	movl	%eax, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$3, %eax
	movl	%eax, -100(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$59, %eax
	movl	%eax, -188(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$4, %eax
	movl	%eax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$58, %eax
	movl	%eax, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$5, %eax
	movl	%eax, -108(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$57, %eax
	movl	%eax, -196(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$6, %eax
	movl	%eax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$56, %eax
	movl	%eax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$7, %eax
	movl	%eax, -116(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$55, %eax
	movl	%eax, -132(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$8, %eax
	movl	%eax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$54, %eax
	movl	%eax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$9, %eax
	movl	%eax, -124(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$53, %eax
	movl	%eax, -140(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$10, %eax
	movl	%eax, %r15d
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$52, %eax
	movl	%eax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$11, %eax
	movl	%eax, %r14d
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$51, %eax
	movl	%eax, -148(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$12, %eax
	movl	%eax, %r13d
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$50, %eax
	movl	%eax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$13, %eax
	movl	%eax, %r12d
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	addl	$49, %eax
	movl	%eax, -156(%rbp)
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	movl	%eax, %edx
	movq	-72(%rbp), %rax
	addl	%edx, %eax
	subl	$14, %eax
	movl	%eax, %ebx
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	addl	$48, %eax
	movl	%eax, -160(%rbp)
	jmp	.L256
.L257:
	movl	-84(%rbp), %eax
	movl	-172(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %eax
	orl	%ecx, %eax
	movl	%eax, -84(%rbp)
	movl	-88(%rbp), %eax
	movl	-176(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %edi
	orl	%esi, %edi
	movl	%edi, -88(%rbp)
	movl	-92(%rbp), %eax
	movl	-180(%rbp), %edi
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	movl	-184(%rbp), %r8d
	cltd
	idivl	%r8d
	movl	%edx, %r9d
	orl	%r8d, %r9d
	movl	%r9d, -96(%rbp)
	movl	-100(%rbp), %eax
	movl	-188(%rbp), %r9d
	cltd
	idivl	%r9d
	movl	%edx, %r10d
	orl	%r9d, %r10d
	movl	%r10d, -100(%rbp)
	movl	-104(%rbp), %eax
	movl	-192(%rbp), %r10d
	cltd
	idivl	%r10d
	movl	%edx, %r11d
	orl	%r10d, %r11d
	movl	%r11d, -104(%rbp)
	movl	-108(%rbp), %eax
	movl	-196(%rbp), %r11d
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %eax
	movl	-136(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %eax
	movl	-140(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -124(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r15d
	orl	-144(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r14d
	orl	-148(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %r13d
	orl	-152(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-156(%rbp)
	movl	%edx, %r12d
	orl	-156(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-160(%rbp)
	movl	%edx, %ebx
	orl	-160(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %eax
	movl	-136(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %eax
	movl	-140(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -124(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r15d
	orl	-144(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r14d
	orl	-148(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %r13d
	orl	-152(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-156(%rbp)
	movl	%edx, %r12d
	orl	-156(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-160(%rbp)
	movl	%edx, %ebx
	orl	-160(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %eax
	movl	-136(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %eax
	movl	-140(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -124(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r15d
	orl	-144(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r14d
	orl	-148(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %r13d
	orl	-152(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-156(%rbp)
	movl	%edx, %r12d
	orl	-156(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-160(%rbp)
	movl	%edx, %ebx
	orl	-160(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %eax
	movl	-136(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %eax
	movl	-140(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -124(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r15d
	orl	-144(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r14d
	orl	-148(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %r13d
	orl	-152(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-156(%rbp)
	movl	%edx, %r12d
	orl	-156(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-160(%rbp)
	movl	%edx, %ebx
	orl	-160(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %eax
	movl	-136(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %eax
	movl	-140(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -124(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r15d
	orl	-144(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r14d
	orl	-148(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %r13d
	orl	-152(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-156(%rbp)
	movl	%edx, %r12d
	orl	-156(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-160(%rbp)
	movl	%edx, %ebx
	orl	-160(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %eax
	movl	-136(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %eax
	movl	-140(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -124(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r15d
	orl	-144(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r14d
	orl	-148(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %r13d
	orl	-152(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-156(%rbp)
	movl	%edx, %r12d
	orl	-156(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-160(%rbp)
	movl	%edx, %ebx
	orl	-160(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %eax
	movl	-136(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %eax
	movl	-140(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -124(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r15d
	orl	-144(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r14d
	orl	-148(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %r13d
	orl	-152(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-156(%rbp)
	movl	%edx, %r12d
	orl	-156(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-160(%rbp)
	movl	%edx, %ebx
	orl	-160(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %eax
	movl	-136(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %eax
	movl	-140(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -124(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r15d
	orl	-144(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r14d
	orl	-148(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %r13d
	orl	-152(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-156(%rbp)
	movl	%edx, %r12d
	orl	-156(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-160(%rbp)
	movl	%edx, %ebx
	orl	-160(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	orl	%esi, %edx
	movl	%edx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	orl	%edi, %edx
	movl	%edx, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	orl	%r8d, %edx
	movl	%edx, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	orl	%r9d, %edx
	movl	%edx, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	orl	%r10d, %edx
	movl	%edx, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	orl	%r11d, %edx
	movl	%edx, -108(%rbp)
	movl	-112(%rbp), %eax
	cltd
	idivl	-128(%rbp)
	movl	%edx, %eax
	movl	-128(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -112(%rbp)
	movl	-116(%rbp), %eax
	cltd
	idivl	-132(%rbp)
	movl	%edx, %eax
	movl	-132(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -116(%rbp)
	movl	-120(%rbp), %eax
	cltd
	idivl	-136(%rbp)
	movl	%edx, %eax
	movl	-136(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -120(%rbp)
	movl	-124(%rbp), %eax
	cltd
	idivl	-140(%rbp)
	movl	%edx, %eax
	movl	-140(%rbp), %edx
	orl	%edx, %eax
	movl	%eax, -124(%rbp)
	movl	%r15d, %eax
	cltd
	idivl	-144(%rbp)
	movl	%edx, %r15d
	orl	-144(%rbp), %r15d
	movl	%r14d, %eax
	cltd
	idivl	-148(%rbp)
	movl	%edx, %r14d
	orl	-148(%rbp), %r14d
	movl	%r13d, %eax
	cltd
	idivl	-152(%rbp)
	movl	%edx, %r13d
	orl	-152(%rbp), %r13d
	movl	%r12d, %eax
	cltd
	idivl	-156(%rbp)
	movl	%edx, %r12d
	orl	-156(%rbp), %r12d
	movl	%ebx, %eax
	cltd
	idivl	-160(%rbp)
	movl	%edx, %ebx
	orl	-160(%rbp), %ebx
	movl	-84(%rbp), %eax
	cltd
	idivl	%ecx
	orl	%ecx, %edx
	movl	%edx, -84(%rbp)
	movl	-88(%rbp), %eax
	cltd
	idivl	%esi
	movl	%edx, %ecx
	orl	%esi, %ecx
	movl	%ecx, -88(%rbp)
	movl	-92(%rbp), %eax
	cltd
	idivl	%edi
	movl	%edx, %esi
	orl	%edi, %esi
	movl	%esi, -92(%rbp)
	movl	-96(%rbp), %eax
	cltd
	idivl	%r8d
	movl	%edx, %edi
	orl	%r8d, %edi
	movl	%edi, -96(%rbp)
	movl	-100(%rbp), %eax
	cltd
	idivl	%r9d
	movl	%edx, %r8d
	orl	%r9d, %r8d
	movl	%r8d, -100(%rbp)
	movl	-104(%rbp), %eax
	cltd
	idivl	%r10d
	movl	%edx, %r8d
	orl	%r10d, %r8d
	movl	%r8d, -104(%rbp)
	movl	-108(%rbp), %eax
	cltd
	idivl	%r11d
	movl	%edx, %r9d
	orl	%r11d, %r9d
	movl	%r9d, -108(%rbp)
	movl	-112(%rbp), %eax
	movl	-128(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	orl	%ecx, %r10d
	movl	%r10d, -112(%rbp)
	movl	-116(%rbp), %eax
	movl	-132(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %r11d
	orl	%esi, %r11d
	movl	%r11d, -116(%rbp)
	movl	-120(%rbp), %eax
	movl	-136(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	orl	%ecx, %r8d
	movl	%r8d, -120(%rbp)
	movl	-124(%rbp), %eax
	movl	-140(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %eax
	orl	%esi, %eax
	movl	%eax, -124(%rbp)
	movl	%r15d, %eax
	movl	-144(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	orl	%ecx, %r15d
	movl	%r14d, %eax
	movl	-148(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %r14d
	orl	%esi, %r14d
	movl	%r13d, %eax
	movl	-152(%rbp), %edi
	cltd
	idivl	%edi
	movl	%edx, %r13d
	orl	%edi, %r13d
	movl	%r12d, %eax
	movl	-156(%rbp), %ecx
	cltd
	idivl	%ecx
	movl	%edx, %r12d
	orl	%ecx, %r12d
	movl	%ebx, %eax
	movl	-160(%rbp), %esi
	cltd
	idivl	%esi
	movl	%edx, %ebx
	orl	%esi, %ebx
.L256:
	movq	-168(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -168(%rbp)
	testq	%rax, %rax
	jne	.L257
	movl	-84(%rbp), %edi
	call	use_int@PLT
	movl	-88(%rbp), %edi
	call	use_int@PLT
	movl	-92(%rbp), %edi
	call	use_int@PLT
	movl	-96(%rbp), %edi
	call	use_int@PLT
	movl	-100(%rbp), %edi
	call	use_int@PLT
	movl	-104(%rbp), %edi
	call	use_int@PLT
	movl	-108(%rbp), %edi
	call	use_int@PLT
	movl	-112(%rbp), %edi
	call	use_int@PLT
	movl	-116(%rbp), %edi
	call	use_int@PLT
	movl	-120(%rbp), %edi
	call	use_int@PLT
	movl	-124(%rbp), %edi
	call	use_int@PLT
	movl	%r15d, %edi
	call	use_int@PLT
	movl	%r14d, %edi
	call	use_int@PLT
	movl	%r13d, %edi
	call	use_int@PLT
	movl	%r12d, %edi
	call	use_int@PLT
	movl	%ebx, %edi
	call	use_int@PLT
	addq	$168, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE88:
	.size	integer_mod_15, .-integer_mod_15
	.globl	integer_mod_benchmarks
	.section	.data.rel.local
	.align 32
	.type	integer_mod_benchmarks, @object
	.size	integer_mod_benchmarks, 128
integer_mod_benchmarks:
	.quad	integer_mod_0
	.quad	integer_mod_1
	.quad	integer_mod_2
	.quad	integer_mod_3
	.quad	integer_mod_4
	.quad	integer_mod_5
	.quad	integer_mod_6
	.quad	integer_mod_7
	.quad	integer_mod_8
	.quad	integer_mod_9
	.quad	integer_mod_10
	.quad	integer_mod_11
	.quad	integer_mod_12
	.quad	integer_mod_13
	.quad	integer_mod_14
	.quad	integer_mod_15
	.text
	.globl	int64_bit_0
	.type	int64_bit_0, @function
int64_bit_0:
.LFB89:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$32, %rsp
	.cfi_offset 14, -24
	.cfi_offset 13, -32
	.cfi_offset 12, -40
	.cfi_offset 3, -48
	movq	%rdi, -56(%rbp)
	movq	%rsi, -64(%rbp)
	movq	-56(%rbp), %r14
	movq	-64(%rbp), %rax
	movq	%rax, -40(%rbp)
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	addq	%rdx, %rax
	addq	$1, %rax
	movq	%rax, %r12
	movq	%r12, %rax
	salq	$32, %rax
	orq	%rax, %r12
	leaq	0(,%r12,4), %rax
	leaq	-1(%rax), %r13
	jmp	.L259
.L260:
	subq	$1, %r13
	xorq	%r13, %rbx
	xorq	%rbx, %r12
	orq	%r12, %rbx
	xorq	%r13, %rbx
	xorq	%rbx, %r12
	orq	%r12, %rbx
	xorq	%r13, %rbx
	xorq	%rbx, %r12
	orq	%r12, %rbx
	xorq	%r13, %rbx
	xorq	%rbx, %r12
	orq	%r12, %rbx
	xorq	%r13, %rbx
	xorq	%rbx, %r12
	orq	%r12, %rbx
	xorq	%r13, %rbx
	xorq	%rbx, %r12
	orq	%r12, %rbx
	xorq	%r13, %rbx
	xorq	%rbx, %r12
	orq	%r12, %rbx
	xorq	%r13, %rbx
	xorq	%rbx, %r12
	orq	%r12, %rbx
	xorq	%r13, %rbx
	xorq	%rbx, %r12
	orq	%r12, %rbx
	xorq	%r13, %rbx
	xorq	%rbx, %r12
	orq	%r12, %rbx
.L259:
	movq	%r14, %rax
	leaq	-1(%rax), %r14
	testq	%rax, %rax
	jne	.L260
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$32, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE89:
	.size	int64_bit_0, .-int64_bit_0
	.globl	int64_bit_1
	.type	int64_bit_1, @function
int64_bit_1:
.LFB90:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %rsi
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$32, %rax
	orq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	addq	$1, %rax
	movq	%rax, %r13
	movq	%r13, %rax
	salq	$32, %rax
	orq	%rax, %r13
	leaq	0(,%r13,4), %rax
	leaq	-1(%rax), %r15
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	movq	%rax, %r14
	movq	%r14, %rax
	salq	$32, %rax
	orq	%rax, %r14
	leaq	0(,%r14,4), %rax
	subq	$1, %rax
	movq	%rax, %rdi
	jmp	.L262
.L263:
	subq	$1, %r15
	movq	%rdi, %rax
	subq	$1, %rax
	movq	%rax, %rdi
	xorq	%r15, %r12
	xorq	%r12, %r13
	orq	%r13, %r12
	xorq	%rax, %rbx
	xorq	%rbx, %r14
	orq	%r14, %rbx
	xorq	%r15, %r12
	xorq	%r12, %r13
	orq	%r13, %r12
	xorq	%rax, %rbx
	xorq	%rbx, %r14
	orq	%r14, %rbx
	xorq	%r15, %r12
	xorq	%r12, %r13
	orq	%r13, %r12
	xorq	%rax, %rbx
	xorq	%rbx, %r14
	orq	%r14, %rbx
	xorq	%r15, %r12
	xorq	%r12, %r13
	orq	%r13, %r12
	xorq	%rax, %rbx
	xorq	%rbx, %r14
	orq	%r14, %rbx
	xorq	%r15, %r12
	xorq	%r12, %r13
	orq	%r13, %r12
	xorq	%rax, %rbx
	xorq	%rbx, %r14
	orq	%r14, %rbx
	xorq	%r15, %r12
	xorq	%r12, %r13
	orq	%r13, %r12
	xorq	%rax, %rbx
	xorq	%rbx, %r14
	orq	%r14, %rbx
	xorq	%r15, %r12
	xorq	%r12, %r13
	orq	%r13, %r12
	xorq	%rax, %rbx
	xorq	%rbx, %r14
	orq	%r14, %rbx
	xorq	%r15, %r12
	xorq	%r12, %r13
	orq	%r13, %r12
	xorq	%rax, %rbx
	xorq	%rbx, %r14
	orq	%r14, %rbx
	xorq	%r15, %r12
	xorq	%r12, %r13
	orq	%r13, %r12
	xorq	%rax, %rbx
	xorq	%rbx, %r14
	orq	%r14, %rbx
	xorq	%r15, %r12
	xorq	%r12, %r13
	orq	%r13, %r12
	xorq	%rax, %rbx
	xorq	%rbx, %r14
	orq	%r14, %rbx
.L262:
	movq	%rsi, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %rsi
	testq	%rax, %rax
	jne	.L263
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE90:
	.size	int64_bit_1, .-int64_bit_1
	.globl	int64_bit_2
	.type	int64_bit_2, @function
int64_bit_2:
.LFB91:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r8
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %r13
	movq	%r13, %rax
	salq	$32, %rax
	orq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	addq	$1, %rax
	movq	%rax, %r14
	movq	%r14, %rax
	salq	$32, %rax
	orq	%rax, %r14
	leaq	0(,%r14,4), %rax
	subq	$1, %rax
	movq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$32, %rax
	orq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	movq	%rax, %r15
	movq	%r15, %rax
	salq	$32, %rax
	orq	%rax, %r15
	leaq	0(,%r15,4), %rax
	subq	$1, %rax
	movq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$1, %rax
	movq	%rax, %rdx
	movq	%rdx, %rax
	salq	$32, %rax
	orq	%rax, %rdx
	movq	%rdx, %r9
	leaq	0(,%rdx,4), %rax
	subq	$1, %rax
	movq	%rax, %rdi
	jmp	.L265
.L266:
	subq	$1, %rcx
	subq	$1, %rsi
	subq	$1, %rdi
	xorq	%rcx, %r13
	xorq	%r13, %r14
	orq	%r14, %r13
	xorq	%rsi, %r12
	xorq	%r12, %r15
	orq	%r15, %r12
	xorq	%rdi, %rbx
	movq	%r9, %rax
	xorq	%rbx, %rax
	orq	%rax, %rbx
	xorq	%rcx, %r13
	xorq	%r13, %r14
	orq	%r14, %r13
	xorq	%rsi, %r12
	xorq	%r12, %r15
	orq	%r15, %r12
	xorq	%rdi, %rbx
	xorq	%rbx, %rax
	orq	%rax, %rbx
	xorq	%rcx, %r13
	xorq	%r13, %r14
	orq	%r14, %r13
	xorq	%rsi, %r12
	xorq	%r12, %r15
	orq	%r15, %r12
	xorq	%rdi, %rbx
	xorq	%rbx, %rax
	orq	%rax, %rbx
	xorq	%rcx, %r13
	xorq	%r13, %r14
	orq	%r14, %r13
	xorq	%rsi, %r12
	xorq	%r12, %r15
	orq	%r15, %r12
	xorq	%rdi, %rbx
	xorq	%rbx, %rax
	orq	%rax, %rbx
	xorq	%rcx, %r13
	xorq	%r13, %r14
	orq	%r14, %r13
	xorq	%rsi, %r12
	xorq	%r12, %r15
	orq	%r15, %r12
	xorq	%rdi, %rbx
	xorq	%rbx, %rax
	orq	%rax, %rbx
	xorq	%rcx, %r13
	xorq	%r13, %r14
	orq	%r14, %r13
	xorq	%rsi, %r12
	xorq	%r12, %r15
	orq	%r15, %r12
	xorq	%rdi, %rbx
	xorq	%rbx, %rax
	orq	%rax, %rbx
	xorq	%rcx, %r13
	xorq	%r13, %r14
	orq	%r14, %r13
	xorq	%rsi, %r12
	xorq	%r12, %r15
	orq	%r15, %r12
	xorq	%rdi, %rbx
	xorq	%rbx, %rax
	orq	%rax, %rbx
	xorq	%rcx, %r13
	xorq	%r13, %r14
	orq	%r14, %r13
	xorq	%rsi, %r12
	xorq	%r12, %r15
	orq	%r15, %r12
	xorq	%rdi, %rbx
	xorq	%rbx, %rax
	orq	%rax, %rbx
	xorq	%rcx, %r13
	xorq	%r13, %r14
	orq	%r14, %r13
	xorq	%rsi, %r12
	xorq	%r12, %r15
	orq	%r15, %r12
	xorq	%rdi, %rbx
	xorq	%rbx, %rax
	orq	%rax, %rbx
	xorq	%rcx, %r13
	xorq	%r13, %r14
	orq	%r14, %r13
	xorq	%rsi, %r12
	xorq	%r12, %r15
	orq	%r15, %r12
	xorq	%rdi, %rbx
	xorq	%rbx, %rax
	movq	%rax, %r9
	orq	%rax, %rbx
.L265:
	movq	%r8, %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, %r8
	testq	%rax, %rax
	jne	.L266
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE91:
	.size	int64_bit_2, .-int64_bit_2
	.globl	int64_bit_3
	.type	int64_bit_3, @function
int64_bit_3:
.LFB92:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r10
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$32, %rax
	orq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	addq	$1, %rax
	movq	%rax, %r15
	movq	%r15, %rax
	salq	$32, %rax
	orq	%rax, %r15
	leaq	0(,%r15,4), %rax
	subq	$1, %rax
	movq	%rax, %r11
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %r13
	movq	%r13, %rax
	salq	$32, %rax
	orq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$32, %rax
	orq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$1, %rax
	movq	%rax, %rdi
	movq	%rdi, %rax
	salq	$32, %rax
	orq	%rax, %rdi
	movq	%rdi, %rcx
	leaq	0(,%rdi,4), %rax
	subq	$1, %rax
	movq	%rax, %r8
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$2, %rax
	movq	%rax, %rdx
	movq	%rdx, %rax
	salq	$32, %rax
	orq	%rax, %rdx
	movq	%rdx, %rsi
	leaq	0(,%rdx,4), %rax
	subq	$1, %rax
	movq	%rax, %r9
	jmp	.L268
.L269:
	movq	%r11, %rdi
	subq	$1, %rdi
	movq	%rdi, %r11
	subq	$1, -96(%rbp)
	movq	-96(%rbp), %rdx
	movq	%rdx, -96(%rbp)
	subq	$1, %r8
	subq	$1, %r9
	xorq	%rdi, %r14
	xorq	%r14, %r15
	orq	%r15, %r14
	xorq	%rdx, %r13
	xorq	%r13, -88(%rbp)
	movq	-88(%rbp), %rax
	orq	%rax, %r13
	xorq	%r8, %r12
	xorq	%r12, %rcx
	orq	%rcx, %r12
	xorq	%r9, %rbx
	xorq	%rbx, %rsi
	orq	%rsi, %rbx
	xorq	%rdi, %r14
	xorq	%r14, %r15
	orq	%r15, %r14
	xorq	%rdx, %r13
	xorq	%r13, %rax
	orq	%rax, %r13
	xorq	%r8, %r12
	xorq	%r12, %rcx
	orq	%rcx, %r12
	xorq	%r9, %rbx
	xorq	%rbx, %rsi
	orq	%rsi, %rbx
	xorq	%rdi, %r14
	xorq	%r14, %r15
	orq	%r15, %r14
	xorq	%rdx, %r13
	xorq	%r13, %rax
	orq	%rax, %r13
	xorq	%r8, %r12
	xorq	%r12, %rcx
	orq	%rcx, %r12
	xorq	%r9, %rbx
	xorq	%rbx, %rsi
	orq	%rsi, %rbx
	xorq	%rdi, %r14
	xorq	%r14, %r15
	orq	%r15, %r14
	xorq	%rdx, %r13
	xorq	%r13, %rax
	orq	%rax, %r13
	xorq	%r8, %r12
	xorq	%r12, %rcx
	orq	%rcx, %r12
	xorq	%r9, %rbx
	xorq	%rbx, %rsi
	orq	%rsi, %rbx
	xorq	%rdi, %r14
	xorq	%r14, %r15
	orq	%r15, %r14
	xorq	%rdx, %r13
	xorq	%r13, %rax
	orq	%rax, %r13
	xorq	%r8, %r12
	xorq	%r12, %rcx
	orq	%rcx, %r12
	xorq	%r9, %rbx
	xorq	%rbx, %rsi
	orq	%rsi, %rbx
	xorq	%rdi, %r14
	xorq	%r14, %r15
	orq	%r15, %r14
	xorq	%rdx, %r13
	xorq	%r13, %rax
	orq	%rax, %r13
	xorq	%r8, %r12
	xorq	%r12, %rcx
	orq	%rcx, %r12
	xorq	%r9, %rbx
	xorq	%rbx, %rsi
	orq	%rsi, %rbx
	xorq	%rdi, %r14
	xorq	%r14, %r15
	orq	%r15, %r14
	xorq	%rdx, %r13
	xorq	%r13, %rax
	orq	%rax, %r13
	xorq	%r8, %r12
	xorq	%r12, %rcx
	orq	%rcx, %r12
	xorq	%r9, %rbx
	xorq	%rbx, %rsi
	orq	%rsi, %rbx
	xorq	%rdi, %r14
	xorq	%r14, %r15
	orq	%r15, %r14
	xorq	%rdx, %r13
	xorq	%r13, %rax
	orq	%rax, %r13
	xorq	%r8, %r12
	xorq	%r12, %rcx
	orq	%rcx, %r12
	xorq	%r9, %rbx
	xorq	%rbx, %rsi
	orq	%rsi, %rbx
	xorq	%rdi, %r14
	xorq	%r14, %r15
	orq	%r15, %r14
	xorq	%rdx, %r13
	xorq	%r13, %rax
	orq	%rax, %r13
	xorq	%r8, %r12
	xorq	%r12, %rcx
	orq	%rcx, %r12
	xorq	%r9, %rbx
	xorq	%rbx, %rsi
	orq	%rsi, %rbx
	xorq	%rdi, %r14
	xorq	%r14, %r15
	orq	%r15, %r14
	xorq	%rdx, %r13
	xorq	%r13, %rax
	movq	%rax, -88(%rbp)
	orq	%rax, %r13
	xorq	%r8, %r12
	xorq	%r12, %rcx
	orq	%rcx, %r12
	xorq	%r9, %rbx
	xorq	%rbx, %rsi
	orq	%rsi, %rbx
.L268:
	movq	%r10, %rax
	leaq	-1(%rax), %rdi
	movq	%rdi, %r10
	testq	%rax, %rax
	jne	.L269
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE92:
	.size	int64_bit_3, .-int64_bit_3
	.globl	int64_bit_4
	.type	int64_bit_4, @function
int64_bit_4:
.LFB93:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -136(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	addq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, %r8
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, %r9
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$1, %rax
	movq	%rax, %r15
	movq	%r15, %rax
	salq	$32, %rax
	orq	%rax, %r15
	leaq	0(,%r15,4), %rax
	subq	$1, %rax
	movq	%rax, %r10
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$32, %rax
	orq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$2, %rax
	movq	%rax, %r13
	movq	%r13, %rax
	salq	$32, %rax
	orq	%rax, %r13
	leaq	0(,%r13,4), %rax
	subq	$1, %rax
	movq	%rax, %r11
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$32, %rax
	orq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$3, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -88(%rbp)
	jmp	.L271
.L272:
	subq	$1, %r8
	subq	$1, %r9
	subq	$1, %r10
	subq	$1, %r11
	subq	$1, -88(%rbp)
	xorq	%r8, -96(%rbp)
	movq	-96(%rbp), %rax
	xorq	%rax, -120(%rbp)
	movq	-120(%rbp), %rdi
	orq	%rdi, %rax
	xorq	%r9, -104(%rbp)
	movq	-104(%rbp), %rcx
	xorq	%rcx, -128(%rbp)
	movq	-128(%rbp), %rdx
	orq	%rdx, %rcx
	xorq	%r10, -112(%rbp)
	movq	-112(%rbp), %rsi
	xorq	%rsi, %r15
	orq	%r15, %rsi
	xorq	%r11, %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-88(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	xorq	%r8, %rax
	xorq	%rax, %rdi
	orq	%rdi, %rax
	xorq	%r9, %rcx
	xorq	%rcx, %rdx
	orq	%rdx, %rcx
	xorq	%r10, %rsi
	xorq	%rsi, %r15
	orq	%r15, %rsi
	xorq	%r11, %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-88(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	xorq	%r8, %rax
	xorq	%rax, %rdi
	orq	%rdi, %rax
	xorq	%r9, %rcx
	xorq	%rcx, %rdx
	orq	%rdx, %rcx
	xorq	%r10, %rsi
	xorq	%rsi, %r15
	orq	%r15, %rsi
	xorq	%r11, %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-88(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	xorq	%r8, %rax
	xorq	%rax, %rdi
	orq	%rdi, %rax
	xorq	%r9, %rcx
	xorq	%rcx, %rdx
	orq	%rdx, %rcx
	xorq	%r10, %rsi
	xorq	%rsi, %r15
	orq	%r15, %rsi
	xorq	%r11, %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-88(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	xorq	%r8, %rax
	xorq	%rax, %rdi
	orq	%rdi, %rax
	xorq	%r9, %rcx
	xorq	%rcx, %rdx
	orq	%rdx, %rcx
	xorq	%r10, %rsi
	xorq	%rsi, %r15
	orq	%r15, %rsi
	xorq	%r11, %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-88(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	xorq	%r8, %rax
	xorq	%rax, %rdi
	orq	%rdi, %rax
	xorq	%r9, %rcx
	xorq	%rcx, %rdx
	orq	%rdx, %rcx
	xorq	%r10, %rsi
	xorq	%rsi, %r15
	orq	%r15, %rsi
	xorq	%r11, %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-88(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	xorq	%r8, %rax
	xorq	%rax, %rdi
	orq	%rdi, %rax
	xorq	%r9, %rcx
	xorq	%rcx, %rdx
	orq	%rdx, %rcx
	xorq	%r10, %rsi
	xorq	%rsi, %r15
	orq	%r15, %rsi
	xorq	%r11, %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-88(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	xorq	%r8, %rax
	xorq	%rax, %rdi
	orq	%rdi, %rax
	xorq	%r9, %rcx
	xorq	%rcx, %rdx
	orq	%rdx, %rcx
	xorq	%r10, %rsi
	xorq	%rsi, %r15
	orq	%r15, %rsi
	xorq	%r11, %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-88(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	xorq	%r8, %rax
	xorq	%rax, %rdi
	orq	%rdi, %rax
	xorq	%r9, %rcx
	xorq	%rcx, %rdx
	orq	%rdx, %rcx
	xorq	%r10, %rsi
	xorq	%rsi, %r15
	orq	%r15, %rsi
	xorq	%r11, %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-88(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	xorq	%r8, %rax
	xorq	%rax, %rdi
	movq	%rdi, -120(%rbp)
	orq	%rdi, %rax
	movq	%rax, -96(%rbp)
	xorq	%r9, %rcx
	xorq	%rcx, %rdx
	movq	%rdx, -128(%rbp)
	orq	%rdx, %rcx
	movq	%rcx, -104(%rbp)
	xorq	%r10, %rsi
	xorq	%rsi, %r15
	orq	%r15, %rsi
	movq	%rsi, -112(%rbp)
	xorq	%r11, %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-88(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
.L271:
	movq	-136(%rbp), %rax
	leaq	-1(%rax), %rdi
	movq	%rdi, -136(%rbp)
	testq	%rax, %rax
	jne	.L272
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE93:
	.size	int64_bit_4, .-int64_bit_4
	.globl	int64_bit_5
	.type	int64_bit_5, @function
int64_bit_5:
.LFB94:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -168(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	addq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -160(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, %r10
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, %r8
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, %r11
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, %r9
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$2, %rax
	movq	%rax, %r15
	movq	%r15, %rax
	salq	$32, %rax
	orq	%rax, %r15
	leaq	0(,%r15,4), %rax
	subq	$1, %rax
	movq	%rax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$32, %rax
	orq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$3, %rax
	movq	%rax, %r13
	movq	%r13, %rax
	salq	$32, %rax
	orq	%rax, %r13
	leaq	0(,%r13,4), %rax
	subq	$1, %rax
	movq	%rax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$32, %rax
	orq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$4, %rax
	movq	%rax, %rdx
	movq	%rdx, %rax
	salq	$32, %rax
	orq	%rax, %rdx
	movq	%rdx, -88(%rbp)
	leaq	0(,%rdx,4), %rax
	subq	$1, %rax
	movq	%rax, -120(%rbp)
	jmp	.L274
.L275:
	subq	$1, %r10
	subq	$1, %r11
	subq	$1, -96(%rbp)
	movq	-96(%rbp), %rdi
	subq	$1, -104(%rbp)
	movq	-104(%rbp), %rbx
	subq	$1, -112(%rbp)
	subq	$1, -120(%rbp)
	xorq	%r10, -128(%rbp)
	movq	-128(%rbp), %rax
	xorq	%rax, -160(%rbp)
	movq	-160(%rbp), %rdx
	orq	%rdx, %rax
	xorq	%r11, -136(%rbp)
	movq	-136(%rbp), %rcx
	xorq	%rcx, %r8
	orq	%r8, %rcx
	movq	%rdi, -96(%rbp)
	xorq	%rdi, -144(%rbp)
	movq	-144(%rbp), %rsi
	xorq	%rsi, %r9
	orq	%r9, %rsi
	movq	%rbx, -104(%rbp)
	xorq	%rbx, -152(%rbp)
	movq	-152(%rbp), %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	xorq	-112(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-120(%rbp), %r12
	xorq	%r12, -88(%rbp)
	movq	-88(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	orq	-88(%rbp), %r12
	xorq	%r10, %rax
	xorq	%rax, %rdx
	orq	%rdx, %rax
	xorq	%r11, %rcx
	xorq	%rcx, %r8
	orq	%r8, %rcx
	movq	-96(%rbp), %rbx
	xorq	%rbx, %rsi
	xorq	%rsi, %r9
	orq	%r9, %rsi
	movq	-104(%rbp), %rbx
	xorq	%rbx, %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	xorq	-112(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-120(%rbp), %r12
	xorq	%r12, -88(%rbp)
	movq	-88(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	orq	-88(%rbp), %r12
	xorq	%r10, %rax
	xorq	%rax, %rdx
	orq	%rdx, %rax
	xorq	%r11, %rcx
	xorq	%rcx, %r8
	orq	%r8, %rcx
	movq	-96(%rbp), %rbx
	xorq	%rbx, %rsi
	xorq	%rsi, %r9
	orq	%r9, %rsi
	movq	-104(%rbp), %rbx
	xorq	%rbx, %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	xorq	-112(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-120(%rbp), %r12
	xorq	%r12, -88(%rbp)
	movq	-88(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	orq	-88(%rbp), %r12
	xorq	%r10, %rax
	xorq	%rax, %rdx
	orq	%rdx, %rax
	xorq	%r11, %rcx
	xorq	%rcx, %r8
	orq	%r8, %rcx
	movq	-96(%rbp), %rbx
	xorq	%rbx, %rsi
	xorq	%rsi, %r9
	orq	%r9, %rsi
	movq	-104(%rbp), %rbx
	xorq	%rbx, %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	xorq	-112(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-120(%rbp), %r12
	xorq	%r12, -88(%rbp)
	movq	-88(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	orq	-88(%rbp), %r12
	xorq	%r10, %rax
	xorq	%rax, %rdx
	orq	%rdx, %rax
	xorq	%r11, %rcx
	xorq	%rcx, %r8
	orq	%r8, %rcx
	movq	-96(%rbp), %rbx
	xorq	%rbx, %rsi
	xorq	%rsi, %r9
	orq	%r9, %rsi
	movq	-104(%rbp), %rbx
	xorq	%rbx, %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	xorq	-112(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-120(%rbp), %r12
	xorq	%r12, -88(%rbp)
	movq	-88(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	orq	-88(%rbp), %r12
	xorq	%r10, %rax
	xorq	%rax, %rdx
	orq	%rdx, %rax
	xorq	%r11, %rcx
	xorq	%rcx, %r8
	orq	%r8, %rcx
	movq	-96(%rbp), %rbx
	xorq	%rbx, %rsi
	xorq	%rsi, %r9
	orq	%r9, %rsi
	movq	-104(%rbp), %rbx
	xorq	%rbx, %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	xorq	-112(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-120(%rbp), %r12
	xorq	%r12, -88(%rbp)
	movq	-88(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	orq	-88(%rbp), %r12
	xorq	%r10, %rax
	xorq	%rax, %rdx
	orq	%rdx, %rax
	xorq	%r11, %rcx
	xorq	%rcx, %r8
	orq	%r8, %rcx
	movq	-96(%rbp), %rbx
	xorq	%rbx, %rsi
	xorq	%rsi, %r9
	orq	%r9, %rsi
	movq	-104(%rbp), %rbx
	xorq	%rbx, %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	xorq	-112(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-120(%rbp), %r12
	xorq	%r12, -88(%rbp)
	movq	-88(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	orq	-88(%rbp), %r12
	xorq	%r10, %rax
	xorq	%rax, %rdx
	orq	%rdx, %rax
	xorq	%r11, %rcx
	xorq	%rcx, %r8
	orq	%r8, %rcx
	movq	-96(%rbp), %rbx
	xorq	%rbx, %rsi
	xorq	%rsi, %r9
	orq	%r9, %rsi
	movq	-104(%rbp), %rbx
	xorq	%rbx, %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	xorq	-112(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-120(%rbp), %r12
	xorq	%r12, -88(%rbp)
	movq	-88(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	orq	-88(%rbp), %r12
	xorq	%r10, %rax
	xorq	%rax, %rdx
	orq	%rdx, %rax
	xorq	%r11, %rcx
	xorq	%rcx, %r8
	orq	%r8, %rcx
	movq	-96(%rbp), %rbx
	xorq	%rbx, %rsi
	xorq	%rsi, %r9
	orq	%r9, %rsi
	movq	-104(%rbp), %rbx
	xorq	%rbx, %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	xorq	-112(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-120(%rbp), %r12
	xorq	%r12, -88(%rbp)
	movq	-88(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	orq	-88(%rbp), %r12
	xorq	%r10, %rax
	xorq	%rax, %rdx
	movq	%rdx, -160(%rbp)
	orq	%rdx, %rax
	movq	%rax, -128(%rbp)
	xorq	%r11, %rcx
	xorq	%rcx, %r8
	orq	%r8, %rcx
	movq	%rcx, -136(%rbp)
	movq	-96(%rbp), %rbx
	xorq	%rbx, %rsi
	xorq	%rsi, %r9
	orq	%r9, %rsi
	movq	%rsi, -144(%rbp)
	movq	-104(%rbp), %rbx
	xorq	%rbx, %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	movq	%rdi, -152(%rbp)
	xorq	-112(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-120(%rbp), %r12
	xorq	%r12, -88(%rbp)
	movq	-88(%rbp), %rax
	movq	%rax, -88(%rbp)
	orq	%rax, %r12
.L274:
	movq	-168(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -168(%rbp)
	testq	%rax, %rax
	jne	.L275
	movl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE94:
	.size	int64_bit_5, .-int64_bit_5
	.globl	int64_bit_6
	.type	int64_bit_6, @function
int64_bit_6:
.LFB95:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -200(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	addq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, %r8
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, %r9
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -192(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$2, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -176(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$3, %rax
	movq	%rax, %r15
	movq	%r15, %rax
	salq	$32, %rax
	orq	%rax, %r15
	leaq	0(,%r15,4), %rax
	subq	$1, %rax
	movq	%rax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$32, %rax
	orq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$4, %rax
	movq	%rax, %r13
	movq	%r13, %rax
	salq	$32, %rax
	orq	%rax, %r13
	leaq	0(,%r13,4), %rax
	subq	$1, %rax
	movq	%rax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$32, %rax
	orq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$5, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -144(%rbp)
	jmp	.L277
.L278:
	subq	$1, -96(%rbp)
	movq	-96(%rbp), %rdx
	subq	$1, -104(%rbp)
	movq	-104(%rbp), %rsi
	subq	$1, -112(%rbp)
	movq	-112(%rbp), %r10
	subq	$1, -120(%rbp)
	movq	-120(%rbp), %r11
	subq	$1, -128(%rbp)
	subq	$1, -136(%rbp)
	subq	$1, -144(%rbp)
	movq	%rdx, -96(%rbp)
	xorq	%rdx, -152(%rbp)
	movq	-152(%rbp), %rax
	movq	%r8, %rcx
	xorq	%rax, %rcx
	movq	%rcx, %r8
	orq	%r8, %rax
	movq	%rsi, -104(%rbp)
	xorq	%rsi, -160(%rbp)
	movq	-160(%rbp), %rcx
	xorq	%rcx, %r9
	orq	%r9, %rcx
	movq	%r10, -112(%rbp)
	xorq	%r10, -184(%rbp)
	movq	-184(%rbp), %rsi
	xorq	%rsi, -192(%rbp)
	movq	-192(%rbp), %r10
	orq	%r10, %rsi
	movq	%r11, -120(%rbp)
	xorq	%r11, -88(%rbp)
	movq	-88(%rbp), %rdi
	xorq	%rdi, -176(%rbp)
	movq	-176(%rbp), %r11
	orq	%r11, %rdi
	movq	%rdi, -88(%rbp)
	movq	-128(%rbp), %rdi
	xorq	%rdi, -168(%rbp)
	movq	-168(%rbp), %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	xorq	-136(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-144(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-96(%rbp), %rdi
	xorq	%rdi, %rax
	xorq	%rax, %r8
	orq	%r8, %rax
	movq	-104(%rbp), %rdi
	xorq	%rdi, %rcx
	xorq	%rcx, %r9
	orq	%r9, %rcx
	movq	-112(%rbp), %rdi
	xorq	%rdi, %rsi
	xorq	%rsi, %r10
	orq	%r10, %rsi
	movq	%rsi, -184(%rbp)
	movq	-120(%rbp), %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %rdi
	xorq	%rdi, %r11
	orq	%r11, %rdi
	movq	-128(%rbp), %rsi
	xorq	%rsi, %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	xorq	-136(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-144(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-96(%rbp), %rsi
	xorq	%rsi, %rax
	xorq	%rax, %r8
	orq	%r8, %rax
	movq	%rax, -152(%rbp)
	movq	-104(%rbp), %rsi
	xorq	%rsi, %rcx
	xorq	%rcx, %r9
	orq	%r9, %rcx
	movq	-112(%rbp), %rax
	xorq	%rax, -184(%rbp)
	movq	-184(%rbp), %rsi
	xorq	%rsi, %r10
	orq	%r10, %rsi
	movq	-120(%rbp), %rax
	xorq	%rax, %rdi
	xorq	%rdi, %r11
	orq	%r11, %rdi
	movq	%rdi, -88(%rbp)
	movq	-128(%rbp), %rdi
	xorq	%rdi, %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	xorq	-136(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-144(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-96(%rbp), %rdi
	xorq	%rdi, -152(%rbp)
	movq	-152(%rbp), %rax
	xorq	%rax, %r8
	orq	%r8, %rax
	movq	-104(%rbp), %rdi
	xorq	%rdi, %rcx
	xorq	%rcx, %r9
	orq	%r9, %rcx
	movq	%rcx, -160(%rbp)
	movq	-112(%rbp), %rdi
	xorq	%rdi, %rsi
	xorq	%rsi, %r10
	orq	%r10, %rsi
	movq	-120(%rbp), %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %rdi
	xorq	%rdi, %r11
	orq	%r11, %rdi
	movq	-128(%rbp), %rcx
	xorq	%rcx, %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	movq	%rdx, -168(%rbp)
	xorq	-136(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-144(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-96(%rbp), %rdx
	xorq	%rdx, %rax
	xorq	%rax, %r8
	orq	%r8, %rax
	movq	-104(%rbp), %rdx
	xorq	%rdx, -160(%rbp)
	movq	-160(%rbp), %rcx
	xorq	%rcx, %r9
	orq	%r9, %rcx
	movq	-112(%rbp), %rdx
	xorq	%rdx, %rsi
	xorq	%rsi, %r10
	orq	%r10, %rsi
	movq	-120(%rbp), %rdx
	xorq	%rdx, %rdi
	xorq	%rdi, %r11
	movq	%r11, -176(%rbp)
	orq	%r11, %rdi
	movq	-128(%rbp), %r11
	xorq	%r11, -168(%rbp)
	movq	-168(%rbp), %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	xorq	-136(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-144(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-96(%rbp), %r11
	xorq	%r11, %rax
	xorq	%rax, %r8
	orq	%r8, %rax
	movq	-104(%rbp), %r11
	xorq	%r11, %rcx
	xorq	%rcx, %r9
	orq	%r9, %rcx
	movq	-112(%rbp), %r11
	xorq	%r11, %rsi
	xorq	%rsi, %r10
	orq	%r10, %rsi
	movq	-120(%rbp), %r11
	xorq	%r11, %rdi
	xorq	%rdi, -176(%rbp)
	movq	-176(%rbp), %r11
	orq	%r11, %rdi
	movq	%rdi, -88(%rbp)
	movq	-128(%rbp), %rdi
	xorq	%rdi, %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	xorq	-136(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-144(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-96(%rbp), %rdi
	xorq	%rdi, %rax
	xorq	%rax, %r8
	orq	%r8, %rax
	movq	%rax, -152(%rbp)
	movq	-104(%rbp), %rdi
	xorq	%rdi, %rcx
	xorq	%rcx, %r9
	orq	%r9, %rcx
	movq	-112(%rbp), %rdi
	xorq	%rdi, %rsi
	xorq	%rsi, %r10
	orq	%r10, %rsi
	movq	-120(%rbp), %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %rdi
	xorq	%rdi, %r11
	orq	%r11, %rdi
	movq	-128(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	movq	%rdx, -168(%rbp)
	xorq	-136(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-144(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-96(%rbp), %rdx
	xorq	%rdx, -152(%rbp)
	movq	-152(%rbp), %rax
	xorq	%rax, %r8
	orq	%r8, %rax
	movq	-104(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, %r9
	orq	%r9, %rcx
	movq	-112(%rbp), %rdx
	xorq	%rdx, %rsi
	xorq	%rsi, %r10
	orq	%r10, %rsi
	movq	-120(%rbp), %rdx
	xorq	%rdx, %rdi
	xorq	%rdi, %r11
	movq	%r11, -176(%rbp)
	orq	%r11, %rdi
	movq	-128(%rbp), %r11
	xorq	%r11, -168(%rbp)
	movq	-168(%rbp), %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	xorq	-136(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-144(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-96(%rbp), %r11
	xorq	%r11, %rax
	xorq	%rax, %r8
	orq	%r8, %rax
	movq	-104(%rbp), %r11
	xorq	%r11, %rcx
	xorq	%rcx, %r9
	orq	%r9, %rcx
	movq	%rcx, -160(%rbp)
	movq	-112(%rbp), %rcx
	xorq	%rcx, %rsi
	xorq	%rsi, %r10
	orq	%r10, %rsi
	movq	-120(%rbp), %r11
	xorq	%r11, %rdi
	xorq	%rdi, -176(%rbp)
	movq	-176(%rbp), %r11
	orq	%r11, %rdi
	movq	-128(%rbp), %rcx
	xorq	%rcx, %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	xorq	-136(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-144(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-96(%rbp), %rcx
	xorq	%rcx, %rax
	xorq	%rax, %r8
	orq	%r8, %rax
	movq	%rax, -152(%rbp)
	movq	-104(%rbp), %rax
	xorq	%rax, -160(%rbp)
	movq	-160(%rbp), %rcx
	xorq	%rcx, %r9
	orq	%r9, %rcx
	movq	%rcx, -160(%rbp)
	movq	-112(%rbp), %rcx
	xorq	%rcx, %rsi
	xorq	%rsi, %r10
	movq	%r10, -192(%rbp)
	orq	%r10, %rsi
	movq	%rsi, -184(%rbp)
	movq	-120(%rbp), %rax
	xorq	%rax, %rdi
	xorq	%rdi, %r11
	movq	%r11, -176(%rbp)
	orq	%r11, %rdi
	movq	%rdi, -88(%rbp)
	movq	-128(%rbp), %rcx
	xorq	%rcx, %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	movq	%rdx, -168(%rbp)
	xorq	-136(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-144(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
.L277:
	movq	-200(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -200(%rbp)
	testq	%rax, %rax
	jne	.L278
	movl	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$168, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE95:
	.size	int64_bit_6, .-int64_bit_6
	.globl	int64_bit_7
	.type	int64_bit_7, @function
int64_bit_7:
.LFB96:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$184, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -224(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	addq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -200(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, %r10
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, %r11
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$2, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$3, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$4, %rax
	movq	%rax, %r15
	movq	%r15, %rax
	salq	$32, %rax
	orq	%rax, %r15
	leaq	0(,%r15,4), %rax
	subq	$1, %rax
	movq	%rax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$32, %rax
	orq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$5, %rax
	movq	%rax, %r13
	movq	%r13, %rax
	salq	$32, %rax
	orq	%rax, %r13
	leaq	0(,%r13,4), %rax
	subq	$1, %rax
	movq	%rax, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$32, %rax
	orq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$6, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -176(%rbp)
	jmp	.L280
.L281:
	subq	$1, -120(%rbp)
	movq	-120(%rbp), %rdi
	subq	$1, -128(%rbp)
	movq	-128(%rbp), %rsi
	subq	$1, -136(%rbp)
	movq	-136(%rbp), %rdx
	subq	$1, -144(%rbp)
	movq	-144(%rbp), %r8
	subq	$1, -152(%rbp)
	subq	$1, -160(%rbp)
	subq	$1, -168(%rbp)
	subq	$1, -176(%rbp)
	movq	%rdi, -120(%rbp)
	xorq	%rdi, -184(%rbp)
	movq	-184(%rbp), %rax
	xorq	%rax, -200(%rbp)
	movq	-200(%rbp), %r9
	orq	%r9, %rax
	movq	%rsi, -128(%rbp)
	xorq	%rsi, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%r10, %rdi
	xorq	%rcx, %rdi
	movq	%rdi, %r10
	orq	%r10, %rcx
	movq	%rcx, -104(%rbp)
	movq	%rdx, -136(%rbp)
	xorq	%rdx, -112(%rbp)
	movq	-112(%rbp), %rsi
	movq	%r11, %rdi
	xorq	%rsi, %rdi
	movq	%rdi, %r11
	orq	%r11, %rsi
	movq	%r8, -144(%rbp)
	xorq	%r8, -208(%rbp)
	movq	-208(%rbp), %rdi
	xorq	%rdi, -88(%rbp)
	movq	-88(%rbp), %rdx
	movq	%rdx, -88(%rbp)
	orq	%rdx, %rdi
	movq	-152(%rbp), %rcx
	xorq	%rcx, -192(%rbp)
	movq	-192(%rbp), %rdx
	xorq	%rdx, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	orq	%r8, %rdx
	movq	-160(%rbp), %rcx
	xorq	%rcx, -216(%rbp)
	movq	-216(%rbp), %r8
	xorq	%r8, %r15
	orq	%r15, %r8
	xorq	-168(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-176(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-120(%rbp), %rcx
	xorq	%rcx, %rax
	xorq	%rax, %r9
	orq	%r9, %rax
	movq	%rax, -184(%rbp)
	movq	-128(%rbp), %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %rcx
	xorq	%rcx, %r10
	orq	%r10, %rcx
	movq	-136(%rbp), %rax
	xorq	%rax, %rsi
	xorq	%rsi, %r11
	orq	%r11, %rsi
	movq	%rsi, -112(%rbp)
	movq	-144(%rbp), %rsi
	xorq	%rsi, %rdi
	xorq	%rdi, -88(%rbp)
	movq	-88(%rbp), %rsi
	movq	%rsi, -88(%rbp)
	movq	%rsi, %rax
	orq	%rax, %rdi
	movq	-152(%rbp), %rsi
	xorq	%rsi, %rdx
	xorq	%rdx, -96(%rbp)
	movq	-96(%rbp), %rax
	movq	%rax, -96(%rbp)
	orq	%rax, %rdx
	movq	-160(%rbp), %rsi
	xorq	%rsi, %r8
	xorq	%r8, %r15
	orq	%r15, %r8
	xorq	-168(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-176(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-120(%rbp), %rsi
	xorq	%rsi, -184(%rbp)
	movq	-184(%rbp), %rax
	xorq	%rax, %r9
	orq	%r9, %rax
	movq	-128(%rbp), %rsi
	xorq	%rsi, %rcx
	xorq	%rcx, %r10
	orq	%r10, %rcx
	movq	%rcx, -104(%rbp)
	movq	-136(%rbp), %rcx
	xorq	%rcx, -112(%rbp)
	movq	-112(%rbp), %rsi
	xorq	%rsi, %r11
	orq	%r11, %rsi
	movq	-144(%rbp), %rcx
	xorq	%rcx, %rdi
	xorq	%rdi, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	orq	%rcx, %rdi
	movq	-152(%rbp), %rcx
	xorq	%rcx, %rdx
	xorq	%rdx, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	orq	%rcx, %rdx
	movq	%rdx, -192(%rbp)
	movq	-160(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, %r15
	orq	%r15, %r8
	xorq	-168(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-176(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-120(%rbp), %rcx
	xorq	%rcx, %rax
	xorq	%rax, %r9
	orq	%r9, %rax
	movq	-128(%rbp), %rdx
	xorq	%rdx, -104(%rbp)
	movq	-104(%rbp), %rcx
	xorq	%rcx, %r10
	orq	%r10, %rcx
	movq	-136(%rbp), %rdx
	xorq	%rdx, %rsi
	xorq	%rsi, %r11
	orq	%r11, %rsi
	movq	%rsi, -112(%rbp)
	movq	-144(%rbp), %rdx
	xorq	%rdx, %rdi
	xorq	%rdi, -88(%rbp)
	movq	-88(%rbp), %rdx
	movq	%rdx, -88(%rbp)
	movq	%rdx, %rsi
	orq	%rsi, %rdi
	movq	-152(%rbp), %rsi
	xorq	%rsi, -192(%rbp)
	movq	-192(%rbp), %rdx
	xorq	%rdx, -96(%rbp)
	movq	-96(%rbp), %rsi
	movq	%rsi, -96(%rbp)
	orq	%rsi, %rdx
	movq	-160(%rbp), %rsi
	xorq	%rsi, %r8
	xorq	%r8, %r15
	orq	%r15, %r8
	xorq	-168(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-176(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-120(%rbp), %rsi
	xorq	%rsi, %rax
	xorq	%rax, %r9
	orq	%r9, %rax
	movq	-128(%rbp), %rsi
	xorq	%rsi, %rcx
	xorq	%rcx, %r10
	orq	%r10, %rcx
	movq	%rcx, -104(%rbp)
	movq	-136(%rbp), %rcx
	xorq	%rcx, -112(%rbp)
	movq	-112(%rbp), %rsi
	xorq	%rsi, %r11
	orq	%r11, %rsi
	movq	-144(%rbp), %rcx
	xorq	%rcx, %rdi
	xorq	%rdi, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	orq	%rcx, %rdi
	movq	-152(%rbp), %rcx
	xorq	%rcx, %rdx
	xorq	%rdx, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	orq	%rcx, %rdx
	movq	-160(%rbp), %rcx
	xorq	%rcx, %r8
	xorq	%r8, %r15
	orq	%r15, %r8
	xorq	-168(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-176(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-120(%rbp), %rcx
	xorq	%rcx, %rax
	xorq	%rax, %r9
	orq	%r9, %rax
	movq	%rax, -184(%rbp)
	movq	-128(%rbp), %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %rcx
	xorq	%rcx, %r10
	orq	%r10, %rcx
	movq	-136(%rbp), %rax
	xorq	%rax, %rsi
	xorq	%rsi, %r11
	orq	%r11, %rsi
	movq	-144(%rbp), %rax
	xorq	%rax, %rdi
	xorq	%rdi, -88(%rbp)
	movq	-88(%rbp), %rax
	movq	%rax, -88(%rbp)
	orq	%rax, %rdi
	movq	-152(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -96(%rbp)
	movq	-96(%rbp), %rax
	movq	%rax, -96(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -192(%rbp)
	movq	-160(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, %r15
	orq	%r15, %r8
	xorq	-168(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-176(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-120(%rbp), %rdx
	xorq	%rdx, -184(%rbp)
	movq	-184(%rbp), %rax
	xorq	%rax, %r9
	orq	%r9, %rax
	movq	-128(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, %r10
	orq	%r10, %rcx
	movq	-136(%rbp), %rdx
	xorq	%rdx, %rsi
	xorq	%rsi, %r11
	orq	%r11, %rsi
	movq	%rsi, -112(%rbp)
	movq	-144(%rbp), %rsi
	xorq	%rsi, %rdi
	xorq	%rdi, -88(%rbp)
	movq	-88(%rbp), %rsi
	movq	%rsi, -88(%rbp)
	orq	%rsi, %rdi
	movq	-152(%rbp), %rsi
	xorq	%rsi, -192(%rbp)
	movq	-192(%rbp), %rdx
	xorq	%rdx, -96(%rbp)
	movq	-96(%rbp), %rsi
	movq	%rsi, -96(%rbp)
	orq	%rsi, %rdx
	movq	-160(%rbp), %rsi
	xorq	%rsi, %r8
	xorq	%r8, %r15
	orq	%r15, %r8
	xorq	-168(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-176(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-120(%rbp), %rsi
	xorq	%rsi, %rax
	xorq	%rax, %r9
	orq	%r9, %rax
	movq	-128(%rbp), %rsi
	xorq	%rsi, %rcx
	xorq	%rcx, %r10
	orq	%r10, %rcx
	movq	%rcx, -104(%rbp)
	movq	-136(%rbp), %rcx
	xorq	%rcx, -112(%rbp)
	movq	-112(%rbp), %rsi
	xorq	%rsi, %r11
	orq	%r11, %rsi
	movq	-144(%rbp), %rcx
	xorq	%rcx, %rdi
	xorq	%rdi, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	orq	%rcx, %rdi
	movq	-152(%rbp), %rcx
	xorq	%rcx, %rdx
	xorq	%rdx, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	orq	%rcx, %rdx
	movq	-160(%rbp), %rcx
	xorq	%rcx, %r8
	xorq	%r8, %r15
	orq	%r15, %r8
	xorq	-168(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-176(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-120(%rbp), %rcx
	xorq	%rcx, %rax
	xorq	%rax, %r9
	movq	%r9, -200(%rbp)
	orq	%r9, %rax
	movq	-128(%rbp), %r9
	xorq	%r9, -104(%rbp)
	movq	-104(%rbp), %rcx
	xorq	%rcx, %r10
	orq	%r10, %rcx
	movq	-136(%rbp), %r9
	xorq	%r9, %rsi
	xorq	%rsi, %r11
	orq	%r11, %rsi
	movq	-144(%rbp), %r9
	xorq	%r9, %rdi
	xorq	%rdi, -88(%rbp)
	movq	-88(%rbp), %r9
	movq	%r9, -88(%rbp)
	orq	%r9, %rdi
	movq	-152(%rbp), %r9
	xorq	%r9, %rdx
	xorq	%rdx, -96(%rbp)
	movq	-96(%rbp), %r9
	movq	%r9, -96(%rbp)
	orq	%r9, %rdx
	movq	-160(%rbp), %r9
	xorq	%r9, %r8
	xorq	%r8, %r15
	orq	%r15, %r8
	xorq	-168(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-176(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-120(%rbp), %r9
	xorq	%r9, %rax
	xorq	%rax, -200(%rbp)
	movq	-200(%rbp), %r9
	movq	%r9, -200(%rbp)
	orq	%r9, %rax
	movq	%rax, -184(%rbp)
	movq	-128(%rbp), %rax
	xorq	%rax, %rcx
	xorq	%rcx, %r10
	orq	%r10, %rcx
	movq	%rcx, -104(%rbp)
	movq	-136(%rbp), %rcx
	xorq	%rcx, %rsi
	xorq	%rsi, %r11
	orq	%r11, %rsi
	movq	%rsi, -112(%rbp)
	movq	-144(%rbp), %rcx
	xorq	%rcx, %rdi
	xorq	%rdi, -88(%rbp)
	movq	-88(%rbp), %rax
	movq	%rax, -88(%rbp)
	orq	%rax, %rdi
	movq	%rdi, -208(%rbp)
	movq	-152(%rbp), %rcx
	xorq	%rcx, %rdx
	xorq	%rdx, -96(%rbp)
	movq	-96(%rbp), %r9
	movq	%r9, -96(%rbp)
	orq	%r9, %rdx
	movq	%rdx, -192(%rbp)
	movq	-160(%rbp), %r9
	xorq	%r9, %r8
	xorq	%r8, %r15
	orq	%r15, %r8
	movq	%r8, -216(%rbp)
	xorq	-168(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-176(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
.L280:
	movq	-224(%rbp), %rax
	leaq	-1(%rax), %rdi
	movq	%rdi, -224(%rbp)
	testq	%rax, %rax
	jne	.L281
	movl	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$184, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE96:
	.size	int64_bit_7, .-int64_bit_7
	.globl	int64_bit_8
	.type	int64_bit_8, @function
int64_bit_8:
.LFB97:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$232, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -264(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	addq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -224(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -256(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -240(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$2, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$3, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -248(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$4, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -112(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$5, %rax
	movq	%rax, %r15
	movq	%r15, %rax
	salq	$32, %rax
	orq	%rax, %r15
	leaq	0(,%r15,4), %rax
	subq	$1, %rax
	movq	%rax, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$32, %rax
	orq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$6, %rax
	movq	%rax, %r13
	movq	%r13, %rax
	salq	$32, %rax
	orq	%rax, %r13
	leaq	0(,%r13,4), %rax
	subq	$1, %rax
	movq	%rax, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$32, %rax
	orq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$7, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -208(%rbp)
	jmp	.L283
.L284:
	subq	$1, -144(%rbp)
	movq	-144(%rbp), %rsi
	subq	$1, -152(%rbp)
	movq	-152(%rbp), %rdi
	subq	$1, -160(%rbp)
	movq	-160(%rbp), %rdx
	subq	$1, -168(%rbp)
	movq	-168(%rbp), %r8
	subq	$1, -176(%rbp)
	movq	-176(%rbp), %r9
	subq	$1, -184(%rbp)
	subq	$1, -192(%rbp)
	subq	$1, -200(%rbp)
	subq	$1, -208(%rbp)
	movq	%rsi, -144(%rbp)
	xorq	%rsi, -120(%rbp)
	movq	-120(%rbp), %rax
	xorq	%rax, -224(%rbp)
	movq	-224(%rbp), %r10
	orq	%r10, %rax
	movq	%rdi, -152(%rbp)
	xorq	%rdi, -128(%rbp)
	movq	-128(%rbp), %rcx
	xorq	%rcx, -256(%rbp)
	movq	-256(%rbp), %r11
	orq	%r11, %rcx
	movq	%rcx, -128(%rbp)
	movq	%rdx, -160(%rbp)
	xorq	%rdx, -232(%rbp)
	movq	-232(%rbp), %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	orq	%rdi, %rsi
	movq	%r8, -168(%rbp)
	xorq	%r8, -240(%rbp)
	movq	-240(%rbp), %rdi
	xorq	%rdi, -96(%rbp)
	movq	-96(%rbp), %rdx
	movq	%rdx, -96(%rbp)
	orq	%rdx, %rdi
	movq	%r9, -176(%rbp)
	xorq	%r9, -136(%rbp)
	movq	-136(%rbp), %rdx
	xorq	%rdx, -104(%rbp)
	movq	-104(%rbp), %r9
	movq	%r9, -104(%rbp)
	orq	%r9, %rdx
	movq	-184(%rbp), %rcx
	xorq	%rcx, -248(%rbp)
	movq	-248(%rbp), %r8
	xorq	%r8, -112(%rbp)
	movq	-112(%rbp), %r9
	movq	%r9, -112(%rbp)
	orq	%r9, %r8
	movq	-192(%rbp), %rcx
	xorq	%rcx, -216(%rbp)
	movq	-216(%rbp), %r9
	xorq	%r9, %r15
	orq	%r15, %r9
	xorq	-200(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-208(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-144(%rbp), %rcx
	xorq	%rcx, %rax
	xorq	%rax, %r10
	orq	%r10, %rax
	movq	%rax, -120(%rbp)
	movq	-152(%rbp), %rax
	xorq	%rax, -128(%rbp)
	movq	-128(%rbp), %rcx
	xorq	%rcx, %r11
	orq	%r11, %rcx
	movq	-160(%rbp), %rax
	xorq	%rax, %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %rax
	movq	%rax, -88(%rbp)
	orq	%rax, %rsi
	movq	-168(%rbp), %rax
	xorq	%rax, %rdi
	xorq	%rdi, -96(%rbp)
	movq	-96(%rbp), %rax
	movq	%rax, -96(%rbp)
	orq	%rax, %rdi
	movq	-176(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -104(%rbp)
	movq	-104(%rbp), %rax
	movq	%rax, -104(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -136(%rbp)
	movq	-184(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -112(%rbp)
	movq	-112(%rbp), %rax
	movq	%rax, -112(%rbp)
	orq	%rax, %r8
	movq	-192(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, %r15
	orq	%r15, %r9
	xorq	-200(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-208(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-144(%rbp), %rdx
	xorq	%rdx, -120(%rbp)
	movq	-120(%rbp), %rax
	xorq	%rax, %r10
	orq	%r10, %rax
	movq	-152(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, %r11
	orq	%r11, %rcx
	movq	%rcx, -128(%rbp)
	movq	-160(%rbp), %rdx
	xorq	%rdx, %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	orq	%rcx, %rsi
	movq	-168(%rbp), %rdx
	xorq	%rdx, %rdi
	xorq	%rdi, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	orq	%rcx, %rdi
	movq	-176(%rbp), %rcx
	xorq	%rcx, -136(%rbp)
	movq	-136(%rbp), %rdx
	xorq	%rdx, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	orq	%rcx, %rdx
	movq	-184(%rbp), %rcx
	xorq	%rcx, %r8
	xorq	%r8, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	orq	%rcx, %r8
	movq	-192(%rbp), %rcx
	xorq	%rcx, %r9
	xorq	%r9, %r15
	orq	%r15, %r9
	xorq	-200(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-208(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-144(%rbp), %rcx
	xorq	%rcx, %rax
	xorq	%rax, %r10
	orq	%r10, %rax
	movq	%rax, -120(%rbp)
	movq	-152(%rbp), %rax
	xorq	%rax, -128(%rbp)
	movq	-128(%rbp), %rcx
	xorq	%rcx, %r11
	orq	%r11, %rcx
	movq	-160(%rbp), %rax
	xorq	%rax, %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %rax
	movq	%rax, -88(%rbp)
	orq	%rax, %rsi
	movq	-168(%rbp), %rax
	xorq	%rax, %rdi
	xorq	%rdi, -96(%rbp)
	movq	-96(%rbp), %rax
	movq	%rax, -96(%rbp)
	orq	%rax, %rdi
	movq	-176(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -104(%rbp)
	movq	-104(%rbp), %rax
	movq	%rax, -104(%rbp)
	orq	%rax, %rdx
	movq	-184(%rbp), %rax
	xorq	%rax, %r8
	xorq	%r8, -112(%rbp)
	movq	-112(%rbp), %rax
	movq	%rax, -112(%rbp)
	orq	%rax, %r8
	movq	-192(%rbp), %rax
	xorq	%rax, %r9
	xorq	%r9, %r15
	orq	%r15, %r9
	movq	%r9, -216(%rbp)
	xorq	-200(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-208(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-144(%rbp), %r9
	xorq	%r9, -120(%rbp)
	movq	-120(%rbp), %rax
	xorq	%rax, %r10
	orq	%r10, %rax
	movq	-152(%rbp), %r9
	xorq	%r9, %rcx
	xorq	%rcx, %r11
	orq	%r11, %rcx
	movq	-160(%rbp), %r9
	xorq	%r9, %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %r9
	movq	%r9, -88(%rbp)
	orq	%r9, %rsi
	movq	-168(%rbp), %r9
	xorq	%r9, %rdi
	xorq	%rdi, -96(%rbp)
	movq	-96(%rbp), %r9
	movq	%r9, -96(%rbp)
	orq	%r9, %rdi
	movq	-176(%rbp), %r9
	xorq	%r9, %rdx
	xorq	%rdx, -104(%rbp)
	movq	-104(%rbp), %r9
	movq	%r9, -104(%rbp)
	orq	%r9, %rdx
	movq	%rdx, -136(%rbp)
	movq	-184(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -112(%rbp)
	movq	-112(%rbp), %r9
	movq	%r9, -112(%rbp)
	orq	%r9, %r8
	movq	-192(%rbp), %rdx
	xorq	%rdx, -216(%rbp)
	movq	-216(%rbp), %r9
	xorq	%r9, %r15
	orq	%r15, %r9
	xorq	-200(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-208(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-144(%rbp), %rdx
	xorq	%rdx, %rax
	xorq	%rax, %r10
	orq	%r10, %rax
	movq	-152(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, %r11
	orq	%r11, %rcx
	movq	%rcx, -128(%rbp)
	movq	-160(%rbp), %rdx
	xorq	%rdx, %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	orq	%rcx, %rsi
	movq	-168(%rbp), %rdx
	xorq	%rdx, %rdi
	xorq	%rdi, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	orq	%rcx, %rdi
	movq	-176(%rbp), %rcx
	xorq	%rcx, -136(%rbp)
	movq	-136(%rbp), %rdx
	xorq	%rdx, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	orq	%rcx, %rdx
	movq	-184(%rbp), %rcx
	xorq	%rcx, %r8
	xorq	%r8, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	orq	%rcx, %r8
	movq	-192(%rbp), %rcx
	xorq	%rcx, %r9
	xorq	%r9, %r15
	orq	%r15, %r9
	xorq	-200(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-208(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-144(%rbp), %rcx
	xorq	%rcx, %rax
	xorq	%rax, %r10
	orq	%r10, %rax
	movq	%rax, -120(%rbp)
	movq	-152(%rbp), %rax
	xorq	%rax, -128(%rbp)
	movq	-128(%rbp), %rcx
	xorq	%rcx, %r11
	orq	%r11, %rcx
	movq	-160(%rbp), %rax
	xorq	%rax, %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %rax
	movq	%rax, -88(%rbp)
	orq	%rax, %rsi
	movq	-168(%rbp), %rax
	xorq	%rax, %rdi
	xorq	%rdi, -96(%rbp)
	movq	-96(%rbp), %rax
	movq	%rax, -96(%rbp)
	orq	%rax, %rdi
	movq	-176(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -104(%rbp)
	movq	-104(%rbp), %rax
	movq	%rax, -104(%rbp)
	orq	%rax, %rdx
	movq	-184(%rbp), %rax
	xorq	%rax, %r8
	xorq	%r8, -112(%rbp)
	movq	-112(%rbp), %rax
	movq	%rax, -112(%rbp)
	orq	%rax, %r8
	movq	-192(%rbp), %rax
	xorq	%rax, %r9
	xorq	%r9, %r15
	orq	%r15, %r9
	movq	%r9, -216(%rbp)
	xorq	-200(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-208(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-144(%rbp), %r9
	xorq	%r9, -120(%rbp)
	movq	-120(%rbp), %rax
	xorq	%rax, %r10
	orq	%r10, %rax
	movq	-152(%rbp), %r9
	xorq	%r9, %rcx
	xorq	%rcx, %r11
	orq	%r11, %rcx
	movq	-160(%rbp), %r9
	xorq	%r9, %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %r9
	movq	%r9, -88(%rbp)
	orq	%r9, %rsi
	movq	-168(%rbp), %r9
	xorq	%r9, %rdi
	xorq	%rdi, -96(%rbp)
	movq	-96(%rbp), %r9
	movq	%r9, -96(%rbp)
	orq	%r9, %rdi
	movq	-176(%rbp), %r9
	xorq	%r9, %rdx
	xorq	%rdx, -104(%rbp)
	movq	-104(%rbp), %r9
	movq	%r9, -104(%rbp)
	orq	%r9, %rdx
	movq	%rdx, -136(%rbp)
	movq	-184(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -112(%rbp)
	movq	-112(%rbp), %r9
	movq	%r9, -112(%rbp)
	orq	%r9, %r8
	movq	-192(%rbp), %rdx
	xorq	%rdx, -216(%rbp)
	movq	-216(%rbp), %r9
	xorq	%r9, %r15
	orq	%r15, %r9
	xorq	-200(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-208(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-144(%rbp), %rdx
	xorq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r10, -224(%rbp)
	orq	%r10, %rax
	movq	-152(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, %r11
	orq	%r11, %rcx
	movq	-160(%rbp), %rdx
	xorq	%rdx, %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %r10
	movq	%r10, -88(%rbp)
	orq	%r10, %rsi
	movq	-168(%rbp), %rdx
	xorq	%rdx, %rdi
	xorq	%rdi, -96(%rbp)
	movq	-96(%rbp), %r10
	movq	%r10, -96(%rbp)
	orq	%r10, %rdi
	movq	-176(%rbp), %r10
	xorq	%r10, -136(%rbp)
	movq	-136(%rbp), %rdx
	xorq	%rdx, -104(%rbp)
	movq	-104(%rbp), %r10
	movq	%r10, -104(%rbp)
	orq	%r10, %rdx
	movq	-184(%rbp), %r10
	xorq	%r10, %r8
	xorq	%r8, -112(%rbp)
	movq	-112(%rbp), %r10
	movq	%r10, -112(%rbp)
	orq	%r10, %r8
	movq	-192(%rbp), %r10
	xorq	%r10, %r9
	xorq	%r9, %r15
	orq	%r15, %r9
	xorq	-200(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-208(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-144(%rbp), %r10
	xorq	%r10, %rax
	xorq	%rax, -224(%rbp)
	movq	-224(%rbp), %r10
	movq	%r10, -224(%rbp)
	orq	%r10, %rax
	movq	%rax, -120(%rbp)
	movq	-152(%rbp), %rax
	xorq	%rax, %rcx
	xorq	%rcx, %r11
	movq	%r11, -256(%rbp)
	orq	%r11, %rcx
	movq	%rcx, -128(%rbp)
	movq	-160(%rbp), %rcx
	xorq	%rcx, %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %rax
	movq	%rax, -88(%rbp)
	orq	%rax, %rsi
	movq	%rsi, -232(%rbp)
	movq	-168(%rbp), %rax
	xorq	%rax, %rdi
	xorq	%rdi, -96(%rbp)
	movq	-96(%rbp), %r10
	movq	%r10, -96(%rbp)
	orq	%r10, %rdi
	movq	%rdi, -240(%rbp)
	movq	-176(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -104(%rbp)
	movq	-104(%rbp), %r10
	movq	%r10, -104(%rbp)
	orq	%r10, %rdx
	movq	%rdx, -136(%rbp)
	movq	-184(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -112(%rbp)
	movq	-112(%rbp), %r10
	movq	%r10, -112(%rbp)
	orq	%r10, %r8
	movq	%r8, -248(%rbp)
	movq	-192(%rbp), %r10
	xorq	%r10, %r9
	xorq	%r9, %r15
	orq	%r15, %r9
	movq	%r9, -216(%rbp)
	xorq	-200(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-208(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
.L283:
	movq	-264(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -264(%rbp)
	testq	%rax, %rax
	jne	.L284
	movl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$232, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE97:
	.size	int64_bit_8, .-int64_bit_8
	.globl	int64_bit_9
	.type	int64_bit_9, @function
int64_bit_9:
.LFB98:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$248, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -288(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	addq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -240(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -248(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -256(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -264(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$2, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$3, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -112(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -272(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$4, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -280(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$5, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$32, %rax
	orq	%rax, %rsi
	movq	%rsi, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$6, %rax
	movq	%rax, %r15
	movq	%r15, %rax
	salq	$32, %rax
	orq	%rax, %r15
	leaq	0(,%r15,4), %rax
	subq	$1, %rax
	movq	%rax, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$32, %rax
	orq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$7, %rax
	movq	%rax, %r13
	movq	%r13, %rax
	salq	$32, %rax
	orq	%rax, %r13
	leaq	0(,%r13,4), %rax
	subq	$1, %rax
	movq	%rax, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$32, %rax
	orq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$8, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -232(%rbp)
	jmp	.L286
.L287:
	subq	$1, -160(%rbp)
	movq	-160(%rbp), %rcx
	subq	$1, -168(%rbp)
	movq	-168(%rbp), %rsi
	subq	$1, -176(%rbp)
	movq	-176(%rbp), %rdi
	subq	$1, -184(%rbp)
	movq	-184(%rbp), %rdx
	subq	$1, -192(%rbp)
	movq	-192(%rbp), %r8
	subq	$1, -200(%rbp)
	movq	-200(%rbp), %r9
	subq	$1, -208(%rbp)
	movq	-208(%rbp), %r10
	subq	$1, -216(%rbp)
	subq	$1, -224(%rbp)
	subq	$1, -232(%rbp)
	movq	%rcx, -160(%rbp)
	xorq	%rcx, -136(%rbp)
	movq	-136(%rbp), %rax
	xorq	%rax, -240(%rbp)
	movq	-240(%rbp), %r11
	movq	%r11, -240(%rbp)
	orq	%r11, %rax
	movq	%rsi, -168(%rbp)
	xorq	%rsi, -248(%rbp)
	movq	-248(%rbp), %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %rsi
	movq	%rsi, -88(%rbp)
	orq	%rsi, %rcx
	movq	%rdi, -176(%rbp)
	xorq	%rdi, -256(%rbp)
	movq	-256(%rbp), %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	orq	%rdi, %rsi
	movq	%rdx, -184(%rbp)
	xorq	%rdx, -264(%rbp)
	movq	-264(%rbp), %rdi
	xorq	%rdi, -104(%rbp)
	movq	-104(%rbp), %rdx
	movq	%rdx, -104(%rbp)
	orq	%rdx, %rdi
	movq	%r8, -192(%rbp)
	xorq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdx
	xorq	%rdx, -112(%rbp)
	movq	-112(%rbp), %r8
	movq	%r8, -112(%rbp)
	orq	%r8, %rdx
	movq	%r9, -200(%rbp)
	xorq	%r9, -272(%rbp)
	movq	-272(%rbp), %r8
	xorq	%r8, -120(%rbp)
	movq	-120(%rbp), %r9
	movq	%r9, -120(%rbp)
	orq	%r9, %r8
	movq	%r10, -208(%rbp)
	xorq	%r10, -280(%rbp)
	movq	-280(%rbp), %r9
	xorq	%r9, -128(%rbp)
	movq	-128(%rbp), %r10
	movq	%r10, -128(%rbp)
	orq	%r10, %r9
	movq	-216(%rbp), %r11
	xorq	%r11, -152(%rbp)
	movq	-152(%rbp), %r10
	xorq	%r10, %r15
	orq	%r15, %r10
	xorq	-224(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-232(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-160(%rbp), %r11
	xorq	%r11, %rax
	xorq	%rax, -240(%rbp)
	movq	-240(%rbp), %r11
	orq	%r11, %rax
	movq	%rax, -136(%rbp)
	movq	-168(%rbp), %rax
	xorq	%rax, %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %rax
	movq	%rax, -88(%rbp)
	orq	%rax, %rcx
	movq	-176(%rbp), %rax
	xorq	%rax, %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %rax
	movq	%rax, -96(%rbp)
	orq	%rax, %rsi
	movq	-184(%rbp), %rax
	xorq	%rax, %rdi
	xorq	%rdi, -104(%rbp)
	movq	-104(%rbp), %rax
	movq	%rax, -104(%rbp)
	orq	%rax, %rdi
	movq	-192(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -112(%rbp)
	movq	-112(%rbp), %rax
	movq	%rax, -112(%rbp)
	orq	%rax, %rdx
	movq	-200(%rbp), %rax
	xorq	%rax, %r8
	xorq	%r8, -120(%rbp)
	movq	-120(%rbp), %rax
	movq	%rax, -120(%rbp)
	orq	%rax, %r8
	movq	-208(%rbp), %rax
	xorq	%rax, %r9
	xorq	%r9, -128(%rbp)
	movq	-128(%rbp), %rax
	movq	%rax, -128(%rbp)
	orq	%rax, %r9
	movq	-216(%rbp), %rax
	xorq	%rax, %r10
	xorq	%r10, %r15
	orq	%r15, %r10
	movq	%r10, -152(%rbp)
	xorq	-224(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-232(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-160(%rbp), %r10
	xorq	%r10, -136(%rbp)
	movq	-136(%rbp), %rax
	xorq	%rax, %r11
	orq	%r11, %rax
	movq	-168(%rbp), %r10
	xorq	%r10, %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %r10
	movq	%r10, -88(%rbp)
	orq	%r10, %rcx
	movq	-176(%rbp), %r10
	xorq	%r10, %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %r10
	movq	%r10, -96(%rbp)
	orq	%r10, %rsi
	movq	-184(%rbp), %r10
	xorq	%r10, %rdi
	xorq	%rdi, -104(%rbp)
	movq	-104(%rbp), %r10
	movq	%r10, -104(%rbp)
	orq	%r10, %rdi
	movq	-192(%rbp), %r10
	xorq	%r10, %rdx
	xorq	%rdx, -112(%rbp)
	movq	-112(%rbp), %r10
	movq	%r10, -112(%rbp)
	orq	%r10, %rdx
	movq	%rdx, -144(%rbp)
	movq	-200(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -120(%rbp)
	movq	-120(%rbp), %r10
	movq	%r10, -120(%rbp)
	orq	%r10, %r8
	movq	-208(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -128(%rbp)
	movq	-128(%rbp), %r10
	movq	%r10, -128(%rbp)
	orq	%r10, %r9
	movq	-216(%rbp), %rdx
	xorq	%rdx, -152(%rbp)
	movq	-152(%rbp), %r10
	xorq	%r10, %r15
	orq	%r15, %r10
	xorq	-224(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-232(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-160(%rbp), %rdx
	xorq	%rdx, %rax
	xorq	%rax, %r11
	orq	%r11, %rax
	movq	%rax, -136(%rbp)
	movq	-168(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %rax
	movq	%rax, -88(%rbp)
	orq	%rax, %rcx
	movq	-176(%rbp), %rdx
	xorq	%rdx, %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %rax
	movq	%rax, -96(%rbp)
	orq	%rax, %rsi
	movq	-184(%rbp), %rdx
	xorq	%rdx, %rdi
	xorq	%rdi, -104(%rbp)
	movq	-104(%rbp), %rax
	movq	%rax, -104(%rbp)
	orq	%rax, %rdi
	movq	-192(%rbp), %rax
	xorq	%rax, -144(%rbp)
	movq	-144(%rbp), %rdx
	xorq	%rdx, -112(%rbp)
	movq	-112(%rbp), %rax
	movq	%rax, -112(%rbp)
	orq	%rax, %rdx
	movq	-200(%rbp), %rax
	xorq	%rax, %r8
	xorq	%r8, -120(%rbp)
	movq	-120(%rbp), %rax
	movq	%rax, -120(%rbp)
	orq	%rax, %r8
	movq	-208(%rbp), %rax
	xorq	%rax, %r9
	xorq	%r9, -128(%rbp)
	movq	-128(%rbp), %rax
	movq	%rax, -128(%rbp)
	orq	%rax, %r9
	movq	-216(%rbp), %rax
	xorq	%rax, %r10
	xorq	%r10, %r15
	orq	%r15, %r10
	movq	%r10, -152(%rbp)
	xorq	-224(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-232(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-160(%rbp), %r10
	xorq	%r10, -136(%rbp)
	movq	-136(%rbp), %rax
	xorq	%rax, %r11
	orq	%r11, %rax
	movq	-168(%rbp), %r10
	xorq	%r10, %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %r10
	movq	%r10, -88(%rbp)
	orq	%r10, %rcx
	movq	%rcx, -248(%rbp)
	movq	-176(%rbp), %rcx
	xorq	%rcx, %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %r10
	movq	%r10, -96(%rbp)
	orq	%r10, %rsi
	movq	-184(%rbp), %rcx
	xorq	%rcx, %rdi
	xorq	%rdi, -104(%rbp)
	movq	-104(%rbp), %r10
	movq	%r10, -104(%rbp)
	orq	%r10, %rdi
	movq	-192(%rbp), %rcx
	xorq	%rcx, %rdx
	xorq	%rdx, -112(%rbp)
	movq	-112(%rbp), %r10
	movq	%r10, -112(%rbp)
	orq	%r10, %rdx
	movq	-200(%rbp), %rcx
	xorq	%rcx, %r8
	xorq	%r8, -120(%rbp)
	movq	-120(%rbp), %r10
	movq	%r10, -120(%rbp)
	orq	%r10, %r8
	movq	-208(%rbp), %rcx
	xorq	%rcx, %r9
	xorq	%r9, -128(%rbp)
	movq	-128(%rbp), %r10
	movq	%r10, -128(%rbp)
	orq	%r10, %r9
	movq	-216(%rbp), %rcx
	xorq	%rcx, -152(%rbp)
	movq	-152(%rbp), %r10
	xorq	%r10, %r15
	orq	%r15, %r10
	xorq	-224(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-232(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-160(%rbp), %rcx
	xorq	%rcx, %rax
	xorq	%rax, %r11
	orq	%r11, %rax
	movq	%rax, -136(%rbp)
	movq	-168(%rbp), %rax
	xorq	%rax, -248(%rbp)
	movq	-248(%rbp), %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %rax
	movq	%rax, -88(%rbp)
	orq	%rax, %rcx
	movq	-176(%rbp), %rax
	xorq	%rax, %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %rax
	movq	%rax, -96(%rbp)
	orq	%rax, %rsi
	movq	-184(%rbp), %rax
	xorq	%rax, %rdi
	xorq	%rdi, -104(%rbp)
	movq	-104(%rbp), %rax
	movq	%rax, -104(%rbp)
	orq	%rax, %rdi
	movq	-192(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -112(%rbp)
	movq	-112(%rbp), %rax
	movq	%rax, -112(%rbp)
	orq	%rax, %rdx
	movq	-200(%rbp), %rax
	xorq	%rax, %r8
	xorq	%r8, -120(%rbp)
	movq	-120(%rbp), %rax
	movq	%rax, -120(%rbp)
	orq	%rax, %r8
	movq	-208(%rbp), %rax
	xorq	%rax, %r9
	xorq	%r9, -128(%rbp)
	movq	-128(%rbp), %rax
	movq	%rax, -128(%rbp)
	orq	%rax, %r9
	movq	-216(%rbp), %rax
	xorq	%rax, %r10
	xorq	%r10, %r15
	orq	%r15, %r10
	movq	%r10, -152(%rbp)
	xorq	-224(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-232(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-160(%rbp), %r10
	xorq	%r10, -136(%rbp)
	movq	-136(%rbp), %rax
	xorq	%rax, %r11
	orq	%r11, %rax
	movq	-168(%rbp), %r10
	xorq	%r10, %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %r10
	movq	%r10, -88(%rbp)
	orq	%r10, %rcx
	movq	-176(%rbp), %r10
	xorq	%r10, %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %r10
	movq	%r10, -96(%rbp)
	orq	%r10, %rsi
	movq	-184(%rbp), %r10
	xorq	%r10, %rdi
	xorq	%rdi, -104(%rbp)
	movq	-104(%rbp), %r10
	movq	%r10, -104(%rbp)
	orq	%r10, %rdi
	movq	-192(%rbp), %r10
	xorq	%r10, %rdx
	xorq	%rdx, -112(%rbp)
	movq	-112(%rbp), %r10
	movq	%r10, -112(%rbp)
	orq	%r10, %rdx
	movq	%rdx, -144(%rbp)
	movq	-200(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -120(%rbp)
	movq	-120(%rbp), %r10
	movq	%r10, -120(%rbp)
	orq	%r10, %r8
	movq	-208(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -128(%rbp)
	movq	-128(%rbp), %r10
	movq	%r10, -128(%rbp)
	orq	%r10, %r9
	movq	-216(%rbp), %rdx
	xorq	%rdx, -152(%rbp)
	movq	-152(%rbp), %r10
	xorq	%r10, %r15
	orq	%r15, %r10
	xorq	-224(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-232(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-160(%rbp), %rdx
	xorq	%rdx, %rax
	xorq	%rax, %r11
	orq	%r11, %rax
	movq	%rax, -136(%rbp)
	movq	-168(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %rdx
	movq	%rdx, -88(%rbp)
	movq	%rdx, %rax
	orq	%rax, %rcx
	movq	-176(%rbp), %rdx
	xorq	%rdx, %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %rax
	movq	%rax, -96(%rbp)
	orq	%rax, %rsi
	movq	-184(%rbp), %rdx
	xorq	%rdx, %rdi
	xorq	%rdi, -104(%rbp)
	movq	-104(%rbp), %rax
	movq	%rax, -104(%rbp)
	orq	%rax, %rdi
	movq	-192(%rbp), %rax
	xorq	%rax, -144(%rbp)
	movq	-144(%rbp), %rdx
	xorq	%rdx, -112(%rbp)
	movq	-112(%rbp), %rax
	movq	%rax, -112(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -144(%rbp)
	movq	-200(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -120(%rbp)
	movq	-120(%rbp), %rax
	movq	%rax, -120(%rbp)
	orq	%rax, %r8
	movq	-208(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -128(%rbp)
	movq	-128(%rbp), %rax
	movq	%rax, -128(%rbp)
	orq	%rax, %r9
	movq	-216(%rbp), %rdx
	xorq	%rdx, %r10
	xorq	%r10, %r15
	orq	%r15, %r10
	xorq	-224(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-232(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-160(%rbp), %rdx
	xorq	%rdx, -136(%rbp)
	movq	-136(%rbp), %rax
	xorq	%rax, %r11
	movq	%r11, -240(%rbp)
	orq	%r11, %rax
	movq	-168(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %r11
	movq	%r11, -88(%rbp)
	orq	%r11, %rcx
	movq	-176(%rbp), %rdx
	xorq	%rdx, %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %r11
	movq	%r11, -96(%rbp)
	orq	%r11, %rsi
	movq	-184(%rbp), %rdx
	xorq	%rdx, %rdi
	xorq	%rdi, -104(%rbp)
	movq	-104(%rbp), %r11
	movq	%r11, -104(%rbp)
	orq	%r11, %rdi
	movq	-192(%rbp), %r11
	xorq	%r11, -144(%rbp)
	movq	-144(%rbp), %rdx
	xorq	%rdx, -112(%rbp)
	movq	-112(%rbp), %r11
	movq	%r11, -112(%rbp)
	orq	%r11, %rdx
	movq	-200(%rbp), %r11
	xorq	%r11, %r8
	xorq	%r8, -120(%rbp)
	movq	-120(%rbp), %r11
	movq	%r11, -120(%rbp)
	orq	%r11, %r8
	movq	-208(%rbp), %r11
	xorq	%r11, %r9
	xorq	%r9, -128(%rbp)
	movq	-128(%rbp), %r11
	movq	%r11, -128(%rbp)
	orq	%r11, %r9
	movq	-216(%rbp), %r11
	xorq	%r11, %r10
	xorq	%r10, %r15
	orq	%r15, %r10
	xorq	-224(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-232(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-160(%rbp), %r11
	xorq	%r11, %rax
	xorq	%rax, -240(%rbp)
	movq	-240(%rbp), %r11
	movq	%r11, -240(%rbp)
	orq	%r11, %rax
	movq	%rax, -136(%rbp)
	movq	-168(%rbp), %rax
	xorq	%rax, %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %rax
	movq	%rax, -88(%rbp)
	orq	%rax, %rcx
	movq	%rcx, -248(%rbp)
	movq	-176(%rbp), %rcx
	xorq	%rcx, %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %r11
	movq	%r11, -96(%rbp)
	orq	%r11, %rsi
	movq	%rsi, -256(%rbp)
	movq	-184(%rbp), %rcx
	xorq	%rcx, %rdi
	xorq	%rdi, -104(%rbp)
	movq	-104(%rbp), %r11
	movq	%r11, -104(%rbp)
	orq	%r11, %rdi
	movq	%rdi, -264(%rbp)
	movq	-192(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -112(%rbp)
	movq	-112(%rbp), %r11
	movq	%r11, -112(%rbp)
	orq	%r11, %rdx
	movq	%rdx, -144(%rbp)
	movq	-200(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -120(%rbp)
	movq	-120(%rbp), %r11
	movq	%r11, -120(%rbp)
	orq	%r11, %r8
	movq	%r8, -272(%rbp)
	movq	-208(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -128(%rbp)
	movq	-128(%rbp), %r11
	movq	%r11, -128(%rbp)
	orq	%r11, %r9
	movq	%r9, -280(%rbp)
	movq	-216(%rbp), %r11
	xorq	%r11, %r10
	xorq	%r10, %r15
	orq	%r15, %r10
	movq	%r10, -152(%rbp)
	xorq	-224(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-232(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
.L286:
	movq	-288(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -288(%rbp)
	testq	%rax, %rax
	jne	.L287
	movl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-264(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-272(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-280(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$248, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE98:
	.size	int64_bit_9, .-int64_bit_9
	.globl	int64_bit_10
	.type	int64_bit_10, @function
int64_bit_10:
.LFB99:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$280, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -312(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	addq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -272(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -280(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -256(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$2, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -112(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -264(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$3, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -288(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$4, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$32, %rax
	orq	%rax, %rsi
	movq	%rsi, -296(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$5, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$32, %rax
	orq	%rax, %rcx
	movq	%rcx, -304(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$6, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$7, %rax
	movq	%rax, %r15
	movq	%r15, %rax
	salq	$32, %rax
	orq	%rax, %r15
	leaq	0(,%r15,4), %rax
	subq	$1, %rax
	movq	%rax, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$32, %rax
	orq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$8, %rax
	movq	%rax, %r13
	movq	%r13, %rax
	salq	$32, %rax
	orq	%rax, %r13
	leaq	0(,%r13,4), %rax
	subq	$1, %rax
	movq	%rax, -240(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$32, %rax
	orq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$9, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -248(%rbp)
	jmp	.L289
.L290:
	subq	$1, -168(%rbp)
	movq	-168(%rbp), %rcx
	subq	$1, -176(%rbp)
	movq	-176(%rbp), %rsi
	subq	$1, -184(%rbp)
	movq	-184(%rbp), %rdi
	subq	$1, -192(%rbp)
	movq	-192(%rbp), %rdx
	subq	$1, -200(%rbp)
	movq	-200(%rbp), %r8
	subq	$1, -208(%rbp)
	movq	-208(%rbp), %r9
	subq	$1, -216(%rbp)
	movq	-216(%rbp), %r10
	subq	$1, -224(%rbp)
	movq	-224(%rbp), %r11
	subq	$1, -232(%rbp)
	subq	$1, -240(%rbp)
	subq	$1, -248(%rbp)
	movq	%rcx, -168(%rbp)
	xorq	%rcx, -160(%rbp)
	movq	-160(%rbp), %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	orq	%rcx, %rax
	movq	%rax, -160(%rbp)
	movq	%rsi, -176(%rbp)
	xorq	%rsi, -272(%rbp)
	movq	-272(%rbp), %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %rsi
	movq	%rsi, -96(%rbp)
	orq	%rsi, %rcx
	movq	%rdi, -184(%rbp)
	xorq	%rdi, -280(%rbp)
	movq	-280(%rbp), %rsi
	xorq	%rsi, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	orq	%rdi, %rsi
	movq	%rdx, -192(%rbp)
	xorq	%rdx, -256(%rbp)
	movq	-256(%rbp), %rdi
	xorq	%rdi, -112(%rbp)
	movq	-112(%rbp), %rdx
	movq	%rdx, -112(%rbp)
	orq	%rdx, %rdi
	movq	%r8, -200(%rbp)
	xorq	%r8, -264(%rbp)
	movq	-264(%rbp), %rdx
	xorq	%rdx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	orq	%r8, %rdx
	movq	%r9, -208(%rbp)
	xorq	%r9, -288(%rbp)
	movq	-288(%rbp), %r8
	xorq	%r8, -128(%rbp)
	movq	-128(%rbp), %r9
	movq	%r9, -128(%rbp)
	orq	%r9, %r8
	movq	%r10, -216(%rbp)
	xorq	%r10, -296(%rbp)
	movq	-296(%rbp), %r9
	xorq	%r9, -136(%rbp)
	movq	-136(%rbp), %r10
	movq	%r10, -136(%rbp)
	orq	%r10, %r9
	movq	%r11, -224(%rbp)
	xorq	%r11, -304(%rbp)
	movq	-304(%rbp), %r10
	xorq	%r10, -144(%rbp)
	movq	-144(%rbp), %r11
	movq	%r11, -144(%rbp)
	orq	%r11, %r10
	movq	-232(%rbp), %rax
	xorq	%rax, -152(%rbp)
	movq	-152(%rbp), %r11
	xorq	%r11, %r15
	orq	%r15, %r11
	movq	%r11, -152(%rbp)
	xorq	-240(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-248(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %r11
	xorq	%r11, -160(%rbp)
	movq	-160(%rbp), %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %r11
	movq	%r11, -88(%rbp)
	orq	%r11, %rax
	movq	%rax, -160(%rbp)
	movq	-176(%rbp), %r11
	xorq	%r11, %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %r11
	movq	%r11, -96(%rbp)
	orq	%r11, %rcx
	movq	-184(%rbp), %r11
	xorq	%r11, %rsi
	xorq	%rsi, -104(%rbp)
	movq	-104(%rbp), %r11
	movq	%r11, -104(%rbp)
	orq	%r11, %rsi
	movq	-192(%rbp), %r11
	xorq	%r11, %rdi
	xorq	%rdi, -112(%rbp)
	movq	-112(%rbp), %r11
	movq	%r11, -112(%rbp)
	orq	%r11, %rdi
	movq	-200(%rbp), %r11
	xorq	%r11, %rdx
	xorq	%rdx, -120(%rbp)
	movq	-120(%rbp), %r11
	movq	%r11, -120(%rbp)
	orq	%r11, %rdx
	movq	-208(%rbp), %r11
	xorq	%r11, %r8
	xorq	%r8, -128(%rbp)
	movq	-128(%rbp), %r11
	movq	%r11, -128(%rbp)
	orq	%r11, %r8
	movq	-216(%rbp), %r11
	xorq	%r11, %r9
	xorq	%r9, -136(%rbp)
	movq	-136(%rbp), %r11
	movq	%r11, -136(%rbp)
	orq	%r11, %r9
	movq	-224(%rbp), %r11
	xorq	%r11, %r10
	xorq	%r10, -144(%rbp)
	movq	-144(%rbp), %r11
	movq	%r11, -144(%rbp)
	orq	%r11, %r10
	movq	-232(%rbp), %rax
	xorq	%rax, -152(%rbp)
	movq	-152(%rbp), %r11
	xorq	%r11, %r15
	orq	%r15, %r11
	movq	%r11, -152(%rbp)
	xorq	-240(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-248(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %r11
	xorq	%r11, -160(%rbp)
	movq	-160(%rbp), %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %r11
	movq	%r11, -88(%rbp)
	orq	%r11, %rax
	movq	%rax, -160(%rbp)
	movq	-176(%rbp), %r11
	xorq	%r11, %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %r11
	movq	%r11, -96(%rbp)
	orq	%r11, %rcx
	movq	-184(%rbp), %r11
	xorq	%r11, %rsi
	xorq	%rsi, -104(%rbp)
	movq	-104(%rbp), %r11
	movq	%r11, -104(%rbp)
	orq	%r11, %rsi
	movq	-192(%rbp), %r11
	xorq	%r11, %rdi
	xorq	%rdi, -112(%rbp)
	movq	-112(%rbp), %r11
	movq	%r11, -112(%rbp)
	orq	%r11, %rdi
	movq	-200(%rbp), %r11
	xorq	%r11, %rdx
	xorq	%rdx, -120(%rbp)
	movq	-120(%rbp), %r11
	movq	%r11, -120(%rbp)
	orq	%r11, %rdx
	movq	-208(%rbp), %r11
	xorq	%r11, %r8
	xorq	%r8, -128(%rbp)
	movq	-128(%rbp), %r11
	movq	%r11, -128(%rbp)
	orq	%r11, %r8
	movq	-216(%rbp), %r11
	xorq	%r11, %r9
	xorq	%r9, -136(%rbp)
	movq	-136(%rbp), %r11
	movq	%r11, -136(%rbp)
	orq	%r11, %r9
	movq	-224(%rbp), %r11
	xorq	%r11, %r10
	xorq	%r10, -144(%rbp)
	movq	-144(%rbp), %r11
	movq	%r11, -144(%rbp)
	orq	%r11, %r10
	movq	-232(%rbp), %rax
	xorq	%rax, -152(%rbp)
	movq	-152(%rbp), %r11
	xorq	%r11, %r15
	orq	%r15, %r11
	movq	%r11, -152(%rbp)
	xorq	-240(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-248(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %r11
	xorq	%r11, -160(%rbp)
	movq	-160(%rbp), %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %r11
	movq	%r11, -88(%rbp)
	orq	%r11, %rax
	movq	%rax, -160(%rbp)
	movq	-176(%rbp), %r11
	xorq	%r11, %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %r11
	movq	%r11, -96(%rbp)
	orq	%r11, %rcx
	movq	-184(%rbp), %r11
	xorq	%r11, %rsi
	xorq	%rsi, -104(%rbp)
	movq	-104(%rbp), %r11
	movq	%r11, -104(%rbp)
	orq	%r11, %rsi
	movq	-192(%rbp), %r11
	xorq	%r11, %rdi
	xorq	%rdi, -112(%rbp)
	movq	-112(%rbp), %r11
	movq	%r11, -112(%rbp)
	orq	%r11, %rdi
	movq	-200(%rbp), %r11
	xorq	%r11, %rdx
	xorq	%rdx, -120(%rbp)
	movq	-120(%rbp), %r11
	movq	%r11, -120(%rbp)
	orq	%r11, %rdx
	movq	-208(%rbp), %r11
	xorq	%r11, %r8
	xorq	%r8, -128(%rbp)
	movq	-128(%rbp), %r11
	movq	%r11, -128(%rbp)
	orq	%r11, %r8
	movq	-216(%rbp), %r11
	xorq	%r11, %r9
	xorq	%r9, -136(%rbp)
	movq	-136(%rbp), %r11
	movq	%r11, -136(%rbp)
	orq	%r11, %r9
	movq	-224(%rbp), %r11
	xorq	%r11, %r10
	xorq	%r10, -144(%rbp)
	movq	-144(%rbp), %r11
	movq	%r11, -144(%rbp)
	orq	%r11, %r10
	movq	-232(%rbp), %rax
	xorq	%rax, -152(%rbp)
	movq	-152(%rbp), %r11
	xorq	%r11, %r15
	orq	%r15, %r11
	movq	%r11, -152(%rbp)
	xorq	-240(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-248(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %r11
	xorq	%r11, -160(%rbp)
	movq	-160(%rbp), %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %r11
	movq	%r11, -88(%rbp)
	orq	%r11, %rax
	movq	%rax, -160(%rbp)
	movq	-176(%rbp), %r11
	xorq	%r11, %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %r11
	movq	%r11, -96(%rbp)
	orq	%r11, %rcx
	movq	-184(%rbp), %r11
	xorq	%r11, %rsi
	xorq	%rsi, -104(%rbp)
	movq	-104(%rbp), %r11
	movq	%r11, -104(%rbp)
	orq	%r11, %rsi
	movq	-192(%rbp), %r11
	xorq	%r11, %rdi
	xorq	%rdi, -112(%rbp)
	movq	-112(%rbp), %r11
	movq	%r11, -112(%rbp)
	orq	%r11, %rdi
	movq	-200(%rbp), %r11
	xorq	%r11, %rdx
	xorq	%rdx, -120(%rbp)
	movq	-120(%rbp), %r11
	movq	%r11, -120(%rbp)
	orq	%r11, %rdx
	movq	-208(%rbp), %r11
	xorq	%r11, %r8
	xorq	%r8, -128(%rbp)
	movq	-128(%rbp), %r11
	movq	%r11, -128(%rbp)
	orq	%r11, %r8
	movq	-216(%rbp), %r11
	xorq	%r11, %r9
	xorq	%r9, -136(%rbp)
	movq	-136(%rbp), %r11
	movq	%r11, -136(%rbp)
	orq	%r11, %r9
	movq	-224(%rbp), %r11
	xorq	%r11, %r10
	xorq	%r10, -144(%rbp)
	movq	-144(%rbp), %r11
	movq	%r11, -144(%rbp)
	orq	%r11, %r10
	movq	-232(%rbp), %rax
	xorq	%rax, -152(%rbp)
	movq	-152(%rbp), %r11
	xorq	%r11, %r15
	orq	%r15, %r11
	movq	%r11, -152(%rbp)
	xorq	-240(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-248(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %r11
	xorq	%r11, -160(%rbp)
	movq	-160(%rbp), %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %r11
	movq	%r11, -88(%rbp)
	orq	%r11, %rax
	movq	%rax, -160(%rbp)
	movq	-176(%rbp), %r11
	xorq	%r11, %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %r11
	movq	%r11, -96(%rbp)
	orq	%r11, %rcx
	movq	-184(%rbp), %r11
	xorq	%r11, %rsi
	xorq	%rsi, -104(%rbp)
	movq	-104(%rbp), %r11
	movq	%r11, -104(%rbp)
	orq	%r11, %rsi
	movq	-192(%rbp), %r11
	xorq	%r11, %rdi
	xorq	%rdi, -112(%rbp)
	movq	-112(%rbp), %r11
	movq	%r11, -112(%rbp)
	orq	%r11, %rdi
	movq	-200(%rbp), %r11
	xorq	%r11, %rdx
	xorq	%rdx, -120(%rbp)
	movq	-120(%rbp), %r11
	movq	%r11, -120(%rbp)
	orq	%r11, %rdx
	movq	-208(%rbp), %r11
	xorq	%r11, %r8
	xorq	%r8, -128(%rbp)
	movq	-128(%rbp), %r11
	movq	%r11, -128(%rbp)
	orq	%r11, %r8
	movq	-216(%rbp), %r11
	xorq	%r11, %r9
	xorq	%r9, -136(%rbp)
	movq	-136(%rbp), %r11
	movq	%r11, -136(%rbp)
	orq	%r11, %r9
	movq	-224(%rbp), %r11
	xorq	%r11, %r10
	xorq	%r10, -144(%rbp)
	movq	-144(%rbp), %r11
	movq	%r11, -144(%rbp)
	orq	%r11, %r10
	movq	-232(%rbp), %rax
	xorq	%rax, -152(%rbp)
	movq	-152(%rbp), %r11
	xorq	%r11, %r15
	orq	%r15, %r11
	movq	%r11, -152(%rbp)
	xorq	-240(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-248(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %r11
	xorq	%r11, -160(%rbp)
	movq	-160(%rbp), %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %r11
	movq	%r11, -88(%rbp)
	orq	%r11, %rax
	movq	%rax, -160(%rbp)
	movq	-176(%rbp), %r11
	xorq	%r11, %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %r11
	movq	%r11, -96(%rbp)
	orq	%r11, %rcx
	movq	-184(%rbp), %r11
	xorq	%r11, %rsi
	xorq	%rsi, -104(%rbp)
	movq	-104(%rbp), %r11
	movq	%r11, -104(%rbp)
	orq	%r11, %rsi
	movq	-192(%rbp), %r11
	xorq	%r11, %rdi
	xorq	%rdi, -112(%rbp)
	movq	-112(%rbp), %r11
	movq	%r11, -112(%rbp)
	orq	%r11, %rdi
	movq	-200(%rbp), %r11
	xorq	%r11, %rdx
	xorq	%rdx, -120(%rbp)
	movq	-120(%rbp), %r11
	movq	%r11, -120(%rbp)
	orq	%r11, %rdx
	movq	-208(%rbp), %r11
	xorq	%r11, %r8
	xorq	%r8, -128(%rbp)
	movq	-128(%rbp), %r11
	movq	%r11, -128(%rbp)
	orq	%r11, %r8
	movq	-216(%rbp), %r11
	xorq	%r11, %r9
	xorq	%r9, -136(%rbp)
	movq	-136(%rbp), %r11
	movq	%r11, -136(%rbp)
	orq	%r11, %r9
	movq	-224(%rbp), %r11
	xorq	%r11, %r10
	xorq	%r10, -144(%rbp)
	movq	-144(%rbp), %r11
	movq	%r11, -144(%rbp)
	orq	%r11, %r10
	movq	-232(%rbp), %rax
	xorq	%rax, -152(%rbp)
	movq	-152(%rbp), %r11
	xorq	%r11, %r15
	orq	%r15, %r11
	movq	%r11, -152(%rbp)
	xorq	-240(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-248(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %r11
	xorq	%r11, -160(%rbp)
	movq	-160(%rbp), %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %r11
	movq	%r11, -88(%rbp)
	orq	%r11, %rax
	movq	-176(%rbp), %r11
	xorq	%r11, %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %r11
	movq	%r11, -96(%rbp)
	orq	%r11, %rcx
	movq	-184(%rbp), %r11
	xorq	%r11, %rsi
	xorq	%rsi, -104(%rbp)
	movq	-104(%rbp), %r11
	movq	%r11, -104(%rbp)
	orq	%r11, %rsi
	movq	-192(%rbp), %r11
	xorq	%r11, %rdi
	xorq	%rdi, -112(%rbp)
	movq	-112(%rbp), %r11
	movq	%r11, -112(%rbp)
	orq	%r11, %rdi
	movq	%rdi, -256(%rbp)
	movq	-200(%rbp), %r11
	xorq	%r11, %rdx
	xorq	%rdx, -120(%rbp)
	movq	-120(%rbp), %r11
	movq	%r11, -120(%rbp)
	orq	%r11, %rdx
	movq	-208(%rbp), %r11
	xorq	%r11, %r8
	xorq	%r8, -128(%rbp)
	movq	-128(%rbp), %r11
	movq	%r11, -128(%rbp)
	orq	%r11, %r8
	movq	-216(%rbp), %r11
	xorq	%r11, %r9
	xorq	%r9, -136(%rbp)
	movq	-136(%rbp), %r11
	movq	%r11, -136(%rbp)
	orq	%r11, %r9
	movq	-224(%rbp), %r11
	xorq	%r11, %r10
	xorq	%r10, -144(%rbp)
	movq	-144(%rbp), %r11
	movq	%r11, -144(%rbp)
	orq	%r11, %r10
	movq	-232(%rbp), %rdi
	xorq	%rdi, -152(%rbp)
	movq	-152(%rbp), %r11
	xorq	%r11, %r15
	orq	%r15, %r11
	movq	%r11, -152(%rbp)
	xorq	-240(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-248(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %r11
	xorq	%r11, %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	movq	%rdi, %r11
	orq	%r11, %rax
	movq	-176(%rbp), %r11
	xorq	%r11, %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %r11
	movq	%r11, -96(%rbp)
	orq	%r11, %rcx
	movq	-184(%rbp), %r11
	xorq	%r11, %rsi
	xorq	%rsi, -104(%rbp)
	movq	-104(%rbp), %r11
	movq	%r11, -104(%rbp)
	orq	%r11, %rsi
	movq	-192(%rbp), %r11
	xorq	%r11, -256(%rbp)
	movq	-256(%rbp), %rdi
	xorq	%rdi, -112(%rbp)
	movq	-112(%rbp), %r11
	movq	%r11, -112(%rbp)
	orq	%r11, %rdi
	movq	-200(%rbp), %r11
	xorq	%r11, %rdx
	xorq	%rdx, -120(%rbp)
	movq	-120(%rbp), %r11
	movq	%r11, -120(%rbp)
	orq	%r11, %rdx
	movq	%rdx, -264(%rbp)
	movq	-208(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdx
	movq	%rdx, -128(%rbp)
	orq	%rdx, %r8
	movq	-216(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -136(%rbp)
	movq	-136(%rbp), %rdx
	movq	%rdx, -136(%rbp)
	orq	%rdx, %r9
	movq	-224(%rbp), %rdx
	xorq	%rdx, %r10
	xorq	%r10, -144(%rbp)
	movq	-144(%rbp), %rdx
	movq	%rdx, -144(%rbp)
	orq	%rdx, %r10
	movq	-232(%rbp), %rdx
	xorq	%rdx, -152(%rbp)
	movq	-152(%rbp), %r11
	xorq	%r11, %r15
	orq	%r15, %r11
	xorq	-240(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-248(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %rdx
	xorq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %rdx
	movq	%rdx, -88(%rbp)
	orq	%rdx, %rax
	movq	%rax, -160(%rbp)
	movq	-176(%rbp), %rax
	xorq	%rax, %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %rax
	movq	%rax, -96(%rbp)
	orq	%rax, %rcx
	movq	%rcx, -272(%rbp)
	movq	-184(%rbp), %rcx
	xorq	%rcx, %rsi
	xorq	%rsi, -104(%rbp)
	movq	-104(%rbp), %rax
	movq	%rax, -104(%rbp)
	orq	%rax, %rsi
	movq	%rsi, -280(%rbp)
	movq	-192(%rbp), %rsi
	xorq	%rsi, %rdi
	xorq	%rdi, -112(%rbp)
	movq	-112(%rbp), %rax
	movq	%rax, -112(%rbp)
	orq	%rax, %rdi
	movq	%rdi, -256(%rbp)
	movq	-200(%rbp), %rcx
	xorq	%rcx, -264(%rbp)
	movq	-264(%rbp), %rdx
	xorq	%rdx, -120(%rbp)
	movq	-120(%rbp), %rax
	movq	%rax, -120(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -264(%rbp)
	movq	-208(%rbp), %rax
	xorq	%rax, %r8
	xorq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdx
	movq	%rdx, -128(%rbp)
	orq	%rdx, %r8
	movq	%r8, -288(%rbp)
	movq	-216(%rbp), %rax
	xorq	%rax, %r9
	xorq	%r9, -136(%rbp)
	movq	-136(%rbp), %rdx
	movq	%rdx, -136(%rbp)
	orq	%rdx, %r9
	movq	%r9, -296(%rbp)
	movq	-224(%rbp), %rax
	xorq	%rax, %r10
	xorq	%r10, -144(%rbp)
	movq	-144(%rbp), %rdx
	movq	%rdx, -144(%rbp)
	orq	%rdx, %r10
	movq	%r10, -304(%rbp)
	movq	-232(%rbp), %rdx
	xorq	%rdx, %r11
	xorq	%r11, %r15
	orq	%r15, %r11
	movq	%r11, -152(%rbp)
	xorq	-240(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-248(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
.L289:
	movq	-312(%rbp), %rax
	leaq	-1(%rax), %rdi
	movq	%rdi, -312(%rbp)
	testq	%rax, %rax
	jne	.L290
	movl	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-272(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-280(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-264(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-288(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-296(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-304(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$280, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE99:
	.size	int64_bit_10, .-int64_bit_10
	.globl	int64_bit_11
	.type	int64_bit_11, @function
int64_bit_11:
.LFB100:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$296, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -336(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -280(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	addq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -288(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -296(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -112(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -264(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$2, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -272(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$3, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -304(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$4, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -312(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$5, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -320(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$6, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -328(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$7, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -160(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$8, %rax
	movq	%rax, %r15
	movq	%r15, %rax
	salq	$32, %rax
	orq	%rax, %r15
	leaq	0(,%r15,4), %rax
	subq	$1, %rax
	movq	%rax, -240(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$32, %rax
	orq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$9, %rax
	movq	%rax, %r13
	movq	%r13, %rax
	salq	$32, %rax
	orq	%rax, %r13
	leaq	0(,%r13,4), %rax
	subq	$1, %rax
	movq	%rax, -248(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	subl	$10, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$32, %rax
	orq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$10, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -256(%rbp)
	jmp	.L292
.L293:
	subq	$1, -168(%rbp)
	movq	-168(%rbp), %rsi
	subq	$1, -176(%rbp)
	movq	-176(%rbp), %rdi
	subq	$1, -184(%rbp)
	movq	-184(%rbp), %rdx
	subq	$1, -192(%rbp)
	movq	-192(%rbp), %r11
	subq	$1, -200(%rbp)
	movq	-200(%rbp), %r8
	subq	$1, -208(%rbp)
	movq	-208(%rbp), %r9
	subq	$1, -216(%rbp)
	movq	-216(%rbp), %r10
	subq	$1, -224(%rbp)
	subq	$1, -232(%rbp)
	subq	$1, -240(%rbp)
	subq	$1, -248(%rbp)
	subq	$1, -256(%rbp)
	movq	%rsi, -168(%rbp)
	xorq	%rsi, -280(%rbp)
	movq	-280(%rbp), %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	orq	%rcx, %rax
	movq	%rdi, -176(%rbp)
	xorq	%rdi, -288(%rbp)
	movq	-288(%rbp), %rcx
	xorq	%rcx, -104(%rbp)
	movq	-104(%rbp), %rsi
	movq	%rsi, -104(%rbp)
	orq	%rsi, %rcx
	movq	%rdx, -184(%rbp)
	xorq	%rdx, -296(%rbp)
	movq	-296(%rbp), %rsi
	xorq	%rsi, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	orq	%rdi, %rsi
	movq	%r11, -192(%rbp)
	xorq	%r11, -264(%rbp)
	movq	-264(%rbp), %rdi
	xorq	%rdi, -120(%rbp)
	movq	-120(%rbp), %rdx
	movq	%rdx, -120(%rbp)
	orq	%rdx, %rdi
	movq	%r8, -200(%rbp)
	xorq	%r8, -272(%rbp)
	movq	-272(%rbp), %rdx
	xorq	%rdx, -128(%rbp)
	movq	-128(%rbp), %r11
	movq	%r11, -128(%rbp)
	orq	%r11, %rdx
	movq	%r9, -208(%rbp)
	xorq	%r9, -304(%rbp)
	movq	-304(%rbp), %r8
	xorq	%r8, -136(%rbp)
	movq	-136(%rbp), %r9
	movq	%r9, -136(%rbp)
	orq	%r9, %r8
	movq	%r10, -216(%rbp)
	movq	%r10, %r11
	xorq	%r11, -312(%rbp)
	movq	-312(%rbp), %r9
	xorq	%r9, -144(%rbp)
	movq	-144(%rbp), %r11
	movq	%r11, -144(%rbp)
	orq	%r11, %r9
	movq	-224(%rbp), %r11
	xorq	%r11, -320(%rbp)
	movq	-320(%rbp), %r10
	xorq	%r10, -152(%rbp)
	movq	-152(%rbp), %r11
	movq	%r11, -152(%rbp)
	orq	%r11, %r10
	movq	%r10, -320(%rbp)
	movq	-232(%rbp), %r10
	xorq	%r10, -328(%rbp)
	movq	-328(%rbp), %r11
	xorq	%r11, -160(%rbp)
	movq	-160(%rbp), %r10
	movq	%r10, -160(%rbp)
	orq	%r10, %r11
	movq	-240(%rbp), %r10
	xorq	%r10, -88(%rbp)
	movq	-88(%rbp), %r10
	xorq	%r10, %r15
	orq	%r15, %r10
	movq	%r10, -88(%rbp)
	xorq	-248(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-256(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %r10
	xorq	%r10, %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %r10
	movq	%r10, -96(%rbp)
	orq	%r10, %rax
	movq	-176(%rbp), %r10
	xorq	%r10, %rcx
	xorq	%rcx, -104(%rbp)
	movq	-104(%rbp), %r10
	movq	%r10, -104(%rbp)
	orq	%r10, %rcx
	movq	-184(%rbp), %r10
	xorq	%r10, %rsi
	xorq	%rsi, -112(%rbp)
	movq	-112(%rbp), %r10
	movq	%r10, -112(%rbp)
	orq	%r10, %rsi
	movq	-192(%rbp), %r10
	xorq	%r10, %rdi
	xorq	%rdi, -120(%rbp)
	movq	-120(%rbp), %r10
	movq	%r10, -120(%rbp)
	orq	%r10, %rdi
	movq	-200(%rbp), %r10
	xorq	%r10, %rdx
	xorq	%rdx, -128(%rbp)
	movq	-128(%rbp), %r10
	movq	%r10, -128(%rbp)
	orq	%r10, %rdx
	movq	-208(%rbp), %r10
	xorq	%r10, %r8
	xorq	%r8, -136(%rbp)
	movq	-136(%rbp), %r10
	movq	%r10, -136(%rbp)
	orq	%r10, %r8
	movq	-216(%rbp), %r10
	xorq	%r10, %r9
	xorq	%r9, -144(%rbp)
	movq	-144(%rbp), %r10
	movq	%r10, -144(%rbp)
	orq	%r10, %r9
	movq	%r9, -312(%rbp)
	movq	-224(%rbp), %r9
	xorq	%r9, -320(%rbp)
	movq	-320(%rbp), %r10
	xorq	%r10, -152(%rbp)
	movq	-152(%rbp), %r9
	movq	%r9, -152(%rbp)
	orq	%r9, %r10
	movq	-232(%rbp), %r9
	xorq	%r9, %r11
	xorq	%r11, -160(%rbp)
	movq	-160(%rbp), %r9
	movq	%r9, -160(%rbp)
	orq	%r9, %r11
	movq	-240(%rbp), %r9
	xorq	%r9, -88(%rbp)
	movq	-88(%rbp), %r9
	xorq	%r9, %r15
	orq	%r15, %r9
	movq	%r9, -88(%rbp)
	xorq	-248(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-256(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %r9
	xorq	%r9, %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %r9
	movq	%r9, -96(%rbp)
	orq	%r9, %rax
	movq	-176(%rbp), %r9
	xorq	%r9, %rcx
	xorq	%rcx, -104(%rbp)
	movq	-104(%rbp), %r9
	movq	%r9, -104(%rbp)
	orq	%r9, %rcx
	movq	-184(%rbp), %r9
	xorq	%r9, %rsi
	xorq	%rsi, -112(%rbp)
	movq	-112(%rbp), %r9
	movq	%r9, -112(%rbp)
	orq	%r9, %rsi
	movq	-192(%rbp), %r9
	xorq	%r9, %rdi
	xorq	%rdi, -120(%rbp)
	movq	-120(%rbp), %r9
	movq	%r9, -120(%rbp)
	orq	%r9, %rdi
	movq	-200(%rbp), %r9
	xorq	%r9, %rdx
	xorq	%rdx, -128(%rbp)
	movq	-128(%rbp), %r9
	movq	%r9, -128(%rbp)
	orq	%r9, %rdx
	movq	-208(%rbp), %r9
	xorq	%r9, %r8
	xorq	%r8, -136(%rbp)
	movq	-136(%rbp), %r9
	movq	%r9, -136(%rbp)
	orq	%r9, %r8
	movq	%r8, -304(%rbp)
	movq	-216(%rbp), %r8
	xorq	%r8, -312(%rbp)
	movq	-312(%rbp), %r9
	xorq	%r9, -144(%rbp)
	movq	-144(%rbp), %r8
	movq	%r8, -144(%rbp)
	orq	%r8, %r9
	movq	-224(%rbp), %r8
	xorq	%r8, %r10
	xorq	%r10, -152(%rbp)
	movq	-152(%rbp), %r8
	movq	%r8, -152(%rbp)
	orq	%r8, %r10
	movq	-232(%rbp), %r8
	xorq	%r8, %r11
	xorq	%r11, -160(%rbp)
	movq	-160(%rbp), %r8
	movq	%r8, -160(%rbp)
	orq	%r8, %r11
	movq	-240(%rbp), %r8
	xorq	%r8, -88(%rbp)
	movq	-88(%rbp), %r8
	xorq	%r8, %r15
	orq	%r15, %r8
	movq	%r8, -88(%rbp)
	xorq	-248(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-256(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %r8
	xorq	%r8, %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	orq	%r8, %rax
	movq	-176(%rbp), %r8
	xorq	%r8, %rcx
	xorq	%rcx, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	orq	%r8, %rcx
	movq	-184(%rbp), %r8
	xorq	%r8, %rsi
	xorq	%rsi, -112(%rbp)
	movq	-112(%rbp), %r8
	movq	%r8, -112(%rbp)
	orq	%r8, %rsi
	movq	-192(%rbp), %r8
	xorq	%r8, %rdi
	xorq	%rdi, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	orq	%r8, %rdi
	movq	-200(%rbp), %r8
	xorq	%r8, %rdx
	xorq	%rdx, -128(%rbp)
	movq	-128(%rbp), %r8
	movq	%r8, -128(%rbp)
	orq	%r8, %rdx
	movq	%rdx, -272(%rbp)
	movq	-208(%rbp), %rdx
	xorq	%rdx, -304(%rbp)
	movq	-304(%rbp), %r8
	xorq	%r8, -136(%rbp)
	movq	-136(%rbp), %rdx
	movq	%rdx, -136(%rbp)
	orq	%rdx, %r8
	movq	-216(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -144(%rbp)
	movq	-144(%rbp), %rdx
	movq	%rdx, -144(%rbp)
	orq	%rdx, %r9
	movq	-224(%rbp), %rdx
	xorq	%rdx, %r10
	xorq	%r10, -152(%rbp)
	movq	-152(%rbp), %rdx
	movq	%rdx, -152(%rbp)
	orq	%rdx, %r10
	movq	-232(%rbp), %rdx
	xorq	%rdx, %r11
	xorq	%r11, -160(%rbp)
	movq	-160(%rbp), %rdx
	movq	%rdx, -160(%rbp)
	orq	%rdx, %r11
	movq	-240(%rbp), %rdx
	xorq	%rdx, -88(%rbp)
	movq	-88(%rbp), %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	movq	%rdx, -88(%rbp)
	xorq	-248(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-256(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %rdx
	xorq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %rdx
	movq	%rdx, -96(%rbp)
	orq	%rdx, %rax
	movq	-176(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, -104(%rbp)
	movq	-104(%rbp), %rdx
	movq	%rdx, -104(%rbp)
	orq	%rdx, %rcx
	movq	-184(%rbp), %rdx
	xorq	%rdx, %rsi
	xorq	%rsi, -112(%rbp)
	movq	-112(%rbp), %rdx
	movq	%rdx, -112(%rbp)
	orq	%rdx, %rsi
	movq	-192(%rbp), %rdx
	xorq	%rdx, %rdi
	xorq	%rdi, -120(%rbp)
	movq	-120(%rbp), %rdx
	movq	%rdx, -120(%rbp)
	orq	%rdx, %rdi
	movq	%rdi, -264(%rbp)
	movq	-200(%rbp), %rdi
	xorq	%rdi, -272(%rbp)
	movq	-272(%rbp), %rdx
	xorq	%rdx, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	orq	%rdi, %rdx
	movq	-208(%rbp), %rdi
	xorq	%rdi, %r8
	xorq	%r8, -136(%rbp)
	movq	-136(%rbp), %rdi
	movq	%rdi, -136(%rbp)
	orq	%rdi, %r8
	movq	-216(%rbp), %rdi
	xorq	%rdi, %r9
	xorq	%r9, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	orq	%rdi, %r9
	movq	-224(%rbp), %rdi
	xorq	%rdi, %r10
	xorq	%r10, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	orq	%rdi, %r10
	movq	-232(%rbp), %rdi
	xorq	%rdi, %r11
	xorq	%r11, -160(%rbp)
	movq	-160(%rbp), %rdi
	movq	%rdi, -160(%rbp)
	orq	%rdi, %r11
	movq	-240(%rbp), %rdi
	xorq	%rdi, -88(%rbp)
	movq	-88(%rbp), %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	movq	%rdi, -88(%rbp)
	xorq	-248(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-256(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %rdi
	xorq	%rdi, %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	orq	%rdi, %rax
	movq	-176(%rbp), %rdi
	xorq	%rdi, %rcx
	xorq	%rcx, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	orq	%rdi, %rcx
	movq	-184(%rbp), %rdi
	xorq	%rdi, %rsi
	xorq	%rsi, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	orq	%rdi, %rsi
	movq	%rsi, -296(%rbp)
	movq	-192(%rbp), %rsi
	xorq	%rsi, -264(%rbp)
	movq	-264(%rbp), %rdi
	xorq	%rdi, -120(%rbp)
	movq	-120(%rbp), %rsi
	movq	%rsi, -120(%rbp)
	orq	%rsi, %rdi
	movq	-200(%rbp), %rsi
	xorq	%rsi, %rdx
	xorq	%rdx, -128(%rbp)
	movq	-128(%rbp), %rsi
	movq	%rsi, -128(%rbp)
	orq	%rsi, %rdx
	movq	-208(%rbp), %rsi
	xorq	%rsi, %r8
	xorq	%r8, -136(%rbp)
	movq	-136(%rbp), %rsi
	movq	%rsi, -136(%rbp)
	orq	%rsi, %r8
	movq	-216(%rbp), %rsi
	xorq	%rsi, %r9
	xorq	%r9, -144(%rbp)
	movq	-144(%rbp), %rsi
	movq	%rsi, -144(%rbp)
	orq	%rsi, %r9
	movq	-224(%rbp), %rsi
	xorq	%rsi, %r10
	xorq	%r10, -152(%rbp)
	movq	-152(%rbp), %rsi
	movq	%rsi, -152(%rbp)
	orq	%rsi, %r10
	movq	-232(%rbp), %rsi
	xorq	%rsi, %r11
	xorq	%r11, -160(%rbp)
	movq	-160(%rbp), %rsi
	movq	%rsi, -160(%rbp)
	orq	%rsi, %r11
	movq	-240(%rbp), %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %rsi
	xorq	%rsi, %r15
	orq	%r15, %rsi
	movq	%rsi, -88(%rbp)
	xorq	-248(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-256(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %rsi
	xorq	%rsi, %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %rsi
	movq	%rsi, -96(%rbp)
	orq	%rsi, %rax
	movq	%rax, -280(%rbp)
	movq	-176(%rbp), %rax
	xorq	%rax, %rcx
	xorq	%rcx, -104(%rbp)
	movq	-104(%rbp), %rsi
	movq	%rsi, -104(%rbp)
	orq	%rsi, %rcx
	movq	-184(%rbp), %rax
	xorq	%rax, -296(%rbp)
	movq	-296(%rbp), %rsi
	xorq	%rsi, -112(%rbp)
	movq	-112(%rbp), %rax
	movq	%rax, -112(%rbp)
	orq	%rax, %rsi
	movq	-192(%rbp), %rax
	xorq	%rax, %rdi
	xorq	%rdi, -120(%rbp)
	movq	-120(%rbp), %rax
	movq	%rax, -120(%rbp)
	orq	%rax, %rdi
	movq	-200(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -128(%rbp)
	movq	-128(%rbp), %rax
	movq	%rax, -128(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -272(%rbp)
	movq	-208(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -136(%rbp)
	movq	-136(%rbp), %rdx
	movq	%rdx, -136(%rbp)
	orq	%rdx, %r8
	movq	-216(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -144(%rbp)
	movq	-144(%rbp), %rdx
	movq	%rdx, -144(%rbp)
	orq	%rdx, %r9
	movq	-224(%rbp), %rdx
	xorq	%rdx, %r10
	xorq	%r10, -152(%rbp)
	movq	-152(%rbp), %rax
	movq	%rax, -152(%rbp)
	orq	%rax, %r10
	movq	-232(%rbp), %rdx
	xorq	%rdx, %r11
	xorq	%r11, -160(%rbp)
	movq	-160(%rbp), %rdx
	movq	%rdx, -160(%rbp)
	orq	%rdx, %r11
	movq	-240(%rbp), %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	movq	%rdx, -88(%rbp)
	xorq	-248(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-256(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %rdx
	xorq	%rdx, -280(%rbp)
	movq	-280(%rbp), %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %rdx
	movq	%rdx, -96(%rbp)
	orq	%rdx, %rax
	movq	-176(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, -104(%rbp)
	movq	-104(%rbp), %rdx
	movq	%rdx, -104(%rbp)
	orq	%rdx, %rcx
	movq	-184(%rbp), %rdx
	xorq	%rdx, %rsi
	xorq	%rsi, -112(%rbp)
	movq	-112(%rbp), %rdx
	movq	%rdx, -112(%rbp)
	orq	%rdx, %rsi
	movq	-192(%rbp), %rdx
	xorq	%rdx, %rdi
	xorq	%rdi, -120(%rbp)
	movq	-120(%rbp), %rdx
	movq	%rdx, -120(%rbp)
	orq	%rdx, %rdi
	movq	%rdi, -264(%rbp)
	movq	-200(%rbp), %rdi
	xorq	%rdi, -272(%rbp)
	movq	-272(%rbp), %rdx
	xorq	%rdx, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	orq	%rdi, %rdx
	movq	-208(%rbp), %rdi
	xorq	%rdi, %r8
	xorq	%r8, -136(%rbp)
	movq	-136(%rbp), %rdi
	movq	%rdi, -136(%rbp)
	orq	%rdi, %r8
	movq	-216(%rbp), %rdi
	xorq	%rdi, %r9
	xorq	%r9, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	orq	%rdi, %r9
	movq	-224(%rbp), %rdi
	xorq	%rdi, %r10
	xorq	%r10, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	orq	%rdi, %r10
	movq	-232(%rbp), %rdi
	xorq	%rdi, %r11
	xorq	%r11, -160(%rbp)
	movq	-160(%rbp), %rdi
	movq	%rdi, -160(%rbp)
	orq	%rdi, %r11
	movq	-240(%rbp), %rdi
	xorq	%rdi, -88(%rbp)
	movq	-88(%rbp), %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	movq	%rdi, -88(%rbp)
	xorq	-248(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-256(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %rdi
	xorq	%rdi, %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	orq	%rdi, %rax
	movq	-176(%rbp), %rdi
	xorq	%rdi, %rcx
	xorq	%rcx, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	orq	%rdi, %rcx
	movq	%rcx, -288(%rbp)
	movq	-184(%rbp), %rcx
	xorq	%rcx, %rsi
	xorq	%rsi, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	orq	%rcx, %rsi
	movq	-192(%rbp), %rcx
	xorq	%rcx, -264(%rbp)
	movq	-264(%rbp), %rdi
	xorq	%rdi, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	orq	%rcx, %rdi
	movq	-200(%rbp), %rcx
	xorq	%rcx, %rdx
	xorq	%rdx, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	orq	%rcx, %rdx
	movq	-208(%rbp), %rcx
	xorq	%rcx, %r8
	xorq	%r8, -136(%rbp)
	movq	-136(%rbp), %rcx
	movq	%rcx, -136(%rbp)
	orq	%rcx, %r8
	movq	-216(%rbp), %rcx
	xorq	%rcx, %r9
	xorq	%r9, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	orq	%rcx, %r9
	movq	-224(%rbp), %rcx
	xorq	%rcx, %r10
	xorq	%r10, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	orq	%rcx, %r10
	movq	-232(%rbp), %rcx
	xorq	%rcx, %r11
	xorq	%r11, -160(%rbp)
	movq	-160(%rbp), %rcx
	movq	%rcx, -160(%rbp)
	orq	%rcx, %r11
	movq	-240(%rbp), %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %rcx
	xorq	%rcx, %r15
	orq	%r15, %rcx
	movq	%rcx, -88(%rbp)
	xorq	-248(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-256(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-168(%rbp), %rcx
	xorq	%rcx, %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	orq	%rcx, %rax
	movq	%rax, -280(%rbp)
	movq	-176(%rbp), %rax
	xorq	%rax, -288(%rbp)
	movq	-288(%rbp), %rcx
	xorq	%rcx, -104(%rbp)
	movq	-104(%rbp), %rax
	movq	%rax, -104(%rbp)
	orq	%rax, %rcx
	movq	%rcx, -288(%rbp)
	movq	-184(%rbp), %rax
	xorq	%rax, %rsi
	xorq	%rsi, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	orq	%rcx, %rsi
	movq	%rsi, -296(%rbp)
	movq	-192(%rbp), %rsi
	xorq	%rsi, %rdi
	xorq	%rdi, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	orq	%rcx, %rdi
	movq	%rdi, -264(%rbp)
	movq	-200(%rbp), %rsi
	xorq	%rsi, %rdx
	xorq	%rdx, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	orq	%rcx, %rdx
	movq	%rdx, -272(%rbp)
	movq	-208(%rbp), %rdi
	xorq	%rdi, %r8
	xorq	%r8, -136(%rbp)
	movq	-136(%rbp), %rcx
	movq	%rcx, -136(%rbp)
	orq	%rcx, %r8
	movq	%r8, -304(%rbp)
	movq	-216(%rbp), %rdi
	xorq	%rdi, %r9
	xorq	%r9, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	orq	%rcx, %r9
	movq	%r9, -312(%rbp)
	movq	-224(%rbp), %rdi
	xorq	%rdi, %r10
	xorq	%r10, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	orq	%rcx, %r10
	movq	%r10, -320(%rbp)
	movq	-232(%rbp), %rdi
	xorq	%rdi, %r11
	xorq	%r11, -160(%rbp)
	movq	-160(%rbp), %rcx
	movq	%rcx, -160(%rbp)
	orq	%rcx, %r11
	movq	%r11, -328(%rbp)
	movq	-240(%rbp), %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %rax
	xorq	%rax, %r15
	orq	%r15, %rax
	movq	%rax, -88(%rbp)
	xorq	-248(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-256(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
.L292:
	movq	-336(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -336(%rbp)
	testq	%rax, %rax
	jne	.L293
	movl	-280(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-288(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-296(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-264(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-272(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-304(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-312(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-320(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-328(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$296, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE100:
	.size	int64_bit_11, .-int64_bit_11
	.globl	int64_bit_12
	.type	int64_bit_12, @function
int64_bit_12:
.LFB101:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$328, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -360(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	addq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -112(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -312(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -320(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$2, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$3, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -240(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -328(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$4, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -248(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -336(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$5, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -256(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$32, %rax
	orq	%rax, %rsi
	movq	%rsi, -344(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$6, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -160(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -264(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -352(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$7, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -168(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -272(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$32, %rax
	orq	%rax, %rsi
	movq	%rsi, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$8, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -176(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -280(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$9, %rax
	movq	%rax, %r15
	movq	%r15, %rax
	salq	$32, %rax
	orq	%rax, %r15
	leaq	0(,%r15,4), %rax
	subq	$1, %rax
	movq	%rax, -288(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	subl	$10, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$32, %rax
	orq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$10, %rax
	movq	%rax, %r13
	movq	%r13, %rax
	salq	$32, %rax
	orq	%rax, %r13
	leaq	0(,%r13,4), %rax
	subq	$1, %rax
	movq	%rax, -296(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	subl	$11, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$32, %rax
	orq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$11, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -304(%rbp)
	jmp	.L295
.L296:
	subq	$1, -208(%rbp)
	movq	-208(%rbp), %rsi
	subq	$1, -216(%rbp)
	movq	-216(%rbp), %rdi
	subq	$1, -224(%rbp)
	movq	-224(%rbp), %rdx
	subq	$1, -232(%rbp)
	movq	-232(%rbp), %r11
	subq	$1, -240(%rbp)
	movq	-240(%rbp), %r8
	subq	$1, -248(%rbp)
	movq	-248(%rbp), %r9
	subq	$1, -256(%rbp)
	movq	-256(%rbp), %r10
	subq	$1, -264(%rbp)
	subq	$1, -272(%rbp)
	subq	$1, -280(%rbp)
	subq	$1, -288(%rbp)
	subq	$1, -296(%rbp)
	subq	$1, -304(%rbp)
	movq	%rsi, -208(%rbp)
	xorq	%rsi, -184(%rbp)
	movq	-184(%rbp), %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %rsi
	movq	%rsi, -104(%rbp)
	orq	%rsi, %rax
	movq	%rdi, -216(%rbp)
	xorq	%rdi, -192(%rbp)
	movq	-192(%rbp), %rcx
	xorq	%rcx, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	orq	%rdi, %rcx
	movq	%rcx, -192(%rbp)
	movq	%rdx, -224(%rbp)
	xorq	%rdx, -312(%rbp)
	movq	-312(%rbp), %rsi
	xorq	%rsi, -120(%rbp)
	movq	-120(%rbp), %rdx
	movq	%rdx, -120(%rbp)
	orq	%rdx, %rsi
	movq	%r11, -232(%rbp)
	xorq	%r11, -320(%rbp)
	movq	-320(%rbp), %rdi
	xorq	%rdi, -128(%rbp)
	movq	-128(%rbp), %r11
	movq	%r11, -128(%rbp)
	orq	%r11, %rdi
	movq	%r8, -240(%rbp)
	xorq	%r8, -200(%rbp)
	movq	-200(%rbp), %rdx
	xorq	%rdx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	orq	%r8, %rdx
	movq	%r9, -248(%rbp)
	xorq	%r9, -328(%rbp)
	movq	-328(%rbp), %r8
	xorq	%r8, -144(%rbp)
	movq	-144(%rbp), %r9
	movq	%r9, -144(%rbp)
	orq	%r9, %r8
	movq	%r10, -256(%rbp)
	movq	%r10, %r11
	xorq	%r11, -336(%rbp)
	movq	-336(%rbp), %r9
	xorq	%r9, -152(%rbp)
	movq	-152(%rbp), %r11
	movq	%r11, -152(%rbp)
	orq	%r11, %r9
	movq	-264(%rbp), %r11
	xorq	%r11, -344(%rbp)
	movq	-344(%rbp), %r10
	xorq	%r10, -160(%rbp)
	movq	-160(%rbp), %r11
	movq	%r11, -160(%rbp)
	orq	%r11, %r10
	movq	%r10, -344(%rbp)
	movq	-272(%rbp), %r10
	xorq	%r10, -352(%rbp)
	movq	-352(%rbp), %r11
	xorq	%r11, -168(%rbp)
	movq	-168(%rbp), %r10
	movq	%r10, -168(%rbp)
	orq	%r10, %r11
	movq	-280(%rbp), %r10
	xorq	%r10, -88(%rbp)
	movq	-88(%rbp), %r10
	movq	%r10, %rcx
	xorq	%r10, -176(%rbp)
	movq	-176(%rbp), %r10
	movq	%r10, -176(%rbp)
	orq	%r10, %rcx
	movq	%rcx, -88(%rbp)
	movq	-288(%rbp), %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %r10
	xorq	%r10, %r15
	orq	%r15, %r10
	movq	%r10, -96(%rbp)
	xorq	-296(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-304(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-208(%rbp), %r10
	xorq	%r10, %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %r10
	movq	%r10, -104(%rbp)
	orq	%r10, %rax
	movq	%rax, -184(%rbp)
	movq	-216(%rbp), %r10
	xorq	%r10, -192(%rbp)
	movq	-192(%rbp), %rcx
	xorq	%rcx, -112(%rbp)
	movq	-112(%rbp), %r10
	movq	%r10, -112(%rbp)
	orq	%r10, %rcx
	movq	-224(%rbp), %r10
	xorq	%r10, %rsi
	xorq	%rsi, -120(%rbp)
	movq	-120(%rbp), %r10
	movq	%r10, -120(%rbp)
	orq	%r10, %rsi
	movq	-232(%rbp), %r10
	xorq	%r10, %rdi
	xorq	%rdi, -128(%rbp)
	movq	-128(%rbp), %r10
	movq	%r10, -128(%rbp)
	orq	%r10, %rdi
	movq	-240(%rbp), %r10
	xorq	%r10, %rdx
	xorq	%rdx, -136(%rbp)
	movq	-136(%rbp), %r10
	movq	%r10, -136(%rbp)
	orq	%r10, %rdx
	movq	-248(%rbp), %r10
	xorq	%r10, %r8
	xorq	%r8, -144(%rbp)
	movq	-144(%rbp), %r10
	movq	%r10, -144(%rbp)
	orq	%r10, %r8
	movq	-256(%rbp), %r10
	xorq	%r10, %r9
	xorq	%r9, -152(%rbp)
	movq	-152(%rbp), %r10
	movq	%r10, -152(%rbp)
	orq	%r10, %r9
	movq	%r9, -336(%rbp)
	movq	-264(%rbp), %r9
	xorq	%r9, -344(%rbp)
	movq	-344(%rbp), %r10
	xorq	%r10, -160(%rbp)
	movq	-160(%rbp), %r9
	movq	%r9, -160(%rbp)
	orq	%r9, %r10
	movq	-272(%rbp), %r9
	xorq	%r9, %r11
	xorq	%r11, -168(%rbp)
	movq	-168(%rbp), %r9
	movq	%r9, -168(%rbp)
	orq	%r9, %r11
	movq	-280(%rbp), %r9
	xorq	%r9, -88(%rbp)
	movq	-88(%rbp), %r9
	movq	%r9, %rax
	xorq	%r9, -176(%rbp)
	movq	-176(%rbp), %r9
	movq	%r9, -176(%rbp)
	orq	%r9, %rax
	movq	%rax, -88(%rbp)
	movq	-288(%rbp), %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %r9
	xorq	%r9, %r15
	orq	%r15, %r9
	movq	%r9, -96(%rbp)
	xorq	-296(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-304(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-208(%rbp), %r9
	xorq	%r9, -184(%rbp)
	movq	-184(%rbp), %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %r9
	movq	%r9, -104(%rbp)
	orq	%r9, %rax
	movq	-216(%rbp), %r9
	xorq	%r9, %rcx
	xorq	%rcx, -112(%rbp)
	movq	-112(%rbp), %r9
	movq	%r9, -112(%rbp)
	orq	%r9, %rcx
	movq	-224(%rbp), %r9
	xorq	%r9, %rsi
	xorq	%rsi, -120(%rbp)
	movq	-120(%rbp), %r9
	movq	%r9, -120(%rbp)
	orq	%r9, %rsi
	movq	%rsi, -312(%rbp)
	movq	-232(%rbp), %r9
	xorq	%r9, %rdi
	xorq	%rdi, -128(%rbp)
	movq	-128(%rbp), %r9
	movq	%r9, -128(%rbp)
	orq	%r9, %rdi
	movq	-240(%rbp), %r9
	xorq	%r9, %rdx
	xorq	%rdx, -136(%rbp)
	movq	-136(%rbp), %r9
	movq	%r9, -136(%rbp)
	orq	%r9, %rdx
	movq	-248(%rbp), %r9
	xorq	%r9, %r8
	xorq	%r8, -144(%rbp)
	movq	-144(%rbp), %r9
	movq	%r9, -144(%rbp)
	orq	%r9, %r8
	movq	%r8, -328(%rbp)
	movq	-256(%rbp), %r8
	xorq	%r8, -336(%rbp)
	movq	-336(%rbp), %r9
	xorq	%r9, -152(%rbp)
	movq	-152(%rbp), %r8
	movq	%r8, -152(%rbp)
	orq	%r8, %r9
	movq	-264(%rbp), %r8
	xorq	%r8, %r10
	xorq	%r10, -160(%rbp)
	movq	-160(%rbp), %r8
	movq	%r8, -160(%rbp)
	orq	%r8, %r10
	movq	-272(%rbp), %r8
	xorq	%r8, %r11
	xorq	%r11, -168(%rbp)
	movq	-168(%rbp), %r8
	movq	%r8, -168(%rbp)
	orq	%r8, %r11
	movq	-280(%rbp), %r8
	xorq	%r8, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, %rsi
	xorq	%r8, -176(%rbp)
	movq	-176(%rbp), %r8
	movq	%r8, -176(%rbp)
	orq	%r8, %rsi
	movq	%rsi, -88(%rbp)
	movq	-288(%rbp), %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %r8
	xorq	%r8, %r15
	orq	%r15, %r8
	movq	%r8, -96(%rbp)
	xorq	-296(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-304(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-208(%rbp), %r8
	xorq	%r8, %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	orq	%r8, %rax
	movq	-216(%rbp), %r8
	xorq	%r8, %rcx
	xorq	%rcx, -112(%rbp)
	movq	-112(%rbp), %r8
	movq	%r8, -112(%rbp)
	orq	%r8, %rcx
	movq	%rcx, -192(%rbp)
	movq	-224(%rbp), %r8
	xorq	%r8, -312(%rbp)
	movq	-312(%rbp), %rsi
	xorq	%rsi, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	orq	%r8, %rsi
	movq	-232(%rbp), %r8
	xorq	%r8, %rdi
	xorq	%rdi, -128(%rbp)
	movq	-128(%rbp), %r8
	movq	%r8, -128(%rbp)
	orq	%r8, %rdi
	movq	-240(%rbp), %r8
	xorq	%r8, %rdx
	xorq	%rdx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	orq	%r8, %rdx
	movq	%rdx, -200(%rbp)
	movq	-248(%rbp), %rdx
	xorq	%rdx, -328(%rbp)
	movq	-328(%rbp), %r8
	xorq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdx
	movq	%rdx, -144(%rbp)
	orq	%rdx, %r8
	movq	-256(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -152(%rbp)
	movq	-152(%rbp), %rdx
	movq	%rdx, -152(%rbp)
	orq	%rdx, %r9
	movq	-264(%rbp), %rdx
	xorq	%rdx, %r10
	xorq	%r10, -160(%rbp)
	movq	-160(%rbp), %rdx
	movq	%rdx, -160(%rbp)
	orq	%rdx, %r10
	movq	-272(%rbp), %rdx
	xorq	%rdx, %r11
	xorq	%r11, -168(%rbp)
	movq	-168(%rbp), %rdx
	movq	%rdx, -168(%rbp)
	orq	%rdx, %r11
	movq	-280(%rbp), %rdx
	xorq	%rdx, -88(%rbp)
	movq	-88(%rbp), %rdx
	movq	%rdx, %rcx
	xorq	%rdx, -176(%rbp)
	movq	-176(%rbp), %rdx
	movq	%rdx, -176(%rbp)
	orq	%rdx, %rcx
	movq	%rcx, -88(%rbp)
	movq	-288(%rbp), %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	movq	%rdx, -96(%rbp)
	xorq	-296(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-304(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-208(%rbp), %rdx
	xorq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %rdx
	movq	%rdx, -104(%rbp)
	orq	%rdx, %rax
	movq	-216(%rbp), %rdx
	xorq	%rdx, -192(%rbp)
	movq	-192(%rbp), %rcx
	xorq	%rcx, -112(%rbp)
	movq	-112(%rbp), %rdx
	movq	%rdx, -112(%rbp)
	orq	%rdx, %rcx
	movq	-224(%rbp), %rdx
	xorq	%rdx, %rsi
	xorq	%rsi, -120(%rbp)
	movq	-120(%rbp), %rdx
	movq	%rdx, -120(%rbp)
	orq	%rdx, %rsi
	movq	-232(%rbp), %rdx
	xorq	%rdx, %rdi
	xorq	%rdi, -128(%rbp)
	movq	-128(%rbp), %rdx
	movq	%rdx, -128(%rbp)
	orq	%rdx, %rdi
	movq	%rdi, -320(%rbp)
	movq	-240(%rbp), %rdi
	xorq	%rdi, -200(%rbp)
	movq	-200(%rbp), %rdx
	xorq	%rdx, -136(%rbp)
	movq	-136(%rbp), %rdi
	movq	%rdi, -136(%rbp)
	orq	%rdi, %rdx
	movq	-248(%rbp), %rdi
	xorq	%rdi, %r8
	xorq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	orq	%rdi, %r8
	movq	-256(%rbp), %rdi
	xorq	%rdi, %r9
	xorq	%r9, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	orq	%rdi, %r9
	movq	%r9, -336(%rbp)
	movq	-264(%rbp), %rdi
	xorq	%rdi, %r10
	xorq	%r10, -160(%rbp)
	movq	-160(%rbp), %rdi
	movq	%rdi, -160(%rbp)
	orq	%rdi, %r10
	movq	-272(%rbp), %rdi
	xorq	%rdi, %r11
	xorq	%r11, -168(%rbp)
	movq	-168(%rbp), %rdi
	movq	%rdi, -168(%rbp)
	orq	%rdi, %r11
	movq	-280(%rbp), %rdi
	xorq	%rdi, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, %r9
	xorq	%rdi, -176(%rbp)
	movq	-176(%rbp), %rdi
	movq	%rdi, -176(%rbp)
	orq	%rdi, %r9
	movq	%r9, -88(%rbp)
	movq	-288(%rbp), %r9
	xorq	%r9, -96(%rbp)
	movq	-96(%rbp), %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	movq	%rdi, -96(%rbp)
	xorq	-296(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-304(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-208(%rbp), %rdi
	xorq	%rdi, %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	orq	%rdi, %rax
	movq	%rax, -184(%rbp)
	movq	-216(%rbp), %rax
	xorq	%rax, %rcx
	xorq	%rcx, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	orq	%rdi, %rcx
	movq	-224(%rbp), %rax
	xorq	%rax, %rsi
	xorq	%rsi, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	orq	%rdi, %rsi
	movq	-232(%rbp), %rax
	xorq	%rax, -320(%rbp)
	movq	-320(%rbp), %rdi
	xorq	%rdi, -128(%rbp)
	movq	-128(%rbp), %r9
	movq	%r9, -128(%rbp)
	movq	%r9, %rax
	orq	%rax, %rdi
	movq	-240(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -136(%rbp)
	movq	-136(%rbp), %rax
	movq	%rax, -136(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -200(%rbp)
	movq	-248(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -144(%rbp)
	movq	-144(%rbp), %rax
	movq	%rax, -144(%rbp)
	orq	%rax, %r8
	movq	-256(%rbp), %rdx
	xorq	%rdx, -336(%rbp)
	movq	-336(%rbp), %r9
	xorq	%r9, -152(%rbp)
	movq	-152(%rbp), %rdx
	movq	%rdx, -152(%rbp)
	orq	%rdx, %r9
	movq	-264(%rbp), %rax
	xorq	%rax, %r10
	xorq	%r10, -160(%rbp)
	movq	-160(%rbp), %rax
	movq	%rax, -160(%rbp)
	orq	%rax, %r10
	movq	-272(%rbp), %rax
	xorq	%rax, %r11
	xorq	%r11, -168(%rbp)
	movq	-168(%rbp), %rdx
	movq	%rdx, -168(%rbp)
	orq	%rdx, %r11
	movq	-280(%rbp), %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %rdx
	xorq	%rdx, -176(%rbp)
	movq	-176(%rbp), %rax
	movq	%rax, -176(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -88(%rbp)
	movq	-288(%rbp), %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	movq	%rdx, -96(%rbp)
	xorq	-296(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-304(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-208(%rbp), %rdx
	xorq	%rdx, -184(%rbp)
	movq	-184(%rbp), %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %rdx
	movq	%rdx, -104(%rbp)
	orq	%rdx, %rax
	movq	-216(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, -112(%rbp)
	movq	-112(%rbp), %rdx
	movq	%rdx, -112(%rbp)
	orq	%rdx, %rcx
	movq	-224(%rbp), %rdx
	xorq	%rdx, %rsi
	xorq	%rsi, -120(%rbp)
	movq	-120(%rbp), %rdx
	movq	%rdx, -120(%rbp)
	orq	%rdx, %rsi
	movq	%rsi, -312(%rbp)
	movq	-232(%rbp), %rdx
	xorq	%rdx, %rdi
	xorq	%rdi, -128(%rbp)
	movq	-128(%rbp), %rdx
	movq	%rdx, -128(%rbp)
	orq	%rdx, %rdi
	movq	%rdi, -320(%rbp)
	movq	-240(%rbp), %rdi
	xorq	%rdi, -200(%rbp)
	movq	-200(%rbp), %rdx
	xorq	%rdx, -136(%rbp)
	movq	-136(%rbp), %rdi
	movq	%rdi, -136(%rbp)
	orq	%rdi, %rdx
	movq	-248(%rbp), %rdi
	xorq	%rdi, %r8
	xorq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	orq	%rdi, %r8
	movq	-256(%rbp), %rdi
	xorq	%rdi, %r9
	xorq	%r9, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	orq	%rdi, %r9
	movq	-264(%rbp), %rdi
	xorq	%rdi, %r10
	xorq	%r10, -160(%rbp)
	movq	-160(%rbp), %rdi
	movq	%rdi, -160(%rbp)
	orq	%rdi, %r10
	movq	-272(%rbp), %rdi
	xorq	%rdi, %r11
	xorq	%r11, -168(%rbp)
	movq	-168(%rbp), %rdi
	movq	%rdi, -168(%rbp)
	orq	%rdi, %r11
	movq	-280(%rbp), %rdi
	xorq	%rdi, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, %rsi
	xorq	%rdi, -176(%rbp)
	movq	-176(%rbp), %rdi
	movq	%rdi, -176(%rbp)
	orq	%rdi, %rsi
	movq	%rsi, -88(%rbp)
	movq	-288(%rbp), %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	movq	%rdi, -96(%rbp)
	xorq	-296(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-304(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-208(%rbp), %rdi
	xorq	%rdi, %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	orq	%rdi, %rax
	movq	%rax, -184(%rbp)
	movq	-216(%rbp), %rax
	xorq	%rax, %rcx
	xorq	%rcx, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	orq	%rdi, %rcx
	movq	-224(%rbp), %rax
	xorq	%rax, -312(%rbp)
	movq	-312(%rbp), %rsi
	xorq	%rsi, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	orq	%rdi, %rsi
	movq	-232(%rbp), %rax
	xorq	%rax, -320(%rbp)
	movq	-320(%rbp), %rdi
	xorq	%rdi, -128(%rbp)
	movq	-128(%rbp), %rax
	movq	%rax, -128(%rbp)
	orq	%rax, %rdi
	movq	-240(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -136(%rbp)
	movq	-136(%rbp), %rax
	movq	%rax, -136(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -200(%rbp)
	movq	-248(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdx
	movq	%rdx, -144(%rbp)
	orq	%rdx, %r8
	movq	-256(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -152(%rbp)
	movq	-152(%rbp), %rax
	movq	%rax, -152(%rbp)
	orq	%rax, %r9
	movq	-264(%rbp), %rdx
	xorq	%rdx, %r10
	xorq	%r10, -160(%rbp)
	movq	-160(%rbp), %rdx
	movq	%rdx, -160(%rbp)
	orq	%rdx, %r10
	movq	-272(%rbp), %rax
	xorq	%rax, %r11
	xorq	%r11, -168(%rbp)
	movq	-168(%rbp), %rax
	movq	%rax, -168(%rbp)
	orq	%rax, %r11
	movq	-280(%rbp), %rdx
	xorq	%rdx, -88(%rbp)
	movq	-88(%rbp), %rdx
	xorq	%rdx, -176(%rbp)
	movq	-176(%rbp), %rax
	movq	%rax, -176(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -88(%rbp)
	movq	-288(%rbp), %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	movq	%rdx, -96(%rbp)
	xorq	-296(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-304(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-208(%rbp), %rdx
	xorq	%rdx, -184(%rbp)
	movq	-184(%rbp), %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %rdx
	movq	%rdx, -104(%rbp)
	orq	%rdx, %rax
	movq	-216(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, -112(%rbp)
	movq	-112(%rbp), %rdx
	movq	%rdx, -112(%rbp)
	orq	%rdx, %rcx
	movq	%rcx, -192(%rbp)
	movq	-224(%rbp), %rcx
	xorq	%rcx, %rsi
	xorq	%rsi, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	orq	%rcx, %rsi
	movq	-232(%rbp), %rcx
	xorq	%rcx, %rdi
	xorq	%rdi, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	orq	%rcx, %rdi
	movq	-240(%rbp), %rcx
	xorq	%rcx, -200(%rbp)
	movq	-200(%rbp), %rdx
	xorq	%rdx, -136(%rbp)
	movq	-136(%rbp), %rcx
	movq	%rcx, -136(%rbp)
	orq	%rcx, %rdx
	movq	-248(%rbp), %rcx
	xorq	%rcx, %r8
	xorq	%r8, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	orq	%rcx, %r8
	movq	%r8, -328(%rbp)
	movq	-256(%rbp), %rcx
	xorq	%rcx, %r9
	xorq	%r9, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	orq	%rcx, %r9
	movq	-264(%rbp), %rcx
	xorq	%rcx, %r10
	xorq	%r10, -160(%rbp)
	movq	-160(%rbp), %rcx
	movq	%rcx, -160(%rbp)
	orq	%rcx, %r10
	movq	-272(%rbp), %rcx
	xorq	%rcx, %r11
	xorq	%r11, -168(%rbp)
	movq	-168(%rbp), %rcx
	movq	%rcx, -168(%rbp)
	orq	%rcx, %r11
	movq	-280(%rbp), %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, %r8
	xorq	%rcx, -176(%rbp)
	movq	-176(%rbp), %rcx
	movq	%rcx, -176(%rbp)
	orq	%rcx, %r8
	movq	%r8, -88(%rbp)
	movq	-288(%rbp), %r8
	xorq	%r8, -96(%rbp)
	movq	-96(%rbp), %rcx
	xorq	%rcx, %r15
	orq	%r15, %rcx
	movq	%rcx, -96(%rbp)
	xorq	-296(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-304(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-208(%rbp), %rcx
	xorq	%rcx, %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	orq	%rcx, %rax
	movq	%rax, -184(%rbp)
	movq	-216(%rbp), %rax
	xorq	%rax, -192(%rbp)
	movq	-192(%rbp), %rcx
	xorq	%rcx, -112(%rbp)
	movq	-112(%rbp), %rax
	movq	%rax, -112(%rbp)
	orq	%rax, %rcx
	movq	%rcx, -192(%rbp)
	movq	-224(%rbp), %rax
	xorq	%rax, %rsi
	xorq	%rsi, -120(%rbp)
	movq	-120(%rbp), %rax
	movq	%rax, -120(%rbp)
	orq	%rax, %rsi
	movq	%rsi, -312(%rbp)
	movq	-232(%rbp), %rax
	xorq	%rax, %rdi
	xorq	%rdi, -128(%rbp)
	movq	-128(%rbp), %rsi
	movq	%rsi, -128(%rbp)
	orq	%rsi, %rdi
	movq	%rdi, -320(%rbp)
	movq	-240(%rbp), %rdi
	xorq	%rdi, %rdx
	xorq	%rdx, -136(%rbp)
	movq	-136(%rbp), %rsi
	movq	%rsi, -136(%rbp)
	orq	%rsi, %rdx
	movq	%rdx, -200(%rbp)
	movq	-248(%rbp), %rdi
	xorq	%rdi, -328(%rbp)
	movq	-328(%rbp), %r8
	xorq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdx
	movq	%rdx, -144(%rbp)
	orq	%rdx, %r8
	movq	%r8, -328(%rbp)
	movq	-256(%rbp), %rdi
	xorq	%rdi, %r9
	xorq	%r9, -152(%rbp)
	movq	-152(%rbp), %rsi
	movq	%rsi, -152(%rbp)
	orq	%rsi, %r9
	movq	%r9, -336(%rbp)
	movq	-264(%rbp), %rdi
	xorq	%rdi, %r10
	xorq	%r10, -160(%rbp)
	movq	-160(%rbp), %rdx
	movq	%rdx, -160(%rbp)
	orq	%rdx, %r10
	movq	%r10, -344(%rbp)
	movq	-272(%rbp), %rdi
	xorq	%rdi, %r11
	xorq	%r11, -168(%rbp)
	movq	-168(%rbp), %rsi
	movq	%rsi, -168(%rbp)
	orq	%rsi, %r11
	movq	%r11, -352(%rbp)
	movq	-280(%rbp), %rdx
	xorq	%rdx, -88(%rbp)
	movq	-88(%rbp), %rax
	xorq	%rax, -176(%rbp)
	movq	-176(%rbp), %rsi
	movq	%rsi, -176(%rbp)
	orq	%rsi, %rax
	movq	%rax, -88(%rbp)
	movq	-288(%rbp), %r8
	xorq	%r8, -96(%rbp)
	movq	-96(%rbp), %rcx
	xorq	%rcx, %r15
	orq	%r15, %rcx
	movq	%rcx, -96(%rbp)
	xorq	-296(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-304(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
.L295:
	movq	-360(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -360(%rbp)
	testq	%rax, %rax
	jne	.L296
	movl	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-312(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-320(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-328(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-336(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-344(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-352(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$328, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE101:
	.size	int64_bit_12, .-int64_bit_12
	.globl	int64_bit_13
	.type	int64_bit_13, @function
int64_bit_13:
.LFB102:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$344, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -384(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	addq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -112(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -240(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -344(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -248(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$2, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -256(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$3, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -264(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -352(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$4, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -272(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -360(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$5, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -160(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -280(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -368(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$6, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -168(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -288(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -376(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$7, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -176(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -296(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$8, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -184(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -304(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$9, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -192(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -312(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	subl	$10, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$10, %rax
	movq	%rax, %r15
	movq	%r15, %rax
	salq	$32, %rax
	orq	%rax, %r15
	leaq	0(,%r15,4), %rax
	subq	$1, %rax
	movq	%rax, -320(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	subl	$11, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$32, %rax
	orq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$11, %rax
	movq	%rax, %r13
	movq	%r13, %rax
	salq	$32, %rax
	orq	%rax, %r13
	leaq	0(,%r13,4), %rax
	subq	$1, %rax
	movq	%rax, -328(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	subl	$12, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$32, %rax
	orq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$12, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -336(%rbp)
	jmp	.L298
.L299:
	subq	$1, -232(%rbp)
	movq	-232(%rbp), %rsi
	subq	$1, -240(%rbp)
	movq	-240(%rbp), %rdi
	subq	$1, -248(%rbp)
	movq	-248(%rbp), %rdx
	subq	$1, -256(%rbp)
	movq	-256(%rbp), %r11
	subq	$1, -264(%rbp)
	movq	-264(%rbp), %r9
	subq	$1, -272(%rbp)
	movq	-272(%rbp), %r10
	subq	$1, -280(%rbp)
	subq	$1, -288(%rbp)
	subq	$1, -296(%rbp)
	subq	$1, -304(%rbp)
	subq	$1, -312(%rbp)
	subq	$1, -320(%rbp)
	subq	$1, -328(%rbp)
	subq	$1, -336(%rbp)
	movq	%rsi, -232(%rbp)
	xorq	%rsi, -208(%rbp)
	movq	-208(%rbp), %rax
	xorq	%rax, -112(%rbp)
	movq	-112(%rbp), %rsi
	movq	%rsi, -112(%rbp)
	orq	%rsi, %rax
	movq	%rax, -208(%rbp)
	movq	%rdi, -240(%rbp)
	xorq	%rdi, -216(%rbp)
	movq	-216(%rbp), %rcx
	xorq	%rcx, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	orq	%rdi, %rcx
	movq	%rdx, -248(%rbp)
	xorq	%rdx, -344(%rbp)
	movq	-344(%rbp), %rsi
	xorq	%rsi, -128(%rbp)
	movq	-128(%rbp), %rdx
	movq	%rdx, -128(%rbp)
	orq	%rdx, %rsi
	movq	%r11, -256(%rbp)
	xorq	%r11, -224(%rbp)
	movq	-224(%rbp), %rdi
	xorq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r11
	movq	%r11, -136(%rbp)
	orq	%r11, %rdi
	movq	%r9, -264(%rbp)
	xorq	%r9, -200(%rbp)
	movq	-200(%rbp), %rdx
	xorq	%rdx, -144(%rbp)
	movq	-144(%rbp), %r8
	movq	%r8, -144(%rbp)
	orq	%r8, %rdx
	movq	%r10, -272(%rbp)
	movq	%r10, %rax
	xorq	%rax, -352(%rbp)
	movq	-352(%rbp), %r8
	xorq	%r8, -152(%rbp)
	movq	-152(%rbp), %r9
	movq	%r9, -152(%rbp)
	orq	%r9, %r8
	movq	-280(%rbp), %rax
	xorq	%rax, -360(%rbp)
	movq	-360(%rbp), %r9
	xorq	%r9, -160(%rbp)
	movq	-160(%rbp), %r11
	movq	%r11, -160(%rbp)
	orq	%r11, %r9
	movq	-288(%rbp), %rax
	xorq	%rax, -368(%rbp)
	movq	-368(%rbp), %r10
	xorq	%r10, -168(%rbp)
	movq	-168(%rbp), %r11
	movq	%r11, -168(%rbp)
	orq	%r11, %r10
	movq	%r10, -368(%rbp)
	movq	-296(%rbp), %rax
	xorq	%rax, -376(%rbp)
	movq	-376(%rbp), %r11
	xorq	%r11, -176(%rbp)
	movq	-176(%rbp), %r10
	movq	%r10, -176(%rbp)
	orq	%r10, %r11
	movq	-304(%rbp), %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %r10
	movq	%r10, %rax
	xorq	%r10, -184(%rbp)
	movq	-184(%rbp), %r10
	movq	%r10, -184(%rbp)
	orq	%r10, %rax
	movq	%rax, -88(%rbp)
	movq	-312(%rbp), %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %r10
	xorq	%r10, -192(%rbp)
	movq	-192(%rbp), %rax
	movq	%rax, -192(%rbp)
	orq	%rax, %r10
	movq	%r10, -96(%rbp)
	movq	-320(%rbp), %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %r10
	xorq	%r10, %r15
	orq	%r15, %r10
	movq	%r10, -104(%rbp)
	xorq	-328(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-336(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-232(%rbp), %r10
	xorq	%r10, -208(%rbp)
	movq	-208(%rbp), %rax
	xorq	%rax, -112(%rbp)
	movq	-112(%rbp), %r10
	movq	%r10, -112(%rbp)
	orq	%r10, %rax
	movq	-240(%rbp), %r10
	xorq	%r10, %rcx
	xorq	%rcx, -120(%rbp)
	movq	-120(%rbp), %r10
	movq	%r10, -120(%rbp)
	orq	%r10, %rcx
	movq	-248(%rbp), %r10
	xorq	%r10, %rsi
	xorq	%rsi, -128(%rbp)
	movq	-128(%rbp), %r10
	movq	%r10, -128(%rbp)
	orq	%r10, %rsi
	movq	-256(%rbp), %r10
	xorq	%r10, %rdi
	xorq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r10
	movq	%r10, -136(%rbp)
	orq	%r10, %rdi
	movq	-264(%rbp), %r10
	xorq	%r10, %rdx
	xorq	%rdx, -144(%rbp)
	movq	-144(%rbp), %r10
	movq	%r10, -144(%rbp)
	orq	%r10, %rdx
	movq	%rdx, -200(%rbp)
	movq	-272(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -152(%rbp)
	movq	-152(%rbp), %r10
	movq	%r10, -152(%rbp)
	orq	%r10, %r8
	movq	-280(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -160(%rbp)
	movq	-160(%rbp), %r10
	movq	%r10, -160(%rbp)
	orq	%r10, %r9
	movq	%r9, -360(%rbp)
	movq	-288(%rbp), %rdx
	xorq	%rdx, -368(%rbp)
	movq	-368(%rbp), %r10
	xorq	%r10, -168(%rbp)
	movq	-168(%rbp), %r9
	movq	%r9, -168(%rbp)
	orq	%r9, %r10
	movq	-296(%rbp), %rdx
	xorq	%rdx, %r11
	xorq	%r11, -176(%rbp)
	movq	-176(%rbp), %r9
	movq	%r9, -176(%rbp)
	orq	%r9, %r11
	movq	-304(%rbp), %rdx
	xorq	%rdx, -88(%rbp)
	movq	-88(%rbp), %r9
	movq	%r9, %rdx
	xorq	%r9, -184(%rbp)
	movq	-184(%rbp), %r9
	movq	%r9, -184(%rbp)
	orq	%r9, %rdx
	movq	%rdx, -88(%rbp)
	movq	-312(%rbp), %rdx
	xorq	%rdx, -96(%rbp)
	movq	-96(%rbp), %r9
	xorq	%r9, -192(%rbp)
	movq	-192(%rbp), %rdx
	movq	%rdx, -192(%rbp)
	orq	%rdx, %r9
	movq	%r9, -96(%rbp)
	movq	-320(%rbp), %rdx
	xorq	%rdx, -104(%rbp)
	movq	-104(%rbp), %r9
	xorq	%r9, %r15
	orq	%r15, %r9
	movq	%r9, -104(%rbp)
	xorq	-328(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-336(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-232(%rbp), %rdx
	xorq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	-112(%rbp), %r9
	movq	%r9, -112(%rbp)
	orq	%r9, %rax
	movq	-240(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, -120(%rbp)
	movq	-120(%rbp), %r9
	movq	%r9, -120(%rbp)
	orq	%r9, %rcx
	movq	%rcx, -216(%rbp)
	movq	-248(%rbp), %rcx
	xorq	%rcx, %rsi
	xorq	%rsi, -128(%rbp)
	movq	-128(%rbp), %r9
	movq	%r9, -128(%rbp)
	orq	%r9, %rsi
	movq	-256(%rbp), %rcx
	xorq	%rcx, %rdi
	xorq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r9
	movq	%r9, -136(%rbp)
	orq	%r9, %rdi
	movq	-264(%rbp), %rcx
	xorq	%rcx, -200(%rbp)
	movq	-200(%rbp), %rdx
	xorq	%rdx, -144(%rbp)
	movq	-144(%rbp), %r9
	movq	%r9, -144(%rbp)
	orq	%r9, %rdx
	movq	-272(%rbp), %rcx
	xorq	%rcx, %r8
	xorq	%r8, -152(%rbp)
	movq	-152(%rbp), %r9
	movq	%r9, -152(%rbp)
	orq	%r9, %r8
	movq	%r8, -352(%rbp)
	movq	-280(%rbp), %rcx
	xorq	%rcx, -360(%rbp)
	movq	-360(%rbp), %r9
	xorq	%r9, -160(%rbp)
	movq	-160(%rbp), %r8
	movq	%r8, -160(%rbp)
	orq	%r8, %r9
	movq	-288(%rbp), %rcx
	xorq	%rcx, %r10
	xorq	%r10, -168(%rbp)
	movq	-168(%rbp), %r8
	movq	%r8, -168(%rbp)
	orq	%r8, %r10
	movq	-296(%rbp), %rcx
	xorq	%rcx, %r11
	xorq	%r11, -176(%rbp)
	movq	-176(%rbp), %r8
	movq	%r8, -176(%rbp)
	orq	%r8, %r11
	movq	-304(%rbp), %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, %rcx
	xorq	%r8, -184(%rbp)
	movq	-184(%rbp), %r8
	movq	%r8, -184(%rbp)
	orq	%r8, %rcx
	movq	%rcx, -88(%rbp)
	movq	-312(%rbp), %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %r8
	xorq	%r8, -192(%rbp)
	movq	-192(%rbp), %rcx
	movq	%rcx, -192(%rbp)
	orq	%rcx, %r8
	movq	%r8, -96(%rbp)
	movq	-320(%rbp), %rcx
	xorq	%rcx, -104(%rbp)
	movq	-104(%rbp), %r8
	xorq	%r8, %r15
	orq	%r15, %r8
	movq	%r8, -104(%rbp)
	xorq	-328(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-336(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-232(%rbp), %rcx
	xorq	%rcx, %rax
	xorq	%rax, -112(%rbp)
	movq	-112(%rbp), %r8
	movq	%r8, -112(%rbp)
	orq	%r8, %rax
	movq	-240(%rbp), %r8
	xorq	%r8, -216(%rbp)
	movq	-216(%rbp), %rcx
	xorq	%rcx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	orq	%r8, %rcx
	movq	-248(%rbp), %r8
	xorq	%r8, %rsi
	xorq	%rsi, -128(%rbp)
	movq	-128(%rbp), %r8
	movq	%r8, -128(%rbp)
	orq	%r8, %rsi
	movq	%rsi, -344(%rbp)
	movq	-256(%rbp), %rsi
	xorq	%rsi, %rdi
	xorq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	orq	%r8, %rdi
	movq	-264(%rbp), %rsi
	xorq	%rsi, %rdx
	xorq	%rdx, -144(%rbp)
	movq	-144(%rbp), %r8
	movq	%r8, -144(%rbp)
	orq	%r8, %rdx
	movq	%rdx, -200(%rbp)
	movq	-272(%rbp), %rsi
	xorq	%rsi, -352(%rbp)
	movq	-352(%rbp), %r8
	xorq	%r8, -152(%rbp)
	movq	-152(%rbp), %rdx
	movq	%rdx, -152(%rbp)
	orq	%rdx, %r8
	movq	-280(%rbp), %rsi
	xorq	%rsi, %r9
	xorq	%r9, -160(%rbp)
	movq	-160(%rbp), %rdx
	movq	%rdx, -160(%rbp)
	orq	%rdx, %r9
	movq	-288(%rbp), %rsi
	xorq	%rsi, %r10
	xorq	%r10, -168(%rbp)
	movq	-168(%rbp), %rdx
	movq	%rdx, -168(%rbp)
	orq	%rdx, %r10
	movq	-296(%rbp), %rsi
	xorq	%rsi, %r11
	xorq	%r11, -176(%rbp)
	movq	-176(%rbp), %rdx
	movq	%rdx, -176(%rbp)
	orq	%rdx, %r11
	movq	-304(%rbp), %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %rdx
	movq	%rdx, %rsi
	xorq	%rdx, -184(%rbp)
	movq	-184(%rbp), %rdx
	movq	%rdx, -184(%rbp)
	orq	%rdx, %rsi
	movq	%rsi, -88(%rbp)
	movq	-312(%rbp), %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %rdx
	xorq	%rdx, -192(%rbp)
	movq	-192(%rbp), %rsi
	movq	%rsi, -192(%rbp)
	orq	%rsi, %rdx
	movq	%rdx, -96(%rbp)
	movq	-320(%rbp), %rsi
	xorq	%rsi, -104(%rbp)
	movq	-104(%rbp), %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	movq	%rdx, -104(%rbp)
	xorq	-328(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-336(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-232(%rbp), %rsi
	xorq	%rsi, %rax
	xorq	%rax, -112(%rbp)
	movq	-112(%rbp), %rdx
	movq	%rdx, -112(%rbp)
	orq	%rdx, %rax
	movq	-240(%rbp), %rsi
	xorq	%rsi, %rcx
	xorq	%rcx, -120(%rbp)
	movq	-120(%rbp), %rdx
	movq	%rdx, -120(%rbp)
	orq	%rdx, %rcx
	movq	%rcx, -216(%rbp)
	movq	-248(%rbp), %rcx
	xorq	%rcx, -344(%rbp)
	movq	-344(%rbp), %rsi
	xorq	%rsi, -128(%rbp)
	movq	-128(%rbp), %rdx
	movq	%rdx, -128(%rbp)
	orq	%rdx, %rsi
	movq	-256(%rbp), %rcx
	xorq	%rcx, %rdi
	xorq	%rdi, -136(%rbp)
	movq	-136(%rbp), %rdx
	movq	%rdx, -136(%rbp)
	orq	%rdx, %rdi
	movq	%rdi, -224(%rbp)
	movq	-264(%rbp), %rcx
	xorq	%rcx, -200(%rbp)
	movq	-200(%rbp), %rdx
	xorq	%rdx, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	orq	%rdi, %rdx
	movq	-272(%rbp), %rcx
	xorq	%rcx, %r8
	xorq	%r8, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	orq	%rdi, %r8
	movq	-280(%rbp), %rcx
	xorq	%rcx, %r9
	xorq	%r9, -160(%rbp)
	movq	-160(%rbp), %rdi
	movq	%rdi, -160(%rbp)
	orq	%rdi, %r9
	movq	-288(%rbp), %rcx
	xorq	%rcx, %r10
	xorq	%r10, -168(%rbp)
	movq	-168(%rbp), %rdi
	movq	%rdi, -168(%rbp)
	orq	%rdi, %r10
	movq	-296(%rbp), %rcx
	xorq	%rcx, %r11
	xorq	%r11, -176(%rbp)
	movq	-176(%rbp), %rdi
	movq	%rdi, -176(%rbp)
	orq	%rdi, %r11
	movq	-304(%rbp), %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, %rcx
	xorq	%rdi, -184(%rbp)
	movq	-184(%rbp), %rdi
	movq	%rdi, -184(%rbp)
	orq	%rdi, %rcx
	movq	%rcx, -88(%rbp)
	movq	-312(%rbp), %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %rdi
	xorq	%rdi, -192(%rbp)
	movq	-192(%rbp), %rcx
	movq	%rcx, -192(%rbp)
	orq	%rcx, %rdi
	movq	%rdi, -96(%rbp)
	movq	-320(%rbp), %rcx
	xorq	%rcx, -104(%rbp)
	movq	-104(%rbp), %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	movq	%rdi, -104(%rbp)
	xorq	-328(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-336(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-232(%rbp), %rcx
	xorq	%rcx, %rax
	xorq	%rax, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	orq	%rdi, %rax
	movq	%rax, -208(%rbp)
	movq	-240(%rbp), %rax
	xorq	%rax, -216(%rbp)
	movq	-216(%rbp), %rcx
	xorq	%rcx, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	orq	%rdi, %rcx
	movq	-248(%rbp), %rax
	xorq	%rax, %rsi
	xorq	%rsi, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	orq	%rdi, %rsi
	movq	-256(%rbp), %rax
	xorq	%rax, -224(%rbp)
	movq	-224(%rbp), %rdi
	xorq	%rdi, -136(%rbp)
	movq	-136(%rbp), %rax
	movq	%rax, -136(%rbp)
	orq	%rax, %rdi
	movq	-264(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -144(%rbp)
	movq	-144(%rbp), %rax
	movq	%rax, -144(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -200(%rbp)
	movq	-272(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -152(%rbp)
	movq	-152(%rbp), %rax
	movq	%rax, -152(%rbp)
	orq	%rax, %r8
	movq	-280(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -160(%rbp)
	movq	-160(%rbp), %rax
	movq	%rax, -160(%rbp)
	orq	%rax, %r9
	movq	-288(%rbp), %rdx
	xorq	%rdx, %r10
	xorq	%r10, -168(%rbp)
	movq	-168(%rbp), %rdx
	movq	%rdx, -168(%rbp)
	orq	%rdx, %r10
	movq	-296(%rbp), %rdx
	xorq	%rdx, %r11
	xorq	%r11, -176(%rbp)
	movq	-176(%rbp), %rax
	movq	%rax, -176(%rbp)
	orq	%rax, %r11
	movq	-304(%rbp), %rdx
	xorq	%rdx, -88(%rbp)
	movq	-88(%rbp), %rdx
	xorq	%rdx, -184(%rbp)
	movq	-184(%rbp), %rax
	movq	%rdx, -88(%rbp)
	movq	%rax, -184(%rbp)
	movq	%rax, %rdx
	orq	%rdx, -88(%rbp)
	movq	-312(%rbp), %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %rdx
	xorq	%rdx, -192(%rbp)
	movq	-192(%rbp), %rax
	movq	%rax, -192(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -96(%rbp)
	movq	-320(%rbp), %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	movq	%rdx, -104(%rbp)
	xorq	-328(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-336(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-232(%rbp), %rdx
	xorq	%rdx, -208(%rbp)
	movq	-208(%rbp), %rax
	xorq	%rax, -112(%rbp)
	movq	-112(%rbp), %rdx
	movq	%rdx, -112(%rbp)
	orq	%rdx, %rax
	movq	-240(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, -120(%rbp)
	movq	-120(%rbp), %rdx
	movq	%rdx, -120(%rbp)
	orq	%rdx, %rcx
	movq	-248(%rbp), %rdx
	xorq	%rdx, %rsi
	xorq	%rsi, -128(%rbp)
	movq	-128(%rbp), %rdx
	movq	%rdx, -128(%rbp)
	orq	%rdx, %rsi
	movq	%rsi, -344(%rbp)
	movq	-256(%rbp), %rsi
	xorq	%rsi, %rdi
	xorq	%rdi, -136(%rbp)
	movq	-136(%rbp), %rdx
	movq	%rdx, -136(%rbp)
	orq	%rdx, %rdi
	movq	%rdi, -224(%rbp)
	movq	-264(%rbp), %rsi
	xorq	%rsi, -200(%rbp)
	movq	-200(%rbp), %rdx
	xorq	%rdx, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	orq	%rdi, %rdx
	movq	-272(%rbp), %rsi
	xorq	%rsi, %r8
	xorq	%r8, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	orq	%rdi, %r8
	movq	-280(%rbp), %rsi
	xorq	%rsi, %r9
	xorq	%r9, -160(%rbp)
	movq	-160(%rbp), %rdi
	movq	%rdi, -160(%rbp)
	orq	%rdi, %r9
	movq	-288(%rbp), %rsi
	xorq	%rsi, %r10
	xorq	%r10, -168(%rbp)
	movq	-168(%rbp), %rdi
	movq	%rdi, -168(%rbp)
	orq	%rdi, %r10
	movq	-296(%rbp), %rsi
	xorq	%rsi, %r11
	xorq	%r11, -176(%rbp)
	movq	-176(%rbp), %rdi
	movq	%rdi, -176(%rbp)
	orq	%rdi, %r11
	movq	-304(%rbp), %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, %rsi
	xorq	%rdi, -184(%rbp)
	movq	-184(%rbp), %rdi
	movq	%rdi, -184(%rbp)
	orq	%rdi, %rsi
	movq	%rsi, -88(%rbp)
	movq	-312(%rbp), %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %rdi
	xorq	%rdi, -192(%rbp)
	movq	-192(%rbp), %rsi
	movq	%rsi, -192(%rbp)
	orq	%rsi, %rdi
	movq	%rdi, -96(%rbp)
	movq	-320(%rbp), %rsi
	xorq	%rsi, -104(%rbp)
	movq	-104(%rbp), %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	movq	%rdi, -104(%rbp)
	xorq	-328(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-336(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-232(%rbp), %rsi
	xorq	%rsi, %rax
	xorq	%rax, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	orq	%rdi, %rax
	movq	%rax, -208(%rbp)
	movq	-240(%rbp), %rsi
	xorq	%rsi, %rcx
	xorq	%rcx, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	orq	%rdi, %rcx
	movq	-248(%rbp), %rax
	xorq	%rax, -344(%rbp)
	movq	-344(%rbp), %rsi
	xorq	%rsi, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	orq	%rdi, %rsi
	movq	-256(%rbp), %rax
	xorq	%rax, -224(%rbp)
	movq	-224(%rbp), %rdi
	xorq	%rdi, -136(%rbp)
	movq	-136(%rbp), %rax
	movq	%rax, -136(%rbp)
	orq	%rax, %rdi
	movq	-264(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -144(%rbp)
	movq	-144(%rbp), %rax
	movq	%rax, -144(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -200(%rbp)
	movq	-272(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -152(%rbp)
	movq	-152(%rbp), %rax
	movq	%rax, -152(%rbp)
	orq	%rax, %r8
	movq	-280(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -160(%rbp)
	movq	-160(%rbp), %rdx
	movq	%rdx, -160(%rbp)
	orq	%rdx, %r9
	movq	-288(%rbp), %rax
	xorq	%rax, %r10
	xorq	%r10, -168(%rbp)
	movq	-168(%rbp), %rax
	movq	%rax, -168(%rbp)
	orq	%rax, %r10
	movq	-296(%rbp), %rdx
	xorq	%rdx, %r11
	xorq	%r11, -176(%rbp)
	movq	-176(%rbp), %rdx
	movq	%rdx, -176(%rbp)
	orq	%rdx, %r11
	movq	-304(%rbp), %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %rdx
	xorq	%rdx, -184(%rbp)
	movq	-184(%rbp), %rax
	movq	%rax, -184(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -88(%rbp)
	movq	-312(%rbp), %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %rdx
	xorq	%rdx, -192(%rbp)
	movq	-192(%rbp), %rax
	movq	%rax, -192(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -96(%rbp)
	movq	-320(%rbp), %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	movq	%rdx, -104(%rbp)
	xorq	-328(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-336(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-232(%rbp), %rdx
	xorq	%rdx, -208(%rbp)
	movq	-208(%rbp), %rax
	xorq	%rax, -112(%rbp)
	movq	-112(%rbp), %rdx
	movq	%rdx, -112(%rbp)
	orq	%rdx, %rax
	movq	-240(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, -120(%rbp)
	movq	-120(%rbp), %rdx
	movq	%rdx, -120(%rbp)
	orq	%rdx, %rcx
	movq	%rcx, -216(%rbp)
	movq	-248(%rbp), %rcx
	xorq	%rcx, %rsi
	xorq	%rsi, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	orq	%rcx, %rsi
	movq	-256(%rbp), %rcx
	xorq	%rcx, %rdi
	xorq	%rdi, -136(%rbp)
	movq	-136(%rbp), %rcx
	movq	%rcx, -136(%rbp)
	orq	%rcx, %rdi
	movq	%rdi, -224(%rbp)
	movq	-264(%rbp), %rdi
	xorq	%rdi, -200(%rbp)
	movq	-200(%rbp), %rdx
	xorq	%rdx, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	orq	%rcx, %rdx
	movq	-272(%rbp), %rdi
	xorq	%rdi, %r8
	xorq	%r8, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	orq	%rcx, %r8
	movq	-280(%rbp), %rdi
	xorq	%rdi, %r9
	xorq	%r9, -160(%rbp)
	movq	-160(%rbp), %rcx
	movq	%rcx, -160(%rbp)
	orq	%rcx, %r9
	movq	-288(%rbp), %rdi
	xorq	%rdi, %r10
	xorq	%r10, -168(%rbp)
	movq	-168(%rbp), %rcx
	movq	%rcx, -168(%rbp)
	orq	%rcx, %r10
	movq	-296(%rbp), %rdi
	xorq	%rdi, %r11
	xorq	%r11, -176(%rbp)
	movq	-176(%rbp), %rcx
	movq	%rcx, -176(%rbp)
	orq	%rcx, %r11
	movq	-304(%rbp), %rdi
	xorq	%rdi, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, %rdi
	xorq	%rcx, -184(%rbp)
	movq	-184(%rbp), %rcx
	movq	%rcx, -184(%rbp)
	orq	%rcx, %rdi
	movq	%rdi, -88(%rbp)
	movq	-312(%rbp), %rdi
	xorq	%rdi, -96(%rbp)
	movq	-96(%rbp), %rcx
	xorq	%rcx, -192(%rbp)
	movq	-192(%rbp), %rdi
	movq	%rdi, -192(%rbp)
	orq	%rdi, %rcx
	movq	%rcx, -96(%rbp)
	movq	-320(%rbp), %rdi
	xorq	%rdi, -104(%rbp)
	movq	-104(%rbp), %rcx
	xorq	%rcx, %r15
	orq	%r15, %rcx
	movq	%rcx, -104(%rbp)
	xorq	-328(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-336(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-232(%rbp), %rdi
	xorq	%rdi, %rax
	xorq	%rax, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	orq	%rcx, %rax
	movq	%rax, -208(%rbp)
	movq	-240(%rbp), %rdi
	xorq	%rdi, -216(%rbp)
	movq	-216(%rbp), %rcx
	xorq	%rcx, -120(%rbp)
	movq	-120(%rbp), %rax
	movq	%rax, -120(%rbp)
	orq	%rax, %rcx
	movq	%rcx, -216(%rbp)
	movq	-248(%rbp), %rdi
	xorq	%rdi, %rsi
	xorq	%rsi, -128(%rbp)
	movq	-128(%rbp), %rax
	movq	%rax, -128(%rbp)
	orq	%rax, %rsi
	movq	%rsi, -344(%rbp)
	movq	-256(%rbp), %rax
	xorq	%rax, -224(%rbp)
	movq	-224(%rbp), %rdi
	xorq	%rdi, -136(%rbp)
	movq	-136(%rbp), %rsi
	movq	%rsi, -136(%rbp)
	orq	%rsi, %rdi
	movq	%rdi, -224(%rbp)
	movq	-264(%rbp), %rdi
	xorq	%rdi, %rdx
	xorq	%rdx, -144(%rbp)
	movq	-144(%rbp), %rsi
	movq	%rsi, -144(%rbp)
	orq	%rsi, %rdx
	movq	%rdx, -200(%rbp)
	movq	-272(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -152(%rbp)
	movq	-152(%rbp), %rsi
	movq	%rsi, -152(%rbp)
	orq	%rsi, %r8
	movq	%r8, -352(%rbp)
	movq	-280(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -160(%rbp)
	movq	-160(%rbp), %rsi
	movq	%rsi, -160(%rbp)
	orq	%rsi, %r9
	movq	%r9, -360(%rbp)
	movq	-288(%rbp), %r8
	xorq	%r8, %r10
	xorq	%r10, -168(%rbp)
	movq	-168(%rbp), %rsi
	movq	%rsi, -168(%rbp)
	orq	%rsi, %r10
	movq	%r10, -368(%rbp)
	movq	-296(%rbp), %rdx
	xorq	%rdx, %r11
	xorq	%r11, -176(%rbp)
	movq	-176(%rbp), %rsi
	movq	%rsi, -176(%rbp)
	orq	%rsi, %r11
	movq	%r11, -376(%rbp)
	movq	-304(%rbp), %r11
	xorq	%r11, -88(%rbp)
	movq	-88(%rbp), %rax
	xorq	%rax, -184(%rbp)
	movq	-184(%rbp), %rsi
	movq	%rsi, -184(%rbp)
	orq	%rsi, %rax
	movq	%rax, -88(%rbp)
	movq	-312(%rbp), %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %rcx
	xorq	%rcx, -192(%rbp)
	movq	-192(%rbp), %rax
	movq	%rax, -192(%rbp)
	orq	%rax, %rcx
	movq	%rcx, -96(%rbp)
	movq	-320(%rbp), %rdi
	xorq	%rdi, -104(%rbp)
	movq	-104(%rbp), %rcx
	xorq	%rcx, %r15
	orq	%r15, %rcx
	movq	%rcx, -104(%rbp)
	xorq	-328(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-336(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
.L298:
	movq	-384(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -384(%rbp)
	testq	%rax, %rax
	jne	.L299
	movl	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-344(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-352(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-360(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-368(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-376(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$344, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE102:
	.size	int64_bit_13, .-int64_bit_13
	.globl	int64_bit_14
	.type	int64_bit_14, @function
int64_bit_14:
.LFB103:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$376, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -408(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	addq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -248(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -256(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -368(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -264(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -240(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$2, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -272(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$3, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -280(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -376(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$4, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -160(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -288(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -384(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$5, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -168(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -296(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -392(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$6, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -176(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -304(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -400(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$7, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -184(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -312(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$8, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -192(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -320(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$9, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -200(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -328(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	subl	$10, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$10, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -208(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -336(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	subl	$11, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$11, %rax
	movq	%rax, %r15
	movq	%r15, %rax
	salq	$32, %rax
	orq	%rax, %r15
	leaq	0(,%r15,4), %rax
	subq	$1, %rax
	movq	%rax, -344(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	subl	$12, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$32, %rax
	orq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$12, %rax
	movq	%rax, %r13
	movq	%r13, %rax
	salq	$32, %rax
	orq	%rax, %r13
	leaq	0(,%r13,4), %rax
	subq	$1, %rax
	movq	%rax, -352(%rbp)
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	subl	$13, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$32, %rax
	orq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$13, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -360(%rbp)
	jmp	.L301
.L302:
	subq	$1, -248(%rbp)
	movq	-248(%rbp), %rsi
	subq	$1, -256(%rbp)
	movq	-256(%rbp), %rdi
	subq	$1, -264(%rbp)
	movq	-264(%rbp), %rdx
	subq	$1, -272(%rbp)
	movq	-272(%rbp), %r11
	subq	$1, -280(%rbp)
	movq	-280(%rbp), %r9
	subq	$1, -288(%rbp)
	movq	-288(%rbp), %r10
	subq	$1, -296(%rbp)
	subq	$1, -304(%rbp)
	subq	$1, -312(%rbp)
	subq	$1, -320(%rbp)
	subq	$1, -328(%rbp)
	subq	$1, -336(%rbp)
	subq	$1, -344(%rbp)
	subq	$1, -352(%rbp)
	subq	$1, -360(%rbp)
	movq	%rsi, -248(%rbp)
	xorq	%rsi, -224(%rbp)
	movq	-224(%rbp), %rax
	xorq	%rax, -120(%rbp)
	movq	-120(%rbp), %rsi
	movq	%rsi, -120(%rbp)
	orq	%rsi, %rax
	movq	%rax, -224(%rbp)
	movq	%rdi, -256(%rbp)
	xorq	%rdi, -232(%rbp)
	movq	-232(%rbp), %rcx
	xorq	%rcx, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	orq	%rdi, %rcx
	movq	%rdx, -264(%rbp)
	xorq	%rdx, -368(%rbp)
	movq	-368(%rbp), %rsi
	xorq	%rsi, -136(%rbp)
	movq	-136(%rbp), %rdx
	movq	%rdx, -136(%rbp)
	orq	%rdx, %rsi
	movq	%r11, -272(%rbp)
	xorq	%r11, -240(%rbp)
	movq	-240(%rbp), %rdi
	xorq	%rdi, -144(%rbp)
	movq	-144(%rbp), %r11
	movq	%r11, -144(%rbp)
	orq	%r11, %rdi
	movq	%r9, -280(%rbp)
	xorq	%r9, -216(%rbp)
	movq	-216(%rbp), %rdx
	xorq	%rdx, -152(%rbp)
	movq	-152(%rbp), %r8
	movq	%r8, -152(%rbp)
	orq	%r8, %rdx
	movq	%r10, -288(%rbp)
	movq	%r10, %rax
	xorq	%rax, -376(%rbp)
	movq	-376(%rbp), %r8
	xorq	%r8, -160(%rbp)
	movq	-160(%rbp), %r9
	movq	%r9, -160(%rbp)
	orq	%r9, %r8
	movq	-296(%rbp), %rax
	xorq	%rax, -384(%rbp)
	movq	-384(%rbp), %r9
	xorq	%r9, -168(%rbp)
	movq	-168(%rbp), %r11
	movq	%r11, -168(%rbp)
	orq	%r11, %r9
	movq	-304(%rbp), %rax
	xorq	%rax, -392(%rbp)
	movq	-392(%rbp), %r10
	xorq	%r10, -176(%rbp)
	movq	-176(%rbp), %r11
	movq	%r11, -176(%rbp)
	orq	%r11, %r10
	movq	%r10, -392(%rbp)
	movq	-312(%rbp), %rax
	xorq	%rax, -400(%rbp)
	movq	-400(%rbp), %r11
	xorq	%r11, -184(%rbp)
	movq	-184(%rbp), %r10
	movq	%r10, -184(%rbp)
	orq	%r10, %r11
	movq	-320(%rbp), %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %r10
	movq	%r10, %rax
	xorq	%r10, -192(%rbp)
	movq	-192(%rbp), %r10
	movq	%r10, -192(%rbp)
	orq	%r10, %rax
	movq	%rax, -88(%rbp)
	movq	-328(%rbp), %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %r10
	xorq	%r10, -200(%rbp)
	movq	-200(%rbp), %rax
	movq	%rax, -200(%rbp)
	orq	%rax, %r10
	movq	%r10, -96(%rbp)
	movq	-336(%rbp), %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %r10
	xorq	%r10, -208(%rbp)
	movq	-208(%rbp), %rax
	movq	%rax, -208(%rbp)
	orq	%rax, %r10
	movq	%r10, -104(%rbp)
	movq	-344(%rbp), %rax
	xorq	%rax, -112(%rbp)
	movq	-112(%rbp), %r10
	xorq	%r10, %r15
	orq	%r15, %r10
	movq	%r10, -112(%rbp)
	xorq	-352(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-360(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-248(%rbp), %r10
	xorq	%r10, -224(%rbp)
	movq	-224(%rbp), %rax
	xorq	%rax, -120(%rbp)
	movq	-120(%rbp), %r10
	movq	%r10, -120(%rbp)
	orq	%r10, %rax
	movq	-256(%rbp), %r10
	xorq	%r10, %rcx
	xorq	%rcx, -128(%rbp)
	movq	-128(%rbp), %r10
	movq	%r10, -128(%rbp)
	orq	%r10, %rcx
	movq	-264(%rbp), %r10
	xorq	%r10, %rsi
	xorq	%rsi, -136(%rbp)
	movq	-136(%rbp), %r10
	movq	%r10, -136(%rbp)
	orq	%r10, %rsi
	movq	-272(%rbp), %r10
	xorq	%r10, %rdi
	xorq	%rdi, -144(%rbp)
	movq	-144(%rbp), %r10
	movq	%r10, -144(%rbp)
	orq	%r10, %rdi
	movq	-280(%rbp), %r10
	xorq	%r10, %rdx
	xorq	%rdx, -152(%rbp)
	movq	-152(%rbp), %r10
	movq	%r10, -152(%rbp)
	orq	%r10, %rdx
	movq	%rdx, -216(%rbp)
	movq	-288(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -160(%rbp)
	movq	-160(%rbp), %r10
	movq	%r10, -160(%rbp)
	orq	%r10, %r8
	movq	-296(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -168(%rbp)
	movq	-168(%rbp), %r10
	movq	%r10, -168(%rbp)
	orq	%r10, %r9
	movq	%r9, -384(%rbp)
	movq	-304(%rbp), %rdx
	xorq	%rdx, -392(%rbp)
	movq	-392(%rbp), %r10
	xorq	%r10, -176(%rbp)
	movq	-176(%rbp), %r9
	movq	%r9, -176(%rbp)
	orq	%r9, %r10
	movq	-312(%rbp), %rdx
	xorq	%rdx, %r11
	xorq	%r11, -184(%rbp)
	movq	-184(%rbp), %r9
	movq	%r9, -184(%rbp)
	orq	%r9, %r11
	movq	-320(%rbp), %rdx
	xorq	%rdx, -88(%rbp)
	movq	-88(%rbp), %r9
	movq	%r9, %rdx
	xorq	%r9, -192(%rbp)
	movq	-192(%rbp), %r9
	movq	%r9, -192(%rbp)
	orq	%r9, %rdx
	movq	%rdx, -88(%rbp)
	movq	-328(%rbp), %rdx
	xorq	%rdx, -96(%rbp)
	movq	-96(%rbp), %r9
	xorq	%r9, -200(%rbp)
	movq	-200(%rbp), %rdx
	movq	%rdx, -200(%rbp)
	orq	%rdx, %r9
	movq	%r9, -96(%rbp)
	movq	-336(%rbp), %rdx
	xorq	%rdx, -104(%rbp)
	movq	-104(%rbp), %r9
	xorq	%r9, -208(%rbp)
	movq	-208(%rbp), %rdx
	movq	%rdx, -208(%rbp)
	orq	%rdx, %r9
	movq	%r9, -104(%rbp)
	movq	-344(%rbp), %rdx
	xorq	%rdx, -112(%rbp)
	movq	-112(%rbp), %r9
	xorq	%r9, %r15
	orq	%r15, %r9
	movq	%r9, -112(%rbp)
	xorq	-352(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-360(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-248(%rbp), %rdx
	xorq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	-120(%rbp), %r9
	movq	%r9, -120(%rbp)
	orq	%r9, %rax
	movq	-256(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, -128(%rbp)
	movq	-128(%rbp), %r9
	movq	%r9, -128(%rbp)
	orq	%r9, %rcx
	movq	%rcx, -232(%rbp)
	movq	-264(%rbp), %rcx
	xorq	%rcx, %rsi
	xorq	%rsi, -136(%rbp)
	movq	-136(%rbp), %r9
	movq	%r9, -136(%rbp)
	orq	%r9, %rsi
	movq	-272(%rbp), %rcx
	xorq	%rcx, %rdi
	xorq	%rdi, -144(%rbp)
	movq	-144(%rbp), %r9
	movq	%r9, -144(%rbp)
	orq	%r9, %rdi
	movq	-280(%rbp), %rcx
	xorq	%rcx, -216(%rbp)
	movq	-216(%rbp), %rdx
	xorq	%rdx, -152(%rbp)
	movq	-152(%rbp), %r9
	movq	%r9, -152(%rbp)
	orq	%r9, %rdx
	movq	-288(%rbp), %rcx
	xorq	%rcx, %r8
	xorq	%r8, -160(%rbp)
	movq	-160(%rbp), %r9
	movq	%r9, -160(%rbp)
	orq	%r9, %r8
	movq	%r8, -376(%rbp)
	movq	-296(%rbp), %rcx
	xorq	%rcx, -384(%rbp)
	movq	-384(%rbp), %r9
	xorq	%r9, -168(%rbp)
	movq	-168(%rbp), %r8
	movq	%r8, -168(%rbp)
	orq	%r8, %r9
	movq	-304(%rbp), %rcx
	xorq	%rcx, %r10
	xorq	%r10, -176(%rbp)
	movq	-176(%rbp), %r8
	movq	%r8, -176(%rbp)
	orq	%r8, %r10
	movq	-312(%rbp), %rcx
	xorq	%rcx, %r11
	xorq	%r11, -184(%rbp)
	movq	-184(%rbp), %r8
	movq	%r8, -184(%rbp)
	orq	%r8, %r11
	movq	-320(%rbp), %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, %rcx
	xorq	%r8, -192(%rbp)
	movq	-192(%rbp), %r8
	movq	%r8, -192(%rbp)
	orq	%r8, %rcx
	movq	%rcx, -88(%rbp)
	movq	-328(%rbp), %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %r8
	xorq	%r8, -200(%rbp)
	movq	-200(%rbp), %rcx
	movq	%rcx, -200(%rbp)
	orq	%rcx, %r8
	movq	%r8, -96(%rbp)
	movq	-336(%rbp), %rcx
	xorq	%rcx, -104(%rbp)
	movq	-104(%rbp), %r8
	xorq	%r8, -208(%rbp)
	movq	-208(%rbp), %rcx
	movq	%rcx, -208(%rbp)
	orq	%rcx, %r8
	movq	%r8, -104(%rbp)
	movq	-344(%rbp), %rcx
	xorq	%rcx, -112(%rbp)
	movq	-112(%rbp), %r8
	xorq	%r8, %r15
	orq	%r15, %r8
	movq	%r8, -112(%rbp)
	xorq	-352(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-360(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-248(%rbp), %rcx
	xorq	%rcx, %rax
	xorq	%rax, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	orq	%r8, %rax
	movq	-256(%rbp), %r8
	xorq	%r8, -232(%rbp)
	movq	-232(%rbp), %rcx
	xorq	%rcx, -128(%rbp)
	movq	-128(%rbp), %r8
	movq	%r8, -128(%rbp)
	orq	%r8, %rcx
	movq	-264(%rbp), %r8
	xorq	%r8, %rsi
	xorq	%rsi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	orq	%r8, %rsi
	movq	%rsi, -368(%rbp)
	movq	-272(%rbp), %rsi
	xorq	%rsi, %rdi
	xorq	%rdi, -144(%rbp)
	movq	-144(%rbp), %r8
	movq	%r8, -144(%rbp)
	orq	%r8, %rdi
	movq	-280(%rbp), %rsi
	xorq	%rsi, %rdx
	xorq	%rdx, -152(%rbp)
	movq	-152(%rbp), %r8
	movq	%r8, -152(%rbp)
	orq	%r8, %rdx
	movq	%rdx, -216(%rbp)
	movq	-288(%rbp), %rsi
	xorq	%rsi, -376(%rbp)
	movq	-376(%rbp), %r8
	xorq	%r8, -160(%rbp)
	movq	-160(%rbp), %rdx
	movq	%rdx, -160(%rbp)
	orq	%rdx, %r8
	movq	-296(%rbp), %rsi
	xorq	%rsi, %r9
	xorq	%r9, -168(%rbp)
	movq	-168(%rbp), %rdx
	movq	%rdx, -168(%rbp)
	orq	%rdx, %r9
	movq	-304(%rbp), %rsi
	xorq	%rsi, %r10
	xorq	%r10, -176(%rbp)
	movq	-176(%rbp), %rdx
	movq	%rdx, -176(%rbp)
	orq	%rdx, %r10
	movq	-312(%rbp), %rsi
	xorq	%rsi, %r11
	xorq	%r11, -184(%rbp)
	movq	-184(%rbp), %rdx
	movq	%rdx, -184(%rbp)
	orq	%rdx, %r11
	movq	-320(%rbp), %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %rdx
	movq	%rdx, %rsi
	xorq	%rdx, -192(%rbp)
	movq	-192(%rbp), %rdx
	movq	%rdx, -192(%rbp)
	orq	%rdx, %rsi
	movq	%rsi, -88(%rbp)
	movq	-328(%rbp), %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %rdx
	xorq	%rdx, -200(%rbp)
	movq	-200(%rbp), %rsi
	movq	%rsi, -200(%rbp)
	orq	%rsi, %rdx
	movq	%rdx, -96(%rbp)
	movq	-336(%rbp), %rsi
	xorq	%rsi, -104(%rbp)
	movq	-104(%rbp), %rdx
	xorq	%rdx, -208(%rbp)
	movq	-208(%rbp), %rsi
	movq	%rsi, -208(%rbp)
	orq	%rsi, %rdx
	movq	%rdx, -104(%rbp)
	movq	-344(%rbp), %rsi
	xorq	%rsi, -112(%rbp)
	movq	-112(%rbp), %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	movq	%rdx, -112(%rbp)
	xorq	-352(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-360(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-248(%rbp), %rsi
	xorq	%rsi, %rax
	xorq	%rax, -120(%rbp)
	movq	-120(%rbp), %rdx
	movq	%rdx, -120(%rbp)
	orq	%rdx, %rax
	movq	-256(%rbp), %rsi
	xorq	%rsi, %rcx
	xorq	%rcx, -128(%rbp)
	movq	-128(%rbp), %rdx
	movq	%rdx, -128(%rbp)
	orq	%rdx, %rcx
	movq	%rcx, -232(%rbp)
	movq	-264(%rbp), %rcx
	xorq	%rcx, -368(%rbp)
	movq	-368(%rbp), %rsi
	xorq	%rsi, -136(%rbp)
	movq	-136(%rbp), %rdx
	movq	%rdx, -136(%rbp)
	orq	%rdx, %rsi
	movq	-272(%rbp), %rcx
	xorq	%rcx, %rdi
	xorq	%rdi, -144(%rbp)
	movq	-144(%rbp), %rdx
	movq	%rdx, -144(%rbp)
	orq	%rdx, %rdi
	movq	%rdi, -240(%rbp)
	movq	-280(%rbp), %rcx
	xorq	%rcx, -216(%rbp)
	movq	-216(%rbp), %rdx
	xorq	%rdx, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	orq	%rdi, %rdx
	movq	-288(%rbp), %rcx
	xorq	%rcx, %r8
	xorq	%r8, -160(%rbp)
	movq	-160(%rbp), %rdi
	movq	%rdi, -160(%rbp)
	orq	%rdi, %r8
	movq	-296(%rbp), %rcx
	xorq	%rcx, %r9
	xorq	%r9, -168(%rbp)
	movq	-168(%rbp), %rdi
	movq	%rdi, -168(%rbp)
	orq	%rdi, %r9
	movq	-304(%rbp), %rcx
	xorq	%rcx, %r10
	xorq	%r10, -176(%rbp)
	movq	-176(%rbp), %rdi
	movq	%rdi, -176(%rbp)
	orq	%rdi, %r10
	movq	-312(%rbp), %rcx
	xorq	%rcx, %r11
	xorq	%r11, -184(%rbp)
	movq	-184(%rbp), %rdi
	movq	%rdi, -184(%rbp)
	orq	%rdi, %r11
	movq	-320(%rbp), %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, %rcx
	xorq	%rdi, -192(%rbp)
	movq	-192(%rbp), %rdi
	movq	%rdi, -192(%rbp)
	orq	%rdi, %rcx
	movq	%rcx, -88(%rbp)
	movq	-328(%rbp), %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %rdi
	xorq	%rdi, -200(%rbp)
	movq	-200(%rbp), %rcx
	movq	%rcx, -200(%rbp)
	orq	%rcx, %rdi
	movq	%rdi, -96(%rbp)
	movq	-336(%rbp), %rcx
	xorq	%rcx, -104(%rbp)
	movq	-104(%rbp), %rdi
	xorq	%rdi, -208(%rbp)
	movq	-208(%rbp), %rcx
	movq	%rcx, -208(%rbp)
	orq	%rcx, %rdi
	movq	%rdi, -104(%rbp)
	movq	-344(%rbp), %rcx
	xorq	%rcx, -112(%rbp)
	movq	-112(%rbp), %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	movq	%rdi, -112(%rbp)
	xorq	-352(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-360(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-248(%rbp), %rcx
	xorq	%rcx, %rax
	xorq	%rax, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	orq	%rdi, %rax
	movq	%rax, -224(%rbp)
	movq	-256(%rbp), %rax
	xorq	%rax, -232(%rbp)
	movq	-232(%rbp), %rcx
	xorq	%rcx, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	orq	%rdi, %rcx
	movq	-264(%rbp), %rax
	xorq	%rax, %rsi
	xorq	%rsi, -136(%rbp)
	movq	-136(%rbp), %rdi
	movq	%rdi, -136(%rbp)
	orq	%rdi, %rsi
	movq	-272(%rbp), %rax
	xorq	%rax, -240(%rbp)
	movq	-240(%rbp), %rdi
	xorq	%rdi, -144(%rbp)
	movq	-144(%rbp), %rax
	movq	%rax, -144(%rbp)
	orq	%rax, %rdi
	movq	-280(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -152(%rbp)
	movq	-152(%rbp), %rax
	movq	%rax, -152(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -216(%rbp)
	movq	-288(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -160(%rbp)
	movq	-160(%rbp), %rax
	movq	%rax, -160(%rbp)
	orq	%rax, %r8
	movq	-296(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -168(%rbp)
	movq	-168(%rbp), %rax
	movq	%rax, -168(%rbp)
	orq	%rax, %r9
	movq	-304(%rbp), %rdx
	xorq	%rdx, %r10
	xorq	%r10, -176(%rbp)
	movq	-176(%rbp), %rdx
	movq	%rdx, -176(%rbp)
	orq	%rdx, %r10
	movq	-312(%rbp), %rdx
	xorq	%rdx, %r11
	xorq	%r11, -184(%rbp)
	movq	-184(%rbp), %rax
	movq	%rax, -184(%rbp)
	orq	%rax, %r11
	movq	-320(%rbp), %rdx
	xorq	%rdx, -88(%rbp)
	movq	-88(%rbp), %rdx
	xorq	%rdx, -192(%rbp)
	movq	-192(%rbp), %rax
	movq	%rax, -192(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -88(%rbp)
	movq	-328(%rbp), %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %rdx
	xorq	%rdx, -200(%rbp)
	movq	-200(%rbp), %rax
	movq	%rax, -200(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -96(%rbp)
	movq	-336(%rbp), %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %rdx
	xorq	%rdx, -208(%rbp)
	movq	-208(%rbp), %rax
	movq	%rax, -208(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -104(%rbp)
	movq	-344(%rbp), %rax
	xorq	%rax, -112(%rbp)
	movq	-112(%rbp), %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	movq	%rdx, -112(%rbp)
	xorq	-352(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-360(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-248(%rbp), %rdx
	xorq	%rdx, -224(%rbp)
	movq	-224(%rbp), %rax
	xorq	%rax, -120(%rbp)
	movq	-120(%rbp), %rdx
	movq	%rdx, -120(%rbp)
	orq	%rdx, %rax
	movq	-256(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, -128(%rbp)
	movq	-128(%rbp), %rdx
	movq	%rdx, -128(%rbp)
	orq	%rdx, %rcx
	movq	-264(%rbp), %rdx
	xorq	%rdx, %rsi
	xorq	%rsi, -136(%rbp)
	movq	-136(%rbp), %rdx
	movq	%rdx, -136(%rbp)
	orq	%rdx, %rsi
	movq	%rsi, -368(%rbp)
	movq	-272(%rbp), %rsi
	xorq	%rsi, %rdi
	xorq	%rdi, -144(%rbp)
	movq	-144(%rbp), %rdx
	movq	%rdx, -144(%rbp)
	orq	%rdx, %rdi
	movq	%rdi, -240(%rbp)
	movq	-280(%rbp), %rsi
	xorq	%rsi, -216(%rbp)
	movq	-216(%rbp), %rdx
	xorq	%rdx, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	orq	%rdi, %rdx
	movq	-288(%rbp), %rsi
	xorq	%rsi, %r8
	xorq	%r8, -160(%rbp)
	movq	-160(%rbp), %rdi
	movq	%rdi, -160(%rbp)
	orq	%rdi, %r8
	movq	-296(%rbp), %rsi
	xorq	%rsi, %r9
	xorq	%r9, -168(%rbp)
	movq	-168(%rbp), %rdi
	movq	%rdi, -168(%rbp)
	orq	%rdi, %r9
	movq	-304(%rbp), %rsi
	xorq	%rsi, %r10
	xorq	%r10, -176(%rbp)
	movq	-176(%rbp), %rdi
	movq	%rdi, -176(%rbp)
	orq	%rdi, %r10
	movq	-312(%rbp), %rsi
	xorq	%rsi, %r11
	xorq	%r11, -184(%rbp)
	movq	-184(%rbp), %rdi
	movq	%rdi, -184(%rbp)
	orq	%rdi, %r11
	movq	-320(%rbp), %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, %rsi
	xorq	%rdi, -192(%rbp)
	movq	-192(%rbp), %rdi
	movq	%rdi, -192(%rbp)
	orq	%rdi, %rsi
	movq	%rsi, -88(%rbp)
	movq	-328(%rbp), %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %rdi
	xorq	%rdi, -200(%rbp)
	movq	-200(%rbp), %rsi
	movq	%rsi, -200(%rbp)
	orq	%rsi, %rdi
	movq	%rdi, -96(%rbp)
	movq	-336(%rbp), %rsi
	xorq	%rsi, -104(%rbp)
	movq	-104(%rbp), %rdi
	xorq	%rdi, -208(%rbp)
	movq	-208(%rbp), %rsi
	movq	%rsi, -208(%rbp)
	orq	%rsi, %rdi
	movq	%rdi, -104(%rbp)
	movq	-344(%rbp), %rsi
	xorq	%rsi, -112(%rbp)
	movq	-112(%rbp), %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	movq	%rdi, -112(%rbp)
	xorq	-352(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-360(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-248(%rbp), %rsi
	xorq	%rsi, %rax
	xorq	%rax, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	orq	%rdi, %rax
	movq	%rax, -224(%rbp)
	movq	-256(%rbp), %rsi
	xorq	%rsi, %rcx
	xorq	%rcx, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	orq	%rdi, %rcx
	movq	-264(%rbp), %rax
	xorq	%rax, -368(%rbp)
	movq	-368(%rbp), %rsi
	xorq	%rsi, -136(%rbp)
	movq	-136(%rbp), %rdi
	movq	%rdi, -136(%rbp)
	orq	%rdi, %rsi
	movq	-272(%rbp), %rax
	xorq	%rax, -240(%rbp)
	movq	-240(%rbp), %rdi
	xorq	%rdi, -144(%rbp)
	movq	-144(%rbp), %rax
	movq	%rax, -144(%rbp)
	orq	%rax, %rdi
	movq	-280(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -152(%rbp)
	movq	-152(%rbp), %rax
	movq	%rax, -152(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -216(%rbp)
	movq	-288(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -160(%rbp)
	movq	-160(%rbp), %rax
	movq	%rax, -160(%rbp)
	orq	%rax, %r8
	movq	-296(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -168(%rbp)
	movq	-168(%rbp), %rdx
	movq	%rdx, -168(%rbp)
	orq	%rdx, %r9
	movq	-304(%rbp), %rdx
	xorq	%rdx, %r10
	xorq	%r10, -176(%rbp)
	movq	-176(%rbp), %rax
	movq	%rax, -176(%rbp)
	orq	%rax, %r10
	movq	-312(%rbp), %rdx
	xorq	%rdx, %r11
	xorq	%r11, -184(%rbp)
	movq	-184(%rbp), %rdx
	movq	%rdx, -184(%rbp)
	orq	%rdx, %r11
	movq	-320(%rbp), %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %rdx
	xorq	%rdx, -192(%rbp)
	movq	-192(%rbp), %rax
	movq	%rax, -192(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -88(%rbp)
	movq	-328(%rbp), %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %rdx
	xorq	%rdx, -200(%rbp)
	movq	-200(%rbp), %rax
	movq	%rax, -200(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -96(%rbp)
	movq	-336(%rbp), %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %rdx
	xorq	%rdx, -208(%rbp)
	movq	-208(%rbp), %rax
	movq	%rax, -208(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -104(%rbp)
	movq	-344(%rbp), %rax
	xorq	%rax, -112(%rbp)
	movq	-112(%rbp), %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	movq	%rdx, -112(%rbp)
	xorq	-352(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-360(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-248(%rbp), %rdx
	xorq	%rdx, -224(%rbp)
	movq	-224(%rbp), %rax
	xorq	%rax, -120(%rbp)
	movq	-120(%rbp), %rdx
	movq	%rdx, -120(%rbp)
	orq	%rdx, %rax
	movq	-256(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, -128(%rbp)
	movq	-128(%rbp), %rdx
	movq	%rdx, -128(%rbp)
	orq	%rdx, %rcx
	movq	%rcx, -232(%rbp)
	movq	-264(%rbp), %rcx
	xorq	%rcx, %rsi
	xorq	%rsi, -136(%rbp)
	movq	-136(%rbp), %rcx
	movq	%rcx, -136(%rbp)
	orq	%rcx, %rsi
	movq	-272(%rbp), %rcx
	xorq	%rcx, %rdi
	xorq	%rdi, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	orq	%rcx, %rdi
	movq	%rdi, -240(%rbp)
	movq	-280(%rbp), %rdi
	xorq	%rdi, -216(%rbp)
	movq	-216(%rbp), %rdx
	xorq	%rdx, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	orq	%rcx, %rdx
	movq	-288(%rbp), %rdi
	xorq	%rdi, %r8
	xorq	%r8, -160(%rbp)
	movq	-160(%rbp), %rcx
	movq	%rcx, -160(%rbp)
	orq	%rcx, %r8
	movq	-296(%rbp), %rdi
	xorq	%rdi, %r9
	xorq	%r9, -168(%rbp)
	movq	-168(%rbp), %rcx
	movq	%rcx, -168(%rbp)
	orq	%rcx, %r9
	movq	-304(%rbp), %rdi
	xorq	%rdi, %r10
	xorq	%r10, -176(%rbp)
	movq	-176(%rbp), %rcx
	movq	%rcx, -176(%rbp)
	orq	%rcx, %r10
	movq	-312(%rbp), %rdi
	xorq	%rdi, %r11
	xorq	%r11, -184(%rbp)
	movq	-184(%rbp), %rcx
	movq	%rcx, -184(%rbp)
	orq	%rcx, %r11
	movq	-320(%rbp), %rdi
	xorq	%rdi, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, %rdi
	xorq	%rcx, -192(%rbp)
	movq	-192(%rbp), %rcx
	movq	%rcx, -192(%rbp)
	orq	%rcx, %rdi
	movq	%rdi, -88(%rbp)
	movq	-328(%rbp), %rdi
	xorq	%rdi, -96(%rbp)
	movq	-96(%rbp), %rcx
	xorq	%rcx, -200(%rbp)
	movq	-200(%rbp), %rdi
	movq	%rdi, -200(%rbp)
	orq	%rdi, %rcx
	movq	%rcx, -96(%rbp)
	movq	-336(%rbp), %rdi
	xorq	%rdi, -104(%rbp)
	movq	-104(%rbp), %rcx
	xorq	%rcx, -208(%rbp)
	movq	-208(%rbp), %rdi
	movq	%rdi, -208(%rbp)
	orq	%rdi, %rcx
	movq	%rcx, -104(%rbp)
	movq	-344(%rbp), %rdi
	xorq	%rdi, -112(%rbp)
	movq	-112(%rbp), %rcx
	xorq	%rcx, %r15
	orq	%r15, %rcx
	movq	%rcx, -112(%rbp)
	xorq	-352(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-360(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-248(%rbp), %rdi
	xorq	%rdi, %rax
	xorq	%rax, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	orq	%rcx, %rax
	movq	%rax, -224(%rbp)
	movq	-256(%rbp), %rax
	xorq	%rax, -232(%rbp)
	movq	-232(%rbp), %rcx
	xorq	%rcx, -128(%rbp)
	movq	-128(%rbp), %rax
	movq	%rax, -128(%rbp)
	orq	%rax, %rcx
	movq	%rcx, -232(%rbp)
	movq	-264(%rbp), %rax
	xorq	%rax, %rsi
	xorq	%rsi, -136(%rbp)
	movq	-136(%rbp), %rax
	movq	%rax, -136(%rbp)
	orq	%rax, %rsi
	movq	%rsi, -368(%rbp)
	movq	-272(%rbp), %rax
	xorq	%rax, -240(%rbp)
	movq	-240(%rbp), %rdi
	xorq	%rdi, -144(%rbp)
	movq	-144(%rbp), %rsi
	movq	%rsi, -144(%rbp)
	orq	%rsi, %rdi
	movq	%rdi, -240(%rbp)
	movq	-280(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -152(%rbp)
	movq	-152(%rbp), %rsi
	movq	%rsi, -152(%rbp)
	orq	%rsi, %rdx
	movq	%rdx, -216(%rbp)
	movq	-288(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -160(%rbp)
	movq	-160(%rbp), %rsi
	movq	%rsi, -160(%rbp)
	orq	%rsi, %r8
	movq	%r8, -376(%rbp)
	movq	-296(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -168(%rbp)
	movq	-168(%rbp), %rsi
	movq	%rsi, -168(%rbp)
	orq	%rsi, %r9
	movq	%r9, -384(%rbp)
	movq	-304(%rbp), %rdx
	xorq	%rdx, %r10
	xorq	%r10, -176(%rbp)
	movq	-176(%rbp), %rsi
	movq	%rsi, -176(%rbp)
	orq	%rsi, %r10
	movq	%r10, -392(%rbp)
	movq	-312(%rbp), %rdx
	xorq	%rdx, %r11
	xorq	%r11, -184(%rbp)
	movq	-184(%rbp), %rsi
	movq	%rsi, -184(%rbp)
	orq	%rsi, %r11
	movq	%r11, -400(%rbp)
	movq	-320(%rbp), %r11
	xorq	%r11, -88(%rbp)
	movq	-88(%rbp), %rax
	xorq	%rax, -192(%rbp)
	movq	-192(%rbp), %rsi
	movq	%rsi, -192(%rbp)
	orq	%rsi, %rax
	movq	%rax, -88(%rbp)
	movq	-328(%rbp), %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %rcx
	xorq	%rcx, -200(%rbp)
	movq	-200(%rbp), %rax
	movq	%rax, -200(%rbp)
	orq	%rax, %rcx
	movq	%rcx, -96(%rbp)
	movq	-336(%rbp), %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %rcx
	xorq	%rcx, -208(%rbp)
	movq	-208(%rbp), %rdi
	movq	%rdi, -208(%rbp)
	orq	%rdi, %rcx
	movq	%rcx, -104(%rbp)
	movq	-344(%rbp), %rdi
	xorq	%rdi, -112(%rbp)
	movq	-112(%rbp), %rcx
	xorq	%rcx, %r15
	orq	%r15, %rcx
	movq	%rcx, -112(%rbp)
	xorq	-352(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-360(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
.L301:
	movq	-408(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -408(%rbp)
	testq	%rax, %rax
	jne	.L302
	movl	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-368(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-376(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-384(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-392(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-400(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$376, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE103:
	.size	int64_bit_14, .-int64_bit_14
	.globl	int64_bit_15
	.type	int64_bit_15, @function
int64_bit_15:
.LFB104:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$392, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -432(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -240(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	addq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -264(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -248(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -272(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -392(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$1, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -280(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -256(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$2, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -288(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$3, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -160(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -296(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -400(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$4, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -168(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -304(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -408(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$5, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -176(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -312(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -416(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$6, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -184(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -320(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -424(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$7, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -192(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -328(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$8, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -200(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -336(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$9, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -208(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -344(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	subl	$10, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$10, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -216(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -352(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	subl	$11, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$11, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -224(%rbp)
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -360(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	subl	$12, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$12, %rax
	movq	%rax, %r15
	movq	%r15, %rax
	salq	$32, %rax
	orq	%rax, %r15
	leaq	0(,%r15,4), %rax
	subq	$1, %rax
	movq	%rax, -368(%rbp)
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	subl	$13, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$32, %rax
	orq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$13, %rax
	movq	%rax, %r13
	movq	%r13, %rax
	salq	$32, %rax
	orq	%rax, %r13
	leaq	0(,%r13,4), %rax
	subq	$1, %rax
	movq	%rax, -376(%rbp)
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	subl	$14, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$32, %rax
	orq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	movslq	%eax, %rdx
	movq	-72(%rbp), %rax
	addq	%rdx, %rax
	subq	$14, %rax
	movq	%rax, %rbx
	movq	%rbx, %rax
	salq	$32, %rax
	orq	%rax, %rbx
	leaq	0(,%rbx,4), %rax
	subq	$1, %rax
	movq	%rax, -384(%rbp)
	jmp	.L304
.L305:
	subq	$1, -264(%rbp)
	movq	-264(%rbp), %rsi
	subq	$1, -272(%rbp)
	movq	-272(%rbp), %rdi
	subq	$1, -280(%rbp)
	movq	-280(%rbp), %rdx
	subq	$1, -288(%rbp)
	movq	-288(%rbp), %r11
	subq	$1, -296(%rbp)
	movq	-296(%rbp), %r9
	subq	$1, -304(%rbp)
	movq	-304(%rbp), %r10
	subq	$1, -312(%rbp)
	subq	$1, -320(%rbp)
	subq	$1, -328(%rbp)
	subq	$1, -336(%rbp)
	subq	$1, -344(%rbp)
	subq	$1, -352(%rbp)
	subq	$1, -360(%rbp)
	subq	$1, -368(%rbp)
	subq	$1, -376(%rbp)
	subq	$1, -384(%rbp)
	movq	%rsi, -264(%rbp)
	xorq	%rsi, -240(%rbp)
	movq	-240(%rbp), %rax
	xorq	%rax, -128(%rbp)
	movq	-128(%rbp), %rsi
	movq	%rsi, -128(%rbp)
	orq	%rsi, %rax
	movq	%rax, -240(%rbp)
	movq	%rdi, -272(%rbp)
	xorq	%rdi, -248(%rbp)
	movq	-248(%rbp), %rcx
	xorq	%rcx, -136(%rbp)
	movq	-136(%rbp), %rdi
	movq	%rdi, -136(%rbp)
	orq	%rdi, %rcx
	movq	%rdx, -280(%rbp)
	xorq	%rdx, -392(%rbp)
	movq	-392(%rbp), %rsi
	xorq	%rsi, -144(%rbp)
	movq	-144(%rbp), %rdx
	movq	%rdx, -144(%rbp)
	orq	%rdx, %rsi
	movq	%r11, -288(%rbp)
	xorq	%r11, -256(%rbp)
	movq	-256(%rbp), %rdi
	xorq	%rdi, -152(%rbp)
	movq	-152(%rbp), %r11
	movq	%r11, -152(%rbp)
	orq	%r11, %rdi
	movq	%r9, -296(%rbp)
	xorq	%r9, -232(%rbp)
	movq	-232(%rbp), %rdx
	xorq	%rdx, -160(%rbp)
	movq	-160(%rbp), %r8
	movq	%r8, -160(%rbp)
	orq	%r8, %rdx
	movq	%r10, -304(%rbp)
	movq	%r10, %rax
	xorq	%rax, -400(%rbp)
	movq	-400(%rbp), %r8
	xorq	%r8, -168(%rbp)
	movq	-168(%rbp), %r9
	movq	%r9, -168(%rbp)
	orq	%r9, %r8
	movq	-312(%rbp), %rax
	xorq	%rax, -408(%rbp)
	movq	-408(%rbp), %r9
	xorq	%r9, -176(%rbp)
	movq	-176(%rbp), %r11
	movq	%r11, -176(%rbp)
	orq	%r11, %r9
	movq	-320(%rbp), %rax
	xorq	%rax, -416(%rbp)
	movq	-416(%rbp), %r10
	xorq	%r10, -184(%rbp)
	movq	-184(%rbp), %r11
	movq	%r11, -184(%rbp)
	orq	%r11, %r10
	movq	%r10, -416(%rbp)
	movq	-328(%rbp), %rax
	xorq	%rax, -424(%rbp)
	movq	-424(%rbp), %r11
	xorq	%r11, -192(%rbp)
	movq	-192(%rbp), %r10
	movq	%r10, -192(%rbp)
	orq	%r10, %r11
	movq	-336(%rbp), %rax
	xorq	%rax, -88(%rbp)
	movq	-88(%rbp), %r10
	movq	%r10, %rax
	xorq	%r10, -200(%rbp)
	movq	-200(%rbp), %r10
	movq	%r10, -200(%rbp)
	orq	%r10, %rax
	movq	%rax, -88(%rbp)
	movq	-344(%rbp), %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %r10
	xorq	%r10, -208(%rbp)
	movq	-208(%rbp), %rax
	movq	%rax, -208(%rbp)
	orq	%rax, %r10
	movq	%r10, -96(%rbp)
	movq	-352(%rbp), %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %r10
	xorq	%r10, -216(%rbp)
	movq	-216(%rbp), %rax
	movq	%rax, -216(%rbp)
	orq	%rax, %r10
	movq	%r10, -104(%rbp)
	movq	-360(%rbp), %rax
	xorq	%rax, -112(%rbp)
	movq	-112(%rbp), %r10
	xorq	%r10, -224(%rbp)
	movq	-224(%rbp), %rax
	movq	%rax, -224(%rbp)
	orq	%rax, %r10
	movq	%r10, -112(%rbp)
	movq	-368(%rbp), %rax
	xorq	%rax, -120(%rbp)
	movq	-120(%rbp), %r10
	xorq	%r10, %r15
	orq	%r15, %r10
	movq	%r10, -120(%rbp)
	xorq	-376(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-384(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-264(%rbp), %r10
	xorq	%r10, -240(%rbp)
	movq	-240(%rbp), %rax
	xorq	%rax, -128(%rbp)
	movq	-128(%rbp), %r10
	movq	%r10, -128(%rbp)
	orq	%r10, %rax
	movq	-272(%rbp), %r10
	xorq	%r10, %rcx
	xorq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r10
	movq	%r10, -136(%rbp)
	orq	%r10, %rcx
	movq	-280(%rbp), %r10
	xorq	%r10, %rsi
	xorq	%rsi, -144(%rbp)
	movq	-144(%rbp), %r10
	movq	%r10, -144(%rbp)
	orq	%r10, %rsi
	movq	-288(%rbp), %r10
	xorq	%r10, %rdi
	xorq	%rdi, -152(%rbp)
	movq	-152(%rbp), %r10
	movq	%r10, -152(%rbp)
	orq	%r10, %rdi
	movq	-296(%rbp), %r10
	xorq	%r10, %rdx
	xorq	%rdx, -160(%rbp)
	movq	-160(%rbp), %r10
	movq	%r10, -160(%rbp)
	orq	%r10, %rdx
	movq	%rdx, -232(%rbp)
	movq	-304(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -168(%rbp)
	movq	-168(%rbp), %r10
	movq	%r10, -168(%rbp)
	orq	%r10, %r8
	movq	-312(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -176(%rbp)
	movq	-176(%rbp), %r10
	movq	%r10, -176(%rbp)
	orq	%r10, %r9
	movq	%r9, -408(%rbp)
	movq	-320(%rbp), %rdx
	xorq	%rdx, -416(%rbp)
	movq	-416(%rbp), %r10
	xorq	%r10, -184(%rbp)
	movq	-184(%rbp), %r9
	movq	%r9, -184(%rbp)
	orq	%r9, %r10
	movq	-328(%rbp), %rdx
	xorq	%rdx, %r11
	xorq	%r11, -192(%rbp)
	movq	-192(%rbp), %r9
	movq	%r9, -192(%rbp)
	orq	%r9, %r11
	movq	-336(%rbp), %rdx
	xorq	%rdx, -88(%rbp)
	movq	-88(%rbp), %r9
	movq	%r9, %rdx
	xorq	%r9, -200(%rbp)
	movq	-200(%rbp), %r9
	movq	%r9, -200(%rbp)
	orq	%r9, %rdx
	movq	%rdx, -88(%rbp)
	movq	-344(%rbp), %rdx
	xorq	%rdx, -96(%rbp)
	movq	-96(%rbp), %r9
	xorq	%r9, -208(%rbp)
	movq	-208(%rbp), %rdx
	movq	%rdx, -208(%rbp)
	orq	%rdx, %r9
	movq	%r9, -96(%rbp)
	movq	-352(%rbp), %rdx
	xorq	%rdx, -104(%rbp)
	movq	-104(%rbp), %r9
	xorq	%r9, -216(%rbp)
	movq	-216(%rbp), %rdx
	movq	%rdx, -216(%rbp)
	orq	%rdx, %r9
	movq	%r9, -104(%rbp)
	movq	-360(%rbp), %rdx
	xorq	%rdx, -112(%rbp)
	movq	-112(%rbp), %r9
	xorq	%r9, -224(%rbp)
	movq	-224(%rbp), %rdx
	movq	%rdx, -224(%rbp)
	orq	%rdx, %r9
	movq	%r9, -112(%rbp)
	movq	-368(%rbp), %rdx
	xorq	%rdx, -120(%rbp)
	movq	-120(%rbp), %r9
	xorq	%r9, %r15
	orq	%r15, %r9
	movq	%r9, -120(%rbp)
	xorq	-376(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-384(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-264(%rbp), %rdx
	xorq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	-128(%rbp), %r9
	movq	%r9, -128(%rbp)
	orq	%r9, %rax
	movq	-272(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r9
	movq	%r9, -136(%rbp)
	orq	%r9, %rcx
	movq	%rcx, -248(%rbp)
	movq	-280(%rbp), %rcx
	xorq	%rcx, %rsi
	xorq	%rsi, -144(%rbp)
	movq	-144(%rbp), %r9
	movq	%r9, -144(%rbp)
	orq	%r9, %rsi
	movq	-288(%rbp), %rcx
	xorq	%rcx, %rdi
	xorq	%rdi, -152(%rbp)
	movq	-152(%rbp), %r9
	movq	%r9, -152(%rbp)
	orq	%r9, %rdi
	movq	-296(%rbp), %rcx
	xorq	%rcx, -232(%rbp)
	movq	-232(%rbp), %rdx
	xorq	%rdx, -160(%rbp)
	movq	-160(%rbp), %r9
	movq	%r9, -160(%rbp)
	orq	%r9, %rdx
	movq	-304(%rbp), %rcx
	xorq	%rcx, %r8
	xorq	%r8, -168(%rbp)
	movq	-168(%rbp), %r9
	movq	%r9, -168(%rbp)
	orq	%r9, %r8
	movq	%r8, -400(%rbp)
	movq	-312(%rbp), %rcx
	xorq	%rcx, -408(%rbp)
	movq	-408(%rbp), %r9
	xorq	%r9, -176(%rbp)
	movq	-176(%rbp), %r8
	movq	%r8, -176(%rbp)
	orq	%r8, %r9
	movq	-320(%rbp), %rcx
	xorq	%rcx, %r10
	xorq	%r10, -184(%rbp)
	movq	-184(%rbp), %r8
	movq	%r8, -184(%rbp)
	orq	%r8, %r10
	movq	-328(%rbp), %rcx
	xorq	%rcx, %r11
	xorq	%r11, -192(%rbp)
	movq	-192(%rbp), %r8
	movq	%r8, -192(%rbp)
	orq	%r8, %r11
	movq	-336(%rbp), %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, %rcx
	xorq	%r8, -200(%rbp)
	movq	-200(%rbp), %r8
	movq	%r8, -200(%rbp)
	orq	%r8, %rcx
	movq	%rcx, -88(%rbp)
	movq	-344(%rbp), %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %r8
	xorq	%r8, -208(%rbp)
	movq	-208(%rbp), %rcx
	movq	%rcx, -208(%rbp)
	orq	%rcx, %r8
	movq	%r8, -96(%rbp)
	movq	-352(%rbp), %rcx
	xorq	%rcx, -104(%rbp)
	movq	-104(%rbp), %r8
	xorq	%r8, -216(%rbp)
	movq	-216(%rbp), %rcx
	movq	%rcx, -216(%rbp)
	orq	%rcx, %r8
	movq	%r8, -104(%rbp)
	movq	-360(%rbp), %rcx
	xorq	%rcx, -112(%rbp)
	movq	-112(%rbp), %r8
	xorq	%r8, -224(%rbp)
	movq	-224(%rbp), %rcx
	movq	%rcx, -224(%rbp)
	orq	%rcx, %r8
	movq	%r8, -112(%rbp)
	movq	-368(%rbp), %rcx
	xorq	%rcx, -120(%rbp)
	movq	-120(%rbp), %r8
	xorq	%r8, %r15
	orq	%r15, %r8
	movq	%r8, -120(%rbp)
	xorq	-376(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-384(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-264(%rbp), %rcx
	xorq	%rcx, %rax
	xorq	%rax, -128(%rbp)
	movq	-128(%rbp), %r8
	movq	%r8, -128(%rbp)
	orq	%r8, %rax
	movq	-272(%rbp), %r8
	xorq	%r8, -248(%rbp)
	movq	-248(%rbp), %rcx
	xorq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	orq	%r8, %rcx
	movq	-280(%rbp), %r8
	xorq	%r8, %rsi
	xorq	%rsi, -144(%rbp)
	movq	-144(%rbp), %r8
	movq	%r8, -144(%rbp)
	orq	%r8, %rsi
	movq	%rsi, -392(%rbp)
	movq	-288(%rbp), %rsi
	xorq	%rsi, %rdi
	xorq	%rdi, -152(%rbp)
	movq	-152(%rbp), %r8
	movq	%r8, -152(%rbp)
	orq	%r8, %rdi
	movq	-296(%rbp), %rsi
	xorq	%rsi, %rdx
	xorq	%rdx, -160(%rbp)
	movq	-160(%rbp), %r8
	movq	%r8, -160(%rbp)
	orq	%r8, %rdx
	movq	%rdx, -232(%rbp)
	movq	-304(%rbp), %rsi
	xorq	%rsi, -400(%rbp)
	movq	-400(%rbp), %r8
	xorq	%r8, -168(%rbp)
	movq	-168(%rbp), %rdx
	movq	%rdx, -168(%rbp)
	orq	%rdx, %r8
	movq	-312(%rbp), %rsi
	xorq	%rsi, %r9
	xorq	%r9, -176(%rbp)
	movq	-176(%rbp), %rdx
	movq	%rdx, -176(%rbp)
	orq	%rdx, %r9
	movq	-320(%rbp), %rsi
	xorq	%rsi, %r10
	xorq	%r10, -184(%rbp)
	movq	-184(%rbp), %rdx
	movq	%rdx, -184(%rbp)
	orq	%rdx, %r10
	movq	-328(%rbp), %rsi
	xorq	%rsi, %r11
	xorq	%r11, -192(%rbp)
	movq	-192(%rbp), %rdx
	movq	%rdx, -192(%rbp)
	orq	%rdx, %r11
	movq	-336(%rbp), %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %rdx
	movq	%rdx, %rsi
	xorq	%rdx, -200(%rbp)
	movq	-200(%rbp), %rdx
	movq	%rdx, -200(%rbp)
	orq	%rdx, %rsi
	movq	%rsi, -88(%rbp)
	movq	-344(%rbp), %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %rdx
	xorq	%rdx, -208(%rbp)
	movq	-208(%rbp), %rsi
	movq	%rsi, -208(%rbp)
	orq	%rsi, %rdx
	movq	%rdx, -96(%rbp)
	movq	-352(%rbp), %rsi
	xorq	%rsi, -104(%rbp)
	movq	-104(%rbp), %rdx
	xorq	%rdx, -216(%rbp)
	movq	-216(%rbp), %rsi
	movq	%rsi, -216(%rbp)
	orq	%rsi, %rdx
	movq	%rdx, -104(%rbp)
	movq	-360(%rbp), %rsi
	xorq	%rsi, -112(%rbp)
	movq	-112(%rbp), %rdx
	xorq	%rdx, -224(%rbp)
	movq	-224(%rbp), %rsi
	movq	%rsi, -224(%rbp)
	orq	%rsi, %rdx
	movq	%rdx, -112(%rbp)
	movq	-368(%rbp), %rsi
	xorq	%rsi, -120(%rbp)
	movq	-120(%rbp), %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	movq	%rdx, -120(%rbp)
	xorq	-376(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-384(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-264(%rbp), %rsi
	xorq	%rsi, %rax
	xorq	%rax, -128(%rbp)
	movq	-128(%rbp), %rdx
	movq	%rdx, -128(%rbp)
	orq	%rdx, %rax
	movq	-272(%rbp), %rsi
	xorq	%rsi, %rcx
	xorq	%rcx, -136(%rbp)
	movq	-136(%rbp), %rdx
	movq	%rdx, -136(%rbp)
	orq	%rdx, %rcx
	movq	%rcx, -248(%rbp)
	movq	-280(%rbp), %rcx
	xorq	%rcx, -392(%rbp)
	movq	-392(%rbp), %rsi
	xorq	%rsi, -144(%rbp)
	movq	-144(%rbp), %rdx
	movq	%rdx, -144(%rbp)
	orq	%rdx, %rsi
	movq	-288(%rbp), %rcx
	xorq	%rcx, %rdi
	xorq	%rdi, -152(%rbp)
	movq	-152(%rbp), %rdx
	movq	%rdx, -152(%rbp)
	orq	%rdx, %rdi
	movq	%rdi, -256(%rbp)
	movq	-296(%rbp), %rcx
	xorq	%rcx, -232(%rbp)
	movq	-232(%rbp), %rdx
	xorq	%rdx, -160(%rbp)
	movq	-160(%rbp), %rdi
	movq	%rdi, -160(%rbp)
	orq	%rdi, %rdx
	movq	-304(%rbp), %rcx
	xorq	%rcx, %r8
	xorq	%r8, -168(%rbp)
	movq	-168(%rbp), %rdi
	movq	%rdi, -168(%rbp)
	orq	%rdi, %r8
	movq	-312(%rbp), %rcx
	xorq	%rcx, %r9
	xorq	%r9, -176(%rbp)
	movq	-176(%rbp), %rdi
	movq	%rdi, -176(%rbp)
	orq	%rdi, %r9
	movq	-320(%rbp), %rcx
	xorq	%rcx, %r10
	xorq	%r10, -184(%rbp)
	movq	-184(%rbp), %rdi
	movq	%rdi, -184(%rbp)
	orq	%rdi, %r10
	movq	-328(%rbp), %rcx
	xorq	%rcx, %r11
	xorq	%r11, -192(%rbp)
	movq	-192(%rbp), %rdi
	movq	%rdi, -192(%rbp)
	orq	%rdi, %r11
	movq	-336(%rbp), %rcx
	xorq	%rcx, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, %rcx
	xorq	%rdi, -200(%rbp)
	movq	-200(%rbp), %rdi
	movq	%rdi, -200(%rbp)
	orq	%rdi, %rcx
	movq	%rcx, -88(%rbp)
	movq	-344(%rbp), %rcx
	xorq	%rcx, -96(%rbp)
	movq	-96(%rbp), %rdi
	xorq	%rdi, -208(%rbp)
	movq	-208(%rbp), %rcx
	movq	%rcx, -208(%rbp)
	orq	%rcx, %rdi
	movq	%rdi, -96(%rbp)
	movq	-352(%rbp), %rcx
	xorq	%rcx, -104(%rbp)
	movq	-104(%rbp), %rdi
	xorq	%rdi, -216(%rbp)
	movq	-216(%rbp), %rcx
	movq	%rcx, -216(%rbp)
	orq	%rcx, %rdi
	movq	%rdi, -104(%rbp)
	movq	-360(%rbp), %rcx
	xorq	%rcx, -112(%rbp)
	movq	-112(%rbp), %rdi
	xorq	%rdi, -224(%rbp)
	movq	-224(%rbp), %rcx
	movq	%rcx, -224(%rbp)
	orq	%rcx, %rdi
	movq	%rdi, -112(%rbp)
	movq	-368(%rbp), %rcx
	xorq	%rcx, -120(%rbp)
	movq	-120(%rbp), %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	movq	%rdi, -120(%rbp)
	xorq	-376(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-384(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-264(%rbp), %rcx
	xorq	%rcx, %rax
	xorq	%rax, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	orq	%rdi, %rax
	movq	%rax, -240(%rbp)
	movq	-272(%rbp), %rax
	xorq	%rax, -248(%rbp)
	movq	-248(%rbp), %rcx
	xorq	%rcx, -136(%rbp)
	movq	-136(%rbp), %rdi
	movq	%rdi, -136(%rbp)
	orq	%rdi, %rcx
	movq	-280(%rbp), %rax
	xorq	%rax, %rsi
	xorq	%rsi, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	orq	%rdi, %rsi
	movq	-288(%rbp), %rax
	xorq	%rax, -256(%rbp)
	movq	-256(%rbp), %rdi
	xorq	%rdi, -152(%rbp)
	movq	-152(%rbp), %rax
	movq	%rax, -152(%rbp)
	orq	%rax, %rdi
	movq	-296(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -160(%rbp)
	movq	-160(%rbp), %rax
	movq	%rax, -160(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -232(%rbp)
	movq	-304(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -168(%rbp)
	movq	-168(%rbp), %rax
	movq	%rax, -168(%rbp)
	orq	%rax, %r8
	movq	-312(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -176(%rbp)
	movq	-176(%rbp), %rax
	movq	%rax, -176(%rbp)
	orq	%rax, %r9
	movq	-320(%rbp), %rdx
	xorq	%rdx, %r10
	xorq	%r10, -184(%rbp)
	movq	-184(%rbp), %rax
	movq	%rax, -184(%rbp)
	orq	%rax, %r10
	movq	-328(%rbp), %rdx
	xorq	%rdx, %r11
	xorq	%r11, -192(%rbp)
	movq	-192(%rbp), %rdx
	movq	%rdx, -192(%rbp)
	orq	%rdx, %r11
	movq	-336(%rbp), %rdx
	xorq	%rdx, -88(%rbp)
	movq	-88(%rbp), %rdx
	xorq	%rdx, -200(%rbp)
	movq	-200(%rbp), %rax
	movq	%rax, -200(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -88(%rbp)
	movq	-344(%rbp), %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %rdx
	xorq	%rdx, -208(%rbp)
	movq	-208(%rbp), %rax
	movq	%rax, -208(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -96(%rbp)
	movq	-352(%rbp), %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %rdx
	xorq	%rdx, -216(%rbp)
	movq	-216(%rbp), %rax
	movq	%rax, -216(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -104(%rbp)
	movq	-360(%rbp), %rax
	xorq	%rax, -112(%rbp)
	movq	-112(%rbp), %rdx
	xorq	%rdx, -224(%rbp)
	movq	-224(%rbp), %rax
	movq	%rax, -224(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -112(%rbp)
	movq	-368(%rbp), %rax
	xorq	%rax, -120(%rbp)
	movq	-120(%rbp), %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	movq	%rdx, -120(%rbp)
	xorq	-376(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-384(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-264(%rbp), %rdx
	xorq	%rdx, -240(%rbp)
	movq	-240(%rbp), %rax
	xorq	%rax, -128(%rbp)
	movq	-128(%rbp), %rdx
	movq	%rdx, -128(%rbp)
	orq	%rdx, %rax
	movq	-272(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, -136(%rbp)
	movq	-136(%rbp), %rdx
	movq	%rdx, -136(%rbp)
	orq	%rdx, %rcx
	movq	-280(%rbp), %rdx
	xorq	%rdx, %rsi
	xorq	%rsi, -144(%rbp)
	movq	-144(%rbp), %rdx
	movq	%rdx, -144(%rbp)
	orq	%rdx, %rsi
	movq	%rsi, -392(%rbp)
	movq	-288(%rbp), %rsi
	xorq	%rsi, %rdi
	xorq	%rdi, -152(%rbp)
	movq	-152(%rbp), %rdx
	movq	%rdx, -152(%rbp)
	orq	%rdx, %rdi
	movq	%rdi, -256(%rbp)
	movq	-296(%rbp), %rsi
	xorq	%rsi, -232(%rbp)
	movq	-232(%rbp), %rdx
	xorq	%rdx, -160(%rbp)
	movq	-160(%rbp), %rdi
	movq	%rdi, -160(%rbp)
	orq	%rdi, %rdx
	movq	-304(%rbp), %rsi
	xorq	%rsi, %r8
	xorq	%r8, -168(%rbp)
	movq	-168(%rbp), %rdi
	movq	%rdi, -168(%rbp)
	orq	%rdi, %r8
	movq	-312(%rbp), %rsi
	xorq	%rsi, %r9
	xorq	%r9, -176(%rbp)
	movq	-176(%rbp), %rdi
	movq	%rdi, -176(%rbp)
	orq	%rdi, %r9
	movq	-320(%rbp), %rsi
	xorq	%rsi, %r10
	xorq	%r10, -184(%rbp)
	movq	-184(%rbp), %rdi
	movq	%rdi, -184(%rbp)
	orq	%rdi, %r10
	movq	-328(%rbp), %rsi
	xorq	%rsi, %r11
	xorq	%r11, -192(%rbp)
	movq	-192(%rbp), %rdi
	movq	%rdi, -192(%rbp)
	orq	%rdi, %r11
	movq	-336(%rbp), %rsi
	xorq	%rsi, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, %rsi
	xorq	%rdi, -200(%rbp)
	movq	-200(%rbp), %rdi
	movq	%rdi, -200(%rbp)
	orq	%rdi, %rsi
	movq	%rsi, -88(%rbp)
	movq	-344(%rbp), %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %rdi
	xorq	%rdi, -208(%rbp)
	movq	-208(%rbp), %rsi
	movq	%rsi, -208(%rbp)
	orq	%rsi, %rdi
	movq	%rdi, -96(%rbp)
	movq	-352(%rbp), %rsi
	xorq	%rsi, -104(%rbp)
	movq	-104(%rbp), %rdi
	xorq	%rdi, -216(%rbp)
	movq	-216(%rbp), %rsi
	movq	%rsi, -216(%rbp)
	orq	%rsi, %rdi
	movq	%rdi, -104(%rbp)
	movq	-360(%rbp), %rsi
	xorq	%rsi, -112(%rbp)
	movq	-112(%rbp), %rdi
	xorq	%rdi, -224(%rbp)
	movq	-224(%rbp), %rsi
	movq	%rsi, -224(%rbp)
	orq	%rsi, %rdi
	movq	%rdi, -112(%rbp)
	movq	-368(%rbp), %rsi
	xorq	%rsi, -120(%rbp)
	movq	-120(%rbp), %rdi
	xorq	%rdi, %r15
	orq	%r15, %rdi
	movq	%rdi, -120(%rbp)
	xorq	-376(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-384(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-264(%rbp), %rsi
	xorq	%rsi, %rax
	xorq	%rax, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	orq	%rdi, %rax
	movq	%rax, -240(%rbp)
	movq	-272(%rbp), %rsi
	xorq	%rsi, %rcx
	xorq	%rcx, -136(%rbp)
	movq	-136(%rbp), %rdi
	movq	%rdi, -136(%rbp)
	orq	%rdi, %rcx
	movq	-280(%rbp), %rax
	xorq	%rax, -392(%rbp)
	movq	-392(%rbp), %rsi
	xorq	%rsi, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	orq	%rdi, %rsi
	movq	-288(%rbp), %rax
	xorq	%rax, -256(%rbp)
	movq	-256(%rbp), %rdi
	xorq	%rdi, -152(%rbp)
	movq	-152(%rbp), %rax
	movq	%rax, -152(%rbp)
	orq	%rax, %rdi
	movq	-296(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -160(%rbp)
	movq	-160(%rbp), %rax
	movq	%rax, -160(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -232(%rbp)
	movq	-304(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -168(%rbp)
	movq	-168(%rbp), %rax
	movq	%rax, -168(%rbp)
	orq	%rax, %r8
	movq	-312(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -176(%rbp)
	movq	-176(%rbp), %rax
	movq	%rax, -176(%rbp)
	orq	%rax, %r9
	movq	-320(%rbp), %rdx
	xorq	%rdx, %r10
	xorq	%r10, -184(%rbp)
	movq	-184(%rbp), %rdx
	movq	%rdx, -184(%rbp)
	orq	%rdx, %r10
	movq	-328(%rbp), %rdx
	xorq	%rdx, %r11
	xorq	%r11, -192(%rbp)
	movq	-192(%rbp), %rax
	movq	%rax, -192(%rbp)
	orq	%rax, %r11
	movq	-336(%rbp), %rdx
	xorq	%rdx, -88(%rbp)
	movq	-88(%rbp), %rdx
	xorq	%rdx, -200(%rbp)
	movq	-200(%rbp), %rax
	movq	%rdx, -88(%rbp)
	movq	%rax, -200(%rbp)
	movq	%rax, %rdx
	orq	%rdx, -88(%rbp)
	movq	-344(%rbp), %rax
	xorq	%rax, -96(%rbp)
	movq	-96(%rbp), %rdx
	xorq	%rdx, -208(%rbp)
	movq	-208(%rbp), %rax
	movq	%rax, -208(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -96(%rbp)
	movq	-352(%rbp), %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %rdx
	xorq	%rdx, -216(%rbp)
	movq	-216(%rbp), %rax
	movq	%rax, -216(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -104(%rbp)
	movq	-360(%rbp), %rax
	xorq	%rax, -112(%rbp)
	movq	-112(%rbp), %rdx
	xorq	%rdx, -224(%rbp)
	movq	-224(%rbp), %rax
	movq	%rax, -224(%rbp)
	orq	%rax, %rdx
	movq	%rdx, -112(%rbp)
	movq	-368(%rbp), %rax
	xorq	%rax, -120(%rbp)
	movq	-120(%rbp), %rdx
	xorq	%rdx, %r15
	orq	%r15, %rdx
	movq	%rdx, -120(%rbp)
	xorq	-376(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-384(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-264(%rbp), %rdx
	xorq	%rdx, -240(%rbp)
	movq	-240(%rbp), %rax
	xorq	%rax, -128(%rbp)
	movq	-128(%rbp), %rdx
	movq	%rdx, -128(%rbp)
	orq	%rdx, %rax
	movq	-272(%rbp), %rdx
	xorq	%rdx, %rcx
	xorq	%rcx, -136(%rbp)
	movq	-136(%rbp), %rdx
	movq	%rdx, -136(%rbp)
	orq	%rdx, %rcx
	movq	%rcx, -248(%rbp)
	movq	-280(%rbp), %rcx
	xorq	%rcx, %rsi
	xorq	%rsi, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	orq	%rcx, %rsi
	movq	-288(%rbp), %rcx
	xorq	%rcx, %rdi
	xorq	%rdi, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	orq	%rcx, %rdi
	movq	%rdi, -256(%rbp)
	movq	-296(%rbp), %rdi
	xorq	%rdi, -232(%rbp)
	movq	-232(%rbp), %rdx
	xorq	%rdx, -160(%rbp)
	movq	-160(%rbp), %rcx
	movq	%rcx, -160(%rbp)
	orq	%rcx, %rdx
	movq	-304(%rbp), %rdi
	xorq	%rdi, %r8
	xorq	%r8, -168(%rbp)
	movq	-168(%rbp), %rcx
	movq	%rcx, -168(%rbp)
	orq	%rcx, %r8
	movq	-312(%rbp), %rdi
	xorq	%rdi, %r9
	xorq	%r9, -176(%rbp)
	movq	-176(%rbp), %rcx
	movq	%rcx, -176(%rbp)
	orq	%rcx, %r9
	movq	-320(%rbp), %rdi
	xorq	%rdi, %r10
	xorq	%r10, -184(%rbp)
	movq	-184(%rbp), %rcx
	movq	%rcx, -184(%rbp)
	orq	%rcx, %r10
	movq	-328(%rbp), %rdi
	xorq	%rdi, %r11
	xorq	%r11, -192(%rbp)
	movq	-192(%rbp), %rcx
	movq	%rcx, -192(%rbp)
	orq	%rcx, %r11
	movq	-336(%rbp), %rdi
	xorq	%rdi, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, %rdi
	xorq	%rcx, -200(%rbp)
	movq	-200(%rbp), %rcx
	movq	%rcx, -200(%rbp)
	orq	%rcx, %rdi
	movq	%rdi, -88(%rbp)
	movq	-344(%rbp), %rdi
	xorq	%rdi, -96(%rbp)
	movq	-96(%rbp), %rcx
	xorq	%rcx, -208(%rbp)
	movq	-208(%rbp), %rdi
	movq	%rdi, -208(%rbp)
	orq	%rdi, %rcx
	movq	%rcx, -96(%rbp)
	movq	-352(%rbp), %rdi
	xorq	%rdi, -104(%rbp)
	movq	-104(%rbp), %rcx
	xorq	%rcx, -216(%rbp)
	movq	-216(%rbp), %rdi
	movq	%rdi, -216(%rbp)
	orq	%rdi, %rcx
	movq	%rcx, -104(%rbp)
	movq	-360(%rbp), %rdi
	xorq	%rdi, -112(%rbp)
	movq	-112(%rbp), %rcx
	xorq	%rcx, -224(%rbp)
	movq	-224(%rbp), %rdi
	movq	%rdi, -224(%rbp)
	orq	%rdi, %rcx
	movq	%rcx, -112(%rbp)
	movq	-368(%rbp), %rdi
	xorq	%rdi, -120(%rbp)
	movq	-120(%rbp), %rcx
	xorq	%rcx, %r15
	orq	%r15, %rcx
	movq	%rcx, -120(%rbp)
	xorq	-376(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-384(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
	movq	-264(%rbp), %rdi
	xorq	%rdi, %rax
	xorq	%rax, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	orq	%rcx, %rax
	movq	%rax, -240(%rbp)
	movq	-272(%rbp), %rax
	xorq	%rax, -248(%rbp)
	movq	-248(%rbp), %rcx
	xorq	%rcx, -136(%rbp)
	movq	-136(%rbp), %rax
	movq	%rax, -136(%rbp)
	orq	%rax, %rcx
	movq	%rcx, -248(%rbp)
	movq	-280(%rbp), %rax
	xorq	%rax, %rsi
	xorq	%rsi, -144(%rbp)
	movq	-144(%rbp), %rax
	movq	%rax, -144(%rbp)
	orq	%rax, %rsi
	movq	%rsi, -392(%rbp)
	movq	-288(%rbp), %rax
	xorq	%rax, -256(%rbp)
	movq	-256(%rbp), %rdi
	xorq	%rdi, -152(%rbp)
	movq	-152(%rbp), %rsi
	movq	%rsi, -152(%rbp)
	orq	%rsi, %rdi
	movq	%rdi, -256(%rbp)
	movq	-296(%rbp), %rax
	xorq	%rax, %rdx
	xorq	%rdx, -160(%rbp)
	movq	-160(%rbp), %rsi
	movq	%rsi, -160(%rbp)
	orq	%rsi, %rdx
	movq	%rdx, -232(%rbp)
	movq	-304(%rbp), %rdx
	xorq	%rdx, %r8
	xorq	%r8, -168(%rbp)
	movq	-168(%rbp), %rsi
	movq	%rsi, -168(%rbp)
	orq	%rsi, %r8
	movq	%r8, -400(%rbp)
	movq	-312(%rbp), %rdx
	xorq	%rdx, %r9
	xorq	%r9, -176(%rbp)
	movq	-176(%rbp), %rsi
	movq	%rsi, -176(%rbp)
	orq	%rsi, %r9
	movq	%r9, -408(%rbp)
	movq	-320(%rbp), %rdx
	xorq	%rdx, %r10
	xorq	%r10, -184(%rbp)
	movq	-184(%rbp), %rsi
	movq	%rsi, -184(%rbp)
	orq	%rsi, %r10
	movq	%r10, -416(%rbp)
	movq	-328(%rbp), %rdx
	xorq	%rdx, %r11
	xorq	%r11, -192(%rbp)
	movq	-192(%rbp), %rsi
	movq	%rsi, -192(%rbp)
	orq	%rsi, %r11
	movq	%r11, -424(%rbp)
	movq	-336(%rbp), %rdx
	xorq	%rdx, -88(%rbp)
	movq	-88(%rbp), %rax
	xorq	%rax, -200(%rbp)
	movq	-200(%rbp), %rsi
	movq	%rsi, -200(%rbp)
	orq	%rsi, %rax
	movq	%rax, -88(%rbp)
	movq	-344(%rbp), %rsi
	xorq	%rsi, -96(%rbp)
	movq	-96(%rbp), %rcx
	xorq	%rcx, -208(%rbp)
	movq	-208(%rbp), %rax
	movq	%rax, -208(%rbp)
	orq	%rax, %rcx
	movq	%rcx, -96(%rbp)
	movq	-352(%rbp), %rax
	xorq	%rax, -104(%rbp)
	movq	-104(%rbp), %rcx
	xorq	%rcx, -216(%rbp)
	movq	-216(%rbp), %rdi
	movq	%rdi, -216(%rbp)
	orq	%rdi, %rcx
	movq	%rcx, -104(%rbp)
	movq	-360(%rbp), %rax
	xorq	%rax, -112(%rbp)
	movq	-112(%rbp), %rcx
	xorq	%rcx, -224(%rbp)
	movq	-224(%rbp), %rdi
	movq	%rdi, -224(%rbp)
	orq	%rdi, %rcx
	movq	%rcx, -112(%rbp)
	movq	-368(%rbp), %rdi
	xorq	%rdi, -120(%rbp)
	movq	-120(%rbp), %rcx
	xorq	%rcx, %r15
	orq	%r15, %rcx
	movq	%rcx, -120(%rbp)
	xorq	-376(%rbp), %r14
	xorq	%r14, %r13
	orq	%r13, %r14
	xorq	-384(%rbp), %r12
	xorq	%r12, %rbx
	orq	%rbx, %r12
.L304:
	movq	-432(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -432(%rbp)
	testq	%rax, %rax
	jne	.L305
	movl	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-392(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-400(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-408(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-416(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-424(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$392, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE104:
	.size	int64_bit_15, .-int64_bit_15
	.globl	int64_bit_benchmarks
	.section	.data.rel.local
	.align 32
	.type	int64_bit_benchmarks, @object
	.size	int64_bit_benchmarks, 128
int64_bit_benchmarks:
	.quad	int64_bit_0
	.quad	int64_bit_1
	.quad	int64_bit_2
	.quad	int64_bit_3
	.quad	int64_bit_4
	.quad	int64_bit_5
	.quad	int64_bit_6
	.quad	int64_bit_7
	.quad	int64_bit_8
	.quad	int64_bit_9
	.quad	int64_bit_10
	.quad	int64_bit_11
	.quad	int64_bit_12
	.quad	int64_bit_13
	.quad	int64_bit_14
	.quad	int64_bit_15
	.text
	.globl	int64_add_0
	.type	int64_add_0, @function
int64_add_0:
.LFB105:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 13, -24
	.cfi_offset 12, -32
	.cfi_offset 3, -40
	movq	%rdi, -56(%rbp)
	movq	%rsi, -64(%rbp)
	movq	-56(%rbp), %r13
	movq	-64(%rbp), %rax
	movq	%rax, -40(%rbp)
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %r12
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	addl	$255, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %r12
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	addl	$21698325, %eax
	movslq	%eax, %rbx
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	jmp	.L307
.L308:
	addq	%rbx, %r12
	subq	%r12, %rbx
	addq	%rbx, %r12
	subq	%r12, %rbx
	addq	%rbx, %r12
	subq	%r12, %rbx
	addq	%rbx, %r12
	subq	%r12, %rbx
	addq	%rbx, %r12
	subq	%r12, %rbx
	addq	%rbx, %r12
	subq	%r12, %rbx
	addq	%rbx, %r12
	subq	%r12, %rbx
	addq	%rbx, %r12
	subq	%r12, %rbx
	addq	%rbx, %r12
	subq	%r12, %rbx
	addq	%rbx, %r12
	subq	%r12, %rbx
.L307:
	movq	%r13, %rax
	leaq	-1(%rax), %r13
	testq	%rax, %rax
	jne	.L308
	movl	%r12d, %edx
	movl	%ebx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE105:
	.size	int64_add_0, .-int64_add_0
	.globl	int64_add_1
	.type	int64_add_1, @function
int64_add_1:
.LFB106:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %r15
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %r14
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$255, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$21698325, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %r13
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$254, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$21698324, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$65534, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	jmp	.L310
.L311:
	addq	%r12, %r14
	subq	%r14, %r12
	addq	%rbx, %r13
	subq	%r13, %rbx
	addq	%r12, %r14
	subq	%r14, %r12
	addq	%rbx, %r13
	subq	%r13, %rbx
	addq	%r12, %r14
	subq	%r14, %r12
	addq	%rbx, %r13
	subq	%r13, %rbx
	addq	%r12, %r14
	subq	%r14, %r12
	addq	%rbx, %r13
	subq	%r13, %rbx
	addq	%r12, %r14
	subq	%r14, %r12
	addq	%rbx, %r13
	subq	%r13, %rbx
	addq	%r12, %r14
	subq	%r14, %r12
	addq	%rbx, %r13
	subq	%r13, %rbx
	addq	%r12, %r14
	subq	%r14, %r12
	addq	%rbx, %r13
	subq	%r13, %rbx
	addq	%r12, %r14
	subq	%r14, %r12
	addq	%rbx, %r13
	subq	%r13, %rbx
	addq	%r12, %r14
	subq	%r14, %r12
	addq	%rbx, %r13
	subq	%r13, %rbx
	addq	%r12, %r14
	subq	%r14, %r12
	addq	%rbx, %r13
	subq	%r13, %rbx
.L310:
	movq	%r15, %rax
	leaq	-1(%rax), %r15
	testq	%rax, %rax
	jne	.L311
	movl	%r14d, %edx
	movl	%r12d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %edx
	movl	%ebx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE106:
	.size	int64_add_1, .-int64_add_1
	.globl	int64_add_2
	.type	int64_add_2, @function
int64_add_2:
.LFB107:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %rsi
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$255, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rcx
	movq	%rcx, %rdi
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$21698325, %eax
	movslq	%eax, %r13
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %r15
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$254, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %r15
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$21698324, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$65534, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %r14
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$253, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$21698323, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$65533, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	jmp	.L313
.L314:
	movq	%rdi, %rax
	addq	%r13, %rax
	subq	%rax, %r13
	addq	%r12, %r15
	subq	%r15, %r12
	addq	%rbx, %r14
	subq	%r14, %rbx
	addq	%r13, %rax
	subq	%rax, %r13
	addq	%r12, %r15
	subq	%r15, %r12
	addq	%rbx, %r14
	subq	%r14, %rbx
	addq	%r13, %rax
	subq	%rax, %r13
	addq	%r12, %r15
	subq	%r15, %r12
	addq	%rbx, %r14
	subq	%r14, %rbx
	addq	%r13, %rax
	subq	%rax, %r13
	addq	%r12, %r15
	subq	%r15, %r12
	addq	%rbx, %r14
	subq	%r14, %rbx
	addq	%r13, %rax
	subq	%rax, %r13
	addq	%r12, %r15
	subq	%r15, %r12
	addq	%rbx, %r14
	subq	%r14, %rbx
	addq	%r13, %rax
	subq	%rax, %r13
	addq	%r12, %r15
	subq	%r15, %r12
	addq	%rbx, %r14
	subq	%r14, %rbx
	addq	%r13, %rax
	subq	%rax, %r13
	addq	%r12, %r15
	subq	%r15, %r12
	addq	%rbx, %r14
	subq	%r14, %rbx
	addq	%r13, %rax
	subq	%rax, %r13
	addq	%r12, %r15
	subq	%r15, %r12
	addq	%rbx, %r14
	subq	%r14, %rbx
	addq	%r13, %rax
	subq	%rax, %r13
	addq	%r12, %r15
	subq	%r15, %r12
	addq	%rbx, %r14
	subq	%r14, %rbx
	addq	%r13, %rax
	movq	%rax, %rdi
	subq	%rax, %r13
	addq	%r12, %r15
	subq	%r15, %r12
	addq	%rbx, %r14
	subq	%r14, %rbx
.L313:
	movq	%rsi, %rax
	leaq	-1(%rax), %rsi
	testq	%rax, %rax
	jne	.L314
	movl	%edi, %edx
	movl	%r13d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r15d, %edx
	movl	%r12d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %edx
	movl	%ebx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE107:
	.size	int64_add_2, .-int64_add_2
	.globl	int64_add_3
	.type	int64_add_3, @function
int64_add_3:
.LFB108:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r9
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$255, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rdi
	movq	%rdi, %r8
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$21698325, %eax
	movslq	%eax, %r14
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$254, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rdx
	movq	%rdx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$21698324, %eax
	movslq	%eax, %r13
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$65534, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$253, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rdi
	movq	%rdi, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$21698323, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$65533, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %r15
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$252, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %r15
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$21698322, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$65532, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	jmp	.L316
.L317:
	movq	%r8, %rax
	addq	%r14, %rax
	subq	%rax, %r14
	addq	%r13, -88(%rbp)
	movq	-88(%rbp), %rcx
	subq	%rcx, %r13
	addq	%r12, -96(%rbp)
	movq	-96(%rbp), %rsi
	subq	%rsi, %r12
	addq	%rbx, %r15
	subq	%r15, %rbx
	addq	%r14, %rax
	subq	%rax, %r14
	addq	%r13, %rcx
	subq	%rcx, %r13
	addq	%r12, %rsi
	subq	%rsi, %r12
	addq	%rbx, %r15
	subq	%r15, %rbx
	addq	%r14, %rax
	subq	%rax, %r14
	addq	%r13, %rcx
	subq	%rcx, %r13
	addq	%r12, %rsi
	subq	%rsi, %r12
	addq	%rbx, %r15
	subq	%r15, %rbx
	addq	%r14, %rax
	subq	%rax, %r14
	addq	%r13, %rcx
	subq	%rcx, %r13
	addq	%r12, %rsi
	subq	%rsi, %r12
	addq	%rbx, %r15
	subq	%r15, %rbx
	addq	%r14, %rax
	subq	%rax, %r14
	addq	%r13, %rcx
	subq	%rcx, %r13
	addq	%r12, %rsi
	subq	%rsi, %r12
	addq	%rbx, %r15
	subq	%r15, %rbx
	addq	%r14, %rax
	subq	%rax, %r14
	addq	%r13, %rcx
	subq	%rcx, %r13
	addq	%r12, %rsi
	subq	%rsi, %r12
	addq	%rbx, %r15
	subq	%r15, %rbx
	addq	%r14, %rax
	subq	%rax, %r14
	addq	%r13, %rcx
	subq	%rcx, %r13
	addq	%r12, %rsi
	subq	%rsi, %r12
	addq	%rbx, %r15
	subq	%r15, %rbx
	addq	%r14, %rax
	subq	%rax, %r14
	addq	%r13, %rcx
	subq	%rcx, %r13
	addq	%r12, %rsi
	subq	%rsi, %r12
	addq	%rbx, %r15
	subq	%r15, %rbx
	addq	%r14, %rax
	subq	%rax, %r14
	addq	%r13, %rcx
	subq	%rcx, %r13
	addq	%r12, %rsi
	subq	%rsi, %r12
	addq	%rbx, %r15
	subq	%r15, %rbx
	addq	%r14, %rax
	movq	%rax, %r8
	subq	%rax, %r14
	addq	%r13, %rcx
	movq	%rcx, -88(%rbp)
	subq	%rcx, %r13
	addq	%r12, %rsi
	movq	%rsi, -96(%rbp)
	subq	%rsi, %r12
	addq	%rbx, %r15
	subq	%r15, %rbx
.L316:
	movq	%r9, %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, %r9
	testq	%rax, %rax
	jne	.L317
	movl	%r8d, %edx
	movl	%r14d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %edx
	movl	%r13d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %edx
	movl	%r12d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r15d, %edx
	movl	%ebx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE108:
	.size	int64_add_3, .-int64_add_3
	.globl	int64_add_4
	.type	int64_add_4, @function
int64_add_4:
.LFB109:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r9
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$255, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, %r8
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$21698325, %eax
	movslq	%eax, %r15
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r15
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$254, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$21698324, %eax
	movslq	%eax, %r14
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$65534, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$253, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$21698323, %eax
	movslq	%eax, %r13
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$65533, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$252, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$21698322, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$65532, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$251, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$21698321, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$65531, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	jmp	.L319
.L320:
	movq	%r8, %rax
	addq	%r15, %rax
	subq	%rax, %r15
	addq	%r14, -88(%rbp)
	movq	-88(%rbp), %rcx
	subq	%rcx, %r14
	addq	%r13, -96(%rbp)
	movq	-96(%rbp), %rsi
	subq	%rsi, %r13
	addq	%r12, -104(%rbp)
	movq	-104(%rbp), %rdi
	subq	%rdi, %r12
	addq	%rbx, -112(%rbp)
	movq	-112(%rbp), %rdx
	subq	%rdx, %rbx
	addq	%r15, %rax
	subq	%rax, %r15
	addq	%r14, %rcx
	subq	%rcx, %r14
	addq	%r13, %rsi
	subq	%rsi, %r13
	addq	%r12, %rdi
	subq	%rdi, %r12
	addq	%rbx, %rdx
	subq	%rdx, %rbx
	addq	%r15, %rax
	subq	%rax, %r15
	addq	%r14, %rcx
	subq	%rcx, %r14
	addq	%r13, %rsi
	subq	%rsi, %r13
	addq	%r12, %rdi
	subq	%rdi, %r12
	addq	%rbx, %rdx
	subq	%rdx, %rbx
	addq	%r15, %rax
	subq	%rax, %r15
	addq	%r14, %rcx
	subq	%rcx, %r14
	addq	%r13, %rsi
	subq	%rsi, %r13
	addq	%r12, %rdi
	subq	%rdi, %r12
	addq	%rbx, %rdx
	subq	%rdx, %rbx
	addq	%r15, %rax
	subq	%rax, %r15
	addq	%r14, %rcx
	subq	%rcx, %r14
	addq	%r13, %rsi
	subq	%rsi, %r13
	addq	%r12, %rdi
	subq	%rdi, %r12
	addq	%rbx, %rdx
	subq	%rdx, %rbx
	addq	%r15, %rax
	subq	%rax, %r15
	addq	%r14, %rcx
	subq	%rcx, %r14
	addq	%r13, %rsi
	subq	%rsi, %r13
	addq	%r12, %rdi
	subq	%rdi, %r12
	addq	%rbx, %rdx
	subq	%rdx, %rbx
	addq	%r15, %rax
	subq	%rax, %r15
	addq	%r14, %rcx
	subq	%rcx, %r14
	addq	%r13, %rsi
	subq	%rsi, %r13
	addq	%r12, %rdi
	subq	%rdi, %r12
	addq	%rbx, %rdx
	subq	%rdx, %rbx
	addq	%r15, %rax
	subq	%rax, %r15
	addq	%r14, %rcx
	subq	%rcx, %r14
	addq	%r13, %rsi
	subq	%rsi, %r13
	addq	%r12, %rdi
	subq	%rdi, %r12
	addq	%rbx, %rdx
	subq	%rdx, %rbx
	addq	%r15, %rax
	subq	%rax, %r15
	addq	%r14, %rcx
	subq	%rcx, %r14
	addq	%r13, %rsi
	subq	%rsi, %r13
	addq	%r12, %rdi
	subq	%rdi, %r12
	addq	%rbx, %rdx
	subq	%rdx, %rbx
	addq	%r15, %rax
	movq	%rax, %r8
	subq	%rax, %r15
	addq	%r14, %rcx
	movq	%rcx, -88(%rbp)
	subq	%rcx, %r14
	addq	%r13, %rsi
	movq	%rsi, -96(%rbp)
	subq	%rsi, %r13
	addq	%r12, %rdi
	movq	%rdi, -104(%rbp)
	subq	%rdi, %r12
	addq	%rbx, %rdx
	movq	%rdx, -112(%rbp)
	subq	%rdx, %rbx
.L319:
	movq	%r9, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %r9
	testq	%rax, %rax
	jne	.L320
	movl	%r8d, %edx
	movl	%r15d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %edx
	movl	%r14d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %edx
	movl	%r13d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %edx
	movl	%r12d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %edx
	movl	%ebx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE109:
	.size	int64_add_4, .-int64_add_4
	.globl	int64_add_5
	.type	int64_add_5, @function
int64_add_5:
.LFB110:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r11
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$255, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, %r10
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$21698325, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, %rcx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$254, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$21698324, %eax
	movslq	%eax, %r15
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$65534, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r15
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$253, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$21698323, %eax
	movslq	%eax, %r14
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$65533, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$252, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$21698322, %eax
	movslq	%eax, %r13
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$65532, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$251, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$21698321, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$65531, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$250, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$21698320, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$65530, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	jmp	.L322
.L323:
	movq	%r10, %rax
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rsi
	subq	%rsi, %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rdi
	subq	%rdi, %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdx
	subq	%rdx, %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %r8
	subq	%r8, %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r9
	subq	%r9, %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%r15, %rsi
	subq	%rsi, %r15
	addq	%r14, %rdi
	subq	%rdi, %r14
	addq	%r13, %rdx
	subq	%rdx, %r13
	addq	%r12, %r8
	subq	%r8, %r12
	addq	%rbx, %r9
	subq	%r9, %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%r15, %rsi
	subq	%rsi, %r15
	addq	%r14, %rdi
	subq	%rdi, %r14
	addq	%r13, %rdx
	subq	%rdx, %r13
	addq	%r12, %r8
	subq	%r8, %r12
	addq	%rbx, %r9
	subq	%r9, %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%r15, %rsi
	subq	%rsi, %r15
	addq	%r14, %rdi
	subq	%rdi, %r14
	addq	%r13, %rdx
	subq	%rdx, %r13
	addq	%r12, %r8
	subq	%r8, %r12
	addq	%rbx, %r9
	subq	%r9, %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%r15, %rsi
	subq	%rsi, %r15
	addq	%r14, %rdi
	subq	%rdi, %r14
	addq	%r13, %rdx
	subq	%rdx, %r13
	addq	%r12, %r8
	subq	%r8, %r12
	addq	%rbx, %r9
	subq	%r9, %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%r15, %rsi
	subq	%rsi, %r15
	addq	%r14, %rdi
	subq	%rdi, %r14
	addq	%r13, %rdx
	subq	%rdx, %r13
	addq	%r12, %r8
	subq	%r8, %r12
	addq	%rbx, %r9
	subq	%r9, %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%r15, %rsi
	subq	%rsi, %r15
	addq	%r14, %rdi
	subq	%rdi, %r14
	addq	%r13, %rdx
	subq	%rdx, %r13
	addq	%r12, %r8
	subq	%r8, %r12
	addq	%rbx, %r9
	subq	%r9, %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%r15, %rsi
	subq	%rsi, %r15
	addq	%r14, %rdi
	subq	%rdi, %r14
	addq	%r13, %rdx
	subq	%rdx, %r13
	addq	%r12, %r8
	subq	%r8, %r12
	addq	%rbx, %r9
	subq	%r9, %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%r15, %rsi
	subq	%rsi, %r15
	addq	%r14, %rdi
	subq	%rdi, %r14
	addq	%r13, %rdx
	subq	%rdx, %r13
	addq	%r12, %r8
	subq	%r8, %r12
	addq	%rbx, %r9
	subq	%r9, %rbx
	addq	%rcx, %rax
	movq	%rax, %r10
	subq	%rax, %rcx
	addq	%r15, %rsi
	movq	%rsi, -88(%rbp)
	subq	%rsi, %r15
	addq	%r14, %rdi
	movq	%rdi, -96(%rbp)
	subq	%rdi, %r14
	addq	%r13, %rdx
	movq	%rdx, -104(%rbp)
	subq	%rdx, %r13
	addq	%r12, %r8
	movq	%r8, -112(%rbp)
	subq	%r8, %r12
	addq	%rbx, %r9
	movq	%r9, -120(%rbp)
	subq	%r9, %rbx
.L322:
	movq	%r11, %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, %r11
	testq	%rax, %rax
	jne	.L323
	movl	%r10d, %edx
	movl	%ecx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %edx
	movl	%r15d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %edx
	movl	%r14d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %edx
	movl	%r13d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %edx
	movl	%r12d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %edx
	movl	%ebx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE110:
	.size	int64_add_5, .-int64_add_5
	.globl	int64_add_6
	.type	int64_add_6, @function
int64_add_6:
.LFB111:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -152(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$255, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$21698325, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, %rcx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$254, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$21698324, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$65534, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$253, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$21698323, %eax
	movslq	%eax, %r15
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$65533, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r15
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$252, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$21698322, %eax
	movslq	%eax, %r14
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$65532, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$251, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$21698321, %eax
	movslq	%eax, %r13
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$65531, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$250, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$21698320, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$65530, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$249, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$21698319, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$65529, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	jmp	.L325
.L326:
	addq	%rcx, -88(%rbp)
	movq	-88(%rbp), %rax
	subq	%rax, %rcx
	movq	-144(%rbp), %rdi
	addq	%rdi, -96(%rbp)
	movq	-96(%rbp), %rsi
	subq	%rsi, %rdi
	addq	%r15, -104(%rbp)
	movq	-104(%rbp), %rdx
	subq	%rdx, %r15
	addq	%r14, -112(%rbp)
	movq	-112(%rbp), %r8
	subq	%r8, %r14
	addq	%r13, -120(%rbp)
	movq	-120(%rbp), %r9
	subq	%r9, %r13
	addq	%r12, -128(%rbp)
	movq	-128(%rbp), %r10
	subq	%r10, %r12
	addq	%rbx, -136(%rbp)
	movq	-136(%rbp), %r11
	subq	%r11, %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r15, %rdx
	subq	%rdx, %r15
	addq	%r14, %r8
	subq	%r8, %r14
	addq	%r13, %r9
	subq	%r9, %r13
	addq	%r12, %r10
	subq	%r10, %r12
	addq	%rbx, %r11
	subq	%r11, %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r15, %rdx
	subq	%rdx, %r15
	addq	%r14, %r8
	subq	%r8, %r14
	addq	%r13, %r9
	subq	%r9, %r13
	addq	%r12, %r10
	subq	%r10, %r12
	addq	%rbx, %r11
	subq	%r11, %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r15, %rdx
	subq	%rdx, %r15
	addq	%r14, %r8
	subq	%r8, %r14
	addq	%r13, %r9
	subq	%r9, %r13
	addq	%r12, %r10
	subq	%r10, %r12
	addq	%rbx, %r11
	subq	%r11, %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r15, %rdx
	subq	%rdx, %r15
	addq	%r14, %r8
	subq	%r8, %r14
	addq	%r13, %r9
	subq	%r9, %r13
	addq	%r12, %r10
	subq	%r10, %r12
	addq	%rbx, %r11
	subq	%r11, %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r15, %rdx
	subq	%rdx, %r15
	addq	%r14, %r8
	subq	%r8, %r14
	addq	%r13, %r9
	subq	%r9, %r13
	addq	%r12, %r10
	subq	%r10, %r12
	addq	%rbx, %r11
	subq	%r11, %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r15, %rdx
	subq	%rdx, %r15
	addq	%r14, %r8
	subq	%r8, %r14
	addq	%r13, %r9
	subq	%r9, %r13
	addq	%r12, %r10
	subq	%r10, %r12
	addq	%rbx, %r11
	subq	%r11, %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r15, %rdx
	subq	%rdx, %r15
	addq	%r14, %r8
	subq	%r8, %r14
	addq	%r13, %r9
	subq	%r9, %r13
	addq	%r12, %r10
	subq	%r10, %r12
	addq	%rbx, %r11
	subq	%r11, %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r15, %rdx
	subq	%rdx, %r15
	addq	%r14, %r8
	subq	%r8, %r14
	addq	%r13, %r9
	subq	%r9, %r13
	addq	%r12, %r10
	subq	%r10, %r12
	addq	%rbx, %r11
	subq	%r11, %rbx
	addq	%rcx, %rax
	movq	%rax, -88(%rbp)
	subq	%rax, %rcx
	addq	%rdi, %rsi
	movq	%rsi, -96(%rbp)
	subq	%rsi, %rdi
	movq	%rdi, -144(%rbp)
	addq	%r15, %rdx
	movq	%rdx, -104(%rbp)
	subq	%rdx, %r15
	addq	%r14, %r8
	movq	%r8, -112(%rbp)
	subq	%r8, %r14
	addq	%r13, %r9
	movq	%r9, -120(%rbp)
	subq	%r9, %r13
	addq	%r12, %r10
	movq	%r10, -128(%rbp)
	subq	%r10, %r12
	addq	%rbx, %r11
	movq	%r11, -136(%rbp)
	subq	%r11, %rbx
.L325:
	movq	-152(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -152(%rbp)
	testq	%rax, %rax
	jne	.L326
	movl	-88(%rbp), %edx
	movl	%ecx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %edx
	movl	-144(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %edx
	movl	%r15d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %edx
	movl	%r14d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %edx
	movl	%r13d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %edx
	movl	%r12d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %edx
	movl	%ebx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE111:
	.size	int64_add_6, .-int64_add_6
	.globl	int64_add_7
	.type	int64_add_7, @function
int64_add_7:
.LFB112:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -176(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$255, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$21698325, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$254, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$21698324, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$65534, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$253, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$21698323, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$65533, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$252, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$21698322, %eax
	movslq	%eax, %r15
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$65532, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r15
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$251, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$21698321, %eax
	movslq	%eax, %r14
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$65531, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$250, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$21698320, %eax
	movslq	%eax, %r13
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$65530, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$249, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$21698319, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$65529, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$248, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$21698318, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$65528, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	jmp	.L328
.L329:
	movq	-152(%rbp), %rcx
	addq	%rcx, -104(%rbp)
	movq	-104(%rbp), %rax
	subq	%rax, %rcx
	movq	-160(%rbp), %rdi
	addq	%rdi, -112(%rbp)
	movq	-112(%rbp), %rsi
	subq	%rsi, %rdi
	movq	-168(%rbp), %r8
	addq	%r8, -120(%rbp)
	movq	-120(%rbp), %rdx
	subq	%rdx, %r8
	addq	%r15, -128(%rbp)
	movq	-128(%rbp), %r9
	subq	%r9, %r15
	addq	%r14, -136(%rbp)
	movq	-136(%rbp), %r10
	subq	%r10, %r14
	addq	%r13, -144(%rbp)
	movq	-144(%rbp), %r11
	subq	%r11, %r13
	addq	%r12, -88(%rbp)
	subq	-88(%rbp), %r12
	addq	%rbx, -96(%rbp)
	subq	-96(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r15, %r9
	subq	%r9, %r15
	addq	%r14, %r10
	subq	%r10, %r14
	addq	%r13, %r11
	subq	%r11, %r13
	addq	%r12, -88(%rbp)
	subq	-88(%rbp), %r12
	addq	%rbx, -96(%rbp)
	subq	-96(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r15, %r9
	subq	%r9, %r15
	addq	%r14, %r10
	subq	%r10, %r14
	addq	%r13, %r11
	subq	%r11, %r13
	addq	%r12, -88(%rbp)
	subq	-88(%rbp), %r12
	addq	%rbx, -96(%rbp)
	subq	-96(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r15, %r9
	subq	%r9, %r15
	addq	%r14, %r10
	subq	%r10, %r14
	addq	%r13, %r11
	subq	%r11, %r13
	addq	%r12, -88(%rbp)
	subq	-88(%rbp), %r12
	addq	%rbx, -96(%rbp)
	subq	-96(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r15, %r9
	subq	%r9, %r15
	addq	%r14, %r10
	subq	%r10, %r14
	addq	%r13, %r11
	subq	%r11, %r13
	addq	%r12, -88(%rbp)
	subq	-88(%rbp), %r12
	addq	%rbx, -96(%rbp)
	subq	-96(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r15, %r9
	subq	%r9, %r15
	addq	%r14, %r10
	subq	%r10, %r14
	addq	%r13, %r11
	subq	%r11, %r13
	addq	%r12, -88(%rbp)
	subq	-88(%rbp), %r12
	addq	%rbx, -96(%rbp)
	subq	-96(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r15, %r9
	subq	%r9, %r15
	addq	%r14, %r10
	subq	%r10, %r14
	addq	%r13, %r11
	subq	%r11, %r13
	addq	%r12, -88(%rbp)
	subq	-88(%rbp), %r12
	addq	%rbx, -96(%rbp)
	subq	-96(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r15, %r9
	subq	%r9, %r15
	addq	%r14, %r10
	subq	%r10, %r14
	addq	%r13, %r11
	subq	%r11, %r13
	addq	%r12, -88(%rbp)
	subq	-88(%rbp), %r12
	addq	%rbx, -96(%rbp)
	subq	-96(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r15, %r9
	subq	%r9, %r15
	addq	%r14, %r10
	subq	%r10, %r14
	addq	%r13, %r11
	subq	%r11, %r13
	addq	%r12, -88(%rbp)
	subq	-88(%rbp), %r12
	addq	%rbx, -96(%rbp)
	subq	-96(%rbp), %rbx
	addq	%rcx, %rax
	movq	%rax, -104(%rbp)
	subq	%rax, %rcx
	movq	%rcx, -152(%rbp)
	addq	%rdi, %rsi
	movq	%rsi, -112(%rbp)
	subq	%rsi, %rdi
	movq	%rdi, -160(%rbp)
	addq	%r8, %rdx
	movq	%rdx, -120(%rbp)
	subq	%rdx, %r8
	movq	%r8, -168(%rbp)
	addq	%r15, %r9
	movq	%r9, -128(%rbp)
	subq	%r9, %r15
	addq	%r14, %r10
	movq	%r10, -136(%rbp)
	subq	%r10, %r14
	addq	%r13, %r11
	movq	%r11, -144(%rbp)
	subq	%r11, %r13
	addq	%r12, -88(%rbp)
	movq	-88(%rbp), %rax
	movq	%rax, -88(%rbp)
	subq	%rax, %r12
	addq	%rbx, -96(%rbp)
	movq	-96(%rbp), %rax
	movq	%rax, -96(%rbp)
	subq	%rax, %rbx
.L328:
	movq	-176(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -176(%rbp)
	testq	%rax, %rax
	jne	.L329
	movl	-104(%rbp), %edx
	movl	-152(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %edx
	movl	-160(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %edx
	movl	-168(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %edx
	movl	%r15d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %edx
	movl	%r14d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %edx
	movl	%r13d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %edx
	movl	%r12d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %edx
	movl	%ebx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE112:
	.size	int64_add_7, .-int64_add_7
	.globl	int64_add_8
	.type	int64_add_8, @function
int64_add_8:
.LFB113:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -192(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$255, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$21698325, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$254, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$21698324, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$65534, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$253, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$21698323, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$65533, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$252, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$21698322, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$65532, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$251, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$21698321, %eax
	movslq	%eax, %r15
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$65531, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r15
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$250, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$21698320, %eax
	movslq	%eax, %r14
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$65530, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$249, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$21698319, %eax
	movslq	%eax, %r13
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$65529, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$248, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rcx
	movq	%rcx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$21698318, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$65528, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$37413, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$247, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rsi
	movq	%rsi, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$21698317, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$65527, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	jmp	.L331
.L332:
	movq	-160(%rbp), %rcx
	addq	%rcx, -120(%rbp)
	movq	-120(%rbp), %rax
	subq	%rax, %rcx
	movq	-168(%rbp), %rdi
	addq	%rdi, -128(%rbp)
	movq	-128(%rbp), %rsi
	subq	%rsi, %rdi
	movq	-176(%rbp), %r8
	addq	%r8, -136(%rbp)
	movq	-136(%rbp), %rdx
	subq	%rdx, %r8
	movq	-184(%rbp), %r10
	addq	%r10, -144(%rbp)
	movq	-144(%rbp), %r9
	subq	%r9, %r10
	addq	%r15, -152(%rbp)
	movq	-152(%rbp), %r11
	subq	%r11, %r15
	addq	%r14, -88(%rbp)
	subq	-88(%rbp), %r14
	addq	%r13, -96(%rbp)
	subq	-96(%rbp), %r13
	addq	%r12, -104(%rbp)
	subq	-104(%rbp), %r12
	addq	%rbx, -112(%rbp)
	subq	-112(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r10, %r9
	subq	%r9, %r10
	addq	%r15, %r11
	subq	%r11, %r15
	addq	%r14, -88(%rbp)
	subq	-88(%rbp), %r14
	addq	%r13, -96(%rbp)
	subq	-96(%rbp), %r13
	addq	%r12, -104(%rbp)
	subq	-104(%rbp), %r12
	addq	%rbx, -112(%rbp)
	subq	-112(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r10, %r9
	subq	%r9, %r10
	addq	%r15, %r11
	subq	%r11, %r15
	addq	%r14, -88(%rbp)
	subq	-88(%rbp), %r14
	addq	%r13, -96(%rbp)
	subq	-96(%rbp), %r13
	addq	%r12, -104(%rbp)
	subq	-104(%rbp), %r12
	addq	%rbx, -112(%rbp)
	subq	-112(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r10, %r9
	subq	%r9, %r10
	addq	%r15, %r11
	subq	%r11, %r15
	addq	%r14, -88(%rbp)
	subq	-88(%rbp), %r14
	addq	%r13, -96(%rbp)
	subq	-96(%rbp), %r13
	addq	%r12, -104(%rbp)
	subq	-104(%rbp), %r12
	addq	%rbx, -112(%rbp)
	subq	-112(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r10, %r9
	subq	%r9, %r10
	addq	%r15, %r11
	subq	%r11, %r15
	addq	%r14, -88(%rbp)
	subq	-88(%rbp), %r14
	addq	%r13, -96(%rbp)
	subq	-96(%rbp), %r13
	addq	%r12, -104(%rbp)
	subq	-104(%rbp), %r12
	addq	%rbx, -112(%rbp)
	subq	-112(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r10, %r9
	subq	%r9, %r10
	addq	%r15, %r11
	subq	%r11, %r15
	addq	%r14, -88(%rbp)
	subq	-88(%rbp), %r14
	addq	%r13, -96(%rbp)
	subq	-96(%rbp), %r13
	addq	%r12, -104(%rbp)
	subq	-104(%rbp), %r12
	addq	%rbx, -112(%rbp)
	subq	-112(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r10, %r9
	subq	%r9, %r10
	addq	%r15, %r11
	subq	%r11, %r15
	addq	%r14, -88(%rbp)
	subq	-88(%rbp), %r14
	addq	%r13, -96(%rbp)
	subq	-96(%rbp), %r13
	addq	%r12, -104(%rbp)
	subq	-104(%rbp), %r12
	addq	%rbx, -112(%rbp)
	subq	-112(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r10, %r9
	subq	%r9, %r10
	addq	%r15, %r11
	subq	%r11, %r15
	addq	%r14, -88(%rbp)
	subq	-88(%rbp), %r14
	addq	%r13, -96(%rbp)
	subq	-96(%rbp), %r13
	addq	%r12, -104(%rbp)
	subq	-104(%rbp), %r12
	addq	%rbx, -112(%rbp)
	subq	-112(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r10, %r9
	subq	%r9, %r10
	addq	%r15, %r11
	subq	%r11, %r15
	addq	%r14, -88(%rbp)
	subq	-88(%rbp), %r14
	addq	%r13, -96(%rbp)
	subq	-96(%rbp), %r13
	addq	%r12, -104(%rbp)
	subq	-104(%rbp), %r12
	addq	%rbx, -112(%rbp)
	subq	-112(%rbp), %rbx
	addq	%rcx, %rax
	movq	%rax, -120(%rbp)
	subq	%rax, %rcx
	movq	%rcx, -160(%rbp)
	addq	%rdi, %rsi
	movq	%rsi, -128(%rbp)
	subq	%rsi, %rdi
	movq	%rdi, -168(%rbp)
	addq	%r8, %rdx
	movq	%rdx, -136(%rbp)
	subq	%rdx, %r8
	movq	%r8, -176(%rbp)
	addq	%r10, %r9
	movq	%r9, -144(%rbp)
	subq	%r9, %r10
	movq	%r10, -184(%rbp)
	addq	%r15, %r11
	movq	%r11, -152(%rbp)
	subq	%r11, %r15
	addq	%r14, -88(%rbp)
	movq	-88(%rbp), %rax
	movq	%rax, -88(%rbp)
	subq	%rax, %r14
	addq	%r13, -96(%rbp)
	movq	-96(%rbp), %rax
	movq	%rax, -96(%rbp)
	subq	%rax, %r13
	addq	%r12, -104(%rbp)
	movq	-104(%rbp), %rax
	movq	%rax, -104(%rbp)
	subq	%rax, %r12
	addq	%rbx, -112(%rbp)
	movq	-112(%rbp), %rax
	movq	%rax, -112(%rbp)
	subq	%rax, %rbx
.L331:
	movq	-192(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -192(%rbp)
	testq	%rax, %rax
	jne	.L332
	movl	-120(%rbp), %edx
	movl	-160(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %edx
	movl	-168(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %edx
	movl	-176(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %edx
	movl	-184(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-152(%rbp), %edx
	movl	%r15d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %edx
	movl	%r14d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %edx
	movl	%r13d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %edx
	movl	%r12d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %edx
	movl	%ebx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$152, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE113:
	.size	int64_add_8, .-int64_add_8
	.globl	int64_add_9
	.type	int64_add_9, @function
int64_add_9:
.LFB114:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -208(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$255, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$21698325, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$254, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$21698324, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$65534, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$253, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$21698323, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$65533, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$252, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$21698322, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$65532, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$251, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$21698321, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$65531, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$250, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$21698320, %eax
	movslq	%eax, %r15
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$65530, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r15
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$249, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rsi
	movq	%rsi, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$21698319, %eax
	movslq	%eax, %r14
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$65529, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$248, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$21698318, %eax
	movslq	%eax, %r13
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$65528, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$37413, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$247, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rsi
	movq	%rsi, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$21698317, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$65527, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$37412, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$246, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$21698316, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$65526, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	jmp	.L334
.L335:
	movq	-136(%rbp), %rcx
	addq	%rcx, -152(%rbp)
	movq	-152(%rbp), %rax
	subq	%rax, %rcx
	movq	%rcx, -136(%rbp)
	movq	-144(%rbp), %rdi
	addq	%rdi, -160(%rbp)
	movq	-160(%rbp), %rsi
	subq	%rsi, %rdi
	movq	-192(%rbp), %r8
	addq	%r8, -168(%rbp)
	movq	-168(%rbp), %rdx
	subq	%rdx, %r8
	movq	-200(%rbp), %r10
	addq	%r10, -176(%rbp)
	movq	-176(%rbp), %r9
	subq	%r9, %r10
	movq	-128(%rbp), %rcx
	addq	%rcx, -184(%rbp)
	movq	-184(%rbp), %r11
	subq	%r11, %rcx
	movq	%rcx, -128(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-136(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -144(%rbp)
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-128(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -128(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -136(%rbp)
	movq	-144(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-128(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -128(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-136(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -144(%rbp)
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-128(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -128(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -136(%rbp)
	movq	-144(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-128(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -128(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-136(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -144(%rbp)
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-128(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -128(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -136(%rbp)
	movq	-144(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-128(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -128(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-136(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -144(%rbp)
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-128(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -128(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -136(%rbp)
	movq	-144(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	addq	%r8, %rdx
	subq	%rdx, %r8
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-128(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -128(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-136(%rbp), %rcx
	addq	%rcx, %rax
	movq	%rax, -152(%rbp)
	subq	%rax, %rcx
	movq	%rcx, -136(%rbp)
	addq	%rdi, %rsi
	movq	%rsi, -160(%rbp)
	subq	%rsi, %rdi
	movq	%rdi, -144(%rbp)
	addq	%r8, %rdx
	movq	%rdx, -168(%rbp)
	subq	%rdx, %r8
	movq	%r8, -192(%rbp)
	addq	%r10, %r9
	movq	%r9, -176(%rbp)
	subq	%r9, %r10
	movq	%r10, -200(%rbp)
	movq	-128(%rbp), %rax
	addq	%rax, %r11
	movq	%r11, -184(%rbp)
	subq	%r11, %rax
	movq	%rax, -128(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	%rcx, %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	%rcx, %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	%rcx, %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	%rcx, %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	%rcx, %rbx
.L334:
	movq	-208(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -208(%rbp)
	testq	%rax, %rax
	jne	.L335
	movl	-152(%rbp), %edx
	movl	-136(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-160(%rbp), %edx
	movl	-144(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-168(%rbp), %edx
	movl	-192(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-176(%rbp), %edx
	movl	-200(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-184(%rbp), %edx
	movl	-128(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %edx
	movl	%r15d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %edx
	movl	%r14d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %edx
	movl	%r13d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %edx
	movl	%r12d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %edx
	movl	%ebx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$168, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE114:
	.size	int64_add_9, .-int64_add_9
	.globl	int64_add_10
	.type	int64_add_10, @function
int64_add_10:
.LFB115:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$184, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -224(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$255, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$21698325, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$254, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$21698324, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$65534, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$253, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$21698323, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$65533, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$252, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$21698322, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$65532, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$251, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$21698321, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$65531, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$250, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$21698320, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$65530, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$249, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$21698319, %eax
	movslq	%eax, %r15
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$65529, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r15
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$248, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rsi
	movq	%rsi, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$21698318, %eax
	movslq	%eax, %r14
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$65528, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$37413, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$247, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$21698317, %eax
	movslq	%eax, %r13
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$65527, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$37412, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$246, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rsi
	movq	%rsi, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$21698316, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$65526, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$37411, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$245, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$21698315, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$65525, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	jmp	.L337
.L338:
	movq	-160(%rbp), %rcx
	addq	%rcx, -176(%rbp)
	movq	-176(%rbp), %rax
	subq	%rax, %rcx
	movq	%rcx, -160(%rbp)
	movq	-168(%rbp), %rdi
	addq	%rdi, -184(%rbp)
	movq	-184(%rbp), %rsi
	subq	%rsi, %rdi
	movq	-136(%rbp), %r8
	addq	%r8, -192(%rbp)
	movq	-192(%rbp), %rdx
	subq	%rdx, %r8
	movq	%r8, -136(%rbp)
	movq	-216(%rbp), %r10
	addq	%r10, -200(%rbp)
	movq	-200(%rbp), %r9
	subq	%r9, %r10
	movq	-144(%rbp), %rcx
	addq	%rcx, -208(%rbp)
	movq	-208(%rbp), %r11
	subq	%r11, %rcx
	movq	%rcx, -144(%rbp)
	movq	-152(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -152(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-160(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -168(%rbp)
	movq	-136(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -136(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-144(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -144(%rbp)
	movq	-152(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -152(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -160(%rbp)
	movq	-168(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-136(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -136(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-144(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -144(%rbp)
	movq	-152(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -152(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-160(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -168(%rbp)
	movq	-136(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -136(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-144(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -144(%rbp)
	movq	-152(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -152(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -160(%rbp)
	movq	-168(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-136(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -136(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-144(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -144(%rbp)
	movq	-152(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -152(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-160(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -168(%rbp)
	movq	-136(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -136(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-144(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -144(%rbp)
	movq	-152(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -152(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -160(%rbp)
	movq	-168(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-136(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -136(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-144(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -144(%rbp)
	movq	-152(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -152(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-160(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -168(%rbp)
	movq	-136(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -136(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-144(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -144(%rbp)
	movq	-152(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -152(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -160(%rbp)
	movq	-168(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-136(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -136(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-144(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -144(%rbp)
	movq	-152(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -152(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %r8
	movq	%r8, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-160(%rbp), %rcx
	addq	%rcx, %rax
	movq	%rax, -176(%rbp)
	subq	%rax, %rcx
	movq	%rcx, -160(%rbp)
	addq	%rdi, %rsi
	movq	%rsi, -184(%rbp)
	subq	%rsi, %rdi
	movq	%rdi, -168(%rbp)
	movq	-136(%rbp), %r8
	addq	%r8, %rdx
	movq	%rdx, -192(%rbp)
	subq	%rdx, %r8
	movq	%r8, -136(%rbp)
	addq	%r10, %r9
	movq	%r9, -200(%rbp)
	subq	%r9, %r10
	movq	%r10, -216(%rbp)
	movq	-144(%rbp), %rax
	addq	%rax, %r11
	movq	%r11, -208(%rbp)
	subq	%r11, %rax
	movq	%rax, -144(%rbp)
	movq	-152(%rbp), %rax
	addq	%rax, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %rax
	movq	%rax, -152(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	%r8, %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	%r8, %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	%rcx, %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %r8
	movq	%r8, -112(%rbp)
	subq	%r8, %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	%rcx, %rbx
.L337:
	movq	-224(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -224(%rbp)
	testq	%rax, %rax
	jne	.L338
	movl	-176(%rbp), %edx
	movl	-160(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-184(%rbp), %edx
	movl	-168(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-192(%rbp), %edx
	movl	-136(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-200(%rbp), %edx
	movl	-216(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-208(%rbp), %edx
	movl	-144(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %edx
	movl	-152(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %edx
	movl	%r15d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %edx
	movl	%r14d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %edx
	movl	%r13d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %edx
	movl	%r12d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %edx
	movl	%ebx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$184, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE115:
	.size	int64_add_10, .-int64_add_10
	.globl	int64_add_11
	.type	int64_add_11, @function
int64_add_11:
.LFB116:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$200, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -240(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$255, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$21698325, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$254, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$21698324, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$65534, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$253, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$21698323, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$65533, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$252, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$21698322, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$65532, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$251, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$21698321, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$65531, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$250, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$21698320, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$65530, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$249, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$21698319, %eax
	movslq	%eax, %r15
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$65529, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r15
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$248, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rsi
	movq	%rsi, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$21698318, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$65528, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$37413, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$247, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rsi
	movq	%rsi, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$21698317, %eax
	movslq	%eax, %r14
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$65527, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$37412, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$246, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$21698316, %eax
	movslq	%eax, %r13
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$65526, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$37411, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$245, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rsi
	movq	%rsi, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$21698315, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$65525, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$37410, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$244, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$21698314, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$65524, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	jmp	.L340
.L341:
	movq	-176(%rbp), %rcx
	addq	%rcx, -192(%rbp)
	movq	-192(%rbp), %rax
	subq	%rax, %rcx
	movq	%rcx, -176(%rbp)
	movq	-184(%rbp), %rdi
	addq	%rdi, -200(%rbp)
	movq	-200(%rbp), %rsi
	subq	%rsi, %rdi
	movq	-144(%rbp), %r8
	addq	%r8, -208(%rbp)
	movq	-208(%rbp), %rdx
	subq	%rdx, %r8
	movq	%r8, -144(%rbp)
	movq	-232(%rbp), %r10
	addq	%r10, -216(%rbp)
	movq	-216(%rbp), %r9
	subq	%r9, %r10
	movq	-152(%rbp), %rcx
	addq	%rcx, -224(%rbp)
	movq	-224(%rbp), %r11
	subq	%r11, %rcx
	movq	%rcx, -152(%rbp)
	movq	-160(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -160(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	movq	-168(%rbp), %r8
	addq	%r8, -136(%rbp)
	movq	-136(%rbp), %rcx
	movq	%rcx, -136(%rbp)
	subq	%rcx, %r8
	movq	%r8, -168(%rbp)
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-176(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -184(%rbp)
	movq	-144(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -144(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-152(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -152(%rbp)
	movq	-160(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -160(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	movq	-168(%rbp), %r8
	addq	%r8, -136(%rbp)
	movq	-136(%rbp), %rdi
	movq	%rdi, -136(%rbp)
	subq	%rdi, %r8
	movq	%r8, -168(%rbp)
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -176(%rbp)
	movq	-184(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-144(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -144(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-152(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -152(%rbp)
	movq	-160(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -160(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	movq	-168(%rbp), %r8
	addq	%r8, -136(%rbp)
	movq	-136(%rbp), %rcx
	movq	%rcx, -136(%rbp)
	subq	%rcx, %r8
	movq	%r8, -168(%rbp)
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-176(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -184(%rbp)
	movq	-144(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -144(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-152(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -152(%rbp)
	movq	-160(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -160(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	movq	-168(%rbp), %r8
	addq	%r8, -136(%rbp)
	movq	-136(%rbp), %rdi
	movq	%rdi, -136(%rbp)
	subq	%rdi, %r8
	movq	%r8, -168(%rbp)
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -176(%rbp)
	movq	-184(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-144(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -144(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-152(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -152(%rbp)
	movq	-160(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -160(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	movq	-168(%rbp), %r8
	addq	%r8, -136(%rbp)
	movq	-136(%rbp), %rcx
	movq	%rcx, -136(%rbp)
	subq	%rcx, %r8
	movq	%r8, -168(%rbp)
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-176(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -184(%rbp)
	movq	-144(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -144(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-152(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -152(%rbp)
	movq	-160(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -160(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	movq	-168(%rbp), %r8
	addq	%r8, -136(%rbp)
	movq	-136(%rbp), %rdi
	movq	%rdi, -136(%rbp)
	subq	%rdi, %r8
	movq	%r8, -168(%rbp)
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -176(%rbp)
	movq	-184(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-144(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -144(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-152(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -152(%rbp)
	movq	-160(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -160(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	movq	-168(%rbp), %r8
	addq	%r8, -136(%rbp)
	movq	-136(%rbp), %rcx
	movq	%rcx, -136(%rbp)
	subq	%rcx, %r8
	movq	%r8, -168(%rbp)
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-176(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -184(%rbp)
	movq	-144(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -144(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-152(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -152(%rbp)
	movq	-160(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -160(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	movq	-168(%rbp), %r8
	addq	%r8, -136(%rbp)
	movq	-136(%rbp), %rdi
	movq	%rdi, -136(%rbp)
	subq	%rdi, %r8
	movq	%r8, -168(%rbp)
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -176(%rbp)
	movq	-184(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-144(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -144(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-152(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -152(%rbp)
	movq	-160(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -160(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	movq	-168(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -168(%rbp)
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-176(%rbp), %rcx
	addq	%rcx, %rax
	movq	%rax, -192(%rbp)
	subq	%rax, %rcx
	movq	%rcx, -176(%rbp)
	addq	%rdi, %rsi
	movq	%rsi, -200(%rbp)
	subq	%rsi, %rdi
	movq	%rdi, -184(%rbp)
	movq	-144(%rbp), %r8
	addq	%r8, %rdx
	movq	%rdx, -208(%rbp)
	subq	%rdx, %r8
	movq	%r8, -144(%rbp)
	addq	%r10, %r9
	movq	%r9, -216(%rbp)
	subq	%r9, %r10
	movq	%r10, -232(%rbp)
	movq	-152(%rbp), %rax
	addq	%rax, %r11
	movq	%r11, -224(%rbp)
	subq	%r11, %rax
	movq	%rax, -152(%rbp)
	movq	-160(%rbp), %rax
	addq	%rax, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %rax
	movq	%rax, -160(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	%r8, %r15
	movq	-168(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -168(%rbp)
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	%rcx, %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	%r8, %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	%rcx, %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	%r8, %rbx
.L340:
	movq	-240(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -240(%rbp)
	testq	%rax, %rax
	jne	.L341
	movl	-192(%rbp), %edx
	movl	-176(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-200(%rbp), %edx
	movl	-184(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-208(%rbp), %edx
	movl	-144(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-216(%rbp), %edx
	movl	-232(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-224(%rbp), %edx
	movl	-152(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %edx
	movl	-160(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %edx
	movl	%r15d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %edx
	movl	-168(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %edx
	movl	%r14d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %edx
	movl	%r13d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %edx
	movl	%r12d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %edx
	movl	%ebx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$200, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE116:
	.size	int64_add_11, .-int64_add_11
	.globl	int64_add_12
	.type	int64_add_12, @function
int64_add_12:
.LFB117:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$216, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -256(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$255, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$21698325, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$254, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$21698324, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$65534, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$253, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$21698323, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$65533, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$252, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$21698322, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$65532, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -248(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$251, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -240(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$21698321, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$65531, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$250, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$21698320, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$65530, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$249, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$21698319, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$65529, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$248, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$21698318, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$65528, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$37413, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$247, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$21698317, %eax
	movslq	%eax, %r15
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$65527, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r15
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$37412, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$246, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rsi
	movq	%rsi, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$21698316, %eax
	movslq	%eax, %r14
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$65526, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$37411, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$245, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$21698315, %eax
	movslq	%eax, %r13
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$65525, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$37410, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$244, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rdx
	movq	%rdx, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$21698314, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$65524, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$37409, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$243, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rsi
	movq	%rsi, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$21698313, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$65523, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	jmp	.L343
.L344:
	movq	-192(%rbp), %rcx
	addq	%rcx, -208(%rbp)
	movq	-208(%rbp), %rax
	subq	%rax, %rcx
	movq	%rcx, -192(%rbp)
	movq	-200(%rbp), %rdi
	addq	%rdi, -216(%rbp)
	movq	-216(%rbp), %rsi
	subq	%rsi, %rdi
	movq	-152(%rbp), %r8
	addq	%r8, -224(%rbp)
	movq	-224(%rbp), %rdx
	subq	%rdx, %r8
	movq	%r8, -152(%rbp)
	movq	-248(%rbp), %r10
	addq	%r10, -232(%rbp)
	movq	-232(%rbp), %r9
	subq	%r9, %r10
	movq	-160(%rbp), %rcx
	addq	%rcx, -240(%rbp)
	movq	-240(%rbp), %r11
	subq	%r11, %rcx
	movq	%rcx, -160(%rbp)
	movq	-168(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -168(%rbp)
	movq	-176(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -176(%rbp)
	movq	-184(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	subq	%rcx, %r8
	movq	%r8, -184(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-192(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -200(%rbp)
	movq	-152(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -152(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-160(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -160(%rbp)
	movq	-168(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -168(%rbp)
	movq	-176(%rbp), %rdi
	addq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -176(%rbp)
	movq	-184(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	subq	%rdi, %r8
	movq	%r8, -184(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -192(%rbp)
	movq	-200(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-152(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -152(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-160(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -160(%rbp)
	movq	-168(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -168(%rbp)
	movq	-176(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -176(%rbp)
	movq	-184(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	subq	%rcx, %r8
	movq	%r8, -184(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-192(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -200(%rbp)
	movq	-152(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -152(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-160(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -160(%rbp)
	movq	-168(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -168(%rbp)
	movq	-176(%rbp), %rdi
	addq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -176(%rbp)
	movq	-184(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	subq	%rdi, %r8
	movq	%r8, -184(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -192(%rbp)
	movq	-200(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-152(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -152(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-160(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -160(%rbp)
	movq	-168(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -168(%rbp)
	movq	-176(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -176(%rbp)
	movq	-184(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	subq	%rcx, %r8
	movq	%r8, -184(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-192(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -200(%rbp)
	movq	-152(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -152(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-160(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -160(%rbp)
	movq	-168(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -168(%rbp)
	movq	-176(%rbp), %rdi
	addq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -176(%rbp)
	movq	-184(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	subq	%rdi, %r8
	movq	%r8, -184(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -192(%rbp)
	movq	-200(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-152(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -152(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-160(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -160(%rbp)
	movq	-168(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -168(%rbp)
	movq	-176(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -176(%rbp)
	movq	-184(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	subq	%rcx, %r8
	movq	%r8, -184(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-192(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -200(%rbp)
	movq	-152(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -152(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-160(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -160(%rbp)
	movq	-168(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -168(%rbp)
	movq	-176(%rbp), %rdi
	addq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -176(%rbp)
	movq	-184(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	subq	%rdi, %r8
	movq	%r8, -184(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -192(%rbp)
	movq	-200(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-152(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -152(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-160(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -160(%rbp)
	movq	-168(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -168(%rbp)
	movq	-176(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -176(%rbp)
	movq	-184(%rbp), %rcx
	addq	%rcx, -144(%rbp)
	movq	-144(%rbp), %r8
	movq	%r8, -144(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -184(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-192(%rbp), %rcx
	addq	%rcx, %rax
	movq	%rax, -208(%rbp)
	subq	%rax, %rcx
	movq	%rcx, -192(%rbp)
	addq	%rdi, %rsi
	movq	%rsi, -216(%rbp)
	subq	%rsi, %rdi
	movq	%rdi, -200(%rbp)
	movq	-152(%rbp), %r8
	addq	%r8, %rdx
	movq	%rdx, -224(%rbp)
	subq	%rdx, %r8
	movq	%r8, -152(%rbp)
	addq	%r10, %r9
	movq	%r9, -232(%rbp)
	subq	%r9, %r10
	movq	%r10, -248(%rbp)
	movq	-160(%rbp), %rax
	addq	%rax, %r11
	movq	%r11, -240(%rbp)
	subq	%r11, %rax
	movq	%rax, -160(%rbp)
	movq	-168(%rbp), %rax
	addq	%rax, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %rax
	movq	%rax, -168(%rbp)
	movq	-176(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -176(%rbp)
	movq	-184(%rbp), %rcx
	addq	%rcx, -144(%rbp)
	movq	-144(%rbp), %r8
	movq	%r8, -144(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -184(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	%r8, %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	%rcx, %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	%r8, %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	%rcx, %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	%r8, %rbx
.L343:
	movq	-256(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -256(%rbp)
	testq	%rax, %rax
	jne	.L344
	movl	-208(%rbp), %edx
	movl	-192(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-216(%rbp), %edx
	movl	-200(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-224(%rbp), %edx
	movl	-152(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-232(%rbp), %edx
	movl	-248(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-240(%rbp), %edx
	movl	-160(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %edx
	movl	-168(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %edx
	movl	-176(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %edx
	movl	-184(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %edx
	movl	%r15d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %edx
	movl	%r14d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %edx
	movl	%r13d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %edx
	movl	%r12d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %edx
	movl	%ebx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$216, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE117:
	.size	int64_add_12, .-int64_add_12
	.globl	int64_add_13
	.type	int64_add_13, @function
int64_add_13:
.LFB118:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$232, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -272(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$255, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$21698325, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$254, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$21698324, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$65534, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$253, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -240(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$21698323, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$65533, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$252, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -248(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$21698322, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$65532, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -264(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$251, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -256(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$21698321, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$65531, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$250, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$21698320, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$65530, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$249, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$21698319, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$65529, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$248, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$21698318, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$65528, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$37413, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$247, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$21698317, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$65527, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$37412, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$246, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$21698316, %eax
	movslq	%eax, %r15
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$65526, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r15
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$37411, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$245, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rdx
	movq	%rdx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$21698315, %eax
	movslq	%eax, %r14
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$65525, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$37410, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$244, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rsi
	movq	%rsi, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$21698314, %eax
	movslq	%eax, %r13
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$65524, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$37409, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$243, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$21698313, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$65523, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$37408, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$242, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rdx
	movq	%rdx, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$21698312, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$65522, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	jmp	.L346
.L347:
	movq	-208(%rbp), %rcx
	addq	%rcx, -224(%rbp)
	movq	-224(%rbp), %rax
	subq	%rax, %rcx
	movq	%rcx, -208(%rbp)
	movq	-216(%rbp), %rdi
	addq	%rdi, -232(%rbp)
	movq	-232(%rbp), %rsi
	subq	%rsi, %rdi
	movq	-160(%rbp), %r8
	addq	%r8, -240(%rbp)
	movq	-240(%rbp), %rdx
	subq	%rdx, %r8
	movq	%r8, -160(%rbp)
	movq	-264(%rbp), %r10
	addq	%r10, -248(%rbp)
	movq	-248(%rbp), %r9
	subq	%r9, %r10
	movq	-168(%rbp), %rcx
	addq	%rcx, -256(%rbp)
	movq	-256(%rbp), %r11
	subq	%r11, %rcx
	movq	%rcx, -168(%rbp)
	movq	-176(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -176(%rbp)
	movq	-184(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -184(%rbp)
	movq	-192(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	subq	%rcx, %r8
	movq	%r8, -192(%rbp)
	movq	-200(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	subq	%rcx, %r8
	movq	%r8, -200(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-208(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -216(%rbp)
	movq	-160(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -160(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-168(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -168(%rbp)
	movq	-176(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -176(%rbp)
	movq	-184(%rbp), %rdi
	addq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -184(%rbp)
	movq	-192(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	subq	%rdi, %r8
	movq	%r8, -192(%rbp)
	movq	-200(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	subq	%rdi, %r8
	movq	%r8, -200(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -208(%rbp)
	movq	-216(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-160(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -160(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-168(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -168(%rbp)
	movq	-176(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -176(%rbp)
	movq	-184(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -184(%rbp)
	movq	-192(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	subq	%rcx, %r8
	movq	%r8, -192(%rbp)
	movq	-200(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	subq	%rcx, %r8
	movq	%r8, -200(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-208(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -216(%rbp)
	movq	-160(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -160(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-168(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -168(%rbp)
	movq	-176(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -176(%rbp)
	movq	-184(%rbp), %rdi
	addq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -184(%rbp)
	movq	-192(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	subq	%rdi, %r8
	movq	%r8, -192(%rbp)
	movq	-200(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	subq	%rdi, %r8
	movq	%r8, -200(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -208(%rbp)
	movq	-216(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-160(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -160(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-168(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -168(%rbp)
	movq	-176(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -176(%rbp)
	movq	-184(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -184(%rbp)
	movq	-192(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	subq	%rcx, %r8
	movq	%r8, -192(%rbp)
	movq	-200(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	subq	%rcx, %r8
	movq	%r8, -200(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-208(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -216(%rbp)
	movq	-160(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -160(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-168(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -168(%rbp)
	movq	-176(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -176(%rbp)
	movq	-184(%rbp), %rdi
	addq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -184(%rbp)
	movq	-192(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	subq	%rdi, %r8
	movq	%r8, -192(%rbp)
	movq	-200(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	subq	%rdi, %r8
	movq	%r8, -200(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -208(%rbp)
	movq	-216(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-160(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -160(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-168(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -168(%rbp)
	movq	-176(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -176(%rbp)
	movq	-184(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -184(%rbp)
	movq	-192(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	subq	%rcx, %r8
	movq	%r8, -192(%rbp)
	movq	-200(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	subq	%rcx, %r8
	movq	%r8, -200(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-208(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -216(%rbp)
	movq	-160(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -160(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-168(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -168(%rbp)
	movq	-176(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -176(%rbp)
	movq	-184(%rbp), %rdi
	addq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -184(%rbp)
	movq	-192(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	subq	%rdi, %r8
	movq	%r8, -192(%rbp)
	movq	-200(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	subq	%rdi, %r8
	movq	%r8, -200(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -208(%rbp)
	movq	-216(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-160(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -160(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-168(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -168(%rbp)
	movq	-176(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -176(%rbp)
	movq	-184(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -184(%rbp)
	movq	-192(%rbp), %rcx
	addq	%rcx, -144(%rbp)
	movq	-144(%rbp), %r8
	movq	%r8, -144(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -192(%rbp)
	movq	-200(%rbp), %rcx
	addq	%rcx, -152(%rbp)
	movq	-152(%rbp), %r8
	movq	%r8, -152(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -200(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-208(%rbp), %rcx
	addq	%rcx, %rax
	movq	%rax, -224(%rbp)
	subq	%rax, %rcx
	movq	%rcx, -208(%rbp)
	addq	%rdi, %rsi
	movq	%rsi, -232(%rbp)
	subq	%rsi, %rdi
	movq	%rdi, -216(%rbp)
	movq	-160(%rbp), %r8
	addq	%r8, %rdx
	movq	%rdx, -240(%rbp)
	subq	%rdx, %r8
	movq	%r8, -160(%rbp)
	addq	%r10, %r9
	movq	%r9, -248(%rbp)
	subq	%r9, %r10
	movq	%r10, -264(%rbp)
	movq	-168(%rbp), %rax
	addq	%rax, %r11
	movq	%r11, -256(%rbp)
	subq	%r11, %rax
	movq	%rax, -168(%rbp)
	movq	-176(%rbp), %rax
	addq	%rax, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %rax
	movq	%rax, -176(%rbp)
	movq	-184(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -184(%rbp)
	movq	-192(%rbp), %rcx
	addq	%rcx, -144(%rbp)
	movq	-144(%rbp), %r8
	movq	%r8, -144(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -192(%rbp)
	movq	-200(%rbp), %rcx
	addq	%rcx, -152(%rbp)
	movq	-152(%rbp), %r8
	movq	%r8, -152(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -200(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %r8
	movq	%r8, -88(%rbp)
	subq	%r8, %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	subq	%rcx, %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %r8
	movq	%r8, -104(%rbp)
	subq	%r8, %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	%rcx, %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	%r8, %rbx
.L346:
	movq	-272(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -272(%rbp)
	testq	%rax, %rax
	jne	.L347
	movl	-224(%rbp), %edx
	movl	-208(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-232(%rbp), %edx
	movl	-216(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-240(%rbp), %edx
	movl	-160(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-248(%rbp), %edx
	movl	-264(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-256(%rbp), %edx
	movl	-168(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %edx
	movl	-176(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %edx
	movl	-184(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %edx
	movl	-192(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-152(%rbp), %edx
	movl	-200(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %edx
	movl	%r15d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %edx
	movl	%r14d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %edx
	movl	%r13d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %edx
	movl	%r12d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %edx
	movl	%ebx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$232, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE118:
	.size	int64_add_13, .-int64_add_13
	.globl	int64_add_14
	.type	int64_add_14, @function
int64_add_14:
.LFB119:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$248, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -288(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$255, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -240(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$21698325, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$254, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -248(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$21698324, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$65534, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$253, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -256(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$21698323, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$65533, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$252, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -264(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$21698322, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$65532, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -280(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$251, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -272(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$21698321, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$65531, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$250, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$21698320, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$65530, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$249, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$21698319, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$65529, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$248, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$21698318, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$65528, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$37413, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$247, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$21698317, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$65527, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$37412, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$246, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$21698316, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$65526, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$37411, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$245, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rdx
	movq	%rdx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$21698315, %eax
	movslq	%eax, %r15
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$65525, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r15
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$37410, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$244, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$21698314, %eax
	movslq	%eax, %r14
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$65524, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$37409, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$243, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rsi
	movq	%rsi, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$21698313, %eax
	movslq	%eax, %r13
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$65523, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$37408, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$242, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rdx
	movq	%rdx, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$21698312, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$65522, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	addl	$37407, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	addl	$241, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	addl	$21698311, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	addl	$65521, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	jmp	.L349
.L350:
	movq	-224(%rbp), %rcx
	addq	%rcx, -240(%rbp)
	movq	-240(%rbp), %rax
	subq	%rax, %rcx
	movq	%rcx, -224(%rbp)
	movq	-232(%rbp), %rdi
	addq	%rdi, -248(%rbp)
	movq	-248(%rbp), %rsi
	subq	%rsi, %rdi
	movq	-168(%rbp), %r8
	addq	%r8, -256(%rbp)
	movq	-256(%rbp), %rdx
	subq	%rdx, %r8
	movq	%r8, -168(%rbp)
	movq	-280(%rbp), %r10
	addq	%r10, -264(%rbp)
	movq	-264(%rbp), %r9
	subq	%r9, %r10
	movq	-176(%rbp), %rcx
	addq	%rcx, -272(%rbp)
	movq	-272(%rbp), %r11
	subq	%r11, %rcx
	movq	%rcx, -176(%rbp)
	movq	-184(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -184(%rbp)
	movq	-192(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -192(%rbp)
	movq	-200(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	subq	%rcx, %r8
	movq	%r8, -200(%rbp)
	movq	-208(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	subq	%rcx, %r8
	movq	%r8, -208(%rbp)
	movq	-216(%rbp), %r8
	addq	%r8, -160(%rbp)
	movq	-160(%rbp), %rcx
	movq	%rcx, -160(%rbp)
	subq	%rcx, %r8
	movq	%r8, -216(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-224(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -232(%rbp)
	movq	-168(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -168(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-176(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -176(%rbp)
	movq	-184(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -184(%rbp)
	movq	-192(%rbp), %rdi
	addq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -192(%rbp)
	movq	-200(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	subq	%rdi, %r8
	movq	%r8, -200(%rbp)
	movq	-208(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	subq	%rdi, %r8
	movq	%r8, -208(%rbp)
	movq	-216(%rbp), %r8
	addq	%r8, -160(%rbp)
	movq	-160(%rbp), %rdi
	movq	%rdi, -160(%rbp)
	subq	%rdi, %r8
	movq	%r8, -216(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -224(%rbp)
	movq	-232(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-168(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -168(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-176(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -176(%rbp)
	movq	-184(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -184(%rbp)
	movq	-192(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -192(%rbp)
	movq	-200(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	subq	%rcx, %r8
	movq	%r8, -200(%rbp)
	movq	-208(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	subq	%rcx, %r8
	movq	%r8, -208(%rbp)
	movq	-216(%rbp), %r8
	addq	%r8, -160(%rbp)
	movq	-160(%rbp), %rcx
	movq	%rcx, -160(%rbp)
	subq	%rcx, %r8
	movq	%r8, -216(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-224(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -232(%rbp)
	movq	-168(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -168(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-176(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -176(%rbp)
	movq	-184(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -184(%rbp)
	movq	-192(%rbp), %rdi
	addq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -192(%rbp)
	movq	-200(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	subq	%rdi, %r8
	movq	%r8, -200(%rbp)
	movq	-208(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	subq	%rdi, %r8
	movq	%r8, -208(%rbp)
	movq	-216(%rbp), %r8
	addq	%r8, -160(%rbp)
	movq	-160(%rbp), %rdi
	movq	%rdi, -160(%rbp)
	subq	%rdi, %r8
	movq	%r8, -216(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -224(%rbp)
	movq	-232(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-168(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -168(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-176(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -176(%rbp)
	movq	-184(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -184(%rbp)
	movq	-192(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -192(%rbp)
	movq	-200(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	subq	%rcx, %r8
	movq	%r8, -200(%rbp)
	movq	-208(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	subq	%rcx, %r8
	movq	%r8, -208(%rbp)
	movq	-216(%rbp), %r8
	addq	%r8, -160(%rbp)
	movq	-160(%rbp), %rcx
	movq	%rcx, -160(%rbp)
	subq	%rcx, %r8
	movq	%r8, -216(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-224(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -232(%rbp)
	movq	-168(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -168(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-176(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -176(%rbp)
	movq	-184(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -184(%rbp)
	movq	-192(%rbp), %rdi
	addq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -192(%rbp)
	movq	-200(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	subq	%rdi, %r8
	movq	%r8, -200(%rbp)
	movq	-208(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	subq	%rdi, %r8
	movq	%r8, -208(%rbp)
	movq	-216(%rbp), %r8
	addq	%r8, -160(%rbp)
	movq	-160(%rbp), %rdi
	movq	%rdi, -160(%rbp)
	subq	%rdi, %r8
	movq	%r8, -216(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -224(%rbp)
	movq	-232(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-168(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -168(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-176(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -176(%rbp)
	movq	-184(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -184(%rbp)
	movq	-192(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -192(%rbp)
	movq	-200(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	subq	%rcx, %r8
	movq	%r8, -200(%rbp)
	movq	-208(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	subq	%rcx, %r8
	movq	%r8, -208(%rbp)
	movq	-216(%rbp), %r8
	addq	%r8, -160(%rbp)
	movq	-160(%rbp), %rcx
	movq	%rcx, -160(%rbp)
	subq	%rcx, %r8
	movq	%r8, -216(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %r8
	movq	%r8, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-224(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -232(%rbp)
	movq	-168(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -168(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-176(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -176(%rbp)
	movq	-184(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -184(%rbp)
	movq	-192(%rbp), %rdi
	addq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -192(%rbp)
	movq	-200(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	subq	%rdi, %r8
	movq	%r8, -200(%rbp)
	movq	-208(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	subq	%rdi, %r8
	movq	%r8, -208(%rbp)
	movq	-216(%rbp), %rdi
	addq	%rdi, -160(%rbp)
	movq	-160(%rbp), %r8
	movq	%r8, -160(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -216(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %r8
	movq	%r8, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -224(%rbp)
	movq	-232(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-168(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -168(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-176(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -176(%rbp)
	movq	-184(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -184(%rbp)
	movq	-192(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -192(%rbp)
	movq	-200(%rbp), %rcx
	addq	%rcx, -144(%rbp)
	movq	-144(%rbp), %r8
	movq	%r8, -144(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -200(%rbp)
	movq	-208(%rbp), %rcx
	addq	%rcx, -152(%rbp)
	movq	-152(%rbp), %r8
	movq	%r8, -152(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -208(%rbp)
	movq	-216(%rbp), %rcx
	addq	%rcx, -160(%rbp)
	movq	-160(%rbp), %r8
	movq	%r8, -160(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -216(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %r8
	movq	%r8, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-224(%rbp), %rcx
	addq	%rcx, %rax
	movq	%rax, -240(%rbp)
	subq	%rax, %rcx
	movq	%rcx, -224(%rbp)
	addq	%rdi, %rsi
	movq	%rsi, -248(%rbp)
	subq	%rsi, %rdi
	movq	%rdi, -232(%rbp)
	movq	-168(%rbp), %r8
	addq	%r8, %rdx
	movq	%rdx, -256(%rbp)
	subq	%rdx, %r8
	movq	%r8, -168(%rbp)
	addq	%r10, %r9
	movq	%r9, -264(%rbp)
	subq	%r9, %r10
	movq	%r10, -280(%rbp)
	movq	-176(%rbp), %rax
	addq	%rax, %r11
	movq	%r11, -272(%rbp)
	subq	%r11, %rax
	movq	%rax, -176(%rbp)
	movq	-184(%rbp), %rax
	addq	%rax, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %rax
	movq	%rax, -184(%rbp)
	movq	-192(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -192(%rbp)
	movq	-200(%rbp), %rcx
	addq	%rcx, -144(%rbp)
	movq	-144(%rbp), %r8
	movq	%r8, -144(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -200(%rbp)
	movq	-208(%rbp), %rcx
	addq	%rcx, -152(%rbp)
	movq	-152(%rbp), %r8
	movq	%r8, -152(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -208(%rbp)
	movq	-216(%rbp), %rcx
	addq	%rcx, -160(%rbp)
	movq	-160(%rbp), %r8
	movq	%r8, -160(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -216(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	%rcx, %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	%r8, %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	%rcx, %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %r8
	movq	%r8, -112(%rbp)
	subq	%r8, %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	%rcx, %rbx
.L349:
	movq	-288(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -288(%rbp)
	testq	%rax, %rax
	jne	.L350
	movl	-240(%rbp), %edx
	movl	-224(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-248(%rbp), %edx
	movl	-232(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-256(%rbp), %edx
	movl	-168(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-264(%rbp), %edx
	movl	-280(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-272(%rbp), %edx
	movl	-176(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %edx
	movl	-184(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %edx
	movl	-192(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %edx
	movl	-200(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-152(%rbp), %edx
	movl	-208(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-160(%rbp), %edx
	movl	-216(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %edx
	movl	%r15d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %edx
	movl	%r14d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %edx
	movl	%r13d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %edx
	movl	%r12d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %edx
	movl	%ebx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$248, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE119:
	.size	int64_add_14, .-int64_add_14
	.globl	int64_add_15
	.type	int64_add_15, @function
int64_add_15:
.LFB120:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$264, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -304(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$255, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -256(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$21698325, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$65535, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -240(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$254, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -264(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$21698324, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$65534, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -248(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$253, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -272(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$21698323, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$65533, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$252, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -280(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$21698322, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$65532, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -296(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$251, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -288(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$21698321, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$65531, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	movq	%rbx, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$250, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$21698320, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$65530, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$249, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$21698319, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$65529, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$248, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$21698318, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$65528, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$37413, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$247, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$21698317, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$65527, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$37412, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$246, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$21698316, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$65526, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rsi
	movq	%rsi, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$37411, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$245, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$21698315, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$65525, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rdx
	movq	%rdx, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$37410, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$244, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rsi
	movq	%rsi, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$21698314, %eax
	movslq	%eax, %r15
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$65524, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r15
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$37409, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$243, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$21698313, %eax
	movslq	%eax, %r14
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$65523, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$37408, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$242, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rdx
	movq	%rdx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$21698312, %eax
	movslq	%eax, %r13
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$65522, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	addl	$37407, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	addl	$241, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rsi
	movq	%rsi, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	addl	$21698311, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	addl	$65521, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	addl	$37406, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	addl	$240, %eax
	cltq
	salq	$30, %rax
	addq	%rax, %rbx
	movq	%rbx, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	addl	$21698310, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	addl	$65520, %eax
	cltq
	salq	$29, %rax
	addq	%rax, %rbx
	jmp	.L352
.L353:
	movq	-240(%rbp), %rcx
	addq	%rcx, -256(%rbp)
	movq	-256(%rbp), %rax
	subq	%rax, %rcx
	movq	%rcx, -240(%rbp)
	movq	-248(%rbp), %rdi
	addq	%rdi, -264(%rbp)
	movq	-264(%rbp), %rsi
	subq	%rsi, %rdi
	movq	-176(%rbp), %r8
	addq	%r8, -272(%rbp)
	movq	-272(%rbp), %rdx
	subq	%rdx, %r8
	movq	%r8, -176(%rbp)
	movq	-296(%rbp), %r10
	addq	%r10, -280(%rbp)
	movq	-280(%rbp), %r9
	subq	%r9, %r10
	movq	-184(%rbp), %rcx
	addq	%rcx, -288(%rbp)
	movq	-288(%rbp), %r11
	subq	%r11, %rcx
	movq	%rcx, -184(%rbp)
	movq	-192(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -192(%rbp)
	movq	-200(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -200(%rbp)
	movq	-208(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	subq	%rcx, %r8
	movq	%r8, -208(%rbp)
	movq	-216(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	subq	%rcx, %r8
	movq	%r8, -216(%rbp)
	movq	-224(%rbp), %r8
	addq	%r8, -160(%rbp)
	movq	-160(%rbp), %rcx
	movq	%rcx, -160(%rbp)
	subq	%rcx, %r8
	movq	%r8, -224(%rbp)
	movq	-232(%rbp), %r8
	addq	%r8, -168(%rbp)
	movq	-168(%rbp), %rcx
	movq	%rcx, -168(%rbp)
	subq	%rcx, %r8
	movq	%r8, -232(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-240(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -248(%rbp)
	movq	-176(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -176(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-184(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -184(%rbp)
	movq	-192(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -192(%rbp)
	movq	-200(%rbp), %rdi
	addq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -200(%rbp)
	movq	-208(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	subq	%rdi, %r8
	movq	%r8, -208(%rbp)
	movq	-216(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	subq	%rdi, %r8
	movq	%r8, -216(%rbp)
	movq	-224(%rbp), %r8
	addq	%r8, -160(%rbp)
	movq	-160(%rbp), %rdi
	movq	%rdi, -160(%rbp)
	subq	%rdi, %r8
	movq	%r8, -224(%rbp)
	movq	-232(%rbp), %r8
	addq	%r8, -168(%rbp)
	movq	-168(%rbp), %rdi
	movq	%rdi, -168(%rbp)
	subq	%rdi, %r8
	movq	%r8, -232(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -240(%rbp)
	movq	-248(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-176(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -176(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-184(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -184(%rbp)
	movq	-192(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -192(%rbp)
	movq	-200(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -200(%rbp)
	movq	-208(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	subq	%rcx, %r8
	movq	%r8, -208(%rbp)
	movq	-216(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	subq	%rcx, %r8
	movq	%r8, -216(%rbp)
	movq	-224(%rbp), %r8
	addq	%r8, -160(%rbp)
	movq	-160(%rbp), %rcx
	movq	%rcx, -160(%rbp)
	subq	%rcx, %r8
	movq	%r8, -224(%rbp)
	movq	-232(%rbp), %r8
	addq	%r8, -168(%rbp)
	movq	-168(%rbp), %rcx
	movq	%rcx, -168(%rbp)
	subq	%rcx, %r8
	movq	%r8, -232(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-240(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -248(%rbp)
	movq	-176(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -176(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-184(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -184(%rbp)
	movq	-192(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -192(%rbp)
	movq	-200(%rbp), %rdi
	addq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -200(%rbp)
	movq	-208(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	subq	%rdi, %r8
	movq	%r8, -208(%rbp)
	movq	-216(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	subq	%rdi, %r8
	movq	%r8, -216(%rbp)
	movq	-224(%rbp), %r8
	addq	%r8, -160(%rbp)
	movq	-160(%rbp), %rdi
	movq	%rdi, -160(%rbp)
	subq	%rdi, %r8
	movq	%r8, -224(%rbp)
	movq	-232(%rbp), %r8
	addq	%r8, -168(%rbp)
	movq	-168(%rbp), %rdi
	movq	%rdi, -168(%rbp)
	subq	%rdi, %r8
	movq	%r8, -232(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -240(%rbp)
	movq	-248(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-176(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -176(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-184(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -184(%rbp)
	movq	-192(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -192(%rbp)
	movq	-200(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -200(%rbp)
	movq	-208(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	subq	%rcx, %r8
	movq	%r8, -208(%rbp)
	movq	-216(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	subq	%rcx, %r8
	movq	%r8, -216(%rbp)
	movq	-224(%rbp), %r8
	addq	%r8, -160(%rbp)
	movq	-160(%rbp), %rcx
	movq	%rcx, -160(%rbp)
	subq	%rcx, %r8
	movq	%r8, -224(%rbp)
	movq	-232(%rbp), %r8
	addq	%r8, -168(%rbp)
	movq	-168(%rbp), %rcx
	movq	%rcx, -168(%rbp)
	subq	%rcx, %r8
	movq	%r8, -232(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rcx
	movq	%rcx, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-240(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -248(%rbp)
	movq	-176(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -176(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-184(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -184(%rbp)
	movq	-192(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -192(%rbp)
	movq	-200(%rbp), %rdi
	addq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -200(%rbp)
	movq	-208(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	subq	%rdi, %r8
	movq	%r8, -208(%rbp)
	movq	-216(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	subq	%rdi, %r8
	movq	%r8, -216(%rbp)
	movq	-224(%rbp), %r8
	addq	%r8, -160(%rbp)
	movq	-160(%rbp), %rdi
	movq	%rdi, -160(%rbp)
	subq	%rdi, %r8
	movq	%r8, -224(%rbp)
	movq	-232(%rbp), %r8
	addq	%r8, -168(%rbp)
	movq	-168(%rbp), %rdi
	movq	%rdi, -168(%rbp)
	subq	%rdi, %r8
	movq	%r8, -232(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %rdi
	movq	%rdi, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %r8
	movq	%r8, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -240(%rbp)
	movq	-248(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-176(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -176(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-184(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -184(%rbp)
	movq	-192(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -192(%rbp)
	movq	-200(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -200(%rbp)
	movq	-208(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rcx
	movq	%rcx, -144(%rbp)
	subq	%rcx, %r8
	movq	%r8, -208(%rbp)
	movq	-216(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rcx
	movq	%rcx, -152(%rbp)
	subq	%rcx, %r8
	movq	%r8, -216(%rbp)
	movq	-224(%rbp), %r8
	addq	%r8, -160(%rbp)
	movq	-160(%rbp), %rcx
	movq	%rcx, -160(%rbp)
	subq	%rcx, %r8
	movq	%r8, -224(%rbp)
	movq	-232(%rbp), %r8
	addq	%r8, -168(%rbp)
	movq	-168(%rbp), %rcx
	movq	%rcx, -168(%rbp)
	subq	%rcx, %r8
	movq	%r8, -232(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %r8
	movq	%r8, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-240(%rbp), %rcx
	addq	%rcx, %rax
	subq	%rax, %rcx
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	%rdi, -248(%rbp)
	movq	-176(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -176(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-184(%rbp), %rdi
	addq	%rdi, %r11
	subq	%r11, %rdi
	movq	%rdi, -184(%rbp)
	movq	-192(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rdi
	movq	%rdi, -128(%rbp)
	subq	%rdi, %r8
	movq	%r8, -192(%rbp)
	movq	-200(%rbp), %rdi
	addq	%rdi, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -200(%rbp)
	movq	-208(%rbp), %r8
	addq	%r8, -144(%rbp)
	movq	-144(%rbp), %rdi
	movq	%rdi, -144(%rbp)
	subq	%rdi, %r8
	movq	%r8, -208(%rbp)
	movq	-216(%rbp), %r8
	addq	%r8, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	%rdi, -152(%rbp)
	subq	%rdi, %r8
	movq	%r8, -216(%rbp)
	movq	-224(%rbp), %rdi
	addq	%rdi, -160(%rbp)
	movq	-160(%rbp), %r8
	movq	%r8, -160(%rbp)
	subq	%r8, %rdi
	movq	%rdi, -224(%rbp)
	movq	-232(%rbp), %r8
	addq	%r8, -168(%rbp)
	movq	-168(%rbp), %rdi
	movq	%rdi, -168(%rbp)
	subq	%rdi, %r8
	movq	%r8, -232(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%rdi, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %r8
	movq	%r8, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	%rdi, -120(%rbp)
	subq	-120(%rbp), %rbx
	addq	%rcx, %rax
	subq	%rax, %rcx
	movq	%rcx, -240(%rbp)
	movq	-248(%rbp), %rdi
	addq	%rdi, %rsi
	subq	%rsi, %rdi
	movq	-176(%rbp), %r8
	addq	%r8, %rdx
	subq	%rdx, %r8
	movq	%r8, -176(%rbp)
	addq	%r10, %r9
	subq	%r9, %r10
	movq	-184(%rbp), %rcx
	addq	%rcx, %r11
	subq	%r11, %rcx
	movq	%rcx, -184(%rbp)
	movq	-192(%rbp), %r8
	addq	%r8, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %r8
	movq	%r8, -192(%rbp)
	movq	-200(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -200(%rbp)
	movq	-208(%rbp), %rcx
	addq	%rcx, -144(%rbp)
	movq	-144(%rbp), %r8
	movq	%r8, -144(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -208(%rbp)
	movq	-216(%rbp), %rcx
	addq	%rcx, -152(%rbp)
	movq	-152(%rbp), %r8
	movq	%r8, -152(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -216(%rbp)
	movq	-224(%rbp), %rcx
	addq	%rcx, -160(%rbp)
	movq	-160(%rbp), %r8
	movq	%r8, -160(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -224(%rbp)
	movq	-232(%rbp), %r8
	addq	%r8, -168(%rbp)
	movq	-168(%rbp), %rcx
	movq	%rcx, -168(%rbp)
	subq	%rcx, %r8
	movq	%r8, -232(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	-88(%rbp), %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	-96(%rbp), %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	-104(%rbp), %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %r8
	movq	%r8, -112(%rbp)
	subq	-112(%rbp), %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	-120(%rbp), %rbx
	movq	-240(%rbp), %rcx
	addq	%rcx, %rax
	movq	%rax, -256(%rbp)
	subq	%rax, %rcx
	movq	%rcx, -240(%rbp)
	addq	%rdi, %rsi
	movq	%rsi, -264(%rbp)
	subq	%rsi, %rdi
	movq	%rdi, -248(%rbp)
	movq	-176(%rbp), %r8
	addq	%r8, %rdx
	movq	%rdx, -272(%rbp)
	subq	%rdx, %r8
	movq	%r8, -176(%rbp)
	addq	%r10, %r9
	movq	%r9, -280(%rbp)
	subq	%r9, %r10
	movq	%r10, -296(%rbp)
	movq	-184(%rbp), %rax
	addq	%rax, %r11
	movq	%r11, -288(%rbp)
	subq	%r11, %rax
	movq	%rax, -184(%rbp)
	movq	-192(%rbp), %rax
	addq	%rax, -128(%rbp)
	movq	-128(%rbp), %rcx
	movq	%rcx, -128(%rbp)
	subq	%rcx, %rax
	movq	%rax, -192(%rbp)
	movq	-200(%rbp), %rcx
	addq	%rcx, -136(%rbp)
	movq	-136(%rbp), %r8
	movq	%r8, -136(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -200(%rbp)
	movq	-208(%rbp), %rcx
	addq	%rcx, -144(%rbp)
	movq	-144(%rbp), %r8
	movq	%r8, -144(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -208(%rbp)
	movq	-216(%rbp), %rcx
	addq	%rcx, -152(%rbp)
	movq	-152(%rbp), %r8
	movq	%r8, -152(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -216(%rbp)
	movq	-224(%rbp), %rcx
	addq	%rcx, -160(%rbp)
	movq	-160(%rbp), %r8
	movq	%r8, -160(%rbp)
	subq	%r8, %rcx
	movq	%rcx, -224(%rbp)
	movq	-232(%rbp), %r8
	addq	%r8, -168(%rbp)
	movq	-168(%rbp), %rcx
	movq	%rcx, -168(%rbp)
	subq	%rcx, %r8
	movq	%r8, -232(%rbp)
	addq	%r15, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	subq	%rcx, %r15
	addq	%r14, -96(%rbp)
	movq	-96(%rbp), %r8
	movq	%r8, -96(%rbp)
	subq	%r8, %r14
	addq	%r13, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	%rcx, -104(%rbp)
	subq	%rcx, %r13
	addq	%r12, -112(%rbp)
	movq	-112(%rbp), %r8
	movq	%r8, -112(%rbp)
	subq	%r8, %r12
	addq	%rbx, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	subq	%rcx, %rbx
.L352:
	movq	-304(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -304(%rbp)
	testq	%rax, %rax
	jne	.L353
	movl	-256(%rbp), %edx
	movl	-240(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-264(%rbp), %edx
	movl	-248(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-272(%rbp), %edx
	movl	-176(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-280(%rbp), %edx
	movl	-296(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-288(%rbp), %edx
	movl	-184(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %edx
	movl	-192(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %edx
	movl	-200(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %edx
	movl	-208(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-152(%rbp), %edx
	movl	-216(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-160(%rbp), %edx
	movl	-224(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-168(%rbp), %edx
	movl	-232(%rbp), %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %edx
	movl	%r15d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %edx
	movl	%r14d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %edx
	movl	%r13d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %edx
	movl	%r12d, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %edx
	movl	%ebx, %eax
	addl	%edx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$264, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE120:
	.size	int64_add_15, .-int64_add_15
	.globl	int64_add_benchmarks
	.section	.data.rel.local
	.align 32
	.type	int64_add_benchmarks, @object
	.size	int64_add_benchmarks, 128
int64_add_benchmarks:
	.quad	int64_add_0
	.quad	int64_add_1
	.quad	int64_add_2
	.quad	int64_add_3
	.quad	int64_add_4
	.quad	int64_add_5
	.quad	int64_add_6
	.quad	int64_add_7
	.quad	int64_add_8
	.quad	int64_add_9
	.quad	int64_add_10
	.quad	int64_add_11
	.quad	int64_add_12
	.quad	int64_add_13
	.quad	int64_add_14
	.quad	int64_add_15
	.text
	.globl	int64_mul_0
	.type	int64_mul_0, @function
int64_mul_0:
.LFB121:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$32, %rsp
	.cfi_offset 14, -24
	.cfi_offset 13, -32
	.cfi_offset 12, -40
	.cfi_offset 3, -48
	movq	%rdi, -56(%rbp)
	movq	%rsi, -64(%rbp)
	movq	-56(%rbp), %r13
	movq	-64(%rbp), %rax
	movq	%rax, -40(%rbp)
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rbx
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	addl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	addl	$5, %eax
	movslq	%eax, %r12
	movq	%rbx, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	subq	%rbx, %rax
	movq	%rax, %r14
	addq	%r14, %rbx
	jmp	.L355
.L356:
	subq	%r14, %rbx
	imulq	%r12, %rbx
	imulq	%r12, %rbx
	imulq	%r12, %rbx
	imulq	%r12, %rbx
	imulq	%r12, %rbx
	imulq	%r12, %rbx
	imulq	%r12, %rbx
	imulq	%r12, %rbx
	imulq	%r12, %rbx
	imulq	%r12, %rbx
.L355:
	movq	%r13, %rax
	leaq	-1(%rax), %r13
	testq	%rax, %rax
	jne	.L356
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$32, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE121:
	.size	int64_mul_0, .-int64_mul_0
	.globl	int64_mul_1
	.type	int64_mul_1, @function
int64_mul_1:
.LFB122:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %r15
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$5, %eax
	movslq	%eax, %r13
	movq	%r12, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	subq	%r12, %rax
	movq	%rax, %rdx
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$4, %eax
	movslq	%eax, %r14
	movq	%rbx, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	subq	%rbx, %rax
	movq	%rax, %rcx
	addq	%rax, %rbx
	jmp	.L358
.L359:
	subq	%rdx, %r12
	subq	%rcx, %rbx
	imulq	%r13, %r12
	imulq	%r14, %rbx
	imulq	%r13, %r12
	imulq	%r14, %rbx
	imulq	%r13, %r12
	imulq	%r14, %rbx
	imulq	%r13, %r12
	imulq	%r14, %rbx
	imulq	%r13, %r12
	imulq	%r14, %rbx
	imulq	%r13, %r12
	imulq	%r14, %rbx
	imulq	%r13, %r12
	imulq	%r14, %rbx
	imulq	%r13, %r12
	imulq	%r14, %rbx
	imulq	%r13, %r12
	imulq	%r14, %rbx
	imulq	%r13, %r12
	imulq	%r14, %rbx
.L358:
	movq	%r15, %rax
	leaq	-1(%rax), %r15
	testq	%rax, %rax
	jne	.L359
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE122:
	.size	int64_mul_1, .-int64_mul_1
	.globl	int64_mul_2
	.type	int64_mul_2, @function
int64_mul_2:
.LFB123:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %rsi
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %r13
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$5, %eax
	movslq	%eax, %r14
	movq	%r13, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	subq	%r13, %rax
	movq	%rax, %rdi
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$4, %eax
	movslq	%eax, %r15
	movq	%r12, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	subq	%r12, %rax
	movq	%rax, %r8
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$3, %eax
	movslq	%eax, %rdx
	movq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%rbx, %rax
	movq	%rax, %r9
	addq	%rax, %rbx
	jmp	.L361
.L362:
	subq	%rdi, %r13
	subq	%r8, %r12
	subq	%r9, %rbx
	imulq	%r14, %r13
	imulq	%r15, %r12
	movq	%rdx, %rax
	imulq	%rax, %rbx
	imulq	%r14, %r13
	imulq	%r15, %r12
	imulq	%rax, %rbx
	imulq	%r14, %r13
	imulq	%r15, %r12
	imulq	%rax, %rbx
	imulq	%r14, %r13
	imulq	%r15, %r12
	imulq	%rax, %rbx
	imulq	%r14, %r13
	imulq	%r15, %r12
	imulq	%rax, %rbx
	imulq	%r14, %r13
	imulq	%r15, %r12
	imulq	%rax, %rbx
	imulq	%r14, %r13
	imulq	%r15, %r12
	imulq	%rax, %rbx
	imulq	%r14, %r13
	imulq	%r15, %r12
	imulq	%rax, %rbx
	imulq	%r14, %r13
	imulq	%r15, %r12
	imulq	%rax, %rbx
	imulq	%r14, %r13
	imulq	%r15, %r12
	imulq	%rax, %rbx
.L361:
	movq	%rsi, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %rsi
	testq	%rax, %rax
	jne	.L362
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE123:
	.size	int64_mul_2, .-int64_mul_2
	.globl	int64_mul_3
	.type	int64_mul_3, @function
int64_mul_3:
.LFB124:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r8
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %r14
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$5, %eax
	movslq	%eax, %r15
	movq	%r14, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	subq	%r14, %rax
	movq	%rax, %r9
	addq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %r13
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$4, %eax
	movslq	%eax, %rdx
	movq	%rdx, %r10
	movq	%rdx, %rax
	imulq	%r13, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%r13, %rax
	movq	%rax, %r11
	addq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %r12
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$3, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rdi
	movq	%rcx, %rax
	imulq	%r12, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	subq	%r12, %rax
	movq	%rax, -88(%rbp)
	addq	%rax, %r12
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$2, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	imulq	%rbx, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	subq	%rbx, %rax
	movq	%rax, -96(%rbp)
	addq	%rax, %rbx
	jmp	.L364
.L365:
	subq	%r9, %r14
	subq	%r11, %r13
	subq	-88(%rbp), %r12
	subq	-96(%rbp), %rbx
	imulq	%r15, %r14
	movq	%r10, %rax
	imulq	%rax, %r13
	imulq	%rdi, %r12
	movq	%rsi, %rdx
	imulq	%rdx, %rbx
	imulq	%r15, %r14
	imulq	%rax, %r13
	imulq	%rdi, %r12
	imulq	%rdx, %rbx
	imulq	%r15, %r14
	imulq	%rax, %r13
	imulq	%rdi, %r12
	imulq	%rdx, %rbx
	imulq	%r15, %r14
	imulq	%rax, %r13
	imulq	%rdi, %r12
	imulq	%rdx, %rbx
	imulq	%r15, %r14
	imulq	%rax, %r13
	imulq	%rdi, %r12
	imulq	%rdx, %rbx
	imulq	%r15, %r14
	imulq	%rax, %r13
	imulq	%rdi, %r12
	imulq	%rdx, %rbx
	imulq	%r15, %r14
	imulq	%rax, %r13
	imulq	%rdi, %r12
	imulq	%rdx, %rbx
	imulq	%r15, %r14
	imulq	%rax, %r13
	imulq	%rdi, %r12
	imulq	%rdx, %rbx
	imulq	%r15, %r14
	imulq	%rax, %r13
	imulq	%rdi, %r12
	imulq	%rdx, %rbx
	imulq	%r15, %r14
	imulq	%rax, %r13
	imulq	%rdi, %r12
	imulq	%rdx, %rbx
.L364:
	movq	%r8, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %r8
	testq	%rax, %rax
	jne	.L365
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE124:
	.size	int64_mul_3, .-int64_mul_3
	.globl	int64_mul_4
	.type	int64_mul_4, @function
int64_mul_4:
.LFB125:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r10
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$5, %eax
	movslq	%eax, %r12
	movq	%rbx, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	subq	%rbx, %rax
	movq	%rax, %r11
	addq	%rax, %rbx
	movq	%rbx, %r9
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$4, %eax
	movslq	%eax, %r13
	movq	%rbx, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	subq	%rbx, %rax
	movq	%rax, -120(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$3, %eax
	movslq	%eax, %r14
	movq	%rdx, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	subq	%rdx, %rax
	movq	%rax, -128(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$2, %eax
	movslq	%eax, %r15
	movq	%rcx, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	subq	%rcx, %rax
	movq	%rax, -136(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rbx
	movq	%rsi, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rsi, %rax
	movq	%rax, %r8
	addq	%rax, %rsi
	movq	%rsi, -112(%rbp)
	jmp	.L367
.L368:
	movq	%r9, %rax
	movq	%r11, %rdi
	subq	%rdi, %rax
	movq	-120(%rbp), %rdi
	subq	%rdi, -88(%rbp)
	movq	-88(%rbp), %rdx
	movq	-128(%rbp), %rdi
	subq	%rdi, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	-136(%rbp), %rdi
	subq	%rdi, -104(%rbp)
	movq	-104(%rbp), %rsi
	subq	%r8, -112(%rbp)
	movq	-112(%rbp), %rdi
	imulq	%r12, %rax
	imulq	%r13, %rdx
	imulq	%r14, %rcx
	imulq	%r15, %rsi
	imulq	%rbx, %rdi
	imulq	%r12, %rax
	imulq	%r13, %rdx
	imulq	%r14, %rcx
	imulq	%r15, %rsi
	imulq	%rbx, %rdi
	imulq	%r12, %rax
	imulq	%r13, %rdx
	imulq	%r14, %rcx
	imulq	%r15, %rsi
	imulq	%rbx, %rdi
	imulq	%r12, %rax
	imulq	%r13, %rdx
	imulq	%r14, %rcx
	imulq	%r15, %rsi
	imulq	%rbx, %rdi
	imulq	%r12, %rax
	imulq	%r13, %rdx
	imulq	%r14, %rcx
	imulq	%r15, %rsi
	imulq	%rbx, %rdi
	imulq	%r12, %rax
	imulq	%r13, %rdx
	imulq	%r14, %rcx
	imulq	%r15, %rsi
	imulq	%rbx, %rdi
	imulq	%r12, %rax
	imulq	%r13, %rdx
	imulq	%r14, %rcx
	imulq	%r15, %rsi
	imulq	%rbx, %rdi
	imulq	%r12, %rax
	imulq	%r13, %rdx
	imulq	%r14, %rcx
	imulq	%r15, %rsi
	imulq	%rbx, %rdi
	imulq	%r12, %rax
	imulq	%r13, %rdx
	imulq	%r14, %rcx
	imulq	%r15, %rsi
	imulq	%rbx, %rdi
	imulq	%r12, %rax
	movq	%rax, %r9
	imulq	%r13, %rdx
	movq	%rdx, -88(%rbp)
	imulq	%r14, %rcx
	movq	%rcx, -96(%rbp)
	imulq	%r15, %rsi
	movq	%rsi, -104(%rbp)
	imulq	%rbx, %rdi
	movq	%rdi, -112(%rbp)
.L367:
	movq	%r10, %rax
	leaq	-1(%rax), %rdi
	movq	%rdi, %r10
	testq	%rax, %rax
	jne	.L368
	movl	%r9d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE125:
	.size	int64_mul_4, .-int64_mul_4
	.globl	int64_mul_5
	.type	int64_mul_5, @function
int64_mul_5:
.LFB126:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -128(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$5, %eax
	movslq	%eax, %r13
	movq	%rdx, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	subq	%rdx, %rax
	movq	%rax, -136(%rbp)
	addq	%rax, %rdx
	movq	%rdx, %r10
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$4, %eax
	movslq	%eax, %r14
	movq	%rcx, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	subq	%rcx, %rax
	movq	%rax, -144(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$3, %eax
	movslq	%eax, %r15
	movq	%rsi, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	subq	%rsi, %rax
	movq	%rax, -152(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$2, %eax
	movslq	%eax, %rbx
	movq	%rbx, %r9
	movq	%rdi, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rdi, %rax
	movq	%rax, -160(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %r12
	movq	%rdx, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	subq	%rdx, %rax
	movq	%rax, -168(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rbx
	movq	%rcx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rcx, %rax
	movq	%rax, %r11
	addq	%rax, %rcx
	movq	%rcx, -120(%rbp)
	jmp	.L370
.L371:
	movq	%r10, %rax
	movq	-136(%rbp), %rsi
	subq	%rsi, %rax
	movq	-144(%rbp), %rsi
	subq	%rsi, -88(%rbp)
	movq	-88(%rbp), %rdx
	movq	-152(%rbp), %rsi
	subq	%rsi, -96(%rbp)
	movq	-96(%rbp), %rcx
	movq	-160(%rbp), %rdi
	subq	%rdi, -104(%rbp)
	movq	-104(%rbp), %rsi
	movq	-168(%rbp), %r10
	subq	%r10, -112(%rbp)
	movq	-112(%rbp), %rdi
	subq	%r11, -120(%rbp)
	movq	-120(%rbp), %r8
	imulq	%r13, %rax
	imulq	%r14, %rdx
	imulq	%r15, %rcx
	imulq	%r9, %rsi
	imulq	%r12, %rdi
	imulq	%rbx, %r8
	imulq	%r13, %rax
	imulq	%r14, %rdx
	imulq	%r15, %rcx
	imulq	%r9, %rsi
	imulq	%r12, %rdi
	imulq	%rbx, %r8
	imulq	%r13, %rax
	imulq	%r14, %rdx
	imulq	%r15, %rcx
	imulq	%r9, %rsi
	imulq	%r12, %rdi
	imulq	%rbx, %r8
	imulq	%r13, %rax
	imulq	%r14, %rdx
	imulq	%r15, %rcx
	imulq	%r9, %rsi
	imulq	%r12, %rdi
	imulq	%rbx, %r8
	imulq	%r13, %rax
	imulq	%r14, %rdx
	imulq	%r15, %rcx
	imulq	%r9, %rsi
	imulq	%r12, %rdi
	imulq	%rbx, %r8
	imulq	%r13, %rax
	imulq	%r14, %rdx
	imulq	%r15, %rcx
	imulq	%r9, %rsi
	imulq	%r12, %rdi
	imulq	%rbx, %r8
	imulq	%r13, %rax
	imulq	%r14, %rdx
	imulq	%r15, %rcx
	imulq	%r9, %rsi
	imulq	%r12, %rdi
	imulq	%rbx, %r8
	imulq	%r13, %rax
	imulq	%r14, %rdx
	imulq	%r15, %rcx
	imulq	%r9, %rsi
	imulq	%r12, %rdi
	imulq	%rbx, %r8
	imulq	%r13, %rax
	imulq	%r14, %rdx
	imulq	%r15, %rcx
	imulq	%r9, %rsi
	imulq	%r12, %rdi
	imulq	%rbx, %r8
	imulq	%r13, %rax
	movq	%rax, %r10
	imulq	%r14, %rdx
	movq	%rdx, -88(%rbp)
	imulq	%r15, %rcx
	movq	%rcx, -96(%rbp)
	imulq	%r9, %rsi
	movq	%rsi, -104(%rbp)
	imulq	%r12, %rdi
	movq	%rdi, -112(%rbp)
	imulq	%rbx, %r8
	movq	%r8, -120(%rbp)
.L370:
	movq	-128(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -128(%rbp)
	testq	%rax, %rax
	jne	.L371
	movl	%r10d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE126:
	.size	int64_mul_5, .-int64_mul_5
	.globl	int64_mul_6
	.type	int64_mul_6, @function
int64_mul_6:
.LFB127:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$184, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -144(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$5, %eax
	movslq	%eax, %r14
	movq	%rcx, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	subq	%rcx, %rax
	movq	%rax, -152(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$4, %eax
	movslq	%eax, %r15
	movq	%rsi, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	subq	%rsi, %rax
	movq	%rax, -160(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$3, %eax
	movslq	%eax, %rbx
	movq	%rbx, -168(%rbp)
	movq	%rdi, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rdi, %rax
	movq	%rax, -176(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$2, %eax
	movslq	%eax, %rdx
	movq	%rdx, -184(%rbp)
	movq	%rcx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%rcx, %rax
	movq	%rax, -192(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %r13
	movq	%rsi, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	subq	%rsi, %rax
	movq	%rax, -200(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %r12
	movq	%rbx, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	subq	%rbx, %rax
	movq	%rax, -208(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rbx
	movq	%rdi, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rdi, %rax
	movq	%rax, -216(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -136(%rbp)
	jmp	.L373
.L374:
	movq	-152(%rbp), %rcx
	subq	%rcx, -88(%rbp)
	movq	-88(%rbp), %rax
	movq	-160(%rbp), %rsi
	subq	%rsi, -96(%rbp)
	movq	-96(%rbp), %rdx
	movq	-176(%rbp), %rsi
	subq	%rsi, -104(%rbp)
	movq	-104(%rbp), %rcx
	movq	-192(%rbp), %rdi
	subq	%rdi, -112(%rbp)
	movq	-112(%rbp), %rsi
	movq	-200(%rbp), %r10
	subq	%r10, -120(%rbp)
	movq	-120(%rbp), %rdi
	movq	-208(%rbp), %r11
	subq	%r11, -128(%rbp)
	movq	-128(%rbp), %r8
	movq	-216(%rbp), %r10
	subq	%r10, -136(%rbp)
	movq	-136(%rbp), %r9
	imulq	%r14, %rax
	imulq	%r15, %rdx
	movq	-168(%rbp), %r10
	imulq	%r10, %rcx
	movq	-184(%rbp), %r11
	imulq	%r11, %rsi
	imulq	%r13, %rdi
	imulq	%r12, %r8
	imulq	%rbx, %r9
	imulq	%r14, %rax
	imulq	%r15, %rdx
	imulq	%r10, %rcx
	imulq	%r11, %rsi
	imulq	%r13, %rdi
	imulq	%r12, %r8
	imulq	%rbx, %r9
	imulq	%r14, %rax
	imulq	%r15, %rdx
	imulq	%r10, %rcx
	imulq	%r11, %rsi
	imulq	%r13, %rdi
	imulq	%r12, %r8
	imulq	%rbx, %r9
	imulq	%r14, %rax
	imulq	%r15, %rdx
	imulq	%r10, %rcx
	imulq	%r11, %rsi
	imulq	%r13, %rdi
	imulq	%r12, %r8
	imulq	%rbx, %r9
	imulq	%r14, %rax
	imulq	%r15, %rdx
	imulq	%r10, %rcx
	imulq	%r11, %rsi
	imulq	%r13, %rdi
	imulq	%r12, %r8
	imulq	%rbx, %r9
	imulq	%r14, %rax
	imulq	%r15, %rdx
	imulq	%r10, %rcx
	imulq	%r11, %rsi
	imulq	%r13, %rdi
	imulq	%r12, %r8
	imulq	%rbx, %r9
	imulq	%r14, %rax
	imulq	%r15, %rdx
	imulq	%r10, %rcx
	imulq	%r11, %rsi
	imulq	%r13, %rdi
	imulq	%r12, %r8
	imulq	%rbx, %r9
	imulq	%r14, %rax
	imulq	%r15, %rdx
	imulq	%r10, %rcx
	imulq	%r11, %rsi
	imulq	%r13, %rdi
	imulq	%r12, %r8
	imulq	%rbx, %r9
	imulq	%r14, %rax
	imulq	%r15, %rdx
	imulq	%r10, %rcx
	imulq	%r11, %rsi
	imulq	%r13, %rdi
	imulq	%r12, %r8
	imulq	%rbx, %r9
	imulq	%r14, %rax
	movq	%rax, -88(%rbp)
	imulq	%r15, %rdx
	movq	%rdx, -96(%rbp)
	imulq	%r10, %rcx
	movq	%rcx, -104(%rbp)
	imulq	%r11, %rsi
	movq	%rsi, -112(%rbp)
	imulq	%r13, %rdi
	movq	%rdi, -120(%rbp)
	imulq	%r12, %r8
	movq	%r8, -128(%rbp)
	imulq	%rbx, %r9
	movq	%r9, -136(%rbp)
.L373:
	movq	-144(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -144(%rbp)
	testq	%rax, %rax
	jne	.L374
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$184, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE127:
	.size	int64_mul_6, .-int64_mul_6
	.globl	int64_mul_7
	.type	int64_mul_7, @function
int64_mul_7:
.LFB128:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$200, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -168(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$5, %eax
	movslq	%eax, %r15
	movq	%rsi, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	subq	%rsi, %rax
	movq	%rax, -176(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$4, %eax
	movslq	%eax, %rbx
	movq	%rbx, -184(%rbp)
	movq	%rdi, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rdi, %rax
	movq	%rax, -192(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$3, %eax
	movslq	%eax, %rdx
	movq	%rdx, -88(%rbp)
	movq	%rsi, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%rsi, %rax
	movq	%rax, -200(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$2, %eax
	movslq	%eax, %rcx
	movq	%rcx, -96(%rbp)
	movq	%rbx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	subq	%rbx, %rax
	movq	%rax, -208(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %r14
	movq	%rdi, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	subq	%rdi, %rax
	movq	%rax, -216(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %r13
	movq	%rsi, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	subq	%rsi, %rax
	movq	%rax, -224(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %r12
	movq	%rbx, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	subq	%rbx, %rax
	movq	%rax, -232(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %rbx
	movq	%rdi, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rdi, %rax
	movq	%rax, -240(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -160(%rbp)
	jmp	.L376
.L377:
	movq	-176(%rbp), %rdx
	subq	%rdx, -104(%rbp)
	movq	-104(%rbp), %rax
	movq	-192(%rbp), %rcx
	subq	%rcx, -112(%rbp)
	movq	-112(%rbp), %rdx
	movq	-200(%rbp), %rsi
	subq	%rsi, -120(%rbp)
	movq	-120(%rbp), %rcx
	movq	-208(%rbp), %rdi
	subq	%rdi, -128(%rbp)
	movq	-128(%rbp), %rsi
	movq	-216(%rbp), %r11
	subq	%r11, -136(%rbp)
	movq	-136(%rbp), %rdi
	movq	-224(%rbp), %r11
	subq	%r11, -144(%rbp)
	movq	-144(%rbp), %r8
	movq	-232(%rbp), %r11
	subq	%r11, -152(%rbp)
	movq	-152(%rbp), %r9
	movq	-240(%rbp), %r11
	subq	%r11, -160(%rbp)
	movq	-160(%rbp), %r10
	imulq	%r15, %rax
	movq	-184(%rbp), %r11
	imulq	%r11, %rdx
	imulq	-88(%rbp), %rcx
	imulq	-96(%rbp), %rsi
	imulq	%r14, %rdi
	imulq	%r13, %r8
	imulq	%r12, %r9
	imulq	%rbx, %r10
	imulq	%r15, %rax
	imulq	%r11, %rdx
	imulq	-88(%rbp), %rcx
	imulq	-96(%rbp), %rsi
	imulq	%r14, %rdi
	imulq	%r13, %r8
	imulq	%r12, %r9
	imulq	%rbx, %r10
	imulq	%r15, %rax
	imulq	%r11, %rdx
	imulq	-88(%rbp), %rcx
	imulq	-96(%rbp), %rsi
	imulq	%r14, %rdi
	imulq	%r13, %r8
	imulq	%r12, %r9
	imulq	%rbx, %r10
	imulq	%r15, %rax
	imulq	%r11, %rdx
	imulq	-88(%rbp), %rcx
	imulq	-96(%rbp), %rsi
	imulq	%r14, %rdi
	imulq	%r13, %r8
	imulq	%r12, %r9
	imulq	%rbx, %r10
	imulq	%r15, %rax
	imulq	%r11, %rdx
	imulq	-88(%rbp), %rcx
	imulq	-96(%rbp), %rsi
	imulq	%r14, %rdi
	imulq	%r13, %r8
	imulq	%r12, %r9
	imulq	%rbx, %r10
	imulq	%r15, %rax
	imulq	%r11, %rdx
	imulq	-88(%rbp), %rcx
	imulq	-96(%rbp), %rsi
	imulq	%r14, %rdi
	imulq	%r13, %r8
	imulq	%r12, %r9
	imulq	%rbx, %r10
	imulq	%r15, %rax
	imulq	%r11, %rdx
	imulq	-88(%rbp), %rcx
	imulq	-96(%rbp), %rsi
	imulq	%r14, %rdi
	imulq	%r13, %r8
	imulq	%r12, %r9
	imulq	%rbx, %r10
	imulq	%r15, %rax
	imulq	%r11, %rdx
	imulq	-88(%rbp), %rcx
	imulq	-96(%rbp), %rsi
	imulq	%r14, %rdi
	imulq	%r13, %r8
	imulq	%r12, %r9
	imulq	%rbx, %r10
	imulq	%r15, %rax
	imulq	%r11, %rdx
	imulq	-88(%rbp), %rcx
	imulq	-96(%rbp), %rsi
	imulq	%r14, %rdi
	imulq	%r13, %r8
	imulq	%r12, %r9
	imulq	%rbx, %r10
	imulq	%r15, %rax
	movq	%rax, -104(%rbp)
	imulq	%r11, %rdx
	movq	%rdx, -112(%rbp)
	imulq	-88(%rbp), %rcx
	movq	%rcx, -120(%rbp)
	imulq	-96(%rbp), %rsi
	movq	%rsi, -128(%rbp)
	imulq	%r14, %rdi
	movq	%rdi, -136(%rbp)
	imulq	%r13, %r8
	movq	%r8, -144(%rbp)
	imulq	%r12, %r9
	movq	%r9, -152(%rbp)
	imulq	%rbx, %r10
	movq	%r10, -160(%rbp)
.L376:
	movq	-168(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -168(%rbp)
	testq	%rax, %rax
	jne	.L377
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$200, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE128:
	.size	int64_mul_7, .-int64_mul_7
	.globl	int64_mul_8
	.type	int64_mul_8, @function
int64_mul_8:
.LFB129:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$232, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -192(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, -88(%rbp)
	movq	%rdi, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rdi, %rax
	movq	%rax, -200(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$4, %eax
	movslq	%eax, %rdx
	movq	%rdx, -96(%rbp)
	movq	%rbx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%rbx, %rax
	movq	%rax, -208(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$3, %eax
	movslq	%eax, %rcx
	movq	%rcx, -104(%rbp)
	movq	%rdi, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	subq	%rdi, %rax
	movq	%rax, -216(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$2, %eax
	movslq	%eax, %rsi
	movq	%rsi, -224(%rbp)
	movq	%rbx, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	subq	%rbx, %rax
	movq	%rax, -232(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %r15
	movq	%rdi, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	subq	%rdi, %rax
	movq	%rax, -240(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %r14
	movq	%rdx, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	subq	%rdx, %rax
	movq	%rax, -248(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %r13
	movq	%rcx, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	subq	%rcx, %rax
	movq	%rax, -256(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %r12
	movq	%rsi, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	subq	%rsi, %rax
	movq	%rax, -264(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$37413, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %rdx
	movq	%rdx, -112(%rbp)
	movq	%rdi, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%rdi, %rax
	movq	%rax, -272(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -184(%rbp)
	jmp	.L379
.L380:
	movq	-200(%rbp), %rdx
	subq	%rdx, -120(%rbp)
	movq	-120(%rbp), %rax
	movq	-208(%rbp), %rcx
	subq	%rcx, -128(%rbp)
	movq	-128(%rbp), %rdx
	movq	-216(%rbp), %rsi
	subq	%rsi, -136(%rbp)
	movq	-136(%rbp), %rcx
	movq	-232(%rbp), %rdi
	subq	%rdi, -144(%rbp)
	movq	-144(%rbp), %rsi
	movq	-240(%rbp), %r8
	subq	%r8, -152(%rbp)
	movq	-152(%rbp), %rdi
	movq	-248(%rbp), %r9
	subq	%r9, -160(%rbp)
	movq	-160(%rbp), %r8
	movq	-256(%rbp), %r10
	subq	%r10, -168(%rbp)
	movq	-168(%rbp), %r9
	movq	-264(%rbp), %r11
	subq	%r11, -176(%rbp)
	movq	-176(%rbp), %r10
	movq	-272(%rbp), %rbx
	subq	%rbx, -184(%rbp)
	movq	-184(%rbp), %r11
	imulq	-88(%rbp), %rax
	imulq	-96(%rbp), %rdx
	imulq	-104(%rbp), %rcx
	movq	-224(%rbp), %rbx
	imulq	%rbx, %rsi
	imulq	%r15, %rdi
	imulq	%r14, %r8
	imulq	%r13, %r9
	imulq	%r12, %r10
	imulq	-112(%rbp), %r11
	imulq	-88(%rbp), %rax
	imulq	-96(%rbp), %rdx
	imulq	-104(%rbp), %rcx
	imulq	%rbx, %rsi
	imulq	%r15, %rdi
	imulq	%r14, %r8
	imulq	%r13, %r9
	imulq	%r12, %r10
	imulq	-112(%rbp), %r11
	imulq	-88(%rbp), %rax
	imulq	-96(%rbp), %rdx
	imulq	-104(%rbp), %rcx
	imulq	%rbx, %rsi
	imulq	%r15, %rdi
	imulq	%r14, %r8
	imulq	%r13, %r9
	imulq	%r12, %r10
	imulq	-112(%rbp), %r11
	imulq	-88(%rbp), %rax
	imulq	-96(%rbp), %rdx
	imulq	-104(%rbp), %rcx
	imulq	%rbx, %rsi
	imulq	%r15, %rdi
	imulq	%r14, %r8
	imulq	%r13, %r9
	imulq	%r12, %r10
	imulq	-112(%rbp), %r11
	imulq	-88(%rbp), %rax
	imulq	-96(%rbp), %rdx
	imulq	-104(%rbp), %rcx
	imulq	%rbx, %rsi
	imulq	%r15, %rdi
	imulq	%r14, %r8
	imulq	%r13, %r9
	imulq	%r12, %r10
	imulq	-112(%rbp), %r11
	imulq	-88(%rbp), %rax
	imulq	-96(%rbp), %rdx
	imulq	-104(%rbp), %rcx
	imulq	%rbx, %rsi
	imulq	%r15, %rdi
	imulq	%r14, %r8
	imulq	%r13, %r9
	imulq	%r12, %r10
	imulq	-112(%rbp), %r11
	imulq	-88(%rbp), %rax
	imulq	-96(%rbp), %rdx
	imulq	-104(%rbp), %rcx
	imulq	%rbx, %rsi
	imulq	%r15, %rdi
	imulq	%r14, %r8
	imulq	%r13, %r9
	imulq	%r12, %r10
	imulq	-112(%rbp), %r11
	imulq	-88(%rbp), %rax
	imulq	-96(%rbp), %rdx
	imulq	-104(%rbp), %rcx
	imulq	%rbx, %rsi
	imulq	%r15, %rdi
	imulq	%r14, %r8
	imulq	%r13, %r9
	imulq	%r12, %r10
	imulq	-112(%rbp), %r11
	imulq	-88(%rbp), %rax
	imulq	-96(%rbp), %rdx
	imulq	-104(%rbp), %rcx
	imulq	%rbx, %rsi
	imulq	%r15, %rdi
	imulq	%r14, %r8
	imulq	%r13, %r9
	imulq	%r12, %r10
	imulq	-112(%rbp), %r11
	imulq	-88(%rbp), %rax
	movq	%rax, -120(%rbp)
	imulq	-96(%rbp), %rdx
	movq	%rdx, -128(%rbp)
	imulq	-104(%rbp), %rcx
	movq	%rcx, -136(%rbp)
	movq	%rbx, %rax
	imulq	%rax, %rsi
	movq	%rsi, -144(%rbp)
	imulq	%r15, %rdi
	movq	%rdi, -152(%rbp)
	imulq	%r14, %r8
	movq	%r8, -160(%rbp)
	imulq	%r13, %r9
	movq	%r9, -168(%rbp)
	imulq	%r12, %r10
	movq	%r10, -176(%rbp)
	imulq	-112(%rbp), %r11
	movq	%r11, -184(%rbp)
.L379:
	movq	-192(%rbp), %rax
	leaq	-1(%rax), %rbx
	movq	%rbx, -192(%rbp)
	testq	%rax, %rax
	jne	.L380
	movl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$232, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE129:
	.size	int64_mul_8, .-int64_mul_8
	.globl	int64_mul_9
	.type	int64_mul_9, @function
int64_mul_9:
.LFB130:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$264, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -216(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, -96(%rbp)
	movq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rdx, %rax
	movq	%rax, -224(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$4, %eax
	movslq	%eax, %rdx
	movq	%rdx, -104(%rbp)
	movq	%rbx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%rbx, %rax
	movq	%rax, -232(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$3, %eax
	movslq	%eax, %rcx
	movq	%rcx, -112(%rbp)
	movq	%rbx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	subq	%rbx, %rax
	movq	%rax, -240(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$2, %eax
	movslq	%eax, %rsi
	movq	%rsi, -120(%rbp)
	movq	%rcx, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	subq	%rcx, %rax
	movq	%rax, -248(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rdi
	movq	%rdi, -128(%rbp)
	movq	%rsi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	subq	%rsi, %rax
	movq	%rax, -256(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %r15
	movq	%rbx, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	subq	%rbx, %rax
	movq	%rax, -264(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %r14
	movq	%rdi, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	subq	%rdi, %rax
	movq	%rax, -272(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %r13
	movq	%rdx, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	subq	%rdx, %rax
	movq	%rax, -280(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$37413, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %rbx
	movq	%rbx, -136(%rbp)
	movq	%rcx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rcx, %rax
	movq	%rax, -288(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$37412, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %rbx
	movq	%rsi, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rsi, %rax
	movq	%rax, -296(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -88(%rbp)
	jmp	.L382
.L383:
	movq	-224(%rbp), %rdi
	subq	%rdi, -144(%rbp)
	movq	-144(%rbp), %rax
	movq	-232(%rbp), %rdi
	subq	%rdi, -152(%rbp)
	movq	-152(%rbp), %rdx
	movq	-240(%rbp), %rdi
	subq	%rdi, -160(%rbp)
	movq	-160(%rbp), %rcx
	movq	-248(%rbp), %rdi
	subq	%rdi, -168(%rbp)
	movq	-168(%rbp), %rsi
	movq	-256(%rbp), %r8
	subq	%r8, -176(%rbp)
	movq	-176(%rbp), %rdi
	movq	-264(%rbp), %r9
	subq	%r9, -184(%rbp)
	movq	-184(%rbp), %r8
	movq	-272(%rbp), %r10
	subq	%r10, -192(%rbp)
	movq	-192(%rbp), %r9
	movq	-280(%rbp), %r11
	subq	%r11, -200(%rbp)
	movq	-200(%rbp), %r10
	movq	-288(%rbp), %r12
	subq	%r12, -208(%rbp)
	movq	-208(%rbp), %r11
	movq	-296(%rbp), %r12
	subq	%r12, -88(%rbp)
	imulq	-96(%rbp), %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	%r15, %r8
	imulq	%r14, %r9
	imulq	%r13, %r10
	imulq	-136(%rbp), %r11
	movq	-88(%rbp), %r12
	imulq	%rbx, %r12
	movq	%r12, -88(%rbp)
	imulq	-96(%rbp), %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	%r15, %r8
	imulq	%r14, %r9
	imulq	%r13, %r10
	imulq	-136(%rbp), %r11
	movq	-88(%rbp), %r12
	imulq	%rbx, %r12
	movq	%r12, -88(%rbp)
	imulq	-96(%rbp), %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	%r15, %r8
	imulq	%r14, %r9
	imulq	%r13, %r10
	imulq	-136(%rbp), %r11
	movq	-88(%rbp), %r12
	imulq	%rbx, %r12
	movq	%r12, -88(%rbp)
	imulq	-96(%rbp), %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	%r15, %r8
	imulq	%r14, %r9
	imulq	%r13, %r10
	imulq	-136(%rbp), %r11
	movq	-88(%rbp), %r12
	imulq	%rbx, %r12
	movq	%r12, -88(%rbp)
	imulq	-96(%rbp), %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	%r15, %r8
	imulq	%r14, %r9
	imulq	%r13, %r10
	imulq	-136(%rbp), %r11
	movq	-88(%rbp), %r12
	imulq	%rbx, %r12
	movq	%r12, -88(%rbp)
	imulq	-96(%rbp), %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	%r15, %r8
	imulq	%r14, %r9
	imulq	%r13, %r10
	imulq	-136(%rbp), %r11
	movq	-88(%rbp), %r12
	imulq	%rbx, %r12
	movq	%r12, -88(%rbp)
	imulq	-96(%rbp), %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	%r15, %r8
	imulq	%r14, %r9
	imulq	%r13, %r10
	imulq	-136(%rbp), %r11
	movq	-88(%rbp), %r12
	imulq	%rbx, %r12
	movq	%r12, -88(%rbp)
	imulq	-96(%rbp), %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	%r15, %r8
	imulq	%r14, %r9
	imulq	%r13, %r10
	imulq	-136(%rbp), %r11
	movq	-88(%rbp), %r12
	imulq	%rbx, %r12
	movq	%r12, -88(%rbp)
	imulq	-96(%rbp), %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	%r15, %r8
	imulq	%r14, %r9
	imulq	%r13, %r10
	imulq	-136(%rbp), %r11
	movq	-88(%rbp), %r12
	imulq	%rbx, %r12
	movq	%r12, -88(%rbp)
	imulq	-96(%rbp), %rax
	movq	%rax, -144(%rbp)
	imulq	-104(%rbp), %rdx
	movq	%rdx, -152(%rbp)
	imulq	-112(%rbp), %rcx
	movq	%rcx, -160(%rbp)
	imulq	-120(%rbp), %rsi
	movq	%rsi, -168(%rbp)
	imulq	-128(%rbp), %rdi
	movq	%rdi, -176(%rbp)
	imulq	%r15, %r8
	movq	%r8, -184(%rbp)
	imulq	%r14, %r9
	movq	%r9, -192(%rbp)
	imulq	%r13, %r10
	movq	%r10, -200(%rbp)
	imulq	-136(%rbp), %r11
	movq	%r11, -208(%rbp)
	movq	-88(%rbp), %r12
	imulq	%rbx, %r12
	movq	%r12, -88(%rbp)
.L382:
	movq	-216(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -216(%rbp)
	testq	%rax, %rax
	jne	.L383
	movl	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$264, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE130:
	.size	int64_mul_9, .-int64_mul_9
	.globl	int64_mul_10
	.type	int64_mul_10, @function
int64_mul_10:
.LFB131:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$280, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -224(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$5, %eax
	movslq	%eax, %rdx
	movq	%rdx, -232(%rbp)
	movq	%rcx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%rcx, %rax
	movq	%rax, -240(%rbp)
	addq	%rax, %rcx
	movq	%rcx, %r12
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$4, %eax
	movslq	%eax, %rcx
	movq	%rcx, -104(%rbp)
	movq	%rsi, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	subq	%rsi, %rax
	movq	%rax, -248(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$3, %eax
	movslq	%eax, %rsi
	movq	%rsi, -112(%rbp)
	movq	%rdi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	subq	%rdi, %rax
	movq	%rax, -256(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$2, %eax
	movslq	%eax, %rbx
	movq	%rbx, -120(%rbp)
	movq	%rcx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rcx, %rax
	movq	%rax, -264(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rdi
	movq	%rdi, -128(%rbp)
	movq	%rsi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	subq	%rsi, %rax
	movq	%rax, -272(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rdx
	movq	%rdx, -136(%rbp)
	movq	%rdi, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%rdi, %rax
	movq	%rax, -280(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %r15
	movq	%rcx, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	subq	%rcx, %rax
	movq	%rax, -288(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %r14
	movq	%rsi, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	subq	%rsi, %rax
	movq	%rax, -296(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$37413, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %r13
	movq	%rdi, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	subq	%rdi, %rax
	movq	%rax, -304(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$37412, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %rsi
	movq	%rsi, -144(%rbp)
	movq	%rdx, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	subq	%rdx, %rax
	movq	%rax, -312(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$37411, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, -152(%rbp)
	movq	%rcx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rcx, %rax
	movq	%rax, -320(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -96(%rbp)
	jmp	.L385
.L386:
	movq	%r12, %rax
	movq	-240(%rbp), %rsi
	subq	%rsi, %rax
	movq	-248(%rbp), %rsi
	subq	%rsi, -160(%rbp)
	movq	-160(%rbp), %rdx
	movq	-256(%rbp), %rsi
	subq	%rsi, -168(%rbp)
	movq	-168(%rbp), %rcx
	movq	-264(%rbp), %rdi
	subq	%rdi, -176(%rbp)
	movq	-176(%rbp), %rsi
	movq	-272(%rbp), %rbx
	subq	%rbx, -184(%rbp)
	movq	-184(%rbp), %rdi
	movq	-280(%rbp), %rbx
	subq	%rbx, -192(%rbp)
	movq	-192(%rbp), %r8
	movq	-288(%rbp), %rbx
	subq	%rbx, -200(%rbp)
	movq	-200(%rbp), %r9
	movq	-296(%rbp), %rbx
	subq	%rbx, -208(%rbp)
	movq	-208(%rbp), %r10
	movq	-304(%rbp), %rbx
	subq	%rbx, -216(%rbp)
	movq	-216(%rbp), %r11
	movq	-312(%rbp), %rbx
	subq	%rbx, -88(%rbp)
	movq	-320(%rbp), %r12
	subq	%r12, -96(%rbp)
	movq	-232(%rbp), %r12
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	-136(%rbp), %r8
	imulq	%r15, %r9
	imulq	%r14, %r10
	imulq	%r13, %r11
	movq	-88(%rbp), %rbx
	imulq	-144(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-152(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	-136(%rbp), %r8
	imulq	%r15, %r9
	imulq	%r14, %r10
	imulq	%r13, %r11
	movq	-88(%rbp), %rbx
	imulq	-144(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-152(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	-136(%rbp), %r8
	imulq	%r15, %r9
	imulq	%r14, %r10
	imulq	%r13, %r11
	movq	-88(%rbp), %rbx
	imulq	-144(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-152(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	-136(%rbp), %r8
	imulq	%r15, %r9
	imulq	%r14, %r10
	imulq	%r13, %r11
	movq	-88(%rbp), %rbx
	imulq	-144(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-152(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	-136(%rbp), %r8
	imulq	%r15, %r9
	imulq	%r14, %r10
	imulq	%r13, %r11
	movq	-88(%rbp), %rbx
	imulq	-144(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-152(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	-136(%rbp), %r8
	imulq	%r15, %r9
	imulq	%r14, %r10
	imulq	%r13, %r11
	movq	-88(%rbp), %rbx
	imulq	-144(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-152(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	-136(%rbp), %r8
	imulq	%r15, %r9
	imulq	%r14, %r10
	imulq	%r13, %r11
	movq	-88(%rbp), %rbx
	imulq	-144(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-152(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	-136(%rbp), %r8
	imulq	%r15, %r9
	imulq	%r14, %r10
	imulq	%r13, %r11
	movq	-88(%rbp), %rbx
	imulq	-144(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-152(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-104(%rbp), %rdx
	imulq	-112(%rbp), %rcx
	imulq	-120(%rbp), %rsi
	imulq	-128(%rbp), %rdi
	imulq	-136(%rbp), %r8
	imulq	%r15, %r9
	imulq	%r14, %r10
	imulq	%r13, %r11
	movq	-88(%rbp), %rbx
	imulq	-144(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-152(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	movq	%rax, %r12
	imulq	-104(%rbp), %rdx
	movq	%rdx, -160(%rbp)
	imulq	-112(%rbp), %rcx
	movq	%rcx, -168(%rbp)
	imulq	-120(%rbp), %rsi
	movq	%rsi, -176(%rbp)
	imulq	-128(%rbp), %rdi
	movq	%rdi, -184(%rbp)
	imulq	-136(%rbp), %r8
	movq	%r8, -192(%rbp)
	imulq	%r15, %r9
	movq	%r9, -200(%rbp)
	imulq	%r14, %r10
	movq	%r10, -208(%rbp)
	imulq	%r13, %r11
	movq	%r11, -216(%rbp)
	movq	-88(%rbp), %rbx
	imulq	-144(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-152(%rbp), %rbx
	movq	%rbx, -96(%rbp)
.L385:
	movq	-224(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -224(%rbp)
	testq	%rax, %rax
	jne	.L386
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$280, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE131:
	.size	int64_mul_10, .-int64_mul_10
	.globl	int64_mul_11
	.type	int64_mul_11, @function
int64_mul_11:
.LFB132:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$312, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -240(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$5, %eax
	movslq	%eax, %rdx
	movq	%rdx, -248(%rbp)
	movq	%rsi, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%rsi, %rax
	movq	%rax, -256(%rbp)
	addq	%rax, %rsi
	movq	%rsi, %r12
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$4, %eax
	movslq	%eax, %rcx
	movq	%rcx, -112(%rbp)
	movq	%rdi, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	subq	%rdi, %rax
	movq	%rax, -264(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$3, %eax
	movslq	%eax, %rsi
	movq	%rsi, -120(%rbp)
	movq	%rdi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	subq	%rdi, %rax
	movq	%rax, -272(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$2, %eax
	movslq	%eax, %rdi
	movq	%rdi, -128(%rbp)
	movq	%rsi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	subq	%rsi, %rax
	movq	%rax, -280(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rbx
	movq	%rbx, -136(%rbp)
	movq	%rsi, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rsi, %rax
	movq	%rax, -288(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rdx
	movq	%rdx, -144(%rbp)
	movq	%rdi, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%rdi, %rax
	movq	%rax, -296(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rcx
	movq	%rcx, -152(%rbp)
	movq	%rdx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	subq	%rdx, %rax
	movq	%rax, -304(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %r15
	movq	%rcx, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	subq	%rcx, %rax
	movq	%rax, -312(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$37413, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %r14
	movq	%rsi, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	subq	%rsi, %rax
	movq	%rax, -320(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$37412, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %r13
	movq	%rdi, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	subq	%rdi, %rax
	movq	%rax, -328(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$37411, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$5, %eax
	movslq	%eax, %rsi
	movq	%rsi, -160(%rbp)
	movq	%rdx, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	subq	%rdx, %rax
	movq	%rax, -336(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$37410, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	subl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	subl	$6, %eax
	movslq	%eax, %rbx
	movq	%rbx, -168(%rbp)
	movq	%rcx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rcx, %rax
	movq	%rax, -344(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -104(%rbp)
	jmp	.L388
.L389:
	movq	%r12, %rax
	movq	-256(%rbp), %rsi
	subq	%rsi, %rax
	movq	-264(%rbp), %rsi
	subq	%rsi, -176(%rbp)
	movq	-176(%rbp), %rdx
	movq	-272(%rbp), %rsi
	subq	%rsi, -184(%rbp)
	movq	-184(%rbp), %rcx
	movq	-280(%rbp), %rdi
	subq	%rdi, -192(%rbp)
	movq	-192(%rbp), %rsi
	movq	-288(%rbp), %rbx
	subq	%rbx, -200(%rbp)
	movq	-200(%rbp), %rdi
	movq	-296(%rbp), %rbx
	subq	%rbx, -208(%rbp)
	movq	-208(%rbp), %r8
	movq	-304(%rbp), %rbx
	subq	%rbx, -216(%rbp)
	movq	-216(%rbp), %r9
	movq	-312(%rbp), %rbx
	subq	%rbx, -224(%rbp)
	movq	-224(%rbp), %r10
	movq	-320(%rbp), %rbx
	subq	%rbx, -232(%rbp)
	movq	-232(%rbp), %r11
	movq	-328(%rbp), %rbx
	subq	%rbx, -88(%rbp)
	movq	-336(%rbp), %r12
	subq	%r12, -96(%rbp)
	movq	-344(%rbp), %r12
	subq	%r12, -104(%rbp)
	movq	-248(%rbp), %r12
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-112(%rbp), %rdx
	imulq	-120(%rbp), %rcx
	imulq	-128(%rbp), %rsi
	imulq	-136(%rbp), %rdi
	imulq	-144(%rbp), %r8
	imulq	-152(%rbp), %r9
	imulq	%r15, %r10
	imulq	%r14, %r11
	movq	-88(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-160(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-168(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-112(%rbp), %rdx
	imulq	-120(%rbp), %rcx
	imulq	-128(%rbp), %rsi
	imulq	-136(%rbp), %rdi
	imulq	-144(%rbp), %r8
	imulq	-152(%rbp), %r9
	imulq	%r15, %r10
	imulq	%r14, %r11
	movq	-88(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-160(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-168(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-112(%rbp), %rdx
	imulq	-120(%rbp), %rcx
	imulq	-128(%rbp), %rsi
	imulq	-136(%rbp), %rdi
	imulq	-144(%rbp), %r8
	imulq	-152(%rbp), %r9
	imulq	%r15, %r10
	imulq	%r14, %r11
	movq	-88(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-160(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-168(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-112(%rbp), %rdx
	imulq	-120(%rbp), %rcx
	imulq	-128(%rbp), %rsi
	imulq	-136(%rbp), %rdi
	imulq	-144(%rbp), %r8
	imulq	-152(%rbp), %r9
	imulq	%r15, %r10
	imulq	%r14, %r11
	movq	-88(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-160(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-168(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-112(%rbp), %rdx
	imulq	-120(%rbp), %rcx
	imulq	-128(%rbp), %rsi
	imulq	-136(%rbp), %rdi
	imulq	-144(%rbp), %r8
	imulq	-152(%rbp), %r9
	imulq	%r15, %r10
	imulq	%r14, %r11
	movq	-88(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-160(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-168(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-112(%rbp), %rdx
	imulq	-120(%rbp), %rcx
	imulq	-128(%rbp), %rsi
	imulq	-136(%rbp), %rdi
	imulq	-144(%rbp), %r8
	imulq	-152(%rbp), %r9
	imulq	%r15, %r10
	imulq	%r14, %r11
	movq	-88(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-160(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-168(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-112(%rbp), %rdx
	imulq	-120(%rbp), %rcx
	imulq	-128(%rbp), %rsi
	imulq	-136(%rbp), %rdi
	imulq	-144(%rbp), %r8
	imulq	-152(%rbp), %r9
	imulq	%r15, %r10
	imulq	%r14, %r11
	movq	-88(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-160(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-168(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-112(%rbp), %rdx
	imulq	-120(%rbp), %rcx
	imulq	-128(%rbp), %rsi
	imulq	-136(%rbp), %rdi
	imulq	-144(%rbp), %r8
	imulq	-152(%rbp), %r9
	imulq	%r15, %r10
	imulq	%r14, %r11
	movq	-88(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-160(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-168(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-112(%rbp), %rdx
	imulq	-120(%rbp), %rcx
	imulq	-128(%rbp), %rsi
	imulq	-136(%rbp), %rdi
	imulq	-144(%rbp), %r8
	imulq	-152(%rbp), %r9
	imulq	%r15, %r10
	imulq	%r14, %r11
	movq	-88(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-160(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-168(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	movq	%rax, %r12
	imulq	-112(%rbp), %rdx
	movq	%rdx, -176(%rbp)
	imulq	-120(%rbp), %rcx
	movq	%rcx, -184(%rbp)
	imulq	-128(%rbp), %rsi
	movq	%rsi, -192(%rbp)
	imulq	-136(%rbp), %rdi
	movq	%rdi, -200(%rbp)
	imulq	-144(%rbp), %r8
	movq	%r8, -208(%rbp)
	imulq	-152(%rbp), %r9
	movq	%r9, -216(%rbp)
	imulq	%r15, %r10
	movq	%r10, -224(%rbp)
	imulq	%r14, %r11
	movq	%r11, -232(%rbp)
	movq	-88(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	-160(%rbp), %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-168(%rbp), %rbx
	movq	%rbx, -104(%rbp)
.L388:
	movq	-240(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -240(%rbp)
	testq	%rax, %rax
	jne	.L389
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$312, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE132:
	.size	int64_mul_11, .-int64_mul_11
	.globl	int64_mul_12
	.type	int64_mul_12, @function
int64_mul_12:
.LFB133:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$328, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -256(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$5, %eax
	movslq	%eax, %rdx
	movq	%rdx, -264(%rbp)
	movq	%rcx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%rcx, %rax
	movq	%rax, -272(%rbp)
	addq	%rax, %rcx
	movq	%rcx, %r12
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$4, %eax
	movslq	%eax, %rcx
	movq	%rcx, -120(%rbp)
	movq	%rdx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	subq	%rdx, %rax
	movq	%rax, -280(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$3, %eax
	movslq	%eax, %rsi
	movq	%rsi, -128(%rbp)
	movq	%rdi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	subq	%rdi, %rax
	movq	%rax, -288(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$2, %eax
	movslq	%eax, %rdi
	movq	%rdi, -136(%rbp)
	movq	%rsi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	subq	%rsi, %rax
	movq	%rax, -296(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rdx
	movq	%rdx, -144(%rbp)
	movq	%rcx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%rcx, %rax
	movq	%rax, -304(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rcx
	movq	%rcx, -152(%rbp)
	movq	%rdx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	subq	%rdx, %rax
	movq	%rax, -312(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rsi
	movq	%rsi, -160(%rbp)
	movq	%rdi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	subq	%rdi, %rax
	movq	%rax, -320(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %rdi
	movq	%rdi, -168(%rbp)
	movq	%rsi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	subq	%rsi, %rax
	movq	%rax, -328(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -240(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$37413, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %r15
	movq	%rdx, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	subq	%rdx, %rax
	movq	%rax, -336(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -248(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$37412, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %r14
	movq	%rcx, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	subq	%rcx, %rax
	movq	%rax, -344(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$37411, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$5, %eax
	movslq	%eax, %r13
	movq	%rsi, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	subq	%rsi, %rax
	movq	%rax, -352(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$37410, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	subl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	subl	$6, %eax
	movslq	%eax, %rbx
	movq	%rbx, -176(%rbp)
	movq	%rdi, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rdi, %rax
	movq	%rax, -360(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$37409, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	subl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	subl	$7, %eax
	movslq	%eax, %rbx
	movq	%rbx, -184(%rbp)
	movq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rdx, %rax
	movq	%rax, -368(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -112(%rbp)
	jmp	.L391
.L392:
	movq	%r12, %rax
	movq	-272(%rbp), %rcx
	subq	%rcx, %rax
	movq	-280(%rbp), %rsi
	subq	%rsi, -192(%rbp)
	movq	-192(%rbp), %rdx
	movq	-288(%rbp), %rsi
	subq	%rsi, -200(%rbp)
	movq	-200(%rbp), %rcx
	movq	-296(%rbp), %rdi
	subq	%rdi, -208(%rbp)
	movq	-208(%rbp), %rsi
	movq	-304(%rbp), %rbx
	subq	%rbx, -216(%rbp)
	movq	-216(%rbp), %rdi
	movq	-312(%rbp), %rbx
	subq	%rbx, -224(%rbp)
	movq	-224(%rbp), %r8
	movq	-320(%rbp), %rbx
	subq	%rbx, -232(%rbp)
	movq	-232(%rbp), %r9
	movq	-328(%rbp), %rbx
	subq	%rbx, -240(%rbp)
	movq	-240(%rbp), %r10
	movq	-336(%rbp), %rbx
	subq	%rbx, -248(%rbp)
	movq	-248(%rbp), %r11
	movq	-344(%rbp), %rbx
	subq	%rbx, -88(%rbp)
	movq	-352(%rbp), %r12
	subq	%r12, -96(%rbp)
	movq	-360(%rbp), %r12
	subq	%r12, -104(%rbp)
	movq	-368(%rbp), %r12
	subq	%r12, -112(%rbp)
	movq	-264(%rbp), %r12
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-120(%rbp), %rdx
	imulq	-128(%rbp), %rcx
	imulq	-136(%rbp), %rsi
	imulq	-144(%rbp), %rdi
	imulq	-152(%rbp), %r8
	imulq	-160(%rbp), %r9
	imulq	-168(%rbp), %r10
	imulq	%r15, %r11
	movq	-88(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-176(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-184(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-120(%rbp), %rdx
	imulq	-128(%rbp), %rcx
	imulq	-136(%rbp), %rsi
	imulq	-144(%rbp), %rdi
	imulq	-152(%rbp), %r8
	imulq	-160(%rbp), %r9
	imulq	-168(%rbp), %r10
	imulq	%r15, %r11
	movq	-88(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-176(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-184(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-120(%rbp), %rdx
	imulq	-128(%rbp), %rcx
	imulq	-136(%rbp), %rsi
	imulq	-144(%rbp), %rdi
	imulq	-152(%rbp), %r8
	imulq	-160(%rbp), %r9
	imulq	-168(%rbp), %r10
	imulq	%r15, %r11
	movq	-88(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-176(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-184(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-120(%rbp), %rdx
	imulq	-128(%rbp), %rcx
	imulq	-136(%rbp), %rsi
	imulq	-144(%rbp), %rdi
	imulq	-152(%rbp), %r8
	imulq	-160(%rbp), %r9
	imulq	-168(%rbp), %r10
	imulq	%r15, %r11
	movq	-88(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-176(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-184(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-120(%rbp), %rdx
	imulq	-128(%rbp), %rcx
	imulq	-136(%rbp), %rsi
	imulq	-144(%rbp), %rdi
	imulq	-152(%rbp), %r8
	imulq	-160(%rbp), %r9
	imulq	-168(%rbp), %r10
	imulq	%r15, %r11
	movq	-88(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-176(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-184(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-120(%rbp), %rdx
	imulq	-128(%rbp), %rcx
	imulq	-136(%rbp), %rsi
	imulq	-144(%rbp), %rdi
	imulq	-152(%rbp), %r8
	imulq	-160(%rbp), %r9
	imulq	-168(%rbp), %r10
	imulq	%r15, %r11
	movq	-88(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-176(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-184(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-120(%rbp), %rdx
	imulq	-128(%rbp), %rcx
	imulq	-136(%rbp), %rsi
	imulq	-144(%rbp), %rdi
	imulq	-152(%rbp), %r8
	imulq	-160(%rbp), %r9
	imulq	-168(%rbp), %r10
	imulq	%r15, %r11
	movq	-88(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-176(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-184(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-120(%rbp), %rdx
	imulq	-128(%rbp), %rcx
	imulq	-136(%rbp), %rsi
	imulq	-144(%rbp), %rdi
	imulq	-152(%rbp), %r8
	imulq	-160(%rbp), %r9
	imulq	-168(%rbp), %r10
	imulq	%r15, %r11
	movq	-88(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-176(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-184(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-120(%rbp), %rdx
	imulq	-128(%rbp), %rcx
	imulq	-136(%rbp), %rsi
	imulq	-144(%rbp), %rdi
	imulq	-152(%rbp), %r8
	imulq	-160(%rbp), %r9
	imulq	-168(%rbp), %r10
	imulq	%r15, %r11
	movq	-88(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-176(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-184(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	movq	%rax, %r12
	imulq	-120(%rbp), %rdx
	movq	%rdx, -192(%rbp)
	imulq	-128(%rbp), %rcx
	movq	%rcx, -200(%rbp)
	imulq	-136(%rbp), %rsi
	movq	%rsi, -208(%rbp)
	imulq	-144(%rbp), %rdi
	movq	%rdi, -216(%rbp)
	imulq	-152(%rbp), %r8
	movq	%r8, -224(%rbp)
	imulq	-160(%rbp), %r9
	movq	%r9, -232(%rbp)
	imulq	-168(%rbp), %r10
	movq	%r10, -240(%rbp)
	imulq	%r15, %r11
	movq	%r11, -248(%rbp)
	movq	-88(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	-176(%rbp), %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-184(%rbp), %rbx
	movq	%rbx, -112(%rbp)
.L391:
	movq	-256(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -256(%rbp)
	testq	%rax, %rax
	jne	.L392
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$328, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE133:
	.size	int64_mul_12, .-int64_mul_12
	.globl	int64_mul_13
	.type	int64_mul_13, @function
int64_mul_13:
.LFB134:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$360, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -272(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$5, %eax
	movslq	%eax, %rdx
	movq	%rdx, -280(%rbp)
	movq	%rcx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%rcx, %rax
	movq	%rax, -288(%rbp)
	addq	%rax, %rcx
	movq	%rcx, %r12
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$4, %eax
	movslq	%eax, %rcx
	movq	%rcx, -128(%rbp)
	movq	%rsi, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	subq	%rsi, %rax
	movq	%rax, -296(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$3, %eax
	movslq	%eax, %rsi
	movq	%rsi, -136(%rbp)
	movq	%rdi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	subq	%rdi, %rax
	movq	%rax, -304(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$2, %eax
	movslq	%eax, %rdi
	movq	%rdi, -144(%rbp)
	movq	%rcx, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	subq	%rcx, %rax
	movq	%rax, -312(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rdx
	movq	%rdx, -152(%rbp)
	movq	%rsi, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%rsi, %rax
	movq	%rax, -320(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rcx
	movq	%rcx, -160(%rbp)
	movq	%rdi, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	subq	%rdi, %rax
	movq	%rax, -328(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -240(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rsi
	movq	%rsi, -168(%rbp)
	movq	%rcx, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	subq	%rcx, %rax
	movq	%rax, -336(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -248(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %rdi
	movq	%rdi, -176(%rbp)
	movq	%rsi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	subq	%rsi, %rax
	movq	%rax, -344(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -256(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$37413, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %rdx
	movq	%rdx, -184(%rbp)
	movq	%rdi, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%rdi, %rax
	movq	%rax, -352(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -264(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$37412, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %r15
	movq	%rdx, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	subq	%rdx, %rax
	movq	%rax, -360(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$37411, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$5, %eax
	movslq	%eax, %r14
	movq	%rcx, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	subq	%rcx, %rax
	movq	%rax, -368(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$37410, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	subl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	subl	$6, %eax
	movslq	%eax, %r13
	movq	%rsi, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	subq	%rsi, %rax
	movq	%rax, -376(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$37409, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	subl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	subl	$7, %eax
	movslq	%eax, %rcx
	movq	%rcx, -192(%rbp)
	movq	%rdi, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	subq	%rdi, %rax
	movq	%rax, -384(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$37408, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	subl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	subl	$8, %eax
	movslq	%eax, %rbx
	movq	%rbx, -200(%rbp)
	movq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rdx, %rax
	movq	%rax, -392(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -120(%rbp)
	jmp	.L394
.L395:
	movq	%r12, %rax
	movq	-288(%rbp), %rcx
	subq	%rcx, %rax
	movq	-296(%rbp), %rsi
	subq	%rsi, -208(%rbp)
	movq	-208(%rbp), %rdx
	movq	-304(%rbp), %rsi
	subq	%rsi, -216(%rbp)
	movq	-216(%rbp), %rcx
	movq	-312(%rbp), %rdi
	subq	%rdi, -224(%rbp)
	movq	-224(%rbp), %rsi
	movq	-320(%rbp), %rbx
	subq	%rbx, -232(%rbp)
	movq	-232(%rbp), %rdi
	movq	-328(%rbp), %rbx
	subq	%rbx, -240(%rbp)
	movq	-240(%rbp), %r8
	movq	-336(%rbp), %rbx
	subq	%rbx, -248(%rbp)
	movq	-248(%rbp), %r9
	movq	-344(%rbp), %rbx
	subq	%rbx, -256(%rbp)
	movq	-256(%rbp), %r10
	movq	-352(%rbp), %rbx
	subq	%rbx, -264(%rbp)
	movq	-264(%rbp), %r11
	movq	-360(%rbp), %rbx
	subq	%rbx, -88(%rbp)
	movq	-368(%rbp), %r12
	subq	%r12, -96(%rbp)
	movq	-376(%rbp), %r12
	subq	%r12, -104(%rbp)
	movq	-384(%rbp), %r12
	subq	%r12, -112(%rbp)
	movq	-392(%rbp), %r12
	subq	%r12, -120(%rbp)
	movq	-280(%rbp), %r12
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-128(%rbp), %rdx
	imulq	-136(%rbp), %rcx
	imulq	-144(%rbp), %rsi
	imulq	-152(%rbp), %rdi
	imulq	-160(%rbp), %r8
	imulq	-168(%rbp), %r9
	imulq	-176(%rbp), %r10
	imulq	-184(%rbp), %r11
	movq	-88(%rbp), %rbx
	imulq	%r15, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-192(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	-120(%rbp), %rbx
	imulq	-200(%rbp), %rbx
	movq	%rbx, -120(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-128(%rbp), %rdx
	imulq	-136(%rbp), %rcx
	imulq	-144(%rbp), %rsi
	imulq	-152(%rbp), %rdi
	imulq	-160(%rbp), %r8
	imulq	-168(%rbp), %r9
	imulq	-176(%rbp), %r10
	imulq	-184(%rbp), %r11
	movq	-88(%rbp), %rbx
	imulq	%r15, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-192(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	-120(%rbp), %rbx
	imulq	-200(%rbp), %rbx
	movq	%rbx, -120(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-128(%rbp), %rdx
	imulq	-136(%rbp), %rcx
	imulq	-144(%rbp), %rsi
	imulq	-152(%rbp), %rdi
	imulq	-160(%rbp), %r8
	imulq	-168(%rbp), %r9
	imulq	-176(%rbp), %r10
	imulq	-184(%rbp), %r11
	movq	-88(%rbp), %rbx
	imulq	%r15, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-192(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	-120(%rbp), %rbx
	imulq	-200(%rbp), %rbx
	movq	%rbx, -120(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-128(%rbp), %rdx
	imulq	-136(%rbp), %rcx
	imulq	-144(%rbp), %rsi
	imulq	-152(%rbp), %rdi
	imulq	-160(%rbp), %r8
	imulq	-168(%rbp), %r9
	imulq	-176(%rbp), %r10
	imulq	-184(%rbp), %r11
	movq	-88(%rbp), %rbx
	imulq	%r15, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-192(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	-120(%rbp), %rbx
	imulq	-200(%rbp), %rbx
	movq	%rbx, -120(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-128(%rbp), %rdx
	imulq	-136(%rbp), %rcx
	imulq	-144(%rbp), %rsi
	imulq	-152(%rbp), %rdi
	imulq	-160(%rbp), %r8
	imulq	-168(%rbp), %r9
	imulq	-176(%rbp), %r10
	imulq	-184(%rbp), %r11
	movq	-88(%rbp), %rbx
	imulq	%r15, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-192(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	-120(%rbp), %rbx
	imulq	-200(%rbp), %rbx
	movq	%rbx, -120(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-128(%rbp), %rdx
	imulq	-136(%rbp), %rcx
	imulq	-144(%rbp), %rsi
	imulq	-152(%rbp), %rdi
	imulq	-160(%rbp), %r8
	imulq	-168(%rbp), %r9
	imulq	-176(%rbp), %r10
	imulq	-184(%rbp), %r11
	movq	-88(%rbp), %rbx
	imulq	%r15, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-192(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	-120(%rbp), %rbx
	imulq	-200(%rbp), %rbx
	movq	%rbx, -120(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-128(%rbp), %rdx
	imulq	-136(%rbp), %rcx
	imulq	-144(%rbp), %rsi
	imulq	-152(%rbp), %rdi
	imulq	-160(%rbp), %r8
	imulq	-168(%rbp), %r9
	imulq	-176(%rbp), %r10
	imulq	-184(%rbp), %r11
	movq	-88(%rbp), %rbx
	imulq	%r15, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-192(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	-120(%rbp), %rbx
	imulq	-200(%rbp), %rbx
	movq	%rbx, -120(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-128(%rbp), %rdx
	imulq	-136(%rbp), %rcx
	imulq	-144(%rbp), %rsi
	imulq	-152(%rbp), %rdi
	imulq	-160(%rbp), %r8
	imulq	-168(%rbp), %r9
	imulq	-176(%rbp), %r10
	imulq	-184(%rbp), %r11
	movq	-88(%rbp), %rbx
	imulq	%r15, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-192(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	-120(%rbp), %rbx
	imulq	-200(%rbp), %rbx
	movq	%rbx, -120(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	imulq	-128(%rbp), %rdx
	imulq	-136(%rbp), %rcx
	imulq	-144(%rbp), %rsi
	imulq	-152(%rbp), %rdi
	imulq	-160(%rbp), %r8
	imulq	-168(%rbp), %r9
	imulq	-176(%rbp), %r10
	imulq	-184(%rbp), %r11
	movq	-88(%rbp), %rbx
	imulq	%r15, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-192(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	-120(%rbp), %rbx
	imulq	-200(%rbp), %rbx
	movq	%rbx, -120(%rbp)
	movq	%r12, %rbx
	imulq	%rbx, %rax
	movq	%rax, %r12
	imulq	-128(%rbp), %rdx
	movq	%rdx, -208(%rbp)
	imulq	-136(%rbp), %rcx
	movq	%rcx, -216(%rbp)
	imulq	-144(%rbp), %rsi
	movq	%rsi, -224(%rbp)
	imulq	-152(%rbp), %rdi
	movq	%rdi, -232(%rbp)
	imulq	-160(%rbp), %r8
	movq	%r8, -240(%rbp)
	imulq	-168(%rbp), %r9
	movq	%r9, -248(%rbp)
	imulq	-176(%rbp), %r10
	movq	%r10, -256(%rbp)
	imulq	-184(%rbp), %r11
	movq	%r11, -264(%rbp)
	movq	-88(%rbp), %rbx
	imulq	%r15, %rbx
	movq	%rbx, -88(%rbp)
	movq	-96(%rbp), %rbx
	imulq	%r14, %rbx
	movq	%rbx, -96(%rbp)
	movq	-104(%rbp), %rbx
	imulq	%r13, %rbx
	movq	%rbx, -104(%rbp)
	movq	-112(%rbp), %rbx
	imulq	-192(%rbp), %rbx
	movq	%rbx, -112(%rbp)
	movq	-120(%rbp), %rbx
	imulq	-200(%rbp), %rbx
	movq	%rbx, -120(%rbp)
.L394:
	movq	-272(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -272(%rbp)
	testq	%rax, %rax
	jne	.L395
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-264(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$360, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE134:
	.size	int64_mul_13, .-int64_mul_13
	.globl	int64_mul_14
	.type	int64_mul_14, @function
int64_mul_14:
.LFB135:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$376, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -288(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, -144(%rbp)
	movq	%rcx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rcx, %rax
	movq	%rax, -296(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$4, %eax
	movslq	%eax, %rcx
	movq	%rcx, -152(%rbp)
	movq	%rbx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	subq	%rbx, %rax
	movq	%rax, -304(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$3, %eax
	movslq	%eax, %rsi
	movq	%rsi, -160(%rbp)
	movq	%rbx, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	subq	%rbx, %rax
	movq	%rax, -312(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$2, %eax
	movslq	%eax, %rbx
	movq	%rbx, -168(%rbp)
	movq	%rsi, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rsi, %rax
	movq	%rax, -320(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -240(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rdi
	movq	%rdi, -176(%rbp)
	movq	%rcx, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	subq	%rcx, %rax
	movq	%rax, -328(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -248(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rdx
	movq	%rdx, -184(%rbp)
	movq	%rdi, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%rdi, %rax
	movq	%rax, -336(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -256(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rcx
	movq	%rcx, -192(%rbp)
	movq	%rsi, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	subq	%rsi, %rax
	movq	%rax, -344(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -264(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %rsi
	movq	%rsi, -200(%rbp)
	movq	%rbx, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	subq	%rbx, %rax
	movq	%rax, -352(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -272(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$37413, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %rbx
	movq	%rbx, -208(%rbp)
	movq	%rdx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rdx, %rax
	movq	%rax, -360(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -280(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$37412, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %rdi
	movq	%rdi, -216(%rbp)
	movq	%rcx, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	subq	%rcx, %rax
	movq	%rax, -368(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$37411, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$5, %eax
	movslq	%eax, %r15
	movq	%rdi, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	subq	%rdi, %rax
	movq	%rax, -376(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$37410, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	subl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	subl	$6, %eax
	movslq	%eax, %r14
	movq	%rsi, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	subq	%rsi, %rax
	movq	%rax, -384(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$37409, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	subl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	subl	$7, %eax
	movslq	%eax, %r13
	movq	%rbx, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	subq	%rbx, %rax
	movq	%rax, -392(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$37408, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	subl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	subl	$8, %eax
	movslq	%eax, %r12
	movq	%rdx, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	subq	%rdx, %rax
	movq	%rax, -400(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	addl	$37407, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	subl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	subl	$9, %eax
	movslq	%eax, %rbx
	movq	%rcx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rcx, %rax
	movq	%rax, -408(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -128(%rbp)
	jmp	.L397
.L398:
	movq	-296(%rbp), %rdi
	subq	%rdi, -224(%rbp)
	movq	-224(%rbp), %rax
	movq	-304(%rbp), %rsi
	subq	%rsi, -136(%rbp)
	movq	-136(%rbp), %rdx
	movq	-312(%rbp), %rdi
	subq	%rdi, -232(%rbp)
	movq	-320(%rbp), %rdi
	subq	%rdi, -240(%rbp)
	movq	-240(%rbp), %rsi
	movq	-328(%rbp), %r10
	subq	%r10, -248(%rbp)
	movq	-248(%rbp), %rdi
	movq	-336(%rbp), %r11
	subq	%r11, -256(%rbp)
	movq	-256(%rbp), %r8
	movq	-344(%rbp), %r10
	subq	%r10, -264(%rbp)
	movq	-264(%rbp), %r9
	movq	-352(%rbp), %r11
	subq	%r11, -272(%rbp)
	movq	-272(%rbp), %r10
	movq	-360(%rbp), %rcx
	subq	%rcx, -280(%rbp)
	movq	-280(%rbp), %r11
	movq	-368(%rbp), %rcx
	subq	%rcx, -88(%rbp)
	movq	-376(%rbp), %rcx
	subq	%rcx, -96(%rbp)
	movq	-384(%rbp), %rcx
	subq	%rcx, -104(%rbp)
	movq	-392(%rbp), %rcx
	subq	%rcx, -112(%rbp)
	movq	-400(%rbp), %rcx
	subq	%rcx, -120(%rbp)
	movq	-408(%rbp), %rcx
	subq	%rcx, -128(%rbp)
	imulq	-144(%rbp), %rax
	imulq	-152(%rbp), %rdx
	movq	%rdx, -136(%rbp)
	movq	-232(%rbp), %rcx
	imulq	-160(%rbp), %rcx
	imulq	-168(%rbp), %rsi
	imulq	-176(%rbp), %rdi
	imulq	-184(%rbp), %r8
	imulq	-192(%rbp), %r9
	imulq	-200(%rbp), %r10
	imulq	-208(%rbp), %r11
	movq	-88(%rbp), %rdx
	imulq	-216(%rbp), %rdx
	movq	%rdx, -88(%rbp)
	movq	-96(%rbp), %rdx
	imulq	%r15, %rdx
	movq	%rdx, -96(%rbp)
	movq	-104(%rbp), %rdx
	imulq	%r14, %rdx
	movq	%rdx, -104(%rbp)
	movq	-112(%rbp), %rdx
	imulq	%r13, %rdx
	movq	%rdx, -112(%rbp)
	movq	-120(%rbp), %rdx
	imulq	%r12, %rdx
	movq	%rdx, -120(%rbp)
	movq	-128(%rbp), %rdx
	imulq	%rbx, %rdx
	movq	%rdx, -128(%rbp)
	imulq	-144(%rbp), %rax
	movq	%rax, -224(%rbp)
	movq	-136(%rbp), %rdx
	imulq	-152(%rbp), %rdx
	imulq	-160(%rbp), %rcx
	imulq	-168(%rbp), %rsi
	imulq	-176(%rbp), %rdi
	imulq	-184(%rbp), %r8
	imulq	-192(%rbp), %r9
	imulq	-200(%rbp), %r10
	imulq	-208(%rbp), %r11
	movq	-88(%rbp), %rax
	imulq	-216(%rbp), %rax
	movq	%rax, -88(%rbp)
	movq	-96(%rbp), %rax
	imulq	%r15, %rax
	movq	%rax, -96(%rbp)
	movq	-104(%rbp), %rax
	imulq	%r14, %rax
	movq	%rax, -104(%rbp)
	movq	-112(%rbp), %rax
	imulq	%r13, %rax
	movq	%rax, -112(%rbp)
	movq	-120(%rbp), %rax
	imulq	%r12, %rax
	movq	%rax, -120(%rbp)
	movq	-128(%rbp), %rax
	imulq	%rbx, %rax
	movq	%rax, -128(%rbp)
	movq	-224(%rbp), %rax
	imulq	-144(%rbp), %rax
	imulq	-152(%rbp), %rdx
	movq	%rdx, -136(%rbp)
	imulq	-160(%rbp), %rcx
	imulq	-168(%rbp), %rsi
	imulq	-176(%rbp), %rdi
	imulq	-184(%rbp), %r8
	imulq	-192(%rbp), %r9
	imulq	-200(%rbp), %r10
	imulq	-208(%rbp), %r11
	movq	-88(%rbp), %rdx
	imulq	-216(%rbp), %rdx
	movq	%rdx, -88(%rbp)
	movq	-96(%rbp), %rdx
	imulq	%r15, %rdx
	movq	%rdx, -96(%rbp)
	movq	-104(%rbp), %rdx
	imulq	%r14, %rdx
	movq	%rdx, -104(%rbp)
	movq	-112(%rbp), %rdx
	imulq	%r13, %rdx
	movq	%rdx, -112(%rbp)
	movq	-120(%rbp), %rdx
	imulq	%r12, %rdx
	movq	%rdx, -120(%rbp)
	movq	-128(%rbp), %rdx
	imulq	%rbx, %rdx
	movq	%rdx, -128(%rbp)
	imulq	-144(%rbp), %rax
	movq	-136(%rbp), %rdx
	imulq	-152(%rbp), %rdx
	imulq	-160(%rbp), %rcx
	movq	%rcx, -232(%rbp)
	imulq	-168(%rbp), %rsi
	imulq	-176(%rbp), %rdi
	imulq	-184(%rbp), %r8
	imulq	-192(%rbp), %r9
	imulq	-200(%rbp), %r10
	imulq	-208(%rbp), %r11
	movq	-88(%rbp), %rcx
	imulq	-216(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	movq	-96(%rbp), %rcx
	imulq	%r15, %rcx
	movq	%rcx, -96(%rbp)
	movq	-104(%rbp), %rcx
	imulq	%r14, %rcx
	movq	%rcx, -104(%rbp)
	movq	-112(%rbp), %rcx
	imulq	%r13, %rcx
	movq	%rcx, -112(%rbp)
	movq	-120(%rbp), %rcx
	imulq	%r12, %rcx
	movq	%rcx, -120(%rbp)
	movq	-128(%rbp), %rcx
	imulq	%rbx, %rcx
	movq	%rcx, -128(%rbp)
	imulq	-144(%rbp), %rax
	imulq	-152(%rbp), %rdx
	movq	-232(%rbp), %rcx
	imulq	-160(%rbp), %rcx
	imulq	-168(%rbp), %rsi
	movq	%rsi, -240(%rbp)
	imulq	-176(%rbp), %rdi
	imulq	-184(%rbp), %r8
	imulq	-192(%rbp), %r9
	imulq	-200(%rbp), %r10
	imulq	-208(%rbp), %r11
	movq	-88(%rbp), %rsi
	imulq	-216(%rbp), %rsi
	movq	%rsi, -88(%rbp)
	movq	-96(%rbp), %rsi
	imulq	%r15, %rsi
	movq	%rsi, -96(%rbp)
	movq	-104(%rbp), %rsi
	imulq	%r14, %rsi
	movq	%rsi, -104(%rbp)
	movq	-112(%rbp), %rsi
	imulq	%r13, %rsi
	movq	%rsi, -112(%rbp)
	movq	-120(%rbp), %rsi
	imulq	%r12, %rsi
	movq	%rsi, -120(%rbp)
	movq	-128(%rbp), %rsi
	imulq	%rbx, %rsi
	movq	%rsi, -128(%rbp)
	imulq	-144(%rbp), %rax
	imulq	-152(%rbp), %rdx
	imulq	-160(%rbp), %rcx
	movq	-240(%rbp), %rsi
	imulq	-168(%rbp), %rsi
	imulq	-176(%rbp), %rdi
	movq	%rdi, -248(%rbp)
	imulq	-184(%rbp), %r8
	imulq	-192(%rbp), %r9
	imulq	-200(%rbp), %r10
	imulq	-208(%rbp), %r11
	movq	-88(%rbp), %rdi
	imulq	-216(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	movq	-96(%rbp), %rdi
	imulq	%r15, %rdi
	movq	%rdi, -96(%rbp)
	movq	-104(%rbp), %rdi
	imulq	%r14, %rdi
	movq	%rdi, -104(%rbp)
	movq	-112(%rbp), %rdi
	imulq	%r13, %rdi
	movq	%rdi, -112(%rbp)
	movq	-120(%rbp), %rdi
	imulq	%r12, %rdi
	movq	%rdi, -120(%rbp)
	movq	-128(%rbp), %rdi
	imulq	%rbx, %rdi
	movq	%rdi, -128(%rbp)
	imulq	-144(%rbp), %rax
	imulq	-152(%rbp), %rdx
	imulq	-160(%rbp), %rcx
	imulq	-168(%rbp), %rsi
	movq	-248(%rbp), %rdi
	imulq	-176(%rbp), %rdi
	imulq	-184(%rbp), %r8
	movq	%r8, -256(%rbp)
	imulq	-192(%rbp), %r9
	imulq	-200(%rbp), %r10
	imulq	-208(%rbp), %r11
	movq	-88(%rbp), %r8
	imulq	-216(%rbp), %r8
	movq	%r8, -88(%rbp)
	movq	-96(%rbp), %r8
	imulq	%r15, %r8
	movq	%r8, -96(%rbp)
	movq	-104(%rbp), %r8
	imulq	%r14, %r8
	movq	%r8, -104(%rbp)
	movq	-112(%rbp), %r8
	imulq	%r13, %r8
	movq	%r8, -112(%rbp)
	movq	-120(%rbp), %r8
	imulq	%r12, %r8
	movq	%r8, -120(%rbp)
	movq	-128(%rbp), %r8
	imulq	%rbx, %r8
	movq	%r8, -128(%rbp)
	imulq	-144(%rbp), %rax
	imulq	-152(%rbp), %rdx
	imulq	-160(%rbp), %rcx
	imulq	-168(%rbp), %rsi
	imulq	-176(%rbp), %rdi
	movq	-256(%rbp), %r8
	imulq	-184(%rbp), %r8
	imulq	-192(%rbp), %r9
	movq	%r9, -264(%rbp)
	imulq	-200(%rbp), %r10
	imulq	-208(%rbp), %r11
	movq	-88(%rbp), %r9
	imulq	-216(%rbp), %r9
	movq	%r9, -88(%rbp)
	movq	-96(%rbp), %r9
	imulq	%r15, %r9
	movq	%r9, -96(%rbp)
	movq	-104(%rbp), %r9
	imulq	%r14, %r9
	movq	%r9, -104(%rbp)
	movq	-112(%rbp), %r9
	imulq	%r13, %r9
	movq	%r9, -112(%rbp)
	movq	-120(%rbp), %r9
	imulq	%r12, %r9
	movq	%r9, -120(%rbp)
	movq	-128(%rbp), %r9
	imulq	%rbx, %r9
	movq	%r9, -128(%rbp)
	imulq	-144(%rbp), %rax
	imulq	-152(%rbp), %rdx
	movq	%rdx, -136(%rbp)
	imulq	-160(%rbp), %rcx
	imulq	-168(%rbp), %rsi
	imulq	-176(%rbp), %rdi
	imulq	-184(%rbp), %r8
	movq	-264(%rbp), %r9
	imulq	-192(%rbp), %r9
	imulq	-200(%rbp), %r10
	imulq	-208(%rbp), %r11
	movq	-88(%rbp), %rdx
	imulq	-216(%rbp), %rdx
	movq	%rdx, -88(%rbp)
	movq	-96(%rbp), %rdx
	imulq	%r15, %rdx
	movq	%rdx, -96(%rbp)
	movq	-104(%rbp), %rdx
	imulq	%r14, %rdx
	movq	%rdx, -104(%rbp)
	movq	-112(%rbp), %rdx
	imulq	%r13, %rdx
	movq	%rdx, -112(%rbp)
	movq	-120(%rbp), %rdx
	imulq	%r12, %rdx
	movq	%rdx, -120(%rbp)
	movq	-128(%rbp), %rdx
	imulq	%rbx, %rdx
	movq	%rdx, -128(%rbp)
	imulq	-144(%rbp), %rax
	movq	%rax, -224(%rbp)
	movq	-136(%rbp), %rdx
	imulq	-152(%rbp), %rdx
	movq	%rdx, -136(%rbp)
	imulq	-160(%rbp), %rcx
	movq	%rcx, -232(%rbp)
	imulq	-168(%rbp), %rsi
	movq	%rsi, -240(%rbp)
	imulq	-176(%rbp), %rdi
	movq	%rdi, -248(%rbp)
	imulq	-184(%rbp), %r8
	movq	%r8, -256(%rbp)
	imulq	-192(%rbp), %r9
	movq	%r9, -264(%rbp)
	imulq	-200(%rbp), %r10
	movq	%r10, -272(%rbp)
	imulq	-208(%rbp), %r11
	movq	%r11, -280(%rbp)
	movq	-88(%rbp), %rdx
	imulq	-216(%rbp), %rdx
	movq	%rdx, -88(%rbp)
	movq	-96(%rbp), %rdx
	imulq	%r15, %rdx
	movq	%rdx, -96(%rbp)
	movq	-104(%rbp), %rdx
	imulq	%r14, %rdx
	movq	%rdx, -104(%rbp)
	movq	-112(%rbp), %rdx
	imulq	%r13, %rdx
	movq	%rdx, -112(%rbp)
	movq	-120(%rbp), %rdx
	imulq	%r12, %rdx
	movq	%rdx, -120(%rbp)
	movq	-128(%rbp), %rdx
	imulq	%rbx, %rdx
	movq	%rdx, -128(%rbp)
.L397:
	movq	-288(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -288(%rbp)
	testq	%rax, %rax
	jne	.L398
	movl	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-264(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-272(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-280(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$376, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE135:
	.size	int64_mul_14, .-int64_mul_14
	.globl	int64_mul_15
	.type	int64_mul_15, @function
int64_mul_15:
.LFB136:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$392, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -304(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37421, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$5, %eax
	movslq	%eax, %rbx
	movq	%rbx, -152(%rbp)
	movq	%rsi, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rsi, %rax
	movq	%rax, -312(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -240(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$37420, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$4, %eax
	movslq	%eax, %rcx
	movq	%rcx, -160(%rbp)
	movq	%rbx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	subq	%rbx, %rax
	movq	%rax, -320(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$37419, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$3, %eax
	movslq	%eax, %rsi
	movq	%rsi, -168(%rbp)
	movq	%rbx, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	subq	%rbx, %rax
	movq	%rax, -328(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -248(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$37418, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$2, %eax
	movslq	%eax, %rbx
	movq	%rbx, -176(%rbp)
	movq	%rsi, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rsi, %rax
	movq	%rax, -336(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -256(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$37417, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$1, %eax
	movslq	%eax, %rdi
	movq	%rdi, -184(%rbp)
	movq	%rbx, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	subq	%rbx, %rax
	movq	%rax, -344(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -264(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$37416, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rcx
	movq	%rcx, -192(%rbp)
	movq	%rdi, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	subq	%rdi, %rax
	movq	%rax, -352(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -272(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$37415, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	subl	$1, %eax
	movslq	%eax, %rsi
	movq	%rsi, -200(%rbp)
	movq	%rdx, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	imulq	%rsi, %rax
	subq	%rdx, %rax
	movq	%rax, -360(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -280(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$37414, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	subl	$2, %eax
	movslq	%eax, %rbx
	movq	%rbx, -208(%rbp)
	movq	%rsi, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rsi, %rax
	movq	%rax, -368(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -288(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$37413, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$1, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	subl	$3, %eax
	movslq	%eax, %rdx
	movq	%rdx, -216(%rbp)
	movq	%rdi, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	imulq	%rdx, %rax
	subq	%rdi, %rax
	movq	%rax, -376(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -296(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$37412, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$2, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	subl	$4, %eax
	movslq	%eax, %rdi
	movq	%rdi, -224(%rbp)
	movq	%rbx, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	imulq	%rdi, %rax
	subq	%rbx, %rax
	movq	%rax, -384(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$37411, %eax
	movslq	%eax, %rdx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$3, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdx
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	subl	$5, %eax
	movslq	%eax, %rcx
	movq	%rcx, -232(%rbp)
	movq	%rdx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	imulq	%rcx, %rax
	subq	%rdx, %rax
	movq	%rax, -392(%rbp)
	addq	%rax, %rdx
	movq	%rdx, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$37410, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	subl	$4, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	subl	$6, %eax
	movslq	%eax, %r15
	movq	%rcx, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	imulq	%r15, %rax
	subq	%rcx, %rax
	movq	%rax, -400(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$37409, %eax
	movslq	%eax, %rsi
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	subl	$5, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rsi
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	subl	$7, %eax
	movslq	%eax, %r14
	movq	%rsi, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	imulq	%r14, %rax
	subq	%rsi, %rax
	movq	%rax, -408(%rbp)
	addq	%rax, %rsi
	movq	%rsi, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$37408, %eax
	movslq	%eax, %rdi
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	subl	$6, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rdi
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	subl	$8, %eax
	movslq	%eax, %r13
	movq	%rdi, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	imulq	%r13, %rax
	subq	%rdi, %rax
	movq	%rax, -416(%rbp)
	addq	%rax, %rdi
	movq	%rdi, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	addl	$37407, %eax
	movslq	%eax, %rbx
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	subl	$7, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rbx
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	subl	$9, %eax
	movslq	%eax, %r12
	movq	%rbx, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	imulq	%r12, %rax
	subq	%rbx, %rax
	movq	%rax, -424(%rbp)
	addq	%rax, %rbx
	movq	%rbx, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	addl	$37406, %eax
	movslq	%eax, %rcx
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	subl	$8, %eax
	cltq
	salq	$32, %rax
	addq	%rax, %rcx
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	subl	$10, %eax
	movslq	%eax, %rbx
	movq	%rcx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	imulq	%rbx, %rax
	subq	%rcx, %rax
	movq	%rax, -432(%rbp)
	addq	%rax, %rcx
	movq	%rcx, -136(%rbp)
	jmp	.L400
.L401:
	movq	-312(%rbp), %rdx
	subq	%rdx, -240(%rbp)
	movq	-240(%rbp), %rax
	movq	-320(%rbp), %rsi
	subq	%rsi, -144(%rbp)
	movq	-144(%rbp), %rdx
	movq	-328(%rbp), %rdi
	subq	%rdi, -248(%rbp)
	movq	-336(%rbp), %rdi
	subq	%rdi, -256(%rbp)
	movq	-256(%rbp), %rsi
	movq	-344(%rbp), %r10
	subq	%r10, -264(%rbp)
	movq	-264(%rbp), %rdi
	movq	-352(%rbp), %r11
	subq	%r11, -272(%rbp)
	movq	-272(%rbp), %r8
	movq	-360(%rbp), %r10
	subq	%r10, -280(%rbp)
	movq	-280(%rbp), %r9
	movq	-368(%rbp), %r11
	subq	%r11, -288(%rbp)
	movq	-288(%rbp), %r10
	movq	-376(%rbp), %rcx
	subq	%rcx, -296(%rbp)
	movq	-296(%rbp), %r11
	movq	-384(%rbp), %rcx
	subq	%rcx, -88(%rbp)
	movq	-392(%rbp), %rcx
	subq	%rcx, -96(%rbp)
	movq	-400(%rbp), %rcx
	subq	%rcx, -104(%rbp)
	movq	-408(%rbp), %rcx
	subq	%rcx, -112(%rbp)
	movq	-416(%rbp), %rcx
	subq	%rcx, -120(%rbp)
	movq	-424(%rbp), %rcx
	subq	%rcx, -128(%rbp)
	movq	-432(%rbp), %rcx
	subq	%rcx, -136(%rbp)
	imulq	-152(%rbp), %rax
	imulq	-160(%rbp), %rdx
	movq	%rdx, -144(%rbp)
	movq	-248(%rbp), %rcx
	imulq	-168(%rbp), %rcx
	imulq	-176(%rbp), %rsi
	imulq	-184(%rbp), %rdi
	imulq	-192(%rbp), %r8
	imulq	-200(%rbp), %r9
	imulq	-208(%rbp), %r10
	imulq	-216(%rbp), %r11
	movq	-88(%rbp), %rdx
	imulq	-224(%rbp), %rdx
	movq	%rdx, -88(%rbp)
	movq	-96(%rbp), %rdx
	imulq	-232(%rbp), %rdx
	movq	%rdx, -96(%rbp)
	movq	-104(%rbp), %rdx
	imulq	%r15, %rdx
	movq	%rdx, -104(%rbp)
	movq	-112(%rbp), %rdx
	imulq	%r14, %rdx
	movq	%rdx, -112(%rbp)
	movq	-120(%rbp), %rdx
	imulq	%r13, %rdx
	movq	%rdx, -120(%rbp)
	movq	-128(%rbp), %rdx
	imulq	%r12, %rdx
	movq	%rdx, -128(%rbp)
	movq	-136(%rbp), %rdx
	imulq	%rbx, %rdx
	movq	%rdx, -136(%rbp)
	imulq	-152(%rbp), %rax
	movq	%rax, -240(%rbp)
	movq	-144(%rbp), %rdx
	imulq	-160(%rbp), %rdx
	imulq	-168(%rbp), %rcx
	imulq	-176(%rbp), %rsi
	imulq	-184(%rbp), %rdi
	imulq	-192(%rbp), %r8
	imulq	-200(%rbp), %r9
	imulq	-208(%rbp), %r10
	imulq	-216(%rbp), %r11
	movq	-88(%rbp), %rax
	imulq	-224(%rbp), %rax
	movq	%rax, -88(%rbp)
	movq	-96(%rbp), %rax
	imulq	-232(%rbp), %rax
	movq	%rax, -96(%rbp)
	movq	-104(%rbp), %rax
	imulq	%r15, %rax
	movq	%rax, -104(%rbp)
	movq	-112(%rbp), %rax
	imulq	%r14, %rax
	movq	%rax, -112(%rbp)
	movq	-120(%rbp), %rax
	imulq	%r13, %rax
	movq	%rax, -120(%rbp)
	movq	-128(%rbp), %rax
	imulq	%r12, %rax
	movq	%rax, -128(%rbp)
	movq	-136(%rbp), %rax
	imulq	%rbx, %rax
	movq	%rax, -136(%rbp)
	movq	-240(%rbp), %rax
	imulq	-152(%rbp), %rax
	imulq	-160(%rbp), %rdx
	movq	%rdx, -144(%rbp)
	imulq	-168(%rbp), %rcx
	imulq	-176(%rbp), %rsi
	imulq	-184(%rbp), %rdi
	imulq	-192(%rbp), %r8
	imulq	-200(%rbp), %r9
	imulq	-208(%rbp), %r10
	imulq	-216(%rbp), %r11
	movq	-88(%rbp), %rdx
	imulq	-224(%rbp), %rdx
	movq	%rdx, -88(%rbp)
	movq	-96(%rbp), %rdx
	imulq	-232(%rbp), %rdx
	movq	%rdx, -96(%rbp)
	movq	-104(%rbp), %rdx
	imulq	%r15, %rdx
	movq	%rdx, -104(%rbp)
	movq	-112(%rbp), %rdx
	imulq	%r14, %rdx
	movq	%rdx, -112(%rbp)
	movq	-120(%rbp), %rdx
	imulq	%r13, %rdx
	movq	%rdx, -120(%rbp)
	movq	-128(%rbp), %rdx
	imulq	%r12, %rdx
	movq	%rdx, -128(%rbp)
	movq	-136(%rbp), %rdx
	imulq	%rbx, %rdx
	movq	%rdx, -136(%rbp)
	imulq	-152(%rbp), %rax
	movq	-144(%rbp), %rdx
	imulq	-160(%rbp), %rdx
	imulq	-168(%rbp), %rcx
	movq	%rcx, -248(%rbp)
	imulq	-176(%rbp), %rsi
	imulq	-184(%rbp), %rdi
	imulq	-192(%rbp), %r8
	imulq	-200(%rbp), %r9
	imulq	-208(%rbp), %r10
	imulq	-216(%rbp), %r11
	movq	-88(%rbp), %rcx
	imulq	-224(%rbp), %rcx
	movq	%rcx, -88(%rbp)
	movq	-96(%rbp), %rcx
	imulq	-232(%rbp), %rcx
	movq	%rcx, -96(%rbp)
	movq	-104(%rbp), %rcx
	imulq	%r15, %rcx
	movq	%rcx, -104(%rbp)
	movq	-112(%rbp), %rcx
	imulq	%r14, %rcx
	movq	%rcx, -112(%rbp)
	movq	-120(%rbp), %rcx
	imulq	%r13, %rcx
	movq	%rcx, -120(%rbp)
	movq	-128(%rbp), %rcx
	imulq	%r12, %rcx
	movq	%rcx, -128(%rbp)
	movq	-136(%rbp), %rcx
	imulq	%rbx, %rcx
	movq	%rcx, -136(%rbp)
	imulq	-152(%rbp), %rax
	imulq	-160(%rbp), %rdx
	movq	-248(%rbp), %rcx
	imulq	-168(%rbp), %rcx
	imulq	-176(%rbp), %rsi
	movq	%rsi, -256(%rbp)
	imulq	-184(%rbp), %rdi
	imulq	-192(%rbp), %r8
	imulq	-200(%rbp), %r9
	imulq	-208(%rbp), %r10
	imulq	-216(%rbp), %r11
	movq	-88(%rbp), %rsi
	imulq	-224(%rbp), %rsi
	movq	%rsi, -88(%rbp)
	movq	-96(%rbp), %rsi
	imulq	-232(%rbp), %rsi
	movq	%rsi, -96(%rbp)
	movq	-104(%rbp), %rsi
	imulq	%r15, %rsi
	movq	%rsi, -104(%rbp)
	movq	-112(%rbp), %rsi
	imulq	%r14, %rsi
	movq	%rsi, -112(%rbp)
	movq	-120(%rbp), %rsi
	imulq	%r13, %rsi
	movq	%rsi, -120(%rbp)
	movq	-128(%rbp), %rsi
	imulq	%r12, %rsi
	movq	%rsi, -128(%rbp)
	movq	-136(%rbp), %rsi
	imulq	%rbx, %rsi
	movq	%rsi, -136(%rbp)
	imulq	-152(%rbp), %rax
	imulq	-160(%rbp), %rdx
	imulq	-168(%rbp), %rcx
	movq	-256(%rbp), %rsi
	imulq	-176(%rbp), %rsi
	imulq	-184(%rbp), %rdi
	movq	%rdi, -264(%rbp)
	imulq	-192(%rbp), %r8
	imulq	-200(%rbp), %r9
	imulq	-208(%rbp), %r10
	imulq	-216(%rbp), %r11
	movq	-88(%rbp), %rdi
	imulq	-224(%rbp), %rdi
	movq	%rdi, -88(%rbp)
	movq	-96(%rbp), %rdi
	imulq	-232(%rbp), %rdi
	movq	%rdi, -96(%rbp)
	movq	-104(%rbp), %rdi
	imulq	%r15, %rdi
	movq	%rdi, -104(%rbp)
	movq	-112(%rbp), %rdi
	imulq	%r14, %rdi
	movq	%rdi, -112(%rbp)
	movq	-120(%rbp), %rdi
	imulq	%r13, %rdi
	movq	%rdi, -120(%rbp)
	movq	-128(%rbp), %rdi
	imulq	%r12, %rdi
	movq	%rdi, -128(%rbp)
	movq	-136(%rbp), %rdi
	imulq	%rbx, %rdi
	movq	%rdi, -136(%rbp)
	imulq	-152(%rbp), %rax
	imulq	-160(%rbp), %rdx
	imulq	-168(%rbp), %rcx
	imulq	-176(%rbp), %rsi
	movq	-264(%rbp), %rdi
	imulq	-184(%rbp), %rdi
	imulq	-192(%rbp), %r8
	movq	%r8, -272(%rbp)
	imulq	-200(%rbp), %r9
	imulq	-208(%rbp), %r10
	imulq	-216(%rbp), %r11
	movq	-88(%rbp), %r8
	imulq	-224(%rbp), %r8
	movq	%r8, -88(%rbp)
	movq	-96(%rbp), %r8
	imulq	-232(%rbp), %r8
	movq	%r8, -96(%rbp)
	movq	-104(%rbp), %r8
	imulq	%r15, %r8
	movq	%r8, -104(%rbp)
	movq	-112(%rbp), %r8
	imulq	%r14, %r8
	movq	%r8, -112(%rbp)
	movq	-120(%rbp), %r8
	imulq	%r13, %r8
	movq	%r8, -120(%rbp)
	movq	-128(%rbp), %r8
	imulq	%r12, %r8
	movq	%r8, -128(%rbp)
	movq	-136(%rbp), %r8
	imulq	%rbx, %r8
	movq	%r8, -136(%rbp)
	imulq	-152(%rbp), %rax
	imulq	-160(%rbp), %rdx
	imulq	-168(%rbp), %rcx
	imulq	-176(%rbp), %rsi
	imulq	-184(%rbp), %rdi
	movq	-272(%rbp), %r8
	imulq	-192(%rbp), %r8
	imulq	-200(%rbp), %r9
	movq	%r9, -280(%rbp)
	imulq	-208(%rbp), %r10
	imulq	-216(%rbp), %r11
	movq	-88(%rbp), %r9
	imulq	-224(%rbp), %r9
	movq	%r9, -88(%rbp)
	movq	-96(%rbp), %r9
	imulq	-232(%rbp), %r9
	movq	%r9, -96(%rbp)
	movq	-104(%rbp), %r9
	imulq	%r15, %r9
	movq	%r9, -104(%rbp)
	movq	-112(%rbp), %r9
	imulq	%r14, %r9
	movq	%r9, -112(%rbp)
	movq	-120(%rbp), %r9
	imulq	%r13, %r9
	movq	%r9, -120(%rbp)
	movq	-128(%rbp), %r9
	imulq	%r12, %r9
	movq	%r9, -128(%rbp)
	movq	-136(%rbp), %r9
	imulq	%rbx, %r9
	movq	%r9, -136(%rbp)
	imulq	-152(%rbp), %rax
	imulq	-160(%rbp), %rdx
	movq	%rdx, -144(%rbp)
	imulq	-168(%rbp), %rcx
	imulq	-176(%rbp), %rsi
	imulq	-184(%rbp), %rdi
	imulq	-192(%rbp), %r8
	movq	-280(%rbp), %r9
	imulq	-200(%rbp), %r9
	imulq	-208(%rbp), %r10
	imulq	-216(%rbp), %r11
	movq	-88(%rbp), %rdx
	imulq	-224(%rbp), %rdx
	movq	%rdx, -88(%rbp)
	movq	-96(%rbp), %rdx
	imulq	-232(%rbp), %rdx
	movq	%rdx, -96(%rbp)
	movq	-104(%rbp), %rdx
	imulq	%r15, %rdx
	movq	%rdx, -104(%rbp)
	movq	-112(%rbp), %rdx
	imulq	%r14, %rdx
	movq	%rdx, -112(%rbp)
	movq	-120(%rbp), %rdx
	imulq	%r13, %rdx
	movq	%rdx, -120(%rbp)
	movq	-128(%rbp), %rdx
	imulq	%r12, %rdx
	movq	%rdx, -128(%rbp)
	movq	-136(%rbp), %rdx
	imulq	%rbx, %rdx
	movq	%rdx, -136(%rbp)
	imulq	-152(%rbp), %rax
	movq	%rax, -240(%rbp)
	movq	-144(%rbp), %rdx
	imulq	-160(%rbp), %rdx
	movq	%rdx, -144(%rbp)
	imulq	-168(%rbp), %rcx
	movq	%rcx, -248(%rbp)
	imulq	-176(%rbp), %rsi
	movq	%rsi, -256(%rbp)
	imulq	-184(%rbp), %rdi
	movq	%rdi, -264(%rbp)
	imulq	-192(%rbp), %r8
	movq	%r8, -272(%rbp)
	imulq	-200(%rbp), %r9
	movq	%r9, -280(%rbp)
	imulq	-208(%rbp), %r10
	movq	%r10, -288(%rbp)
	imulq	-216(%rbp), %r11
	movq	%r11, -296(%rbp)
	movq	-88(%rbp), %rdx
	imulq	-224(%rbp), %rdx
	movq	%rdx, -88(%rbp)
	movq	-96(%rbp), %rdx
	imulq	-232(%rbp), %rdx
	movq	%rdx, -96(%rbp)
	movq	-104(%rbp), %rdx
	imulq	%r15, %rdx
	movq	%rdx, -104(%rbp)
	movq	-112(%rbp), %rdx
	imulq	%r14, %rdx
	movq	%rdx, -112(%rbp)
	movq	-120(%rbp), %rdx
	imulq	%r13, %rdx
	movq	%rdx, -120(%rbp)
	movq	-128(%rbp), %rdx
	imulq	%r12, %rdx
	movq	%rdx, -128(%rbp)
	movq	-136(%rbp), %rdx
	imulq	%rbx, %rdx
	movq	%rdx, -136(%rbp)
.L400:
	movq	-304(%rbp), %rax
	leaq	-1(%rax), %rdx
	movq	%rdx, -304(%rbp)
	testq	%rax, %rax
	jne	.L401
	movl	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-264(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-272(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-280(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-288(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-296(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$392, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE136:
	.size	int64_mul_15, .-int64_mul_15
	.globl	int64_mul_benchmarks
	.section	.data.rel.local
	.align 32
	.type	int64_mul_benchmarks, @object
	.size	int64_mul_benchmarks, 128
int64_mul_benchmarks:
	.quad	int64_mul_0
	.quad	int64_mul_1
	.quad	int64_mul_2
	.quad	int64_mul_3
	.quad	int64_mul_4
	.quad	int64_mul_5
	.quad	int64_mul_6
	.quad	int64_mul_7
	.quad	int64_mul_8
	.quad	int64_mul_9
	.quad	int64_mul_10
	.quad	int64_mul_11
	.quad	int64_mul_12
	.quad	int64_mul_13
	.quad	int64_mul_14
	.quad	int64_mul_15
	.text
	.globl	int64_div_0
	.type	int64_div_0, @function
int64_div_0:
.LFB137:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 13, -24
	.cfi_offset 12, -32
	.cfi_offset 3, -40
	movq	%rdi, -56(%rbp)
	movq	%rsi, -64(%rbp)
	movq	-56(%rbp), %r13
	movq	-64(%rbp), %rax
	movq	%rax, -40(%rbp)
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, %r12
	jmp	.L403
.L404:
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
.L403:
	movq	%r13, %rax
	leaq	-1(%rax), %r13
	testq	%rax, %rax
	jne	.L404
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE137:
	.size	int64_div_0, .-int64_div_0
	.globl	int64_div_1
	.type	int64_div_1, @function
int64_div_1:
.LFB138:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %r15
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$33, %rax
	addq	%rax, %r12
	leaq	17(%r12), %rax
	salq	$13, %rax
	movq	%rax, %r13
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, %r14
	jmp	.L406
.L407:
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
.L406:
	movq	%r15, %rax
	leaq	-1(%rax), %r15
	testq	%rax, %rax
	jne	.L407
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE138:
	.size	int64_div_1, .-int64_div_1
	.globl	int64_div_2
	.type	int64_div_2, @function
int64_div_2:
.LFB139:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %rsi
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movslq	%eax, %r13
	movq	%r13, %rax
	salq	$33, %rax
	addq	%rax, %r13
	leaq	17(%r13), %rax
	salq	$13, %rax
	movq	%rax, %r14
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$33, %rax
	addq	%rax, %r12
	leaq	17(%r12), %rax
	salq	$13, %rax
	movq	%rax, %r15
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -88(%rbp)
	jmp	.L409
.L410:
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
.L409:
	movq	%rsi, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %rsi
	testq	%rax, %rax
	jne	.L410
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE139:
	.size	int64_div_2, .-int64_div_2
	.globl	int64_div_3
	.type	int64_div_3, @function
int64_div_3:
.LFB140:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %rsi
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$33, %rax
	addq	%rax, %r14
	leaq	17(%r14), %rax
	salq	$13, %rax
	movq	%rax, %r15
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movslq	%eax, %r13
	movq	%r13, %rax
	salq	$33, %rax
	addq	%rax, %r13
	leaq	17(%r13), %rax
	salq	$13, %rax
	movq	%rax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$33, %rax
	addq	%rax, %r12
	leaq	17(%r12), %rax
	salq	$13, %rax
	movq	%rax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -104(%rbp)
	jmp	.L412
.L413:
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
.L412:
	movq	%rsi, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %rsi
	testq	%rax, %rax
	jne	.L413
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE140:
	.size	int64_div_3, .-int64_div_3
	.globl	int64_div_4
	.type	int64_div_4, @function
int64_div_4:
.LFB141:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %rsi
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movslq	%eax, %r15
	movq	%r15, %rax
	salq	$33, %rax
	addq	%rax, %r15
	leaq	17(%r15), %rax
	salq	$13, %rax
	movq	%rax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$33, %rax
	addq	%rax, %r14
	leaq	17(%r14), %rax
	salq	$13, %rax
	movq	%rax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movslq	%eax, %r13
	movq	%r13, %rax
	salq	$33, %rax
	addq	%rax, %r13
	leaq	17(%r13), %rax
	salq	$13, %rax
	movq	%rax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$33, %rax
	addq	%rax, %r12
	leaq	17(%r12), %rax
	salq	$13, %rax
	movq	%rax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -120(%rbp)
	jmp	.L415
.L416:
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
.L415:
	movq	%rsi, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %rsi
	testq	%rax, %rax
	jne	.L416
	movl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE141:
	.size	int64_div_4, .-int64_div_4
	.globl	int64_div_5
	.type	int64_div_5, @function
int64_div_5:
.LFB142:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r8
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, %rdi
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movslq	%eax, %r15
	movq	%r15, %rax
	salq	$33, %rax
	addq	%rax, %r15
	leaq	17(%r15), %rax
	salq	$13, %rax
	movq	%rax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$33, %rax
	addq	%rax, %r14
	leaq	17(%r14), %rax
	salq	$13, %rax
	movq	%rax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movslq	%eax, %r13
	movq	%r13, %rax
	salq	$33, %rax
	addq	%rax, %r13
	leaq	17(%r13), %rax
	salq	$13, %rax
	movq	%rax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$33, %rax
	addq	%rax, %r12
	leaq	17(%r12), %rax
	salq	$13, %rax
	movq	%rax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$32, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -128(%rbp)
	jmp	.L418
.L419:
	movq	-88(%rbp), %rax
	movq	%rdi, %rsi
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rsi
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rsi
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
.L418:
	movq	%r8, %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, %r8
	testq	%rax, %rax
	jne	.L419
	movl	%edi, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE142:
	.size	int64_div_5, .-int64_div_5
	.globl	int64_div_6
	.type	int64_div_6, @function
int64_div_6:
.LFB143:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r8
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -144(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movslq	%eax, %r15
	movq	%r15, %rax
	salq	$33, %rax
	addq	%rax, %r15
	leaq	17(%r15), %rax
	salq	$13, %rax
	movq	%rax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$33, %rax
	addq	%rax, %r14
	leaq	17(%r14), %rax
	salq	$13, %rax
	movq	%rax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movslq	%eax, %r13
	movq	%r13, %rax
	salq	$33, %rax
	addq	%rax, %r13
	leaq	17(%r13), %rax
	salq	$13, %rax
	movq	%rax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$32, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$33, %rax
	addq	%rax, %r12
	leaq	17(%r12), %rax
	salq	$13, %rax
	movq	%rax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -136(%rbp)
	jmp	.L421
.L422:
	movq	-88(%rbp), %rax
	movq	%rcx, %rdi
	cqto
	idivq	%rdi
	movq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	-144(%rbp)
	movq	%rax, %rdi
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rsi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rsi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rdi
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rsi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rdi
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, -144(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
.L421:
	movq	%r8, %rax
	leaq	-1(%rax), %rdi
	movq	%rdi, %r8
	testq	%rax, %rax
	jne	.L422
	movl	%ecx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE143:
	.size	int64_div_6, .-int64_div_6
	.globl	int64_div_7
	.type	int64_div_7, @function
int64_div_7:
.LFB144:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r9
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, %r8
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -152(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movslq	%eax, %rdi
	movq	%rdi, %rax
	salq	$33, %rax
	addq	%rax, %rdi
	movq	%rdi, -160(%rbp)
	leaq	17(%rdi), %rax
	salq	$13, %rax
	movq	%rax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movslq	%eax, %r15
	movq	%r15, %rax
	salq	$33, %rax
	addq	%rax, %r15
	leaq	17(%r15), %rax
	salq	$13, %rax
	movq	%rax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$33, %rax
	addq	%rax, %r14
	leaq	17(%r14), %rax
	salq	$13, %rax
	movq	%rax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$32, %eax
	movslq	%eax, %r13
	movq	%r13, %rax
	salq	$33, %rax
	addq	%rax, %r13
	leaq	17(%r13), %rax
	salq	$13, %rax
	movq	%rax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$33, %rax
	addq	%rax, %r12
	leaq	17(%r12), %rax
	salq	$13, %rax
	movq	%rax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$30, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -144(%rbp)
	jmp	.L424
.L425:
	movq	-88(%rbp), %rax
	movq	%r8, %rcx
	cqto
	idivq	%rcx
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	-152(%rbp)
	movq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	-160(%rbp)
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %r8
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, -152(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, -160(%rbp)
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
.L424:
	movq	%r9, %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, %r9
	testq	%rax, %rax
	jne	.L425
	movl	%r8d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE144:
	.size	int64_div_7, .-int64_div_7
	.globl	int64_div_8
	.type	int64_div_8, @function
int64_div_8:
.LFB145:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -184(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, %r9
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -160(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movslq	%eax, %rdi
	movq	%rdi, %rax
	salq	$33, %rax
	addq	%rax, %rdi
	movq	%rdi, -168(%rbp)
	leaq	17(%rdi), %rax
	salq	$13, %rax
	movq	%rax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movslq	%eax, %r15
	movq	%r15, %rax
	salq	$33, %rax
	addq	%rax, %r15
	leaq	17(%r15), %rax
	salq	$13, %rax
	movq	%rax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$33, %rax
	addq	%rax, %r14
	leaq	17(%r14), %rax
	salq	$13, %rax
	movq	%rax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$32, %eax
	movslq	%eax, %r13
	movq	%r13, %rax
	salq	$33, %rax
	addq	%rax, %r13
	leaq	17(%r13), %rax
	salq	$13, %rax
	movq	%rax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$33, %rax
	addq	%rax, %r12
	leaq	17(%r12), %rax
	salq	$13, %rax
	movq	%rax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$30, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, -176(%rbp)
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$29, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -152(%rbp)
	jmp	.L427
.L428:
	movq	-88(%rbp), %rax
	movq	%r9, %rsi
	cqto
	idivq	%rsi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	-160(%rbp)
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	-168(%rbp)
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	-176(%rbp)
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r9
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r10
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r11
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r9
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r10
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r11
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %r9
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, -160(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, -168(%rbp)
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-128(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-136(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, -176(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
.L427:
	movq	-184(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -184(%rbp)
	testq	%rax, %rax
	jne	.L428
	movl	%r9d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$152, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE145:
	.size	int64_div_8, .-int64_div_8
	.globl	int64_div_9
	.type	int64_div_9, @function
int64_div_9:
.LFB146:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -200(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, %r10
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -168(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movslq	%eax, %rdi
	movq	%rdi, %rax
	salq	$33, %rax
	addq	%rax, %rdi
	movq	%rdi, -176(%rbp)
	leaq	17(%rdi), %rax
	salq	$13, %rax
	movq	%rax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, -184(%rbp)
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movslq	%eax, %r15
	movq	%r15, %rax
	salq	$33, %rax
	addq	%rax, %r15
	leaq	17(%r15), %rax
	salq	$13, %rax
	movq	%rax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$32, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$33, %rax
	addq	%rax, %r14
	leaq	17(%r14), %rax
	salq	$13, %rax
	movq	%rax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movslq	%eax, %r13
	movq	%r13, %rax
	salq	$33, %rax
	addq	%rax, %r13
	leaq	17(%r13), %rax
	salq	$13, %rax
	movq	%rax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$30, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -192(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$29, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$33, %rax
	addq	%rax, %r12
	leaq	17(%r12), %rax
	salq	$13, %rax
	movq	%rax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$28, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -160(%rbp)
	jmp	.L430
.L431:
	movq	-88(%rbp), %rax
	movq	%r10, %rdi
	cqto
	idivq	%rdi
	movq	%rax, %r8
	movq	-96(%rbp), %rax
	cqto
	idivq	-168(%rbp)
	movq	%rax, %r9
	movq	-104(%rbp), %rax
	cqto
	idivq	-176(%rbp)
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	-184(%rbp)
	movq	%rax, %rdi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	-192(%rbp)
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-160(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r10
	movq	-96(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r11
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-160(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r8
	movq	-96(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r9
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-160(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r10
	movq	-96(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r11
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-160(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r8
	movq	-96(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r9
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-160(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r10
	movq	-96(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r11
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-160(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r8
	movq	-96(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r9
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-160(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r10
	movq	-96(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r11
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-160(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r8
	movq	-96(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r9
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-160(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r10
	movq	-96(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, -168(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, -176(%rbp)
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, -184(%rbp)
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, -192(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-160(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
.L430:
	movq	-200(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -200(%rbp)
	testq	%rax, %rax
	jne	.L431
	movl	%r10d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$168, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE146:
	.size	int64_div_9, .-int64_div_9
	.globl	int64_div_10
	.type	int64_div_10, @function
int64_div_10:
.LFB147:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$184, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -216(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, %r11
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -176(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movslq	%eax, %rdi
	movq	%rdi, %rax
	salq	$33, %rax
	addq	%rax, %rdi
	movq	%rdi, -184(%rbp)
	leaq	17(%rdi), %rax
	salq	$13, %rax
	movq	%rax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, -192(%rbp)
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movslq	%eax, %r15
	movq	%r15, %rax
	salq	$33, %rax
	addq	%rax, %r15
	leaq	17(%r15), %rax
	salq	$13, %rax
	movq	%rax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$32, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$33, %rax
	addq	%rax, %r14
	leaq	17(%r14), %rax
	salq	$13, %rax
	movq	%rax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movslq	%eax, %r13
	movq	%r13, %rax
	salq	$33, %rax
	addq	%rax, %r13
	leaq	17(%r13), %rax
	salq	$13, %rax
	movq	%rax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$30, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -200(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$29, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	movq	%rbx, -208(%rbp)
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$28, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$33, %rax
	addq	%rax, %r12
	leaq	17(%r12), %rax
	salq	$13, %rax
	movq	%rax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$27, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -168(%rbp)
	jmp	.L433
.L434:
	movq	-88(%rbp), %rax
	movq	%r11, %rdi
	cqto
	idivq	%rdi
	movq	%rax, %r8
	movq	-96(%rbp), %rax
	cqto
	idivq	-176(%rbp)
	movq	%rax, %r9
	movq	-104(%rbp), %rax
	cqto
	idivq	-184(%rbp)
	movq	%rax, %r10
	movq	-112(%rbp), %rax
	cqto
	idivq	-192(%rbp)
	movq	%rax, %rsi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	-200(%rbp)
	movq	%rax, %rdi
	movq	-152(%rbp), %rax
	cqto
	idivq	-208(%rbp)
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-168(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r11
	movq	-96(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r8
	movq	-104(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r9
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-168(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r10
	movq	-96(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r11
	movq	-104(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-168(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r9
	movq	-96(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r10
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r11
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-168(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r8
	movq	-96(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r9
	movq	-104(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r10
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-168(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r11
	movq	-96(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r8
	movq	-104(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r9
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-168(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r10
	movq	-96(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r11
	movq	-104(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-168(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r9
	movq	-96(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r10
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r11
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-168(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r8
	movq	-96(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r9
	movq	-104(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r10
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-168(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r11
	movq	-96(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, -176(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, -184(%rbp)
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, -192(%rbp)
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-136(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, -200(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, -208(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-168(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
.L433:
	movq	-216(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -216(%rbp)
	testq	%rax, %rax
	jne	.L434
	movl	%r11d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$184, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE147:
	.size	int64_div_10, .-int64_div_10
	.globl	int64_div_11
	.type	int64_div_11, @function
int64_div_11:
.LFB148:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$200, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -240(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, -184(%rbp)
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -192(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movslq	%eax, %rdi
	movq	%rdi, %rax
	salq	$33, %rax
	addq	%rax, %rdi
	movq	%rdi, -200(%rbp)
	leaq	17(%rdi), %rax
	salq	$13, %rax
	movq	%rax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, -208(%rbp)
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -216(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$32, %eax
	movslq	%eax, %r15
	movq	%r15, %rax
	salq	$33, %rax
	addq	%rax, %r15
	leaq	17(%r15), %rax
	salq	$13, %rax
	movq	%rax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$33, %rax
	addq	%rax, %r14
	leaq	17(%r14), %rax
	salq	$13, %rax
	movq	%rax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$30, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	movq	%rbx, -224(%rbp)
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$29, %eax
	movslq	%eax, %rdi
	movq	%rdi, %rax
	salq	$33, %rax
	addq	%rax, %rdi
	movq	%rdi, -232(%rbp)
	leaq	17(%rdi), %rax
	salq	$13, %rax
	movq	%rax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$28, %eax
	movslq	%eax, %r13
	movq	%r13, %rax
	salq	$33, %rax
	addq	%rax, %r13
	leaq	17(%r13), %rax
	salq	$13, %rax
	movq	%rax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$27, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$33, %rax
	addq	%rax, %r12
	leaq	17(%r12), %rax
	salq	$13, %rax
	movq	%rax, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$26, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -176(%rbp)
	jmp	.L436
.L437:
	movq	-88(%rbp), %rax
	cqto
	idivq	-184(%rbp)
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	-192(%rbp)
	movq	%rax, %r8
	movq	-104(%rbp), %rax
	cqto
	idivq	-200(%rbp)
	movq	%rax, %r9
	movq	-112(%rbp), %rax
	cqto
	idivq	-208(%rbp)
	movq	%rax, %r10
	movq	-120(%rbp), %rax
	cqto
	idivq	-216(%rbp)
	movq	%rax, %r11
	movq	-128(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-136(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-144(%rbp), %rax
	cqto
	idivq	-224(%rbp)
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	-232(%rbp)
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-168(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-176(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-104(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-112(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-120(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-128(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-136(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-168(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-176(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-104(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-112(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-120(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-128(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-136(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-168(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-176(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-104(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-112(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-120(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-128(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-136(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-168(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-176(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-104(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-112(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-120(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-128(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-136(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-168(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-176(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-104(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-112(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-120(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-128(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-136(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-168(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-176(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-104(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-112(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-120(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-128(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-136(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-168(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-176(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-104(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-112(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-120(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-128(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-136(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-168(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-176(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-96(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-104(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-112(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-120(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-128(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-136(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-168(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-176(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, -184(%rbp)
	movq	-96(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, -192(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, -200(%rbp)
	movq	-112(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, -208(%rbp)
	movq	-120(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, -216(%rbp)
	movq	-128(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-136(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, -224(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, -232(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-168(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-176(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
.L436:
	movq	-240(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -240(%rbp)
	testq	%rax, %rax
	jne	.L437
	movl	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$200, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE148:
	.size	int64_div_11, .-int64_div_11
	.globl	int64_div_12
	.type	int64_div_12, @function
int64_div_12:
.LFB149:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$216, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -256(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, -200(%rbp)
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -208(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movslq	%eax, %rdi
	movq	%rdi, %rax
	salq	$33, %rax
	addq	%rax, %rdi
	movq	%rdi, -216(%rbp)
	leaq	17(%rdi), %rax
	salq	$13, %rax
	movq	%rax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, -224(%rbp)
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -232(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$32, %eax
	movslq	%eax, %r15
	movq	%r15, %rax
	salq	$33, %rax
	addq	%rax, %r15
	leaq	17(%r15), %rax
	salq	$13, %rax
	movq	%rax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$33, %rax
	addq	%rax, %r14
	leaq	17(%r14), %rax
	salq	$13, %rax
	movq	%rax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$30, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$29, %eax
	movslq	%eax, %rdi
	movq	%rdi, %rax
	salq	$33, %rax
	addq	%rax, %rdi
	movq	%rdi, -240(%rbp)
	leaq	17(%rdi), %rax
	salq	$13, %rax
	movq	%rax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$28, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, -248(%rbp)
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$27, %eax
	movslq	%eax, %r13
	movq	%r13, %rax
	salq	$33, %rax
	addq	%rax, %r13
	leaq	17(%r13), %rax
	salq	$13, %rax
	movq	%rax, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$26, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$33, %rax
	addq	%rax, %r12
	leaq	17(%r12), %rax
	salq	$13, %rax
	movq	%rax, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$25, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -192(%rbp)
	jmp	.L439
.L440:
	movq	-96(%rbp), %rax
	cqto
	idivq	-200(%rbp)
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	-208(%rbp)
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	-216(%rbp)
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	-224(%rbp)
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	-232(%rbp)
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-144(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	-240(%rbp)
	movq	%rax, %rdi
	movq	-168(%rbp), %rax
	cqto
	idivq	-248(%rbp)
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-184(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-192(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-144(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-184(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-192(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-144(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-184(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-192(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-144(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-184(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-192(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-144(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-184(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-192(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-144(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-184(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-192(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-144(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-184(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-192(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-144(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-184(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-192(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-144(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-184(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-192(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, -200(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, -208(%rbp)
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, -216(%rbp)
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, -224(%rbp)
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, -232(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-144(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, -240(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, -248(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-184(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-192(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
.L439:
	movq	-256(%rbp), %rax
	leaq	-1(%rax), %rdi
	movq	%rdi, -256(%rbp)
	testq	%rax, %rax
	jne	.L440
	movl	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$216, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE149:
	.size	int64_div_12, .-int64_div_12
	.globl	int64_div_13
	.type	int64_div_13, @function
int64_div_13:
.LFB150:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$232, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -264(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, %r8
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -216(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, -224(%rbp)
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movslq	%eax, %rdi
	movq	%rdi, %rax
	salq	$33, %rax
	addq	%rax, %rdi
	movq	%rdi, -232(%rbp)
	leaq	17(%rdi), %rax
	salq	$13, %rax
	movq	%rax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -88(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$32, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movslq	%eax, %r15
	movq	%r15, %rax
	salq	$33, %rax
	addq	%rax, %r15
	leaq	17(%r15), %rax
	salq	$13, %rax
	movq	%rax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$30, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, -240(%rbp)
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$29, %eax
	movslq	%eax, %rdi
	movq	%rdi, %rax
	salq	$33, %rax
	addq	%rax, %rdi
	movq	%rdi, -248(%rbp)
	leaq	17(%rdi), %rax
	salq	$13, %rax
	movq	%rax, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$28, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -256(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$27, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$33, %rax
	addq	%rax, %r14
	leaq	17(%r14), %rax
	salq	$13, %rax
	movq	%rax, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$26, %eax
	movslq	%eax, %r13
	movq	%r13, %rax
	salq	$33, %rax
	addq	%rax, %r13
	leaq	17(%r13), %rax
	salq	$13, %rax
	movq	%rax, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$25, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$33, %rax
	addq	%rax, %r12
	leaq	17(%r12), %rax
	salq	$13, %rax
	movq	%rax, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$24, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -208(%rbp)
	jmp	.L442
.L443:
	movq	-104(%rbp), %rax
	movq	%r8, %rcx
	cqto
	idivq	%rcx
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	-216(%rbp)
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	-224(%rbp)
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	-232(%rbp)
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-160(%rbp), %rax
	cqto
	idivq	-240(%rbp)
	movq	%rax, %rcx
	movq	-168(%rbp), %rax
	cqto
	idivq	-248(%rbp)
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	-256(%rbp)
	movq	%rax, %rdi
	movq	-184(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-192(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-200(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-208(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-184(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-192(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-200(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-208(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-184(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-192(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-200(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-208(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-184(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-192(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-200(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-208(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-184(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-192(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-200(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-208(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-184(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-192(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-200(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-208(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-184(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-192(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-200(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-208(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-184(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-192(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-200(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-208(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-136(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-184(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-192(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-200(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-208(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-104(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-112(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, -216(%rbp)
	movq	-120(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, -224(%rbp)
	movq	-128(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, -232(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, -240(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, -248(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, -256(%rbp)
	movq	-184(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-192(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-200(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-208(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
.L442:
	movq	-264(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -264(%rbp)
	testq	%rax, %rax
	jne	.L443
	movl	%r8d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$232, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE150:
	.size	int64_div_13, .-int64_div_13
	.globl	int64_div_14
	.type	int64_div_14, @function
int64_div_14:
.LFB151:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$248, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -288(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movslq	%eax, %r15
	movq	%r15, %rax
	salq	$33, %rax
	addq	%rax, %r15
	leaq	17(%r15), %rax
	salq	$13, %rax
	movq	%rax, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, -232(%rbp)
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -240(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, -248(%rbp)
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -256(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$32, %eax
	movslq	%eax, %rdi
	movq	%rdi, %rax
	salq	$33, %rax
	addq	%rax, %rdi
	movq	%rdi, -88(%rbp)
	leaq	17(%rdi), %rax
	salq	$13, %rax
	movq	%rax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	movq	%rbx, -96(%rbp)
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$30, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, -104(%rbp)
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$29, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -264(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$28, %eax
	movslq	%eax, %rdi
	movq	%rdi, %rax
	salq	$33, %rax
	addq	%rax, %rdi
	movq	%rdi, -272(%rbp)
	leaq	17(%rdi), %rax
	salq	$13, %rax
	movq	%rax, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$27, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	movq	%rbx, -280(%rbp)
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$26, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$33, %rax
	addq	%rax, %r14
	leaq	17(%r14), %rax
	salq	$13, %rax
	movq	%rax, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$25, %eax
	movslq	%eax, %r13
	movq	%r13, %rax
	salq	$33, %rax
	addq	%rax, %r13
	leaq	17(%r13), %rax
	salq	$13, %rax
	movq	%rax, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$24, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$33, %rax
	addq	%rax, %r12
	leaq	17(%r12), %rax
	salq	$13, %rax
	movq	%rax, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	addl	$23, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -224(%rbp)
	jmp	.L445
.L446:
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	-232(%rbp)
	movq	%rax, %r8
	movq	-128(%rbp), %rax
	cqto
	idivq	-240(%rbp)
	movq	%rax, %r9
	movq	-136(%rbp), %rax
	cqto
	idivq	-248(%rbp)
	movq	%rax, %r10
	movq	-144(%rbp), %rax
	cqto
	idivq	-256(%rbp)
	movq	%rax, %r11
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	-264(%rbp)
	movq	%rax, %rsi
	movq	-184(%rbp), %rax
	cqto
	idivq	-272(%rbp)
	movq	%rax, %rdi
	movq	-192(%rbp), %rax
	cqto
	idivq	-280(%rbp)
	movq	%rax, %rcx
	movq	-200(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-208(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-216(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-224(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-128(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-136(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-144(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-184(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-192(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-200(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-208(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-216(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-224(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-128(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-136(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-144(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-184(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-192(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-200(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-208(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-216(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-224(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-128(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-136(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-144(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-184(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-192(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-200(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-208(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-216(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-224(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-128(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-136(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-144(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-184(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-192(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-200(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-208(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-216(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-224(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-128(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-136(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-144(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-184(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-192(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-200(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-208(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-216(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-224(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-128(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-136(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-144(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-184(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-192(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-200(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-208(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-216(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-224(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-128(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-136(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-144(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-184(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-192(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-200(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-208(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-216(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-224(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-128(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-136(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-144(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-184(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-192(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-200(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-208(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-216(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-224(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-112(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-120(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, -232(%rbp)
	movq	-128(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, -240(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, -248(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, -256(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, -264(%rbp)
	movq	-184(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, -272(%rbp)
	movq	-192(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, -280(%rbp)
	movq	-200(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-208(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-216(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-224(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
.L445:
	movq	-288(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -288(%rbp)
	testq	%rax, %rax
	jne	.L446
	movl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-264(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-272(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-280(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$248, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE151:
	.size	int64_div_14, .-int64_div_14
	.globl	int64_div_15
	.type	int64_div_15, @function
int64_div_15:
.LFB152:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$264, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -304(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	addl	$37, %eax
	movslq	%eax, %r15
	movq	%r15, %rax
	salq	$33, %rax
	addq	%rax, %r15
	leaq	17(%r15), %rax
	salq	$13, %rax
	movq	%rax, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	addl	$36, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, -112(%rbp)
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	addl	$35, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -256(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	addl	$34, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, -264(%rbp)
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	addl	$33, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -272(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	addl	$32, %eax
	movslq	%eax, %rdi
	movq	%rdi, %rax
	salq	$33, %rax
	addq	%rax, %rdi
	movq	%rdi, -280(%rbp)
	leaq	17(%rdi), %rax
	salq	$13, %rax
	movq	%rax, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	addl	$31, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	movq	%rbx, -88(%rbp)
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	addl	$30, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, -96(%rbp)
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	addl	$29, %eax
	movslq	%eax, %rsi
	movq	%rsi, %rax
	salq	$33, %rax
	addq	%rax, %rsi
	movq	%rsi, -104(%rbp)
	leaq	17(%rsi), %rax
	salq	$13, %rax
	movq	%rax, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	addl	$28, %eax
	movslq	%eax, %rdi
	movq	%rdi, %rax
	salq	$33, %rax
	addq	%rax, %rdi
	movq	%rdi, -248(%rbp)
	leaq	17(%rdi), %rax
	salq	$13, %rax
	movq	%rax, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	addl	$27, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	movq	%rbx, -288(%rbp)
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	addl	$26, %eax
	movslq	%eax, %rcx
	movq	%rcx, %rax
	salq	$33, %rax
	addq	%rax, %rcx
	movq	%rcx, -296(%rbp)
	leaq	17(%rcx), %rax
	salq	$13, %rax
	movq	%rax, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	addl	$25, %eax
	movslq	%eax, %r14
	movq	%r14, %rax
	salq	$33, %rax
	addq	%rax, %r14
	leaq	17(%r14), %rax
	salq	$13, %rax
	movq	%rax, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	addl	$24, %eax
	movslq	%eax, %r13
	movq	%r13, %rax
	salq	$33, %rax
	addq	%rax, %r13
	leaq	17(%r13), %rax
	salq	$13, %rax
	movq	%rax, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	addl	$23, %eax
	movslq	%eax, %r12
	movq	%r12, %rax
	salq	$33, %rax
	addq	%rax, %r12
	leaq	17(%r12), %rax
	salq	$13, %rax
	movq	%rax, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	addl	$22, %eax
	movslq	%eax, %rbx
	movq	%rbx, %rax
	salq	$33, %rax
	addq	%rax, %rbx
	leaq	17(%rbx), %rax
	salq	$13, %rax
	movq	%rax, -240(%rbp)
	jmp	.L448
.L449:
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	-112(%rbp)
	movq	%rax, %rcx
	movq	-136(%rbp), %rax
	cqto
	idivq	-256(%rbp)
	movq	%rax, %r8
	movq	-144(%rbp), %rax
	cqto
	idivq	-264(%rbp)
	movq	%rax, %r9
	movq	-152(%rbp), %rax
	cqto
	idivq	-272(%rbp)
	movq	%rax, %r10
	movq	-160(%rbp), %rax
	cqto
	idivq	-280(%rbp)
	movq	%rax, %r11
	movq	-168(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-184(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-192(%rbp), %rax
	cqto
	idivq	-248(%rbp)
	movq	%rax, -248(%rbp)
	movq	-200(%rbp), %rax
	cqto
	idivq	-288(%rbp)
	movq	%rax, %rdi
	movq	-208(%rbp), %rax
	cqto
	idivq	-296(%rbp)
	movq	%rax, %rsi
	movq	-216(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-224(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-232(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-240(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-152(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-160(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-168(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-184(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-192(%rbp), %rax
	cqto
	idivq	-248(%rbp)
	movq	%rax, -248(%rbp)
	movq	-200(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-208(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-216(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-224(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-232(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-240(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, -112(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-152(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-160(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-168(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-184(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-192(%rbp), %rax
	cqto
	idivq	-248(%rbp)
	movq	%rax, %rcx
	movq	-200(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-208(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-216(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-224(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-232(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-240(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	-112(%rbp)
	movq	%rax, -112(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-152(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-160(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-168(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-184(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-192(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-200(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-208(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-216(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-224(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-232(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-240(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	-112(%rbp)
	movq	%rax, -112(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-152(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-160(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-168(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-184(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-192(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-200(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-208(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-216(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-224(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-232(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-240(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	-112(%rbp)
	movq	%rax, -112(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-152(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-160(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-168(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-184(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-192(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-200(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-208(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-216(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-224(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-232(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-240(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	-112(%rbp)
	movq	%rax, -112(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-152(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-160(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-168(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-184(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-192(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-200(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-208(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-216(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-224(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-232(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-240(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	-112(%rbp)
	movq	%rax, -112(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-152(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-160(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-168(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-184(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-192(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-200(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-208(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-216(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-224(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-232(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-240(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	-112(%rbp)
	movq	%rax, -112(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, %r8
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, %r9
	movq	-152(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, %r10
	movq	-160(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, %r11
	movq	-168(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-184(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-192(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, %rcx
	movq	-200(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, %rdi
	movq	-208(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, %rsi
	movq	-216(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-224(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-232(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-240(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
	movq	-120(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rax, %r15
	movq	-128(%rbp), %rax
	cqto
	idivq	-112(%rbp)
	movq	%rax, -112(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rax, -256(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rax, -264(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rax, -272(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rax, -280(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rax, -88(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rax, -96(%rbp)
	movq	-184(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rax, -104(%rbp)
	movq	-192(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rax, -248(%rbp)
	movq	-200(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rax, -288(%rbp)
	movq	-208(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rax, -296(%rbp)
	movq	-216(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rax, %r14
	movq	-224(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rax, %r13
	movq	-232(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rax, %r12
	movq	-240(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rax, %rbx
.L448:
	movq	-304(%rbp), %rax
	leaq	-1(%rax), %rdi
	movq	%rdi, -304(%rbp)
	testq	%rax, %rax
	jne	.L449
	movl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-264(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-272(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-280(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-288(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-296(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$264, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE152:
	.size	int64_div_15, .-int64_div_15
	.globl	int64_div_benchmarks
	.section	.data.rel.local
	.align 32
	.type	int64_div_benchmarks, @object
	.size	int64_div_benchmarks, 128
int64_div_benchmarks:
	.quad	int64_div_0
	.quad	int64_div_1
	.quad	int64_div_2
	.quad	int64_div_3
	.quad	int64_div_4
	.quad	int64_div_5
	.quad	int64_div_6
	.quad	int64_div_7
	.quad	int64_div_8
	.quad	int64_div_9
	.quad	int64_div_10
	.quad	int64_div_11
	.quad	int64_div_12
	.quad	int64_div_13
	.quad	int64_div_14
	.quad	int64_div_15
	.text
	.globl	int64_mod_0
	.type	int64_mod_0, @function
int64_mod_0:
.LFB153:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 13, -24
	.cfi_offset 12, -32
	.cfi_offset 3, -40
	movq	%rdi, -56(%rbp)
	movq	%rsi, -64(%rbp)
	movq	-56(%rbp), %r13
	movq	-64(%rbp), %rax
	movq	%rax, -40(%rbp)
	movq	-40(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %rbx
	movl	$0, %r12d
	jmp	.L451
.L452:
	addq	$1, %r12
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r12, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
.L451:
	movq	%r13, %rax
	leaq	-1(%rax), %r13
	testq	%rax, %rax
	jne	.L452
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE153:
	.size	int64_mod_0, .-int64_mod_0
	.globl	int64_mod_1
	.type	int64_mod_1, @function
int64_mod_1:
.LFB154:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %r15
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %r12
	movl	$0, %r13d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %rbx
	movl	$0, %r14d
	jmp	.L454
.L455:
	addq	$1, %r13
	addq	$1, %r14
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r13, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%r14, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
.L454:
	movq	%r15, %rax
	leaq	-1(%rax), %r15
	testq	%rax, %rax
	jne	.L455
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE154:
	.size	int64_mod_1, .-int64_mod_1
	.globl	int64_mod_2
	.type	int64_mod_2, @function
int64_mod_2:
.LFB155:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %rsi
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %r13
	movl	$0, %r14d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %r12
	movl	$0, %r15d
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %rbx
	movq	$0, -88(%rbp)
	jmp	.L457
.L458:
	addq	$1, %r14
	addq	$1, %r15
	addq	$1, -88(%rbp)
	movq	-88(%rbp), %rcx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%rcx, -88(%rbp)
	movq	%rcx, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r14, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%r15, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-88(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
.L457:
	movq	%rsi, %rax
	leaq	-1(%rax), %rsi
	testq	%rax, %rax
	jne	.L458
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE155:
	.size	int64_mod_2, .-int64_mod_2
	.globl	int64_mod_3
	.type	int64_mod_3, @function
int64_mod_3:
.LFB156:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r8
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %r14
	movl	$0, %r15d
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %r13
	movq	$0, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %r12
	movq	$0, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movslq	%eax, %rbx
	movq	$0, -104(%rbp)
	jmp	.L460
.L461:
	addq	$1, %r15
	addq	$1, -88(%rbp)
	movq	-88(%rbp), %rcx
	addq	$1, -96(%rbp)
	movq	-96(%rbp), %rsi
	addq	$1, -104(%rbp)
	movq	-104(%rbp), %rdi
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	%rcx, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%rsi, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%rdi, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	%rcx, -88(%rbp)
	movq	%rcx, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%rsi, -96(%rbp)
	movq	%rsi, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%rdi, -104(%rbp)
	movq	%rdi, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%r15, %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-88(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-96(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-104(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
.L460:
	movq	%r8, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %r8
	testq	%rax, %rax
	jne	.L461
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE156:
	.size	int64_mod_3, .-int64_mod_3
	.globl	int64_mod_4
	.type	int64_mod_4, @function
int64_mod_4:
.LFB157:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, %r10
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	movslq	%eax, %r15
	movq	$0, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %r14
	movq	$0, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %r13
	movq	$0, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movslq	%eax, %r12
	movq	$0, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movslq	%eax, %rbx
	movq	$0, -120(%rbp)
	jmp	.L463
.L464:
	addq	$1, -88(%rbp)
	movq	-88(%rbp), %rcx
	addq	$1, -96(%rbp)
	movq	-96(%rbp), %rsi
	addq	$1, -104(%rbp)
	movq	-104(%rbp), %rdi
	addq	$1, -112(%rbp)
	movq	-112(%rbp), %r8
	addq	$1, -120(%rbp)
	movq	-120(%rbp), %r9
	movq	%rcx, %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	%rsi, %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	%rdi, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%r8, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%r9, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	%rcx, -88(%rbp)
	movq	%rcx, %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	%rsi, -96(%rbp)
	movq	%rsi, %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	%rdi, -104(%rbp)
	movq	%rdi, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%r8, -112(%rbp)
	movq	%r8, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%r9, -120(%rbp)
	movq	%r9, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-96(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-104(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-112(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-120(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
.L463:
	movq	%r10, %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, %r10
	testq	%rax, %rax
	jne	.L464
	movl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE157:
	.size	int64_mod_4, .-int64_mod_4
	.globl	int64_mod_5
	.type	int64_mod_5, @function
int64_mod_5:
.LFB158:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -136(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	cltq
	movq	%rax, %rcx
	movq	$0, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	movslq	%eax, %r15
	movq	$0, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %r14
	movq	$0, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movslq	%eax, %r13
	movq	$0, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movslq	%eax, %r12
	movq	$0, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %rbx
	movq	$0, -128(%rbp)
	jmp	.L466
.L467:
	addq	$1, -88(%rbp)
	movq	-88(%rbp), %rsi
	addq	$1, -96(%rbp)
	movq	-96(%rbp), %rdi
	addq	$1, -104(%rbp)
	movq	-104(%rbp), %r8
	addq	$1, -112(%rbp)
	movq	-112(%rbp), %r9
	addq	$1, -120(%rbp)
	movq	-120(%rbp), %r10
	addq	$1, -128(%rbp)
	movq	-128(%rbp), %r11
	movq	%rsi, -88(%rbp)
	movq	%rsi, %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rdi, %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	%r8, %rsi
	movq	%r8, %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	%r9, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%r10, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%r11, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rdi, -96(%rbp)
	movq	%rdi, %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	%rsi, -104(%rbp)
	movq	%rsi, %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	%r9, -112(%rbp)
	movq	%r9, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%r10, -120(%rbp)
	movq	%r10, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	%r11, -128(%rbp)
	movq	%r11, %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-104(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-112(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-120(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-128(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
.L466:
	movq	-136(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -136(%rbp)
	testq	%rax, %rax
	jne	.L467
	movl	%ecx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE158:
	.size	int64_mod_5, .-int64_mod_5
	.globl	int64_mod_6
	.type	int64_mod_6, @function
int64_mod_6:
.LFB159:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -152(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	cltq
	movq	%rax, %rcx
	movq	$0, -88(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	cltq
	movq	%rax, -144(%rbp)
	movq	$0, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	movslq	%eax, %r15
	movq	$0, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	movslq	%eax, %r14
	movq	$0, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	movslq	%eax, %r13
	movq	$0, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	movslq	%eax, %r12
	movq	$0, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	movslq	%eax, %rbx
	movq	$0, -136(%rbp)
	jmp	.L469
.L470:
	addq	$1, -88(%rbp)
	movq	-88(%rbp), %rdi
	addq	$1, -96(%rbp)
	movq	-96(%rbp), %rsi
	addq	$1, -104(%rbp)
	movq	-104(%rbp), %r8
	addq	$1, -112(%rbp)
	movq	-112(%rbp), %r9
	addq	$1, -120(%rbp)
	movq	-120(%rbp), %r10
	addq	$1, -128(%rbp)
	movq	-128(%rbp), %r11
	addq	$1, -136(%rbp)
	movq	%rdi, -88(%rbp)
	movq	%rdi, %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rsi, -96(%rbp)
	movq	%rsi, %rax
	movq	-144(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%r8, %rdi
	movq	%r8, %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	%r9, %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	%r10, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%r11, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rdi, -104(%rbp)
	movq	%rdi, %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	%r9, -112(%rbp)
	movq	%r9, %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	%r10, -120(%rbp)
	movq	%r10, %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	%r11, -128(%rbp)
	movq	%r11, %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
	movq	-88(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-96(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -144(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%r15
	movq	%rdx, %rax
	xorq	%rax, %r15
	movq	-112(%rbp), %rax
	cqto
	idivq	%r14
	movq	%rdx, %rax
	xorq	%rax, %r14
	movq	-120(%rbp), %rax
	cqto
	idivq	%r13
	movq	%rdx, %rax
	xorq	%rax, %r13
	movq	-128(%rbp), %rax
	cqto
	idivq	%r12
	movq	%rdx, %rax
	xorq	%rax, %r12
	movq	-136(%rbp), %rax
	cqto
	idivq	%rbx
	movq	%rdx, %rax
	xorq	%rax, %rbx
.L469:
	movq	-152(%rbp), %rax
	leaq	-1(%rax), %rdi
	movq	%rdi, -152(%rbp)
	testq	%rax, %rax
	jne	.L470
	movl	%ecx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r15d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r14d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r13d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%r12d, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	%ebx, %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE159:
	.size	int64_mod_6, .-int64_mod_6
	.globl	int64_mod_7
	.type	int64_mod_7, @function
int64_mod_7:
.LFB160:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -176(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	cltq
	movq	%rax, -120(%rbp)
	movq	$0, -96(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	cltq
	movq	%rax, -128(%rbp)
	movq	$0, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	cltq
	movq	%rax, -136(%rbp)
	movq	$0, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	cltq
	movq	%rax, -144(%rbp)
	movl	$0, %r15d
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	cltq
	movq	%rax, -152(%rbp)
	movl	$0, %r14d
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	cltq
	movq	%rax, -160(%rbp)
	movl	$0, %r13d
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	cltq
	movq	%rax, -168(%rbp)
	movl	$0, %r12d
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	movq	%rax, -88(%rbp)
	movl	$0, %ebx
	jmp	.L472
.L473:
	addq	$1, -96(%rbp)
	movq	-96(%rbp), %rsi
	addq	$1, -104(%rbp)
	movq	-104(%rbp), %rdi
	addq	$1, -112(%rbp)
	movq	-112(%rbp), %r8
	addq	$1, %r15
	addq	$1, %r14
	addq	$1, %r13
	addq	$1, %r12
	addq	$1, %rbx
	movq	%rsi, -96(%rbp)
	movq	%rsi, %rax
	movq	-120(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rdi, -104(%rbp)
	movq	%rdi, %rax
	movq	-128(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%r8, -112(%rbp)
	movq	%r8, %rax
	movq	-136(%rbp), %rdi
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%r15, %rax
	movq	-144(%rbp), %r8
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r14, %rax
	movq	-152(%rbp), %r9
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r13, %rax
	movq	-160(%rbp), %r10
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r12, %rax
	movq	-168(%rbp), %r11
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%rbx, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%r15, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%rbx, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%r15, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%rbx, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%r15, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%rbx, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%r15, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%rbx, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%r15, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%rbx, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%r15, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%rbx, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%r15, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%rbx, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-104(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%r15, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%rbx, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-96(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -120(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -128(%rbp)
	movq	-112(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%rdi, -136(%rbp)
	movq	%r15, %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r8, -144(%rbp)
	movq	%r14, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r9, -152(%rbp)
	movq	%r13, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r10, -160(%rbp)
	movq	%r12, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r11, -168(%rbp)
	movq	%rbx, %rax
	movq	-88(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -88(%rbp)
.L472:
	movq	-176(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -176(%rbp)
	testq	%rax, %rax
	jne	.L473
	movl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE160:
	.size	int64_mod_7, .-int64_mod_7
	.globl	int64_mod_8
	.type	int64_mod_8, @function
int64_mod_8:
.LFB161:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -192(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	cltq
	movq	%rax, -136(%rbp)
	movq	$0, -104(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	cltq
	movq	%rax, -144(%rbp)
	movq	$0, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	cltq
	movq	%rax, -152(%rbp)
	movq	$0, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	cltq
	movq	%rax, -160(%rbp)
	movq	$0, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	cltq
	movq	%rax, -168(%rbp)
	movl	$0, %r15d
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	cltq
	movq	%rax, -176(%rbp)
	movl	$0, %r14d
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	cltq
	movq	%rax, -184(%rbp)
	movl	$0, %r13d
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	movq	%rax, -88(%rbp)
	movl	$0, %r12d
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	cltq
	movq	%rax, -96(%rbp)
	movl	$0, %ebx
	jmp	.L475
.L476:
	addq	$1, -104(%rbp)
	movq	-104(%rbp), %rcx
	addq	$1, -112(%rbp)
	movq	-112(%rbp), %rsi
	addq	$1, -120(%rbp)
	movq	-120(%rbp), %rdi
	addq	$1, -128(%rbp)
	movq	-128(%rbp), %r8
	addq	$1, %r15
	addq	$1, %r14
	addq	$1, %r13
	addq	$1, %r12
	addq	$1, %rbx
	movq	%rcx, -104(%rbp)
	movq	%rcx, %rax
	movq	-136(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rsi, -112(%rbp)
	movq	%rsi, %rax
	movq	-144(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rdi, -120(%rbp)
	movq	%rdi, %rax
	movq	-152(%rbp), %rdi
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%r8, -128(%rbp)
	movq	%r8, %rax
	movq	-160(%rbp), %r8
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r15, %rax
	movq	-168(%rbp), %r9
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r14, %rax
	movq	-176(%rbp), %r10
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r13, %rax
	movq	-184(%rbp), %r11
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r12, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-120(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-128(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r15, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r14, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r13, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r12, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-120(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-128(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r15, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r14, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r13, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r12, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-120(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-128(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r15, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r14, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r13, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r12, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-120(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-128(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r15, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r14, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r13, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r12, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-120(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-128(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r15, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r14, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r13, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r12, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-120(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-128(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r15, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r14, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r13, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r12, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-120(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-128(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r15, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r14, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r13, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r12, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-120(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-128(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r15, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r14, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r13, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r12, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-104(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -136(%rbp)
	movq	-112(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -144(%rbp)
	movq	-120(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%rdi, -152(%rbp)
	movq	-128(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r8, -160(%rbp)
	movq	%r15, %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r9, -168(%rbp)
	movq	%r14, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r10, -176(%rbp)
	movq	%r13, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r11, -184(%rbp)
	movq	%r12, %rax
	movq	-88(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -88(%rbp)
	movq	%rbx, %rax
	movq	-96(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -96(%rbp)
.L475:
	movq	-192(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -192(%rbp)
	testq	%rax, %rax
	jne	.L476
	movl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$152, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE161:
	.size	int64_mod_8, .-int64_mod_8
	.globl	int64_mod_9
	.type	int64_mod_9, @function
int64_mod_9:
.LFB162:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -208(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	cltq
	movq	%rax, -152(%rbp)
	movq	$0, -112(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	cltq
	movq	%rax, -160(%rbp)
	movq	$0, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	cltq
	movq	%rax, -168(%rbp)
	movq	$0, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	cltq
	movq	%rax, -176(%rbp)
	movq	$0, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	cltq
	movq	%rax, -184(%rbp)
	movq	$0, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	cltq
	movq	%rax, -192(%rbp)
	movl	$0, %r15d
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	cltq
	movq	%rax, -200(%rbp)
	movl	$0, %r14d
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	movq	%rax, -88(%rbp)
	movl	$0, %r13d
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	cltq
	movq	%rax, -96(%rbp)
	movl	$0, %r12d
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	cltq
	movq	%rax, -104(%rbp)
	movl	$0, %ebx
	jmp	.L478
.L479:
	addq	$1, -112(%rbp)
	movq	-112(%rbp), %rsi
	addq	$1, -120(%rbp)
	movq	-120(%rbp), %rdi
	addq	$1, -128(%rbp)
	movq	-128(%rbp), %r8
	addq	$1, -136(%rbp)
	movq	-136(%rbp), %r9
	addq	$1, -144(%rbp)
	movq	-144(%rbp), %r10
	addq	$1, %r15
	addq	$1, %r14
	addq	$1, %r13
	addq	$1, %r12
	addq	$1, %rbx
	movq	%rsi, -112(%rbp)
	movq	%rsi, %rax
	movq	-152(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rdi, -120(%rbp)
	movq	%rdi, %rax
	movq	-160(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%r8, -128(%rbp)
	movq	%r8, %rax
	movq	-168(%rbp), %rdi
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%r9, -136(%rbp)
	movq	%r9, %rax
	movq	-176(%rbp), %r8
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r10, -144(%rbp)
	movq	%r10, %rax
	movq	-184(%rbp), %r9
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r15, %rax
	movq	-192(%rbp), %r10
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r14, %rax
	movq	-200(%rbp), %r11
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-112(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-120(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-128(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r15, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r14, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-112(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-120(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-128(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r15, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r14, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-112(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-120(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-128(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r15, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r14, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-112(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-120(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-128(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r15, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r14, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-112(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-120(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-128(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r15, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r14, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-112(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-120(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-128(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r15, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r14, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-112(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-120(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-128(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r15, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r14, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-112(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-120(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-128(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r15, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r14, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r13, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-112(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -152(%rbp)
	movq	-120(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -160(%rbp)
	movq	-128(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%rdi, -168(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r8, -176(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r9, -184(%rbp)
	movq	%r15, %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r10, -192(%rbp)
	movq	%r14, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r11, -200(%rbp)
	movq	%r13, %rax
	movq	-88(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -88(%rbp)
	movq	%r12, %rax
	movq	-96(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -96(%rbp)
	movq	%rbx, %rax
	movq	-104(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -104(%rbp)
.L478:
	movq	-208(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -208(%rbp)
	testq	%rax, %rax
	jne	.L479
	movl	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$168, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE162:
	.size	int64_mod_9, .-int64_mod_9
	.globl	int64_mod_10
	.type	int64_mod_10, @function
int64_mod_10:
.LFB163:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$184, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -224(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	cltq
	movq	%rax, -168(%rbp)
	movq	$0, -120(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	cltq
	movq	%rax, -176(%rbp)
	movq	$0, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	cltq
	movq	%rax, -184(%rbp)
	movq	$0, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	cltq
	movq	%rax, -192(%rbp)
	movq	$0, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	cltq
	movq	%rax, -200(%rbp)
	movq	$0, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	cltq
	movq	%rax, -208(%rbp)
	movq	$0, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	cltq
	movq	%rax, -216(%rbp)
	movl	$0, %r15d
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	movq	%rax, -88(%rbp)
	movl	$0, %r14d
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	cltq
	movq	%rax, -96(%rbp)
	movl	$0, %r13d
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	cltq
	movq	%rax, -104(%rbp)
	movl	$0, %r12d
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	cltq
	movq	%rax, -112(%rbp)
	movl	$0, %ebx
	jmp	.L481
.L482:
	addq	$1, -120(%rbp)
	movq	-120(%rbp), %rdi
	addq	$1, -128(%rbp)
	movq	-128(%rbp), %rsi
	addq	$1, -136(%rbp)
	movq	-136(%rbp), %r8
	addq	$1, -144(%rbp)
	movq	-144(%rbp), %r9
	addq	$1, -152(%rbp)
	movq	-152(%rbp), %r10
	addq	$1, -160(%rbp)
	movq	-160(%rbp), %r11
	addq	$1, %r15
	addq	$1, %r14
	addq	$1, %r13
	addq	$1, %r12
	addq	$1, %rbx
	movq	%rdi, -120(%rbp)
	movq	%rdi, %rax
	movq	-168(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rsi, -128(%rbp)
	movq	%rsi, %rax
	movq	-176(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%r8, -136(%rbp)
	movq	%r8, %rax
	movq	-184(%rbp), %rdi
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%r9, -144(%rbp)
	movq	%r9, %rax
	movq	-192(%rbp), %r8
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r10, -152(%rbp)
	movq	%r10, %rax
	movq	-200(%rbp), %r9
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r11, -160(%rbp)
	movq	%r11, %rax
	movq	-208(%rbp), %r10
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r15, %rax
	movq	-216(%rbp), %r11
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r14, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	-120(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-128(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-136(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-144(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-152(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-160(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r15, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r14, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	-120(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-128(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-136(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-144(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-152(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-160(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r15, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r14, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	-120(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-128(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-136(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-144(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-152(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-160(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r15, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r14, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	-120(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-128(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-136(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-144(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-152(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-160(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r15, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r14, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	-120(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-128(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-136(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-144(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-152(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-160(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r15, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r14, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	-120(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-128(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-136(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-144(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-152(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-160(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r15, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r14, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	-120(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-128(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-136(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-144(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-152(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-160(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r15, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r14, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	-120(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-128(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-136(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-144(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-152(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-160(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r15, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r14, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	-120(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -168(%rbp)
	movq	-128(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -176(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%rdi, -184(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r8, -192(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r9, -200(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r10, -208(%rbp)
	movq	%r15, %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r11, -216(%rbp)
	movq	%r14, %rax
	movq	-88(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -88(%rbp)
	movq	%r13, %rax
	movq	-96(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -96(%rbp)
	movq	%r12, %rax
	movq	-104(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -104(%rbp)
	movq	%rbx, %rax
	movq	-112(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -112(%rbp)
.L481:
	movq	-224(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -224(%rbp)
	testq	%rax, %rax
	jne	.L482
	movl	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$184, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE163:
	.size	int64_mod_10, .-int64_mod_10
	.globl	int64_mod_11
	.type	int64_mod_11, @function
int64_mod_11:
.LFB164:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$200, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -240(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	cltq
	movq	%rax, -184(%rbp)
	movq	$0, -128(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	cltq
	movq	%rax, -192(%rbp)
	movq	$0, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	cltq
	movq	%rax, -200(%rbp)
	movq	$0, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	cltq
	movq	%rax, -208(%rbp)
	movq	$0, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	cltq
	movq	%rax, -216(%rbp)
	movq	$0, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	cltq
	movq	%rax, -224(%rbp)
	movq	$0, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	cltq
	movq	%rax, -232(%rbp)
	movq	$0, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	movq	%rax, -88(%rbp)
	movl	$0, %r15d
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	cltq
	movq	%rax, -96(%rbp)
	movl	$0, %r14d
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	cltq
	movq	%rax, -104(%rbp)
	movl	$0, %r13d
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	cltq
	movq	%rax, -112(%rbp)
	movl	$0, %r12d
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	cltq
	movq	%rax, -120(%rbp)
	movl	$0, %ebx
	jmp	.L484
.L485:
	addq	$1, -128(%rbp)
	movq	-128(%rbp), %rcx
	addq	$1, -136(%rbp)
	movq	-136(%rbp), %rsi
	addq	$1, -144(%rbp)
	movq	-144(%rbp), %rdi
	addq	$1, -152(%rbp)
	movq	-152(%rbp), %r8
	addq	$1, -160(%rbp)
	movq	-160(%rbp), %r9
	addq	$1, -168(%rbp)
	movq	-168(%rbp), %r10
	addq	$1, -176(%rbp)
	movq	-176(%rbp), %r11
	addq	$1, %r15
	addq	$1, %r14
	addq	$1, %r13
	addq	$1, %r12
	addq	$1, %rbx
	movq	%rcx, -128(%rbp)
	movq	%rcx, %rax
	movq	-184(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rsi, -136(%rbp)
	movq	%rsi, %rax
	movq	-192(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rdi, -144(%rbp)
	movq	%rdi, %rax
	movq	-200(%rbp), %rdi
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%r8, -152(%rbp)
	movq	%r8, %rax
	movq	-208(%rbp), %r8
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r9, -160(%rbp)
	movq	%r9, %rax
	movq	-216(%rbp), %r9
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r10, -168(%rbp)
	movq	%r10, %rax
	movq	-224(%rbp), %r10
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r11, -176(%rbp)
	movq	%r11, %rax
	movq	-232(%rbp), %r11
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r15, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	-128(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-136(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-152(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-160(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-168(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-176(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r15, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	-128(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-136(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-152(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-160(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-168(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-176(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r15, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	-128(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-136(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-152(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-160(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-168(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-176(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r15, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	-128(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-136(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-152(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-160(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-168(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-176(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r15, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	-128(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-136(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-152(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-160(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-168(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-176(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r15, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	-128(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-136(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-152(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-160(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-168(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-176(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r15, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	-128(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-136(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-152(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-160(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-168(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-176(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r15, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	-128(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-136(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-152(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-160(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-168(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-176(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r15, %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	-128(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -184(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -192(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%rdi, -200(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r8, -208(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r9, -216(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r10, -224(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r11, -232(%rbp)
	movq	%r15, %rax
	movq	-88(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -88(%rbp)
	movq	%r14, %rax
	movq	-96(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -96(%rbp)
	movq	%r13, %rax
	movq	-104(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -104(%rbp)
	movq	%r12, %rax
	movq	-112(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -112(%rbp)
	movq	%rbx, %rax
	movq	-120(%rbp), %rdi
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%rdi, -120(%rbp)
.L484:
	movq	-240(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -240(%rbp)
	testq	%rax, %rax
	jne	.L485
	movl	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$200, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE164:
	.size	int64_mod_11, .-int64_mod_11
	.globl	int64_mod_12
	.type	int64_mod_12, @function
int64_mod_12:
.LFB165:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$216, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -256(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	cltq
	movq	%rax, -200(%rbp)
	movq	$0, -136(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	cltq
	movq	%rax, -208(%rbp)
	movq	$0, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	cltq
	movq	%rax, -216(%rbp)
	movq	$0, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	cltq
	movq	%rax, -224(%rbp)
	movq	$0, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	cltq
	movq	%rax, -232(%rbp)
	movq	$0, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	cltq
	movq	%rax, -240(%rbp)
	movq	$0, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	cltq
	movq	%rax, -248(%rbp)
	movq	$0, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	movq	%rax, -88(%rbp)
	movq	$0, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	cltq
	movq	%rax, -96(%rbp)
	movl	$0, %r15d
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	cltq
	movq	%rax, -104(%rbp)
	movl	$0, %r14d
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	cltq
	movq	%rax, -112(%rbp)
	movl	$0, %r13d
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	cltq
	movq	%rax, -120(%rbp)
	movl	$0, %r12d
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	cltq
	movq	%rax, -128(%rbp)
	movl	$0, %ebx
	jmp	.L487
.L488:
	addq	$1, -136(%rbp)
	movq	-136(%rbp), %rsi
	addq	$1, -144(%rbp)
	movq	-144(%rbp), %rdi
	addq	$1, -152(%rbp)
	movq	-152(%rbp), %r8
	addq	$1, -160(%rbp)
	movq	-160(%rbp), %r9
	addq	$1, -168(%rbp)
	movq	-168(%rbp), %r10
	addq	$1, -176(%rbp)
	movq	-176(%rbp), %r11
	addq	$1, -184(%rbp)
	addq	$1, -192(%rbp)
	addq	$1, %r15
	addq	$1, %r14
	addq	$1, %r13
	addq	$1, %r12
	addq	$1, %rbx
	movq	%rsi, -136(%rbp)
	movq	%rsi, %rax
	movq	-200(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rdi, -144(%rbp)
	movq	%rdi, %rax
	movq	-208(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%r8, -152(%rbp)
	movq	%r8, %rax
	movq	-216(%rbp), %rdi
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%r9, -160(%rbp)
	movq	%r9, %rax
	movq	-224(%rbp), %r8
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r10, -168(%rbp)
	movq	%r10, %rax
	movq	-232(%rbp), %r9
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r11, -176(%rbp)
	movq	%r11, %rax
	movq	-240(%rbp), %r10
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-184(%rbp), %rax
	movq	-248(%rbp), %r11
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-192(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-160(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-168(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-176(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-184(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-192(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-160(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-168(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-176(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-184(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-192(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-160(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-168(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-176(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-184(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-192(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-160(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-168(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-176(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-184(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-192(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-160(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-168(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-176(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-184(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-192(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-160(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-168(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-176(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-184(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-192(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-160(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-168(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-176(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-184(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-192(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-152(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-160(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-168(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-176(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-184(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-192(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	-136(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -200(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -208(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%rdi, -216(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r8, -224(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r9, -232(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r10, -240(%rbp)
	movq	-184(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r11, -248(%rbp)
	movq	-192(%rbp), %rax
	movq	-88(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -88(%rbp)
	movq	%r15, %rax
	movq	-96(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -96(%rbp)
	movq	%r14, %rax
	movq	-104(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -104(%rbp)
	movq	%r13, %rax
	movq	-112(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -112(%rbp)
	movq	%r12, %rax
	movq	-120(%rbp), %rdi
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%rdi, -120(%rbp)
	movq	%rbx, %rax
	movq	-128(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -128(%rbp)
.L487:
	movq	-256(%rbp), %rax
	leaq	-1(%rax), %rsi
	movq	%rsi, -256(%rbp)
	testq	%rax, %rax
	jne	.L488
	movl	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$216, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE165:
	.size	int64_mod_12, .-int64_mod_12
	.globl	int64_mod_13
	.type	int64_mod_13, @function
int64_mod_13:
.LFB166:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$232, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -272(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	cltq
	movq	%rax, -216(%rbp)
	movq	$0, -144(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	cltq
	movq	%rax, -224(%rbp)
	movq	$0, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	cltq
	movq	%rax, -232(%rbp)
	movq	$0, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	cltq
	movq	%rax, -240(%rbp)
	movq	$0, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	cltq
	movq	%rax, -248(%rbp)
	movq	$0, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	cltq
	movq	%rax, -256(%rbp)
	movq	$0, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	cltq
	movq	%rax, -264(%rbp)
	movq	$0, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	movq	%rax, -88(%rbp)
	movq	$0, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	cltq
	movq	%rax, -96(%rbp)
	movq	$0, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	cltq
	movq	%rax, -104(%rbp)
	movl	$0, %r15d
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	cltq
	movq	%rax, -112(%rbp)
	movl	$0, %r14d
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	cltq
	movq	%rax, -120(%rbp)
	movl	$0, %r13d
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	cltq
	movq	%rax, -128(%rbp)
	movl	$0, %r12d
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	cltq
	movq	%rax, -136(%rbp)
	movl	$0, %ebx
	jmp	.L490
.L491:
	addq	$1, -144(%rbp)
	movq	-144(%rbp), %rdi
	addq	$1, -152(%rbp)
	movq	-152(%rbp), %rsi
	addq	$1, -160(%rbp)
	movq	-160(%rbp), %r8
	addq	$1, -168(%rbp)
	movq	-168(%rbp), %r9
	addq	$1, -176(%rbp)
	movq	-176(%rbp), %r10
	addq	$1, -184(%rbp)
	movq	-184(%rbp), %r11
	addq	$1, -192(%rbp)
	addq	$1, -200(%rbp)
	addq	$1, -208(%rbp)
	addq	$1, %r15
	addq	$1, %r14
	addq	$1, %r13
	addq	$1, %r12
	addq	$1, %rbx
	movq	%rdi, -144(%rbp)
	movq	%rdi, %rax
	movq	-216(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rsi, -152(%rbp)
	movq	%rsi, %rax
	movq	-224(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%r8, -160(%rbp)
	movq	%r8, %rax
	movq	-232(%rbp), %rdi
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%r9, -168(%rbp)
	movq	%r9, %rax
	movq	-240(%rbp), %r8
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r10, -176(%rbp)
	movq	%r10, %rax
	movq	-248(%rbp), %r9
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r11, -184(%rbp)
	movq	%r11, %rax
	movq	-256(%rbp), %r10
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-192(%rbp), %rax
	movq	-264(%rbp), %r11
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-200(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-208(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-152(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-168(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-176(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-184(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-192(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-200(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-208(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-152(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-168(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-176(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-184(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-192(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-200(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-208(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-152(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-168(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-176(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-184(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-192(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-200(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-208(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-152(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-168(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-176(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-184(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-192(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-200(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-208(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-152(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-168(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-176(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-184(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-192(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-200(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-208(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-152(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-168(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-176(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-184(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-192(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-200(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-208(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-152(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-168(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-176(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-184(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-192(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-200(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-208(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-152(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-168(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-176(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-184(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-192(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-200(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-208(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	-144(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -216(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -224(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%rdi, -232(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r8, -240(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r9, -248(%rbp)
	movq	-184(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r10, -256(%rbp)
	movq	-192(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r11, -264(%rbp)
	movq	-200(%rbp), %rax
	movq	-88(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -88(%rbp)
	movq	-208(%rbp), %rax
	movq	-96(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -96(%rbp)
	movq	%r15, %rax
	movq	-104(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -104(%rbp)
	movq	%r14, %rax
	movq	-112(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -112(%rbp)
	movq	%r13, %rax
	movq	-120(%rbp), %rdi
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%rdi, -120(%rbp)
	movq	%r12, %rax
	movq	-128(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -128(%rbp)
	movq	%rbx, %rax
	movq	-136(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -136(%rbp)
.L490:
	movq	-272(%rbp), %rax
	leaq	-1(%rax), %rcx
	movq	%rcx, -272(%rbp)
	testq	%rax, %rax
	jne	.L491
	movl	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-264(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$232, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE166:
	.size	int64_mod_13, .-int64_mod_13
	.globl	int64_mod_14
	.type	int64_mod_14, @function
int64_mod_14:
.LFB167:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$248, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -288(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	cltq
	movq	%rax, -232(%rbp)
	movq	$0, -152(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	cltq
	movq	%rax, -240(%rbp)
	movq	$0, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	cltq
	movq	%rax, -248(%rbp)
	movq	$0, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	cltq
	movq	%rax, -256(%rbp)
	movq	$0, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	cltq
	movq	%rax, -264(%rbp)
	movq	$0, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	cltq
	movq	%rax, -272(%rbp)
	movq	$0, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	cltq
	movq	%rax, -280(%rbp)
	movq	$0, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	movq	%rax, -88(%rbp)
	movq	$0, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	cltq
	movq	%rax, -96(%rbp)
	movq	$0, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	cltq
	movq	%rax, -104(%rbp)
	movq	$0, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	cltq
	movq	%rax, -112(%rbp)
	movl	$0, %r15d
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	cltq
	movq	%rax, -120(%rbp)
	movl	$0, %r14d
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	cltq
	movq	%rax, -128(%rbp)
	movl	$0, %r13d
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	cltq
	movq	%rax, -136(%rbp)
	movl	$0, %r12d
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	cltq
	movq	%rax, -144(%rbp)
	movl	$0, %ebx
	jmp	.L493
.L494:
	addq	$1, -152(%rbp)
	movq	-152(%rbp), %rdi
	addq	$1, -160(%rbp)
	movq	-160(%rbp), %rsi
	addq	$1, -168(%rbp)
	movq	-168(%rbp), %r8
	addq	$1, -176(%rbp)
	movq	-176(%rbp), %r9
	addq	$1, -184(%rbp)
	movq	-184(%rbp), %r10
	addq	$1, -192(%rbp)
	movq	-192(%rbp), %r11
	addq	$1, -200(%rbp)
	addq	$1, -208(%rbp)
	addq	$1, -216(%rbp)
	addq	$1, -224(%rbp)
	addq	$1, %r15
	addq	$1, %r14
	addq	$1, %r13
	addq	$1, %r12
	addq	$1, %rbx
	movq	%rdi, -152(%rbp)
	movq	%rdi, %rax
	movq	-232(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rsi, -160(%rbp)
	movq	%rsi, %rax
	movq	-240(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%r8, -168(%rbp)
	movq	%r8, %rax
	movq	-248(%rbp), %rdi
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%r9, -176(%rbp)
	movq	%r9, %rax
	movq	-256(%rbp), %r8
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r10, -184(%rbp)
	movq	%r10, %rax
	movq	-264(%rbp), %r9
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r11, -192(%rbp)
	movq	%r11, %rax
	movq	-272(%rbp), %r10
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-200(%rbp), %rax
	movq	-280(%rbp), %r11
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-208(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-216(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-168(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-176(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-184(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-192(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-200(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-208(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-216(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-168(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-176(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-184(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-192(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-200(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-208(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-216(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-168(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-176(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-184(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-192(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-200(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-208(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-216(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-168(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-176(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-184(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-192(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-200(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-208(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-216(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-168(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-176(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-184(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-192(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-200(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-208(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-216(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-168(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-176(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-184(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-192(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-200(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-208(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-216(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-168(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-176(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-184(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-192(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-200(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-208(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-216(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-160(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-168(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-176(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-184(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-192(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-200(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-208(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-216(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	-152(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -232(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -240(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%rdi, -248(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r8, -256(%rbp)
	movq	-184(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r9, -264(%rbp)
	movq	-192(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r10, -272(%rbp)
	movq	-200(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r11, -280(%rbp)
	movq	-208(%rbp), %rax
	movq	-88(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -88(%rbp)
	movq	-216(%rbp), %rax
	movq	-96(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -96(%rbp)
	movq	-224(%rbp), %rax
	movq	-104(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -104(%rbp)
	movq	%r15, %rax
	movq	-112(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -112(%rbp)
	movq	%r14, %rax
	movq	-120(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -120(%rbp)
	movq	%r13, %rax
	movq	-128(%rbp), %rdi
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%rdi, -128(%rbp)
	movq	%r12, %rax
	movq	-136(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -136(%rbp)
	movq	%rbx, %rax
	movq	-144(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -144(%rbp)
.L493:
	movq	-288(%rbp), %rax
	leaq	-1(%rax), %rdi
	movq	%rdi, -288(%rbp)
	testq	%rax, %rax
	jne	.L494
	movl	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-264(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-272(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-280(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$248, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE167:
	.size	int64_mod_14, .-int64_mod_14
	.globl	int64_mod_15
	.type	int64_mod_15, @function
int64_mod_15:
.LFB168:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$264, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, -72(%rbp)
	movq	%rsi, -80(%rbp)
	movq	-72(%rbp), %rax
	movq	%rax, -304(%rbp)
	movq	-80(%rbp), %rax
	movq	%rax, -56(%rbp)
	movq	-56(%rbp), %rax
	movl	12(%rax), %eax
	cltq
	movq	%rax, -248(%rbp)
	movq	$0, -160(%rbp)
	movq	-56(%rbp), %rax
	movl	16(%rax), %eax
	cltq
	movq	%rax, -256(%rbp)
	movq	$0, -168(%rbp)
	movq	-56(%rbp), %rax
	movl	20(%rax), %eax
	cltq
	movq	%rax, -264(%rbp)
	movq	$0, -176(%rbp)
	movq	-56(%rbp), %rax
	movl	24(%rax), %eax
	cltq
	movq	%rax, -272(%rbp)
	movq	$0, -184(%rbp)
	movq	-56(%rbp), %rax
	movl	28(%rax), %eax
	cltq
	movq	%rax, -280(%rbp)
	movq	$0, -192(%rbp)
	movq	-56(%rbp), %rax
	movl	32(%rax), %eax
	cltq
	movq	%rax, -288(%rbp)
	movq	$0, -200(%rbp)
	movq	-56(%rbp), %rax
	movl	36(%rax), %eax
	cltq
	movq	%rax, -296(%rbp)
	movq	$0, -208(%rbp)
	movq	-56(%rbp), %rax
	movl	40(%rax), %eax
	cltq
	movq	%rax, -88(%rbp)
	movq	$0, -216(%rbp)
	movq	-56(%rbp), %rax
	movl	44(%rax), %eax
	cltq
	movq	%rax, -96(%rbp)
	movq	$0, -224(%rbp)
	movq	-56(%rbp), %rax
	movl	48(%rax), %eax
	cltq
	movq	%rax, -104(%rbp)
	movq	$0, -232(%rbp)
	movq	-56(%rbp), %rax
	movl	52(%rax), %eax
	cltq
	movq	%rax, -112(%rbp)
	movq	$0, -240(%rbp)
	movq	-56(%rbp), %rax
	movl	56(%rax), %eax
	cltq
	movq	%rax, -120(%rbp)
	movl	$0, %r15d
	movq	-56(%rbp), %rax
	movl	60(%rax), %eax
	cltq
	movq	%rax, -128(%rbp)
	movl	$0, %r14d
	movq	-56(%rbp), %rax
	movl	64(%rax), %eax
	cltq
	movq	%rax, -136(%rbp)
	movl	$0, %r13d
	movq	-56(%rbp), %rax
	movl	68(%rax), %eax
	cltq
	movq	%rax, -144(%rbp)
	movl	$0, %r12d
	movq	-56(%rbp), %rax
	movl	72(%rax), %eax
	cltq
	movq	%rax, -152(%rbp)
	movl	$0, %ebx
	jmp	.L496
.L497:
	addq	$1, -160(%rbp)
	movq	-160(%rbp), %rdi
	addq	$1, -168(%rbp)
	movq	-168(%rbp), %rsi
	addq	$1, -176(%rbp)
	movq	-176(%rbp), %r8
	addq	$1, -184(%rbp)
	movq	-184(%rbp), %r9
	addq	$1, -192(%rbp)
	movq	-192(%rbp), %r10
	addq	$1, -200(%rbp)
	movq	-200(%rbp), %r11
	addq	$1, -208(%rbp)
	addq	$1, -216(%rbp)
	addq	$1, -224(%rbp)
	addq	$1, -232(%rbp)
	addq	$1, -240(%rbp)
	addq	$1, %r15
	addq	$1, %r14
	addq	$1, %r13
	addq	$1, %r12
	addq	$1, %rbx
	movq	%rdi, -160(%rbp)
	movq	%rdi, %rax
	movq	-248(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rsi, -168(%rbp)
	movq	%rsi, %rax
	movq	-256(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%r8, -176(%rbp)
	movq	%r8, %rax
	movq	-264(%rbp), %rdi
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%r9, -184(%rbp)
	movq	%r9, %rax
	movq	-272(%rbp), %r8
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r10, -192(%rbp)
	movq	%r10, %rax
	movq	-280(%rbp), %r9
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r11, -200(%rbp)
	movq	%r11, %rax
	movq	-288(%rbp), %r10
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-208(%rbp), %rax
	movq	-296(%rbp), %r11
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-216(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-232(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-240(%rbp), %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-152(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -152(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-184(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-192(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-200(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-208(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-216(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-232(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-240(%rbp), %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-152(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -152(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-184(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-192(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-200(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-208(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-216(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-232(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-240(%rbp), %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-152(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -152(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-184(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-192(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-200(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-208(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-216(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-232(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-240(%rbp), %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-152(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -152(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-184(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-192(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-200(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-208(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-216(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-232(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-240(%rbp), %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-152(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -152(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-184(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-192(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-200(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-208(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-216(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-232(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-240(%rbp), %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-152(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -152(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-184(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-192(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-200(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-208(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-216(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-232(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-240(%rbp), %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-152(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -152(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-184(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-192(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-200(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-208(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-216(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-232(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-240(%rbp), %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-152(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -152(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	-184(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	-192(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	-200(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	-208(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	-216(%rbp), %rax
	cqto
	idivq	-88(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -88(%rbp)
	movq	-224(%rbp), %rax
	cqto
	idivq	-96(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -96(%rbp)
	movq	-232(%rbp), %rax
	cqto
	idivq	-104(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -104(%rbp)
	movq	-240(%rbp), %rax
	cqto
	idivq	-112(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -112(%rbp)
	movq	%r15, %rax
	cqto
	idivq	-120(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -120(%rbp)
	movq	%r14, %rax
	cqto
	idivq	-128(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -128(%rbp)
	movq	%r13, %rax
	cqto
	idivq	-136(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -136(%rbp)
	movq	%r12, %rax
	cqto
	idivq	-144(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -144(%rbp)
	movq	%rbx, %rax
	cqto
	idivq	-152(%rbp)
	movq	%rdx, %rax
	xorq	%rax, -152(%rbp)
	movq	-160(%rbp), %rax
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -248(%rbp)
	movq	-168(%rbp), %rax
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -256(%rbp)
	movq	-176(%rbp), %rax
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%rdi, -264(%rbp)
	movq	-184(%rbp), %rax
	cqto
	idivq	%r8
	movq	%rdx, %rax
	xorq	%rax, %r8
	movq	%r8, -272(%rbp)
	movq	-192(%rbp), %rax
	cqto
	idivq	%r9
	movq	%rdx, %rax
	xorq	%rax, %r9
	movq	%r9, -280(%rbp)
	movq	-200(%rbp), %rax
	cqto
	idivq	%r10
	movq	%rdx, %rax
	xorq	%rax, %r10
	movq	%r10, -288(%rbp)
	movq	-208(%rbp), %rax
	cqto
	idivq	%r11
	movq	%rdx, %rax
	xorq	%rax, %r11
	movq	%r11, -296(%rbp)
	movq	-216(%rbp), %rax
	movq	-88(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -88(%rbp)
	movq	-224(%rbp), %rax
	movq	-96(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -96(%rbp)
	movq	-232(%rbp), %rax
	movq	-104(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -104(%rbp)
	movq	-240(%rbp), %rax
	movq	-112(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -112(%rbp)
	movq	%r15, %rax
	movq	-120(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -120(%rbp)
	movq	%r14, %rax
	movq	-128(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -128(%rbp)
	movq	%r13, %rax
	movq	-136(%rbp), %rdi
	cqto
	idivq	%rdi
	movq	%rdx, %rax
	xorq	%rax, %rdi
	movq	%rdi, -136(%rbp)
	movq	%r12, %rax
	movq	-144(%rbp), %rcx
	cqto
	idivq	%rcx
	movq	%rdx, %rax
	xorq	%rax, %rcx
	movq	%rcx, -144(%rbp)
	movq	%rbx, %rax
	movq	-152(%rbp), %rsi
	cqto
	idivq	%rsi
	movq	%rdx, %rax
	xorq	%rax, %rsi
	movq	%rsi, -152(%rbp)
.L496:
	movq	-304(%rbp), %rax
	leaq	-1(%rax), %rdi
	movq	%rdi, -304(%rbp)
	testq	%rax, %rax
	jne	.L497
	movl	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-264(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-272(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-280(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-288(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-296(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movl	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	addq	$264, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE168:
	.size	int64_mod_15, .-int64_mod_15
	.globl	int64_mod_benchmarks
	.section	.data.rel.local
	.align 32
	.type	int64_mod_benchmarks, @object
	.size	int64_mod_benchmarks, 128
int64_mod_benchmarks:
	.quad	int64_mod_0
	.quad	int64_mod_1
	.quad	int64_mod_2
	.quad	int64_mod_3
	.quad	int64_mod_4
	.quad	int64_mod_5
	.quad	int64_mod_6
	.quad	int64_mod_7
	.quad	int64_mod_8
	.quad	int64_mod_9
	.quad	int64_mod_10
	.quad	int64_mod_11
	.quad	int64_mod_12
	.quad	int64_mod_13
	.quad	int64_mod_14
	.quad	int64_mod_15
	.text
	.globl	float_add_0
	.type	float_add_0, @function
float_add_0:
.LFB169:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movaps	%xmm1, %xmm6
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	jmp	.L499
.L500:
	movaps	%xmm6, %xmm2
	addss	%xmm0, %xmm2
	movaps	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	addss	%xmm5, %xmm2
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm4
	addss	%xmm3, %xmm4
	movaps	%xmm4, %xmm5
	addss	%xmm4, %xmm5
	movaps	%xmm5, %xmm6
	addss	%xmm5, %xmm6
	movaps	%xmm6, %xmm7
	addss	%xmm6, %xmm7
	movaps	%xmm7, %xmm1
	addss	%xmm7, %xmm1
	movaps	%xmm1, %xmm2
	addss	%xmm1, %xmm2
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm4
	addss	%xmm3, %xmm4
	movaps	%xmm4, %xmm6
.L499:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L500
	cvttss2sil	%xmm6, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE169:
	.size	float_add_0, .-float_add_0
	.globl	float_add_1
	.type	float_add_1, @function
float_add_1:
.LFB170:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movaps	%xmm1, %xmm4
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2ssl	%eax, %xmm7
	movaps	%xmm7, %xmm8
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	movaps	%xmm0, %xmm9
	jmp	.L502
.L503:
	movaps	%xmm4, %xmm2
	movaps	%xmm8, %xmm7
	addss	%xmm7, %xmm2
	movss	-52(%rbp), %xmm1
	movaps	%xmm9, %xmm3
	addss	%xmm3, %xmm1
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm1, %xmm3
	addss	%xmm1, %xmm3
	movaps	%xmm3, %xmm1
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm1, %xmm4
	addss	%xmm1, %xmm4
	movaps	%xmm4, %xmm1
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm1, %xmm5
	addss	%xmm1, %xmm5
	movaps	%xmm5, %xmm1
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm1, %xmm0
	addss	%xmm1, %xmm0
	movaps	%xmm0, %xmm1
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm1, %xmm3
	addss	%xmm1, %xmm3
	movaps	%xmm3, %xmm1
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm1, %xmm4
	addss	%xmm1, %xmm4
	movaps	%xmm4, %xmm1
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm1, %xmm5
	addss	%xmm1, %xmm5
	movaps	%xmm5, %xmm1
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movss	%xmm6, -52(%rbp)
.L502:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L503
	cvttss2sil	%xmm4, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE170:
	.size	float_add_1, .-float_add_1
	.globl	float_add_2
	.type	float_add_2, @function
float_add_2:
.LFB171:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movaps	%xmm1, %xmm6
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movaps	%xmm3, %xmm8
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movaps	%xmm4, %xmm9
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2ssl	%eax, %xmm5
	movaps	%xmm5, %xmm10
	jmp	.L505
.L506:
	movaps	%xmm6, %xmm2
	movaps	%xmm8, %xmm6
	addss	%xmm6, %xmm2
	movss	-52(%rbp), %xmm1
	movaps	%xmm9, %xmm7
	addss	%xmm7, %xmm1
	movss	-56(%rbp), %xmm0
	movaps	%xmm10, %xmm3
	addss	%xmm3, %xmm0
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm1, %xmm3
	addss	%xmm1, %xmm3
	movaps	%xmm3, %xmm1
	movaps	%xmm0, %xmm3
	addss	%xmm0, %xmm3
	movaps	%xmm3, %xmm0
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm1, %xmm4
	addss	%xmm1, %xmm4
	movaps	%xmm4, %xmm1
	movaps	%xmm0, %xmm4
	addss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm1, %xmm5
	addss	%xmm1, %xmm5
	movaps	%xmm5, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm1, %xmm3
	addss	%xmm1, %xmm3
	movaps	%xmm3, %xmm1
	movaps	%xmm0, %xmm3
	addss	%xmm0, %xmm3
	movaps	%xmm3, %xmm0
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm1, %xmm4
	addss	%xmm1, %xmm4
	movaps	%xmm4, %xmm1
	movaps	%xmm0, %xmm4
	addss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm1, %xmm5
	addss	%xmm1, %xmm5
	movaps	%xmm5, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm6
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movss	%xmm7, -52(%rbp)
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movss	%xmm7, -56(%rbp)
.L505:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L506
	cvttss2sil	%xmm6, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE171:
	.size	float_add_2, .-float_add_2
	.globl	float_add_3
	.type	float_add_3, @function
float_add_3:
.LFB172:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movaps	%xmm1, %xmm4
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2ssl	%eax, %xmm7
	movaps	%xmm7, %xmm8
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2ssl	%eax, %xmm7
	movaps	%xmm7, %xmm9
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2ssl	%eax, %xmm7
	movaps	%xmm7, %xmm10
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm6, %xmm6
	cvtsi2ssl	%eax, %xmm6
	movaps	%xmm6, %xmm11
	jmp	.L508
.L509:
	movaps	%xmm4, %xmm2
	movaps	%xmm8, %xmm7
	addss	%xmm7, %xmm2
	movss	-52(%rbp), %xmm5
	movaps	%xmm9, %xmm0
	addss	%xmm0, %xmm5
	movss	-56(%rbp), %xmm0
	movaps	%xmm10, %xmm3
	addss	%xmm3, %xmm0
	movss	-60(%rbp), %xmm1
	movaps	%xmm11, %xmm6
	addss	%xmm6, %xmm1
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm1, %xmm3
	addss	%xmm1, %xmm3
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm3, %xmm7
	addss	%xmm3, %xmm7
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm7, %xmm5
	addss	%xmm7, %xmm5
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	movaps	%xmm6, %xmm1
	addss	%xmm6, %xmm1
	movaps	%xmm1, %xmm6
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm6, %xmm1
	addss	%xmm6, %xmm1
	movaps	%xmm1, %xmm6
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm3, %xmm1
	addss	%xmm3, %xmm1
	movaps	%xmm1, %xmm3
	movaps	%xmm6, %xmm1
	addss	%xmm6, %xmm1
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm4, %xmm5
	addss	%xmm4, %xmm5
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm3, %xmm7
	addss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm5, %xmm6
	addss	%xmm5, %xmm6
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movss	%xmm5, -52(%rbp)
	movaps	%xmm1, %xmm0
	addss	%xmm1, %xmm0
	movss	%xmm0, -56(%rbp)
	movaps	%xmm6, %xmm7
	addss	%xmm6, %xmm7
	movss	%xmm7, -60(%rbp)
.L508:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L509
	cvttss2sil	%xmm4, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE172:
	.size	float_add_3, .-float_add_3
	.globl	float_add_4
	.type	float_add_4, @function
float_add_4:
.LFB173:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movaps	%xmm1, %xmm2
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2ssl	%eax, %xmm5
	movaps	%xmm5, %xmm8
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2ssl	%eax, %xmm5
	movaps	%xmm5, %xmm9
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2ssl	%eax, %xmm5
	movaps	%xmm5, %xmm10
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm6, %xmm6
	cvtsi2ssl	%eax, %xmm6
	movaps	%xmm6, %xmm11
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2ssl	%eax, %xmm7
	movaps	%xmm7, %xmm12
	jmp	.L511
.L512:
	movaps	%xmm8, %xmm5
	addss	%xmm5, %xmm2
	movss	-52(%rbp), %xmm5
	movaps	%xmm9, %xmm6
	addss	%xmm6, %xmm5
	movss	-56(%rbp), %xmm1
	movaps	%xmm10, %xmm7
	addss	%xmm7, %xmm1
	movss	-60(%rbp), %xmm0
	movaps	%xmm11, %xmm6
	addss	%xmm6, %xmm0
	movss	-64(%rbp), %xmm7
	movaps	%xmm12, %xmm6
	addss	%xmm6, %xmm7
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm3
	addss	%xmm0, %xmm3
	movaps	%xmm3, %xmm0
	movaps	%xmm7, %xmm3
	addss	%xmm7, %xmm3
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm6, %xmm1
	addss	%xmm6, %xmm1
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm7, %xmm6
	addss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	movaps	%xmm0, %xmm4
	addss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	movaps	%xmm1, %xmm5
	addss	%xmm1, %xmm5
	movaps	%xmm5, %xmm1
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm7, %xmm6
	addss	%xmm7, %xmm6
	movaps	%xmm0, %xmm4
	addss	%xmm0, %xmm4
	movaps	%xmm1, %xmm0
	addss	%xmm1, %xmm0
	movaps	%xmm0, %xmm1
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm3, %xmm7
	addss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	movaps	%xmm6, %xmm7
	addss	%xmm6, %xmm7
	movaps	%xmm4, %xmm0
	addss	%xmm4, %xmm0
	movaps	%xmm0, %xmm4
	movaps	%xmm1, %xmm0
	addss	%xmm1, %xmm0
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm3, %xmm1
	addss	%xmm3, %xmm1
	movaps	%xmm1, %xmm3
	movaps	%xmm7, %xmm6
	addss	%xmm7, %xmm6
	movaps	%xmm4, %xmm5
	addss	%xmm4, %xmm5
	movaps	%xmm5, %xmm4
	movaps	%xmm0, %xmm1
	addss	%xmm0, %xmm1
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movss	%xmm5, -52(%rbp)
	movaps	%xmm6, %xmm0
	addss	%xmm6, %xmm0
	movss	%xmm0, -56(%rbp)
	movaps	%xmm4, %xmm0
	addss	%xmm4, %xmm0
	movss	%xmm0, -60(%rbp)
	movaps	%xmm1, %xmm5
	addss	%xmm1, %xmm5
	movss	%xmm5, -64(%rbp)
.L511:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L512
	cvttss2sil	%xmm2, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE173:
	.size	float_add_4, .-float_add_4
	.globl	float_add_5
	.type	float_add_5, @function
float_add_5:
.LFB174:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movaps	%xmm1, %xmm2
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movaps	%xmm3, %xmm9
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movaps	%xmm3, %xmm10
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2ssl	%eax, %xmm5
	movaps	%xmm5, %xmm11
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm6, %xmm6
	cvtsi2ssl	%eax, %xmm6
	movaps	%xmm6, %xmm12
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movaps	%xmm4, %xmm13
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2ssl	%eax, %xmm7
	movaps	%xmm7, %xmm14
	jmp	.L514
.L515:
	movaps	%xmm9, %xmm3
	addss	%xmm3, %xmm2
	movss	-52(%rbp), %xmm5
	movaps	%xmm10, %xmm6
	addss	%xmm6, %xmm5
	movss	-56(%rbp), %xmm1
	movaps	%xmm11, %xmm4
	addss	%xmm4, %xmm1
	movss	-60(%rbp), %xmm0
	movaps	%xmm12, %xmm7
	addss	%xmm7, %xmm0
	movss	-64(%rbp), %xmm7
	movaps	%xmm13, %xmm3
	addss	%xmm3, %xmm7
	movss	-68(%rbp), %xmm8
	movaps	%xmm14, %xmm6
	addss	%xmm6, %xmm8
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm3
	addss	%xmm0, %xmm3
	movaps	%xmm3, %xmm0
	movaps	%xmm7, %xmm3
	addss	%xmm7, %xmm3
	movaps	%xmm8, %xmm6
	addss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm8, %xmm5
	addss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm8, %xmm5
	addss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm6, %xmm1
	addss	%xmm6, %xmm1
	movaps	%xmm8, %xmm5
	addss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm7, %xmm6
	addss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	movaps	%xmm0, %xmm4
	addss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	movaps	%xmm1, %xmm4
	addss	%xmm1, %xmm4
	movaps	%xmm4, %xmm1
	movaps	%xmm8, %xmm6
	addss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm7, %xmm6
	addss	%xmm7, %xmm6
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm1, %xmm4
	addss	%xmm1, %xmm4
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm3, %xmm7
	addss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	movaps	%xmm6, %xmm7
	addss	%xmm6, %xmm7
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm4, %xmm5
	addss	%xmm4, %xmm5
	movaps	%xmm5, %xmm4
	movaps	%xmm8, %xmm5
	addss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm7, %xmm6
	addss	%xmm7, %xmm6
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm3, %xmm1
	addss	%xmm3, %xmm1
	movss	%xmm1, -52(%rbp)
	movaps	%xmm6, %xmm0
	addss	%xmm6, %xmm0
	movss	%xmm0, -56(%rbp)
	movaps	%xmm5, %xmm6
	addss	%xmm5, %xmm6
	movss	%xmm6, -60(%rbp)
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movss	%xmm7, -64(%rbp)
	movaps	%xmm8, %xmm6
	addss	%xmm8, %xmm6
	movss	%xmm6, -68(%rbp)
.L514:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L515
	cvttss2sil	%xmm2, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE174:
	.size	float_add_5, .-float_add_5
	.globl	float_add_6
	.type	float_add_6, @function
float_add_6:
.LFB175:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movaps	%xmm1, %xmm2
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2ssl	%eax, %xmm5
	movaps	%xmm5, %xmm11
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movaps	%xmm3, %xmm12
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm6, %xmm6
	cvtsi2ssl	%eax, %xmm6
	movaps	%xmm6, %xmm13
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movaps	%xmm4, %xmm14
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2ssl	%eax, %xmm5
	movaps	%xmm5, %xmm15
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2ssl	%eax, %xmm7
	movss	%xmm7, -76(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movss	%xmm3, -80(%rbp)
	jmp	.L517
.L518:
	movaps	%xmm11, %xmm6
	addss	%xmm6, %xmm2
	movss	-52(%rbp), %xmm5
	movaps	%xmm12, %xmm4
	addss	%xmm4, %xmm5
	movss	-56(%rbp), %xmm1
	movaps	%xmm13, %xmm7
	addss	%xmm7, %xmm1
	movss	-60(%rbp), %xmm0
	movaps	%xmm14, %xmm3
	addss	%xmm3, %xmm0
	movss	-64(%rbp), %xmm8
	movaps	%xmm15, %xmm6
	addss	%xmm6, %xmm8
	movss	-68(%rbp), %xmm9
	addss	-76(%rbp), %xmm9
	movss	-72(%rbp), %xmm10
	addss	-80(%rbp), %xmm10
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm3
	addss	%xmm8, %xmm3
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm7, %xmm5
	addss	%xmm7, %xmm5
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm3, %xmm7
	addss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movss	%xmm6, -52(%rbp)
	movaps	%xmm1, %xmm5
	addss	%xmm1, %xmm5
	movss	%xmm5, -56(%rbp)
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movss	%xmm6, -60(%rbp)
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movss	%xmm7, -64(%rbp)
	movaps	%xmm9, %xmm3
	addss	%xmm9, %xmm3
	movss	%xmm3, -68(%rbp)
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movss	%xmm4, -72(%rbp)
.L517:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L518
	cvttss2sil	%xmm2, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE175:
	.size	float_add_6, .-float_add_6
	.globl	float_add_7
	.type	float_add_7, @function
float_add_7:
.LFB176:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movaps	%xmm1, %xmm12
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movaps	%xmm2, %xmm13
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movaps	%xmm4, %xmm14
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movaps	%xmm2, %xmm15
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movss	%xmm3, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movss	%xmm4, -84(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm6, %xmm6
	cvtsi2ssl	%eax, %xmm6
	movss	%xmm6, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2ssl	%eax, %xmm7
	movss	%xmm7, -92(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -96(%rbp)
	jmp	.L520
.L521:
	movaps	%xmm12, %xmm2
	movaps	%xmm13, %xmm3
	addss	%xmm3, %xmm2
	movss	-52(%rbp), %xmm5
	movaps	%xmm14, %xmm4
	addss	%xmm4, %xmm5
	movss	-56(%rbp), %xmm1
	movaps	%xmm15, %xmm6
	addss	%xmm6, %xmm1
	movss	-60(%rbp), %xmm0
	addss	-80(%rbp), %xmm0
	movss	-64(%rbp), %xmm8
	addss	-84(%rbp), %xmm8
	movss	-68(%rbp), %xmm9
	addss	-88(%rbp), %xmm9
	movss	-72(%rbp), %xmm10
	addss	-92(%rbp), %xmm10
	movss	-76(%rbp), %xmm11
	addss	-96(%rbp), %xmm11
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm3
	addss	%xmm8, %xmm3
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm6
	addss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm3
	addss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm7
	addss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm7, %xmm5
	addss	%xmm7, %xmm5
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	movaps	%xmm11, %xmm4
	addss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm6
	addss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm3, %xmm7
	addss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm7
	addss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm12
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movss	%xmm6, -52(%rbp)
	movaps	%xmm1, %xmm5
	addss	%xmm1, %xmm5
	movss	%xmm5, -56(%rbp)
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movss	%xmm6, -60(%rbp)
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movss	%xmm7, -64(%rbp)
	movaps	%xmm9, %xmm3
	addss	%xmm9, %xmm3
	movss	%xmm3, -68(%rbp)
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movss	%xmm4, -72(%rbp)
	movaps	%xmm11, %xmm3
	addss	%xmm11, %xmm3
	movss	%xmm3, -76(%rbp)
.L520:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L521
	cvttss2sil	%xmm12, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE176:
	.size	float_add_7, .-float_add_7
	.globl	float_add_8
	.type	float_add_8, @function
float_add_8:
.LFB177:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movaps	%xmm1, %xmm13
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movaps	%xmm2, %xmm14
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movaps	%xmm2, %xmm15
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movss	%xmm3, -84(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movss	%xmm4, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -92(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movss	%xmm3, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movss	%xmm4, -100(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm6, %xmm6
	cvtsi2ssl	%eax, %xmm6
	movss	%xmm6, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -108(%rbp)
	jmp	.L523
.L524:
	movaps	%xmm13, %xmm2
	movaps	%xmm14, %xmm3
	addss	%xmm3, %xmm2
	movss	-52(%rbp), %xmm5
	movaps	%xmm15, %xmm7
	addss	%xmm7, %xmm5
	movss	-56(%rbp), %xmm1
	addss	-84(%rbp), %xmm1
	movss	-60(%rbp), %xmm0
	addss	-88(%rbp), %xmm0
	movss	-64(%rbp), %xmm8
	addss	-92(%rbp), %xmm8
	movss	-68(%rbp), %xmm9
	addss	-96(%rbp), %xmm9
	movss	-72(%rbp), %xmm10
	addss	-100(%rbp), %xmm10
	movss	-76(%rbp), %xmm11
	addss	-104(%rbp), %xmm11
	movss	-80(%rbp), %xmm12
	addss	-108(%rbp), %xmm12
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm3
	addss	%xmm8, %xmm3
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm6
	addss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm12, %xmm4
	addss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm6
	addss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm3
	addss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	movaps	%xmm12, %xmm3
	addss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm7
	addss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	movaps	%xmm12, %xmm7
	addss	%xmm12, %xmm7
	movaps	%xmm7, %xmm12
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm5
	addss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm7, %xmm5
	addss	%xmm7, %xmm5
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	movaps	%xmm11, %xmm4
	addss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	movaps	%xmm12, %xmm4
	addss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm6
	addss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm12, %xmm6
	addss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm3, %xmm7
	addss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm7
	addss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	movaps	%xmm12, %xmm7
	addss	%xmm12, %xmm7
	movaps	%xmm7, %xmm12
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm5
	addss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm13
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movss	%xmm6, -52(%rbp)
	movaps	%xmm1, %xmm5
	addss	%xmm1, %xmm5
	movss	%xmm5, -56(%rbp)
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movss	%xmm6, -60(%rbp)
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movss	%xmm7, -64(%rbp)
	movaps	%xmm9, %xmm3
	addss	%xmm9, %xmm3
	movss	%xmm3, -68(%rbp)
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movss	%xmm4, -72(%rbp)
	movaps	%xmm11, %xmm3
	addss	%xmm11, %xmm3
	movss	%xmm3, -76(%rbp)
	movaps	%xmm12, %xmm2
	addss	%xmm12, %xmm2
	movss	%xmm2, -80(%rbp)
.L523:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L524
	cvttss2sil	%xmm13, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE177:
	.size	float_add_8, .-float_add_8
	.globl	float_add_9
	.type	float_add_9, @function
float_add_9:
.LFB178:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movaps	%xmm1, %xmm14
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movaps	%xmm2, %xmm15
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -92(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movss	%xmm3, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -100(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movss	%xmm4, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movss	%xmm3, -108(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2ssl	%eax, %xmm5
	movss	%xmm5, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -116(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movss	%xmm4, -120(%rbp)
	jmp	.L526
.L527:
	movaps	%xmm14, %xmm2
	movaps	%xmm15, %xmm6
	addss	%xmm6, %xmm2
	movss	-52(%rbp), %xmm5
	addss	-88(%rbp), %xmm5
	movss	-56(%rbp), %xmm1
	addss	-92(%rbp), %xmm1
	movss	-60(%rbp), %xmm0
	addss	-96(%rbp), %xmm0
	movss	-64(%rbp), %xmm8
	addss	-100(%rbp), %xmm8
	movss	-68(%rbp), %xmm9
	addss	-104(%rbp), %xmm9
	movss	-72(%rbp), %xmm10
	addss	-108(%rbp), %xmm10
	movss	-76(%rbp), %xmm11
	addss	-112(%rbp), %xmm11
	movss	-80(%rbp), %xmm12
	addss	-116(%rbp), %xmm12
	movss	-84(%rbp), %xmm13
	addss	-120(%rbp), %xmm13
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm3
	addss	%xmm8, %xmm3
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm6
	addss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm12, %xmm4
	addss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	movaps	%xmm13, %xmm4
	addss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm6
	addss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	movaps	%xmm13, %xmm6
	addss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm3
	addss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	movaps	%xmm12, %xmm3
	addss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	movaps	%xmm13, %xmm3
	addss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm7
	addss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	movaps	%xmm12, %xmm7
	addss	%xmm12, %xmm7
	movaps	%xmm7, %xmm12
	movaps	%xmm13, %xmm3
	addss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm5
	addss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	movaps	%xmm13, %xmm5
	addss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm7, %xmm5
	addss	%xmm7, %xmm5
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	movaps	%xmm11, %xmm4
	addss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	movaps	%xmm12, %xmm4
	addss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	movaps	%xmm13, %xmm4
	addss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm6
	addss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm12, %xmm6
	addss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	movaps	%xmm13, %xmm7
	addss	%xmm13, %xmm7
	movaps	%xmm7, %xmm13
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm3, %xmm7
	addss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm7
	addss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	movaps	%xmm12, %xmm7
	addss	%xmm12, %xmm7
	movaps	%xmm7, %xmm12
	movaps	%xmm13, %xmm6
	addss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm5
	addss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	movaps	%xmm13, %xmm7
	addss	%xmm13, %xmm7
	movaps	%xmm7, %xmm13
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm14
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movss	%xmm6, -52(%rbp)
	movaps	%xmm1, %xmm5
	addss	%xmm1, %xmm5
	movss	%xmm5, -56(%rbp)
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movss	%xmm6, -60(%rbp)
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movss	%xmm7, -64(%rbp)
	movaps	%xmm9, %xmm3
	addss	%xmm9, %xmm3
	movss	%xmm3, -68(%rbp)
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movss	%xmm4, -72(%rbp)
	movaps	%xmm11, %xmm3
	addss	%xmm11, %xmm3
	movss	%xmm3, -76(%rbp)
	movaps	%xmm12, %xmm2
	addss	%xmm12, %xmm2
	movss	%xmm2, -80(%rbp)
	movaps	%xmm13, %xmm2
	addss	%xmm13, %xmm2
	movss	%xmm2, -84(%rbp)
.L526:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L527
	cvttss2sil	%xmm14, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE178:
	.size	float_add_9, .-float_add_9
	.globl	float_add_10
	.type	float_add_10, @function
float_add_10:
.LFB179:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movaps	%xmm1, %xmm14
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movaps	%xmm2, %xmm15
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -92(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -100(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movss	%xmm4, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -108(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movss	%xmm3, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movss	%xmm4, -116(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2ssl	%eax, %xmm5
	movss	%xmm5, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2ssl	%eax, %xmm7
	movss	%xmm7, -124(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm6, %xmm6
	cvtsi2ssl	%eax, %xmm6
	movss	%xmm6, -128(%rbp)
	jmp	.L529
.L530:
	movaps	%xmm14, %xmm2
	movaps	%xmm15, %xmm3
	addss	%xmm3, %xmm2
	movss	-52(%rbp), %xmm5
	addss	-92(%rbp), %xmm5
	movss	-56(%rbp), %xmm1
	addss	-96(%rbp), %xmm1
	movss	-60(%rbp), %xmm6
	addss	-100(%rbp), %xmm6
	movss	-64(%rbp), %xmm0
	addss	-104(%rbp), %xmm0
	movss	-68(%rbp), %xmm8
	addss	-108(%rbp), %xmm8
	movss	-72(%rbp), %xmm9
	addss	-112(%rbp), %xmm9
	movss	-76(%rbp), %xmm10
	addss	-116(%rbp), %xmm10
	movss	-80(%rbp), %xmm11
	addss	-120(%rbp), %xmm11
	movss	-84(%rbp), %xmm12
	addss	-124(%rbp), %xmm12
	movss	-88(%rbp), %xmm13
	addss	-128(%rbp), %xmm13
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm6, %xmm3
	addss	%xmm6, %xmm3
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	movaps	%xmm11, %xmm4
	addss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	movaps	%xmm12, %xmm7
	addss	%xmm12, %xmm7
	movaps	%xmm7, %xmm12
	movaps	%xmm13, %xmm4
	addss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm6
	addss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	movaps	%xmm9, %xmm5
	addss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm6
	addss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm12, %xmm6
	addss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	movaps	%xmm13, %xmm7
	addss	%xmm13, %xmm7
	movaps	%xmm7, %xmm13
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm3
	addss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	movaps	%xmm9, %xmm3
	addss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm3
	addss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	movaps	%xmm12, %xmm3
	addss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	movaps	%xmm13, %xmm3
	addss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm9, %xmm3
	addss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm3
	addss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	movaps	%xmm12, %xmm3
	addss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	movaps	%xmm13, %xmm6
	addss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm5, %xmm6
	addss	%xmm5, %xmm6
	movaps	%xmm6, %xmm5
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm6
	addss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm7
	addss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	movaps	%xmm12, %xmm7
	addss	%xmm12, %xmm7
	movaps	%xmm7, %xmm12
	movaps	%xmm13, %xmm3
	addss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm9, %xmm4
	addss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	movaps	%xmm11, %xmm4
	addss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	movaps	%xmm12, %xmm4
	addss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	movaps	%xmm13, %xmm4
	addss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm6
	addss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm5
	addss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm5
	addss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	movaps	%xmm13, %xmm5
	addss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm3, %xmm7
	addss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm5
	addss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	movaps	%xmm9, %xmm5
	addss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	movaps	%xmm10, %xmm5
	addss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	movaps	%xmm11, %xmm6
	addss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm12, %xmm6
	addss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	movaps	%xmm13, %xmm7
	addss	%xmm13, %xmm7
	movaps	%xmm7, %xmm13
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm5
	addss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	movaps	%xmm13, %xmm6
	addss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm14
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movss	%xmm6, -52(%rbp)
	movaps	%xmm1, %xmm5
	addss	%xmm1, %xmm5
	movss	%xmm5, -56(%rbp)
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movss	%xmm7, -60(%rbp)
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movss	%xmm6, -64(%rbp)
	movaps	%xmm8, %xmm3
	addss	%xmm8, %xmm3
	movss	%xmm3, -68(%rbp)
	movaps	%xmm9, %xmm3
	addss	%xmm9, %xmm3
	movss	%xmm3, -72(%rbp)
	movaps	%xmm10, %xmm2
	addss	%xmm10, %xmm2
	movss	%xmm2, -76(%rbp)
	movaps	%xmm11, %xmm2
	addss	%xmm11, %xmm2
	movss	%xmm2, -80(%rbp)
	movaps	%xmm12, %xmm2
	addss	%xmm12, %xmm2
	movss	%xmm2, -84(%rbp)
	movaps	%xmm13, %xmm2
	addss	%xmm13, %xmm2
	movss	%xmm2, -88(%rbp)
.L529:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L530
	cvttss2sil	%xmm14, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE179:
	.size	float_add_10, .-float_add_10
	.globl	float_add_11
	.type	float_add_11, @function
float_add_11:
.LFB180:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movaps	%xmm1, %xmm15
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -100(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -108(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movss	%xmm3, -116(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movss	%xmm4, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -124(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movss	%xmm3, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movss	%xmm4, -132(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2ssl	%eax, %xmm5
	movss	%xmm5, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2ssl	%eax, %xmm7
	movss	%xmm7, -140(%rbp)
	jmp	.L532
.L533:
	movaps	%xmm15, %xmm2
	addss	-96(%rbp), %xmm2
	movss	-52(%rbp), %xmm5
	addss	-100(%rbp), %xmm5
	movss	-56(%rbp), %xmm1
	addss	-104(%rbp), %xmm1
	movss	-60(%rbp), %xmm6
	addss	-108(%rbp), %xmm6
	movss	-64(%rbp), %xmm0
	addss	-112(%rbp), %xmm0
	movss	-68(%rbp), %xmm8
	addss	-116(%rbp), %xmm8
	movss	-72(%rbp), %xmm9
	addss	-120(%rbp), %xmm9
	movss	-76(%rbp), %xmm10
	addss	-124(%rbp), %xmm10
	movss	-80(%rbp), %xmm11
	addss	-128(%rbp), %xmm11
	movss	-84(%rbp), %xmm12
	addss	-132(%rbp), %xmm12
	movss	-88(%rbp), %xmm13
	addss	-136(%rbp), %xmm13
	movss	-92(%rbp), %xmm14
	addss	-140(%rbp), %xmm14
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm6, %xmm3
	addss	%xmm6, %xmm3
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	movaps	%xmm11, %xmm4
	addss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	movaps	%xmm12, %xmm7
	addss	%xmm12, %xmm7
	movaps	%xmm7, %xmm12
	movaps	%xmm13, %xmm4
	addss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	movaps	%xmm14, %xmm4
	addss	%xmm14, %xmm4
	movaps	%xmm4, %xmm14
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm6
	addss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	movaps	%xmm9, %xmm5
	addss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm6
	addss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm12, %xmm6
	addss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	movaps	%xmm13, %xmm7
	addss	%xmm13, %xmm7
	movaps	%xmm7, %xmm13
	movaps	%xmm14, %xmm5
	addss	%xmm14, %xmm5
	movaps	%xmm5, %xmm14
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm3
	addss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	movaps	%xmm9, %xmm3
	addss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm3
	addss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	movaps	%xmm12, %xmm3
	addss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	movaps	%xmm13, %xmm3
	addss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	movaps	%xmm14, %xmm3
	addss	%xmm14, %xmm3
	movaps	%xmm3, %xmm14
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm9, %xmm3
	addss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm3
	addss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	movaps	%xmm12, %xmm3
	addss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	movaps	%xmm13, %xmm6
	addss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	movaps	%xmm14, %xmm7
	addss	%xmm14, %xmm7
	movaps	%xmm7, %xmm14
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm5, %xmm6
	addss	%xmm5, %xmm6
	movaps	%xmm6, %xmm5
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm6
	addss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm7
	addss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	movaps	%xmm12, %xmm7
	addss	%xmm12, %xmm7
	movaps	%xmm7, %xmm12
	movaps	%xmm13, %xmm3
	addss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	movaps	%xmm14, %xmm6
	addss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm9, %xmm4
	addss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	movaps	%xmm11, %xmm4
	addss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	movaps	%xmm12, %xmm4
	addss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	movaps	%xmm13, %xmm4
	addss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	movaps	%xmm14, %xmm4
	addss	%xmm14, %xmm4
	movaps	%xmm4, %xmm14
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm6
	addss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm5
	addss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm5
	addss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	movaps	%xmm13, %xmm5
	addss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	movaps	%xmm14, %xmm5
	addss	%xmm14, %xmm5
	movaps	%xmm5, %xmm14
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm3, %xmm7
	addss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm5
	addss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	movaps	%xmm9, %xmm5
	addss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	movaps	%xmm10, %xmm5
	addss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	movaps	%xmm11, %xmm6
	addss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm12, %xmm6
	addss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	movaps	%xmm13, %xmm7
	addss	%xmm13, %xmm7
	movaps	%xmm7, %xmm13
	movaps	%xmm14, %xmm7
	addss	%xmm14, %xmm7
	movaps	%xmm7, %xmm14
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm5
	addss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	movaps	%xmm13, %xmm6
	addss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	movaps	%xmm14, %xmm6
	addss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm15
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movss	%xmm6, -52(%rbp)
	movaps	%xmm1, %xmm5
	addss	%xmm1, %xmm5
	movss	%xmm5, -56(%rbp)
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movss	%xmm7, -60(%rbp)
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movss	%xmm6, -64(%rbp)
	movaps	%xmm8, %xmm3
	addss	%xmm8, %xmm3
	movss	%xmm3, -68(%rbp)
	movaps	%xmm9, %xmm3
	addss	%xmm9, %xmm3
	movss	%xmm3, -72(%rbp)
	movaps	%xmm10, %xmm2
	addss	%xmm10, %xmm2
	movss	%xmm2, -76(%rbp)
	movaps	%xmm11, %xmm2
	addss	%xmm11, %xmm2
	movss	%xmm2, -80(%rbp)
	movaps	%xmm12, %xmm2
	addss	%xmm12, %xmm2
	movss	%xmm2, -84(%rbp)
	movaps	%xmm13, %xmm2
	addss	%xmm13, %xmm2
	movss	%xmm2, -88(%rbp)
	movaps	%xmm14, %xmm2
	addss	%xmm14, %xmm2
	movss	%xmm2, -92(%rbp)
.L532:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L533
	cvttss2sil	%xmm15, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE180:
	.size	float_add_11, .-float_add_11
	.globl	float_add_12
	.type	float_add_12, @function
float_add_12:
.LFB181:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -108(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -116(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -124(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movss	%xmm3, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -132(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movss	%xmm4, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm6, %xmm6
	cvtsi2ssl	%eax, %xmm6
	movss	%xmm6, -140(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movss	%xmm3, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -148(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movss	%xmm4, -152(%rbp)
	jmp	.L535
.L536:
	movss	-52(%rbp), %xmm2
	addss	-104(%rbp), %xmm2
	movss	-56(%rbp), %xmm5
	addss	-108(%rbp), %xmm5
	movss	-60(%rbp), %xmm1
	addss	-112(%rbp), %xmm1
	movss	-64(%rbp), %xmm6
	addss	-116(%rbp), %xmm6
	movss	-68(%rbp), %xmm0
	addss	-120(%rbp), %xmm0
	movss	-72(%rbp), %xmm8
	addss	-124(%rbp), %xmm8
	movss	-76(%rbp), %xmm9
	addss	-128(%rbp), %xmm9
	movss	-80(%rbp), %xmm10
	addss	-132(%rbp), %xmm10
	movss	-84(%rbp), %xmm11
	addss	-136(%rbp), %xmm11
	movss	-88(%rbp), %xmm12
	addss	-140(%rbp), %xmm12
	movss	-92(%rbp), %xmm13
	addss	-144(%rbp), %xmm13
	movss	-96(%rbp), %xmm14
	addss	-148(%rbp), %xmm14
	movss	-100(%rbp), %xmm15
	addss	-152(%rbp), %xmm15
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm6, %xmm3
	addss	%xmm6, %xmm3
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	movaps	%xmm11, %xmm4
	addss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	movaps	%xmm12, %xmm7
	addss	%xmm12, %xmm7
	movaps	%xmm7, %xmm12
	movaps	%xmm13, %xmm4
	addss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	movaps	%xmm14, %xmm4
	addss	%xmm14, %xmm4
	movaps	%xmm4, %xmm14
	movaps	%xmm15, %xmm4
	addss	%xmm15, %xmm4
	movaps	%xmm4, %xmm15
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm6
	addss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	movaps	%xmm9, %xmm5
	addss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm6
	addss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm12, %xmm6
	addss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	movaps	%xmm13, %xmm7
	addss	%xmm13, %xmm7
	movaps	%xmm7, %xmm13
	movaps	%xmm14, %xmm5
	addss	%xmm14, %xmm5
	movaps	%xmm5, %xmm14
	movaps	%xmm15, %xmm5
	addss	%xmm15, %xmm5
	movaps	%xmm5, %xmm15
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm3
	addss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	movaps	%xmm9, %xmm3
	addss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm3
	addss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	movaps	%xmm12, %xmm3
	addss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	movaps	%xmm13, %xmm3
	addss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	movaps	%xmm14, %xmm3
	addss	%xmm14, %xmm3
	movaps	%xmm3, %xmm14
	movaps	%xmm15, %xmm3
	addss	%xmm15, %xmm3
	movaps	%xmm3, %xmm15
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm9, %xmm3
	addss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm3
	addss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	movaps	%xmm12, %xmm3
	addss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	movaps	%xmm13, %xmm6
	addss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	movaps	%xmm14, %xmm7
	addss	%xmm14, %xmm7
	movaps	%xmm7, %xmm14
	movaps	%xmm15, %xmm3
	addss	%xmm15, %xmm3
	movaps	%xmm3, %xmm15
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm5, %xmm6
	addss	%xmm5, %xmm6
	movaps	%xmm6, %xmm5
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm6
	addss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm7
	addss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	movaps	%xmm12, %xmm7
	addss	%xmm12, %xmm7
	movaps	%xmm7, %xmm12
	movaps	%xmm13, %xmm3
	addss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	movaps	%xmm14, %xmm6
	addss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	movaps	%xmm15, %xmm7
	addss	%xmm15, %xmm7
	movaps	%xmm7, %xmm15
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm9, %xmm4
	addss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	movaps	%xmm11, %xmm4
	addss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	movaps	%xmm12, %xmm4
	addss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	movaps	%xmm13, %xmm4
	addss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	movaps	%xmm14, %xmm4
	addss	%xmm14, %xmm4
	movaps	%xmm4, %xmm14
	movaps	%xmm15, %xmm4
	addss	%xmm15, %xmm4
	movaps	%xmm4, %xmm15
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm6
	addss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm5
	addss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm5
	addss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	movaps	%xmm13, %xmm5
	addss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	movaps	%xmm14, %xmm5
	addss	%xmm14, %xmm5
	movaps	%xmm5, %xmm14
	movaps	%xmm15, %xmm6
	addss	%xmm15, %xmm6
	movaps	%xmm6, %xmm15
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm3, %xmm7
	addss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm5
	addss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	movaps	%xmm9, %xmm5
	addss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	movaps	%xmm10, %xmm5
	addss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	movaps	%xmm11, %xmm6
	addss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm12, %xmm6
	addss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	movaps	%xmm13, %xmm7
	addss	%xmm13, %xmm7
	movaps	%xmm7, %xmm13
	movaps	%xmm14, %xmm7
	addss	%xmm14, %xmm7
	movaps	%xmm7, %xmm14
	movaps	%xmm15, %xmm5
	addss	%xmm15, %xmm5
	movaps	%xmm5, %xmm15
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm5
	addss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	movaps	%xmm13, %xmm6
	addss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	movaps	%xmm14, %xmm6
	addss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	movaps	%xmm15, %xmm7
	addss	%xmm15, %xmm7
	movaps	%xmm7, %xmm15
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movss	%xmm7, -52(%rbp)
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movss	%xmm6, -56(%rbp)
	movaps	%xmm1, %xmm5
	addss	%xmm1, %xmm5
	movss	%xmm5, -60(%rbp)
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movss	%xmm7, -64(%rbp)
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movss	%xmm6, -68(%rbp)
	movaps	%xmm8, %xmm3
	addss	%xmm8, %xmm3
	movss	%xmm3, -72(%rbp)
	movaps	%xmm9, %xmm3
	addss	%xmm9, %xmm3
	movss	%xmm3, -76(%rbp)
	movaps	%xmm10, %xmm2
	addss	%xmm10, %xmm2
	movss	%xmm2, -80(%rbp)
	movaps	%xmm11, %xmm2
	addss	%xmm11, %xmm2
	movss	%xmm2, -84(%rbp)
	movaps	%xmm12, %xmm2
	addss	%xmm12, %xmm2
	movss	%xmm2, -88(%rbp)
	movaps	%xmm13, %xmm2
	addss	%xmm13, %xmm2
	movss	%xmm2, -92(%rbp)
	movaps	%xmm14, %xmm2
	addss	%xmm14, %xmm2
	movss	%xmm2, -96(%rbp)
	movaps	%xmm15, %xmm2
	addss	%xmm15, %xmm2
	movss	%xmm2, -100(%rbp)
.L535:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L536
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE181:
	.size	float_add_12, .-float_add_12
	.globl	float_add_13
	.type	float_add_13, @function
float_add_13:
.LFB182:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -108(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -116(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -124(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -132(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movss	%xmm4, -140(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movss	%xmm3, -148(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movss	%xmm4, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -156(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2ssl	%eax, %xmm3
	movss	%xmm3, -160(%rbp)
	jmp	.L538
.L539:
	movss	-56(%rbp), %xmm2
	addss	-108(%rbp), %xmm2
	movss	-60(%rbp), %xmm5
	addss	-112(%rbp), %xmm5
	movss	-64(%rbp), %xmm1
	addss	-116(%rbp), %xmm1
	movss	-68(%rbp), %xmm6
	addss	-120(%rbp), %xmm6
	movss	-72(%rbp), %xmm0
	addss	-124(%rbp), %xmm0
	movss	-76(%rbp), %xmm8
	addss	-128(%rbp), %xmm8
	movss	-80(%rbp), %xmm9
	addss	-132(%rbp), %xmm9
	movss	-84(%rbp), %xmm10
	addss	-136(%rbp), %xmm10
	movss	-88(%rbp), %xmm11
	addss	-140(%rbp), %xmm11
	movss	-92(%rbp), %xmm12
	addss	-144(%rbp), %xmm12
	movss	-96(%rbp), %xmm13
	addss	-148(%rbp), %xmm13
	movss	-100(%rbp), %xmm14
	addss	-152(%rbp), %xmm14
	movss	-104(%rbp), %xmm15
	addss	-156(%rbp), %xmm15
	movss	-52(%rbp), %xmm4
	addss	-160(%rbp), %xmm4
	movss	%xmm4, -52(%rbp)
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm6, %xmm3
	addss	%xmm6, %xmm3
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	movaps	%xmm11, %xmm4
	addss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	movaps	%xmm12, %xmm4
	addss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	movaps	%xmm13, %xmm4
	addss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	movaps	%xmm14, %xmm4
	addss	%xmm14, %xmm4
	movaps	%xmm4, %xmm14
	movaps	%xmm15, %xmm4
	addss	%xmm15, %xmm4
	movaps	%xmm4, %xmm15
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm4
	addss	%xmm6, %xmm4
	movss	%xmm4, -52(%rbp)
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm6
	addss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	movaps	%xmm9, %xmm5
	addss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	movaps	%xmm10, %xmm5
	addss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	movaps	%xmm11, %xmm7
	addss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	movaps	%xmm12, %xmm6
	addss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	movaps	%xmm13, %xmm5
	addss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	movaps	%xmm14, %xmm7
	addss	%xmm14, %xmm7
	movaps	%xmm7, %xmm14
	movaps	%xmm15, %xmm5
	addss	%xmm15, %xmm5
	movaps	%xmm5, %xmm15
	movss	-52(%rbp), %xmm5
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movss	%xmm7, -52(%rbp)
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm3
	addss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	movaps	%xmm9, %xmm3
	addss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm3
	addss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	movaps	%xmm12, %xmm3
	addss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	movaps	%xmm13, %xmm3
	addss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	movaps	%xmm14, %xmm3
	addss	%xmm14, %xmm3
	movaps	%xmm3, %xmm14
	movaps	%xmm15, %xmm3
	addss	%xmm15, %xmm3
	movaps	%xmm3, %xmm15
	movss	-52(%rbp), %xmm7
	movaps	%xmm7, %xmm3
	addss	%xmm7, %xmm3
	movss	%xmm3, -52(%rbp)
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	movaps	%xmm0, %xmm1
	addss	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm3
	addss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	movaps	%xmm12, %xmm7
	addss	%xmm12, %xmm7
	movaps	%xmm7, %xmm12
	movaps	%xmm13, %xmm1
	addss	%xmm13, %xmm1
	movaps	%xmm1, %xmm13
	movaps	%xmm14, %xmm1
	addss	%xmm14, %xmm1
	movaps	%xmm1, %xmm14
	movaps	%xmm15, %xmm1
	addss	%xmm15, %xmm1
	movaps	%xmm1, %xmm15
	movss	-52(%rbp), %xmm3
	movaps	%xmm3, %xmm1
	addss	%xmm3, %xmm1
	movss	%xmm1, -52(%rbp)
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	movaps	%xmm6, %xmm7
	addss	%xmm6, %xmm7
	movaps	%xmm5, %xmm6
	addss	%xmm5, %xmm6
	movaps	%xmm6, %xmm5
	movaps	%xmm0, %xmm3
	addss	%xmm0, %xmm3
	movaps	%xmm3, %xmm0
	movaps	%xmm8, %xmm1
	addss	%xmm8, %xmm1
	movaps	%xmm1, %xmm8
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm6
	addss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm12, %xmm1
	addss	%xmm12, %xmm1
	movaps	%xmm1, %xmm12
	movaps	%xmm13, %xmm3
	addss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	movaps	%xmm14, %xmm1
	addss	%xmm14, %xmm1
	movaps	%xmm1, %xmm14
	movaps	%xmm15, %xmm3
	addss	%xmm15, %xmm3
	movaps	%xmm3, %xmm15
	movss	-52(%rbp), %xmm1
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm7, %xmm6
	addss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm4
	addss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	movaps	%xmm9, %xmm4
	addss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	movaps	%xmm11, %xmm4
	addss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	movaps	%xmm12, %xmm4
	addss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	movaps	%xmm13, %xmm6
	addss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	movaps	%xmm14, %xmm4
	addss	%xmm14, %xmm4
	movaps	%xmm4, %xmm14
	movaps	%xmm15, %xmm6
	addss	%xmm15, %xmm6
	movaps	%xmm6, %xmm15
	movaps	%xmm1, %xmm4
	addss	%xmm1, %xmm4
	movss	%xmm4, -52(%rbp)
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm7, %xmm6
	addss	%xmm7, %xmm6
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm0, %xmm7
	addss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	movaps	%xmm8, %xmm1
	addss	%xmm8, %xmm1
	movaps	%xmm1, %xmm8
	movaps	%xmm9, %xmm5
	addss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm5
	addss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	movaps	%xmm13, %xmm7
	addss	%xmm13, %xmm7
	movaps	%xmm7, %xmm13
	movaps	%xmm14, %xmm5
	addss	%xmm14, %xmm5
	movaps	%xmm5, %xmm14
	movaps	%xmm15, %xmm7
	addss	%xmm15, %xmm7
	movaps	%xmm7, %xmm15
	movss	-52(%rbp), %xmm1
	movaps	%xmm1, %xmm5
	addss	%xmm1, %xmm5
	movss	%xmm5, -52(%rbp)
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm3, %xmm7
	addss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	movaps	%xmm6, %xmm7
	addss	%xmm6, %xmm7
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm0, %xmm1
	addss	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	movaps	%xmm8, %xmm1
	addss	%xmm8, %xmm1
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm5
	addss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm6
	addss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	movaps	%xmm13, %xmm5
	addss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	movaps	%xmm14, %xmm6
	addss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	movaps	%xmm15, %xmm5
	addss	%xmm15, %xmm5
	movaps	%xmm5, %xmm15
	movss	-52(%rbp), %xmm5
	movaps	%xmm5, %xmm8
	addss	%xmm5, %xmm8
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm7, %xmm5
	addss	%xmm7, %xmm5
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm7
	addss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	movaps	%xmm12, %xmm7
	addss	%xmm12, %xmm7
	movaps	%xmm7, %xmm12
	movaps	%xmm13, %xmm6
	addss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	movaps	%xmm14, %xmm7
	addss	%xmm14, %xmm7
	movaps	%xmm7, %xmm14
	movaps	%xmm15, %xmm6
	addss	%xmm15, %xmm6
	movaps	%xmm6, %xmm15
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movss	%xmm7, -56(%rbp)
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movss	%xmm6, -60(%rbp)
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movss	%xmm7, -64(%rbp)
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movss	%xmm3, -68(%rbp)
	movaps	%xmm0, %xmm3
	addss	%xmm0, %xmm3
	movss	%xmm3, -72(%rbp)
	movaps	%xmm1, %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, -76(%rbp)
	movaps	%xmm9, %xmm2
	addss	%xmm9, %xmm2
	movss	%xmm2, -80(%rbp)
	movaps	%xmm10, %xmm2
	addss	%xmm10, %xmm2
	movss	%xmm2, -84(%rbp)
	movaps	%xmm11, %xmm2
	addss	%xmm11, %xmm2
	movss	%xmm2, -88(%rbp)
	movaps	%xmm12, %xmm2
	addss	%xmm12, %xmm2
	movss	%xmm2, -92(%rbp)
	movaps	%xmm13, %xmm2
	addss	%xmm13, %xmm2
	movss	%xmm2, -96(%rbp)
	movaps	%xmm14, %xmm2
	addss	%xmm14, %xmm2
	movss	%xmm2, -100(%rbp)
	movaps	%xmm15, %xmm2
	addss	%xmm15, %xmm2
	movss	%xmm2, -104(%rbp)
	movaps	%xmm8, %xmm2
	addss	%xmm8, %xmm2
	movss	%xmm2, -52(%rbp)
.L538:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L539
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE182:
	.size	float_add_13, .-float_add_13
	.globl	float_add_14
	.type	float_add_14, @function
float_add_14:
.LFB183:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -116(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -124(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -132(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -140(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -148(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movss	%xmm4, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -156(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -108(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movss	%xmm4, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -164(%rbp)
	movq	-24(%rbp), %rax
	movsd	192(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2ssl	%eax, %xmm4
	movss	%xmm4, -168(%rbp)
	jmp	.L541
.L542:
	movss	-60(%rbp), %xmm2
	addss	-112(%rbp), %xmm2
	movss	-64(%rbp), %xmm5
	addss	-116(%rbp), %xmm5
	movss	-68(%rbp), %xmm7
	addss	-120(%rbp), %xmm7
	movss	-72(%rbp), %xmm1
	addss	-124(%rbp), %xmm1
	movss	-76(%rbp), %xmm0
	addss	-128(%rbp), %xmm0
	movss	-80(%rbp), %xmm8
	addss	-132(%rbp), %xmm8
	movss	-84(%rbp), %xmm9
	addss	-136(%rbp), %xmm9
	movss	-88(%rbp), %xmm10
	addss	-140(%rbp), %xmm10
	movss	-92(%rbp), %xmm11
	addss	-144(%rbp), %xmm11
	movss	-96(%rbp), %xmm12
	addss	-148(%rbp), %xmm12
	movss	-100(%rbp), %xmm13
	addss	-152(%rbp), %xmm13
	movss	-104(%rbp), %xmm14
	addss	-156(%rbp), %xmm14
	movss	-108(%rbp), %xmm15
	addss	-160(%rbp), %xmm15
	movss	-56(%rbp), %xmm3
	addss	-164(%rbp), %xmm3
	movss	%xmm3, -56(%rbp)
	movss	-52(%rbp), %xmm6
	addss	-168(%rbp), %xmm6
	movss	%xmm6, -52(%rbp)
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm7, %xmm6
	addss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	movaps	%xmm1, %xmm3
	addss	%xmm1, %xmm3
	movaps	%xmm3, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm3
	addss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	movaps	%xmm9, %xmm4
	addss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm4
	addss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	movaps	%xmm12, %xmm4
	addss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	movaps	%xmm13, %xmm3
	addss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	movaps	%xmm14, %xmm4
	addss	%xmm14, %xmm4
	movaps	%xmm4, %xmm14
	movaps	%xmm15, %xmm4
	addss	%xmm15, %xmm4
	movaps	%xmm4, %xmm15
	movss	-56(%rbp), %xmm4
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movss	%xmm3, -56(%rbp)
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm4
	addss	%xmm6, %xmm4
	movss	%xmm4, -52(%rbp)
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm7, %xmm5
	addss	%xmm7, %xmm5
	movaps	%xmm5, %xmm7
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm8, %xmm3
	addss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	movaps	%xmm9, %xmm5
	addss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	movaps	%xmm10, %xmm5
	addss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	movaps	%xmm11, %xmm3
	addss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	movaps	%xmm12, %xmm3
	addss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	movaps	%xmm13, %xmm5
	addss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	movaps	%xmm14, %xmm3
	addss	%xmm14, %xmm3
	movaps	%xmm3, %xmm14
	movaps	%xmm15, %xmm5
	addss	%xmm15, %xmm5
	movaps	%xmm5, %xmm15
	movss	-56(%rbp), %xmm3
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movss	%xmm5, -56(%rbp)
	movss	-52(%rbp), %xmm5
	movaps	%xmm5, %xmm3
	addss	%xmm5, %xmm3
	movss	%xmm3, -52(%rbp)
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm7, %xmm5
	addss	%xmm7, %xmm5
	movaps	%xmm1, %xmm3
	addss	%xmm1, %xmm3
	movaps	%xmm3, %xmm1
	movaps	%xmm0, %xmm3
	addss	%xmm0, %xmm3
	movaps	%xmm3, %xmm0
	movaps	%xmm8, %xmm6
	addss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm6
	addss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm12, %xmm7
	addss	%xmm12, %xmm7
	movaps	%xmm7, %xmm12
	movaps	%xmm13, %xmm7
	addss	%xmm13, %xmm7
	movaps	%xmm7, %xmm13
	movaps	%xmm14, %xmm6
	addss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	movaps	%xmm15, %xmm6
	addss	%xmm15, %xmm6
	movaps	%xmm6, %xmm15
	movss	-56(%rbp), %xmm6
	movaps	%xmm6, %xmm7
	addss	%xmm6, %xmm7
	movss	%xmm7, -56(%rbp)
	movss	-52(%rbp), %xmm3
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movss	%xmm6, -52(%rbp)
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm5, %xmm6
	addss	%xmm5, %xmm6
	movaps	%xmm1, %xmm3
	addss	%xmm1, %xmm3
	movaps	%xmm3, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm8, %xmm5
	addss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm5
	addss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	movaps	%xmm13, %xmm3
	addss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	movaps	%xmm14, %xmm5
	addss	%xmm14, %xmm5
	movaps	%xmm5, %xmm14
	movaps	%xmm15, %xmm7
	addss	%xmm15, %xmm7
	movaps	%xmm7, %xmm15
	movss	-56(%rbp), %xmm7
	movaps	%xmm7, %xmm3
	addss	%xmm7, %xmm3
	movss	%xmm3, -56(%rbp)
	movss	-52(%rbp), %xmm7
	movaps	%xmm7, %xmm5
	addss	%xmm7, %xmm5
	movss	%xmm5, -52(%rbp)
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	movaps	%xmm6, %xmm7
	addss	%xmm6, %xmm7
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm3
	addss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	movaps	%xmm9, %xmm3
	addss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm3
	addss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	movaps	%xmm12, %xmm6
	addss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	movaps	%xmm13, %xmm5
	addss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	movaps	%xmm14, %xmm3
	addss	%xmm14, %xmm3
	movaps	%xmm3, %xmm14
	movaps	%xmm15, %xmm3
	addss	%xmm15, %xmm3
	movaps	%xmm3, %xmm15
	movss	-56(%rbp), %xmm3
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movss	%xmm6, -56(%rbp)
	movss	-52(%rbp), %xmm5
	movaps	%xmm5, %xmm3
	addss	%xmm5, %xmm3
	movss	%xmm3, -52(%rbp)
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm7, %xmm6
	addss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	movaps	%xmm1, %xmm4
	addss	%xmm1, %xmm4
	movaps	%xmm4, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm8, %xmm5
	addss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	movaps	%xmm9, %xmm4
	addss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	movaps	%xmm11, %xmm4
	addss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	movaps	%xmm12, %xmm4
	addss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	movaps	%xmm13, %xmm4
	addss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	movaps	%xmm14, %xmm4
	addss	%xmm14, %xmm4
	movaps	%xmm4, %xmm14
	movaps	%xmm15, %xmm4
	addss	%xmm15, %xmm4
	movaps	%xmm4, %xmm15
	movss	-56(%rbp), %xmm6
	movaps	%xmm6, %xmm4
	addss	%xmm6, %xmm4
	movss	%xmm4, -56(%rbp)
	movss	-52(%rbp), %xmm4
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movss	%xmm6, -52(%rbp)
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm7, %xmm6
	addss	%xmm7, %xmm6
	movaps	%xmm1, %xmm4
	addss	%xmm1, %xmm4
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm9, %xmm5
	addss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	movaps	%xmm10, %xmm5
	addss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	movaps	%xmm11, %xmm7
	addss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	movaps	%xmm12, %xmm7
	addss	%xmm12, %xmm7
	movaps	%xmm7, %xmm12
	movaps	%xmm13, %xmm1
	addss	%xmm13, %xmm1
	movaps	%xmm1, %xmm13
	movaps	%xmm14, %xmm1
	addss	%xmm14, %xmm1
	movaps	%xmm1, %xmm14
	movaps	%xmm15, %xmm1
	addss	%xmm15, %xmm1
	movaps	%xmm1, %xmm15
	movss	-56(%rbp), %xmm5
	movaps	%xmm5, %xmm1
	addss	%xmm5, %xmm1
	movss	%xmm1, -56(%rbp)
	movss	-52(%rbp), %xmm5
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movss	%xmm7, -52(%rbp)
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm3, %xmm7
	addss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	movaps	%xmm6, %xmm7
	addss	%xmm6, %xmm7
	movaps	%xmm4, %xmm1
	addss	%xmm4, %xmm1
	movaps	%xmm1, %xmm4
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm6
	addss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm1
	addss	%xmm12, %xmm1
	movaps	%xmm1, %xmm12
	movaps	%xmm13, %xmm6
	addss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	movaps	%xmm14, %xmm1
	addss	%xmm14, %xmm1
	movaps	%xmm1, %xmm14
	movaps	%xmm15, %xmm5
	addss	%xmm15, %xmm5
	movaps	%xmm5, %xmm15
	movss	-56(%rbp), %xmm1
	movaps	%xmm1, %xmm5
	addss	%xmm1, %xmm5
	movaps	%xmm5, %xmm1
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	addss	%xmm6, %xmm5
	movss	%xmm5, -52(%rbp)
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm7, %xmm5
	addss	%xmm7, %xmm5
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm7
	addss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm6
	addss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm12, %xmm6
	addss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	movaps	%xmm13, %xmm7
	addss	%xmm13, %xmm7
	movaps	%xmm7, %xmm13
	movaps	%xmm14, %xmm7
	addss	%xmm14, %xmm7
	movaps	%xmm7, %xmm14
	movaps	%xmm15, %xmm6
	addss	%xmm15, %xmm6
	movaps	%xmm6, %xmm15
	movaps	%xmm1, %xmm7
	addss	%xmm1, %xmm7
	movss	%xmm7, -56(%rbp)
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm1
	addss	%xmm6, %xmm1
	movss	%xmm1, -52(%rbp)
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movss	%xmm7, -60(%rbp)
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movss	%xmm6, -64(%rbp)
	movaps	%xmm5, %xmm1
	addss	%xmm5, %xmm1
	movss	%xmm1, -68(%rbp)
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movss	%xmm3, -72(%rbp)
	movaps	%xmm0, %xmm3
	addss	%xmm0, %xmm3
	movss	%xmm3, -76(%rbp)
	movaps	%xmm8, %xmm3
	addss	%xmm8, %xmm3
	movss	%xmm3, -80(%rbp)
	movaps	%xmm9, %xmm2
	addss	%xmm9, %xmm2
	movss	%xmm2, -84(%rbp)
	movaps	%xmm10, %xmm2
	addss	%xmm10, %xmm2
	movss	%xmm2, -88(%rbp)
	movaps	%xmm11, %xmm2
	addss	%xmm11, %xmm2
	movss	%xmm2, -92(%rbp)
	movaps	%xmm12, %xmm2
	addss	%xmm12, %xmm2
	movss	%xmm2, -96(%rbp)
	movaps	%xmm13, %xmm2
	addss	%xmm13, %xmm2
	movss	%xmm2, -100(%rbp)
	movaps	%xmm14, %xmm2
	addss	%xmm14, %xmm2
	movss	%xmm2, -104(%rbp)
	movaps	%xmm15, %xmm2
	addss	%xmm15, %xmm2
	movss	%xmm2, -108(%rbp)
	movss	-56(%rbp), %xmm7
	movaps	%xmm7, %xmm2
	addss	%xmm7, %xmm2
	movss	%xmm2, -56(%rbp)
	movss	-52(%rbp), %xmm1
	movaps	%xmm1, %xmm2
	addss	%xmm1, %xmm2
	movss	%xmm2, -52(%rbp)
.L541:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L542
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE183:
	.size	float_add_14, .-float_add_14
	.globl	float_add_15
	.type	float_add_15, @function
float_add_15:
.LFB184:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -116(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -124(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -132(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -140(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -148(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -156(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -108(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -164(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	192(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -172(%rbp)
	movq	-24(%rbp), %rax
	movsd	200(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC3(%rip), %xmm0
	addss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2ssl	%eax, %xmm2
	movss	%xmm2, -176(%rbp)
	jmp	.L544
.L545:
	movss	-64(%rbp), %xmm2
	addss	-116(%rbp), %xmm2
	movss	-68(%rbp), %xmm5
	addss	-120(%rbp), %xmm5
	movss	-72(%rbp), %xmm7
	addss	-124(%rbp), %xmm7
	movss	-76(%rbp), %xmm1
	addss	-128(%rbp), %xmm1
	movss	-80(%rbp), %xmm0
	addss	-132(%rbp), %xmm0
	movss	-84(%rbp), %xmm8
	addss	-136(%rbp), %xmm8
	movss	-88(%rbp), %xmm9
	addss	-140(%rbp), %xmm9
	movss	-92(%rbp), %xmm10
	addss	-144(%rbp), %xmm10
	movss	-96(%rbp), %xmm11
	addss	-148(%rbp), %xmm11
	movss	-100(%rbp), %xmm12
	addss	-152(%rbp), %xmm12
	movss	-104(%rbp), %xmm13
	addss	-156(%rbp), %xmm13
	movss	-108(%rbp), %xmm14
	addss	-160(%rbp), %xmm14
	movss	-112(%rbp), %xmm15
	addss	-164(%rbp), %xmm15
	movss	-60(%rbp), %xmm4
	addss	-168(%rbp), %xmm4
	movss	%xmm4, -60(%rbp)
	movss	-52(%rbp), %xmm6
	addss	-172(%rbp), %xmm6
	movss	%xmm6, -52(%rbp)
	movss	-56(%rbp), %xmm4
	addss	-176(%rbp), %xmm4
	movss	%xmm4, -56(%rbp)
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	movaps	%xmm7, %xmm6
	addss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	movaps	%xmm1, %xmm3
	addss	%xmm1, %xmm3
	movaps	%xmm3, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm3
	addss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	movaps	%xmm11, %xmm4
	addss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	movaps	%xmm12, %xmm3
	addss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	movaps	%xmm13, %xmm3
	addss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	movaps	%xmm14, %xmm3
	addss	%xmm14, %xmm3
	movaps	%xmm3, %xmm14
	movaps	%xmm15, %xmm4
	addss	%xmm15, %xmm4
	movaps	%xmm4, %xmm15
	movss	-60(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	addss	%xmm3, %xmm4
	movss	%xmm4, -60(%rbp)
	movss	-52(%rbp), %xmm3
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movss	%xmm6, -52(%rbp)
	movss	-56(%rbp), %xmm4
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movss	%xmm3, -56(%rbp)
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movaps	%xmm7, %xmm5
	addss	%xmm7, %xmm5
	movaps	%xmm5, %xmm7
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm8, %xmm5
	addss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	movaps	%xmm9, %xmm3
	addss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm3
	addss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	movaps	%xmm12, %xmm3
	addss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	movaps	%xmm13, %xmm6
	addss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	movaps	%xmm14, %xmm5
	addss	%xmm14, %xmm5
	movaps	%xmm5, %xmm14
	movaps	%xmm15, %xmm3
	addss	%xmm15, %xmm3
	movaps	%xmm3, %xmm15
	movss	-60(%rbp), %xmm5
	movaps	%xmm5, %xmm3
	addss	%xmm5, %xmm3
	movss	%xmm3, -60(%rbp)
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	addss	%xmm6, %xmm5
	movss	%xmm5, -52(%rbp)
	movss	-56(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	addss	%xmm6, %xmm5
	movss	%xmm5, -56(%rbp)
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm7, %xmm5
	addss	%xmm7, %xmm5
	movaps	%xmm1, %xmm3
	addss	%xmm1, %xmm3
	movaps	%xmm3, %xmm1
	movaps	%xmm0, %xmm3
	addss	%xmm0, %xmm3
	movaps	%xmm3, %xmm0
	movaps	%xmm8, %xmm6
	addss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm7
	addss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	movaps	%xmm12, %xmm7
	addss	%xmm12, %xmm7
	movaps	%xmm7, %xmm12
	movaps	%xmm13, %xmm7
	addss	%xmm13, %xmm7
	movaps	%xmm7, %xmm13
	movaps	%xmm14, %xmm6
	addss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	movaps	%xmm15, %xmm6
	addss	%xmm15, %xmm6
	movaps	%xmm6, %xmm15
	movss	-60(%rbp), %xmm3
	movaps	%xmm3, %xmm7
	addss	%xmm3, %xmm7
	movss	%xmm7, -60(%rbp)
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm3
	addss	%xmm6, %xmm3
	movss	%xmm3, -52(%rbp)
	movss	-56(%rbp), %xmm3
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movss	%xmm6, -56(%rbp)
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm5, %xmm6
	addss	%xmm5, %xmm6
	movaps	%xmm1, %xmm3
	addss	%xmm1, %xmm3
	movaps	%xmm3, %xmm1
	movaps	%xmm0, %xmm5
	addss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	movaps	%xmm8, %xmm5
	addss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	movaps	%xmm9, %xmm5
	addss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	movaps	%xmm10, %xmm5
	addss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm5
	addss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	movaps	%xmm13, %xmm3
	addss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	movaps	%xmm14, %xmm7
	addss	%xmm14, %xmm7
	movaps	%xmm7, %xmm14
	movaps	%xmm15, %xmm5
	addss	%xmm15, %xmm5
	movaps	%xmm5, %xmm15
	movss	-60(%rbp), %xmm7
	movaps	%xmm7, %xmm5
	addss	%xmm7, %xmm5
	movss	%xmm5, -60(%rbp)
	movss	-52(%rbp), %xmm3
	movaps	%xmm3, %xmm7
	addss	%xmm3, %xmm7
	movss	%xmm7, -52(%rbp)
	movss	-56(%rbp), %xmm3
	movaps	%xmm3, %xmm7
	addss	%xmm3, %xmm7
	movss	%xmm7, -56(%rbp)
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	movaps	%xmm6, %xmm7
	addss	%xmm6, %xmm7
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm8, %xmm3
	addss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm3
	addss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	movaps	%xmm11, %xmm6
	addss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm12, %xmm6
	addss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	movaps	%xmm13, %xmm5
	addss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	movaps	%xmm14, %xmm3
	addss	%xmm14, %xmm3
	movaps	%xmm3, %xmm14
	movaps	%xmm15, %xmm3
	addss	%xmm15, %xmm3
	movaps	%xmm3, %xmm15
	movss	-60(%rbp), %xmm5
	movaps	%xmm5, %xmm6
	addss	%xmm5, %xmm6
	movss	%xmm6, -60(%rbp)
	movss	-52(%rbp), %xmm3
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movss	%xmm5, -52(%rbp)
	movss	-56(%rbp), %xmm5
	movaps	%xmm5, %xmm3
	addss	%xmm5, %xmm3
	movss	%xmm3, -56(%rbp)
	movaps	%xmm2, %xmm3
	addss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movaps	%xmm7, %xmm6
	addss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	movaps	%xmm1, %xmm4
	addss	%xmm1, %xmm4
	movaps	%xmm4, %xmm1
	movaps	%xmm0, %xmm4
	addss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	movaps	%xmm8, %xmm5
	addss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	movaps	%xmm9, %xmm5
	addss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	movaps	%xmm10, %xmm4
	addss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	movaps	%xmm11, %xmm4
	addss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	movaps	%xmm12, %xmm4
	addss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	movaps	%xmm13, %xmm4
	addss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	movaps	%xmm14, %xmm4
	addss	%xmm14, %xmm4
	movaps	%xmm4, %xmm14
	movaps	%xmm15, %xmm4
	addss	%xmm15, %xmm4
	movaps	%xmm4, %xmm15
	movss	-60(%rbp), %xmm6
	movaps	%xmm6, %xmm4
	addss	%xmm6, %xmm4
	movss	%xmm4, -60(%rbp)
	movss	-52(%rbp), %xmm5
	movaps	%xmm5, %xmm6
	addss	%xmm5, %xmm6
	movss	%xmm6, -52(%rbp)
	movss	-56(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	addss	%xmm6, %xmm5
	movss	%xmm5, -56(%rbp)
	movaps	%xmm2, %xmm4
	addss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm7, %xmm6
	addss	%xmm7, %xmm6
	movaps	%xmm1, %xmm4
	addss	%xmm1, %xmm4
	movaps	%xmm4, %xmm1
	movaps	%xmm0, %xmm4
	addss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	movaps	%xmm8, %xmm5
	addss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm7
	addss	%xmm10, %xmm7
	movaps	%xmm7, %xmm10
	movaps	%xmm11, %xmm7
	addss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	movaps	%xmm12, %xmm4
	addss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	movaps	%xmm13, %xmm4
	addss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	movaps	%xmm14, %xmm5
	addss	%xmm14, %xmm5
	movaps	%xmm5, %xmm14
	movaps	%xmm15, %xmm7
	addss	%xmm15, %xmm7
	movaps	%xmm7, %xmm15
	movss	-60(%rbp), %xmm4
	movaps	%xmm4, %xmm7
	addss	%xmm4, %xmm7
	movss	%xmm7, -60(%rbp)
	movss	-52(%rbp), %xmm7
	movaps	%xmm7, %xmm4
	addss	%xmm7, %xmm4
	movss	%xmm4, -52(%rbp)
	movss	-56(%rbp), %xmm5
	movaps	%xmm5, %xmm4
	addss	%xmm5, %xmm4
	movss	%xmm4, -56(%rbp)
	movaps	%xmm2, %xmm5
	addss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm3, %xmm7
	addss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	movaps	%xmm6, %xmm7
	addss	%xmm6, %xmm7
	movaps	%xmm1, %xmm4
	addss	%xmm1, %xmm4
	movaps	%xmm4, %xmm1
	movaps	%xmm0, %xmm4
	addss	%xmm0, %xmm4
	movaps	%xmm8, %xmm0
	addss	%xmm8, %xmm0
	movaps	%xmm9, %xmm6
	addss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm10, %xmm5
	addss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	movaps	%xmm11, %xmm5
	addss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	movaps	%xmm12, %xmm5
	addss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	movaps	%xmm13, %xmm6
	addss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	movaps	%xmm14, %xmm6
	addss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	movaps	%xmm15, %xmm5
	addss	%xmm15, %xmm5
	movaps	%xmm5, %xmm15
	movss	-60(%rbp), %xmm5
	movaps	%xmm5, %xmm8
	addss	%xmm5, %xmm8
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	addss	%xmm6, %xmm5
	movss	%xmm5, -52(%rbp)
	movss	-56(%rbp), %xmm5
	movaps	%xmm5, %xmm6
	addss	%xmm5, %xmm6
	movss	%xmm6, -56(%rbp)
	movaps	%xmm2, %xmm6
	addss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm3, %xmm5
	addss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	movaps	%xmm7, %xmm5
	addss	%xmm7, %xmm5
	movaps	%xmm1, %xmm6
	addss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm4, %xmm6
	addss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm0, %xmm6
	addss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movaps	%xmm9, %xmm7
	addss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movaps	%xmm10, %xmm6
	addss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movaps	%xmm11, %xmm6
	addss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm12, %xmm7
	addss	%xmm12, %xmm7
	movaps	%xmm7, %xmm12
	movaps	%xmm13, %xmm7
	addss	%xmm13, %xmm7
	movaps	%xmm7, %xmm13
	movaps	%xmm14, %xmm7
	addss	%xmm14, %xmm7
	movaps	%xmm7, %xmm14
	movaps	%xmm15, %xmm6
	addss	%xmm15, %xmm6
	movaps	%xmm6, %xmm15
	movaps	%xmm8, %xmm6
	addss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm7
	addss	%xmm6, %xmm7
	movss	%xmm7, -52(%rbp)
	movss	-56(%rbp), %xmm7
	movaps	%xmm7, %xmm6
	addss	%xmm7, %xmm6
	movss	%xmm6, -56(%rbp)
	movaps	%xmm2, %xmm7
	addss	%xmm2, %xmm7
	movss	%xmm7, -64(%rbp)
	movaps	%xmm3, %xmm6
	addss	%xmm3, %xmm6
	movss	%xmm6, -68(%rbp)
	movaps	%xmm5, %xmm7
	addss	%xmm5, %xmm7
	movss	%xmm7, -72(%rbp)
	movaps	%xmm1, %xmm3
	addss	%xmm1, %xmm3
	movss	%xmm3, -76(%rbp)
	movaps	%xmm4, %xmm3
	addss	%xmm4, %xmm3
	movss	%xmm3, -80(%rbp)
	movaps	%xmm0, %xmm3
	addss	%xmm0, %xmm3
	movss	%xmm3, -84(%rbp)
	movaps	%xmm9, %xmm3
	addss	%xmm9, %xmm3
	movss	%xmm3, -88(%rbp)
	movaps	%xmm10, %xmm2
	addss	%xmm10, %xmm2
	movss	%xmm2, -92(%rbp)
	movaps	%xmm11, %xmm2
	addss	%xmm11, %xmm2
	movss	%xmm2, -96(%rbp)
	movaps	%xmm12, %xmm2
	addss	%xmm12, %xmm2
	movss	%xmm2, -100(%rbp)
	movaps	%xmm13, %xmm2
	addss	%xmm13, %xmm2
	movss	%xmm2, -104(%rbp)
	movaps	%xmm14, %xmm2
	addss	%xmm14, %xmm2
	movss	%xmm2, -108(%rbp)
	movaps	%xmm15, %xmm2
	addss	%xmm15, %xmm2
	movss	%xmm2, -112(%rbp)
	movaps	%xmm8, %xmm2
	addss	%xmm8, %xmm2
	movss	%xmm2, -60(%rbp)
	movss	-52(%rbp), %xmm7
	movaps	%xmm7, %xmm2
	addss	%xmm7, %xmm2
	movss	%xmm2, -52(%rbp)
	movss	-56(%rbp), %xmm6
	movaps	%xmm6, %xmm2
	addss	%xmm6, %xmm2
	movss	%xmm2, -56(%rbp)
.L544:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L545
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE184:
	.size	float_add_15, .-float_add_15
	.globl	float_add_benchmarks
	.section	.data.rel.local
	.align 32
	.type	float_add_benchmarks, @object
	.size	float_add_benchmarks, 128
float_add_benchmarks:
	.quad	float_add_0
	.quad	float_add_1
	.quad	float_add_2
	.quad	float_add_3
	.quad	float_add_4
	.quad	float_add_5
	.quad	float_add_6
	.quad	float_add_7
	.quad	float_add_8
	.quad	float_add_9
	.quad	float_add_10
	.quad	float_add_11
	.quad	float_add_12
	.quad	float_add_13
	.quad	float_add_14
	.quad	float_add_15
	.text
	.globl	float_mul_0
	.type	float_mul_0, @function
float_mul_0:
.LFB185:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm7
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	cvtsd2ss	%xmm0, %xmm0
	movss	%xmm0, -52(%rbp)
	jmp	.L547
.L548:
	movaps	%xmm7, %xmm6
	movaps	%xmm6, %xmm2
	mulss	%xmm6, %xmm2
	movss	-52(%rbp), %xmm1
	mulss	%xmm1, %xmm2
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm3, %xmm4
	mulss	%xmm1, %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm7, %xmm2
	mulss	%xmm1, %xmm2
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm3, %xmm4
	mulss	%xmm1, %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm4, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm7
.L547:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L548
	cvttss2sil	%xmm7, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE185:
	.size	float_mul_0, .-float_mul_0
	.globl	float_mul_1
	.type	float_mul_1, @function
float_mul_1:
.LFB186:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm4
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm0, %xmm6
	movss	%xmm6, -56(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm0, %xmm5
	movss	%xmm5, -60(%rbp)
	jmp	.L550
.L551:
	movaps	%xmm4, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm7, %xmm2
	movss	-56(%rbp), %xmm1
	mulss	%xmm1, %xmm2
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm6, %xmm5
	movss	-60(%rbp), %xmm0
	mulss	%xmm0, %xmm5
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm1, %xmm2
	movaps	%xmm5, %xmm7
	mulss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm1, %xmm2
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm1, %xmm2
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	mulss	%xmm1, %xmm2
	movaps	%xmm4, %xmm7
	mulss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	movaps	%xmm0, %xmm6
	mulss	%xmm6, %xmm4
	movaps	%xmm2, %xmm7
	mulss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm1, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	movaps	%xmm0, %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm7, %xmm4
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm1, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	movaps	%xmm6, %xmm7
	mulss	%xmm7, %xmm3
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm1, %xmm2
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm6, %xmm3
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm1, %xmm2
	movaps	%xmm3, %xmm7
	mulss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	movaps	%xmm6, %xmm7
	mulss	%xmm7, %xmm3
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm3, %xmm1
	mulss	%xmm3, %xmm1
	mulss	%xmm7, %xmm1
	movss	%xmm1, -52(%rbp)
.L550:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L551
	cvttss2sil	%xmm4, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE186:
	.size	float_mul_1, .-float_mul_1
	.globl	float_mul_2
	.type	float_mul_2, @function
float_mul_2:
.LFB187:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm2
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm3, %xmm3
	cvtsd2ss	%xmm0, %xmm3
	movss	%xmm3, -60(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm0, %xmm6
	movss	%xmm6, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm3, %xmm3
	cvtsd2ss	%xmm0, %xmm3
	movss	%xmm3, -68(%rbp)
	jmp	.L553
.L554:
	movaps	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm7, %xmm2
	movss	-60(%rbp), %xmm4
	mulss	%xmm4, %xmm2
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm6, %xmm5
	movss	-64(%rbp), %xmm6
	mulss	%xmm6, %xmm5
	movss	-56(%rbp), %xmm0
	movaps	%xmm0, %xmm1
	mulss	%xmm0, %xmm1
	movss	-68(%rbp), %xmm8
	mulss	%xmm8, %xmm1
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm4, %xmm0
	mulss	%xmm0, %xmm2
	movaps	%xmm5, %xmm7
	mulss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	movaps	%xmm6, %xmm3
	mulss	%xmm3, %xmm5
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	%xmm8, %xmm1
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	movaps	%xmm6, %xmm3
	mulss	%xmm3, %xmm4
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	%xmm8, %xmm1
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm5, %xmm4
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	movaps	%xmm8, %xmm3
	mulss	%xmm3, %xmm1
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm4, %xmm7
	mulss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	mulss	%xmm5, %xmm4
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	movaps	%xmm8, %xmm6
	mulss	%xmm6, %xmm1
	movaps	%xmm2, %xmm7
	mulss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm5, %xmm4
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm6, %xmm1
	mulss	%xmm1, %xmm7
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm5, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm0, %xmm2
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	movaps	%xmm5, %xmm4
	mulss	%xmm4, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm0, %xmm5
	mulss	%xmm5, %xmm2
	movaps	%xmm3, %xmm7
	mulss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	mulss	%xmm4, %xmm3
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	movaps	%xmm1, %xmm0
	mulss	%xmm0, %xmm7
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	mulss	%xmm5, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	%xmm4, %xmm5
	movss	%xmm5, -52(%rbp)
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm1, %xmm0
	mulss	%xmm0, %xmm6
	movss	%xmm6, -56(%rbp)
.L553:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L554
	cvttss2sil	%xmm2, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE187:
	.size	float_mul_2, .-float_mul_2
	.globl	float_mul_3
	.type	float_mul_3, @function
float_mul_3:
.LFB188:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm4
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm0, %xmm5
	movss	%xmm5, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm0, %xmm5
	movss	%xmm5, -68(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm0, %xmm6
	movss	%xmm6, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm0, %xmm5
	movss	%xmm5, -76(%rbp)
	jmp	.L556
.L557:
	movaps	%xmm4, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm7, %xmm2
	movss	-64(%rbp), %xmm7
	mulss	%xmm7, %xmm2
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm6, %xmm5
	movss	-68(%rbp), %xmm8
	mulss	%xmm8, %xmm5
	movss	-56(%rbp), %xmm6
	movaps	%xmm6, %xmm1
	mulss	%xmm6, %xmm1
	movss	-72(%rbp), %xmm9
	mulss	%xmm9, %xmm1
	movss	-60(%rbp), %xmm4
	movaps	%xmm4, %xmm0
	mulss	%xmm4, %xmm0
	movss	-76(%rbp), %xmm10
	mulss	%xmm10, %xmm0
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm7, %xmm6
	mulss	%xmm6, %xmm2
	movaps	%xmm5, %xmm7
	mulss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	mulss	%xmm8, %xmm5
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	%xmm9, %xmm1
	movaps	%xmm0, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm0
	mulss	%xmm10, %xmm0
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	movaps	%xmm6, %xmm3
	mulss	%xmm3, %xmm2
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	%xmm9, %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	%xmm10, %xmm0
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm3, %xmm5
	mulss	%xmm5, %xmm2
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	%xmm9, %xmm1
	movaps	%xmm0, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm0
	mulss	%xmm10, %xmm0
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm5, %xmm3
	mulss	%xmm3, %xmm2
	movaps	%xmm4, %xmm7
	mulss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	%xmm10, %xmm0
	movaps	%xmm2, %xmm7
	mulss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm3, %xmm1
	mulss	%xmm1, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm9, %xmm7
	movaps	%xmm0, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm0
	mulss	%xmm10, %xmm0
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm1, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm9, %xmm7
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	%xmm10, %xmm0
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm1, %xmm2
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm0, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	mulss	%xmm10, %xmm0
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm1, %xmm4
	mulss	%xmm4, %xmm2
	movaps	%xmm3, %xmm7
	mulss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	movaps	%xmm8, %xmm1
	mulss	%xmm1, %xmm3
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm9, %xmm7
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	%xmm10, %xmm0
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	movaps	%xmm8, %xmm1
	mulss	%xmm1, %xmm5
	movss	%xmm5, -52(%rbp)
	movaps	%xmm7, %xmm5
	mulss	%xmm7, %xmm5
	mulss	%xmm9, %xmm5
	movss	%xmm5, -56(%rbp)
	movaps	%xmm0, %xmm3
	mulss	%xmm0, %xmm3
	mulss	%xmm10, %xmm3
	movss	%xmm3, -60(%rbp)
.L556:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L557
	cvttss2sil	%xmm4, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE188:
	.size	float_mul_3, .-float_mul_3
	.globl	float_mul_4
	.type	float_mul_4, @function
float_mul_4:
.LFB189:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm2
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm3, %xmm3
	cvtsd2ss	%xmm0, %xmm3
	movss	%xmm3, -68(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm0, %xmm7
	movss	%xmm7, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm3, %xmm3
	cvtsd2ss	%xmm0, %xmm3
	movss	%xmm3, -76(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm0, %xmm6
	movss	%xmm6, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm0, %xmm7
	movss	%xmm7, -84(%rbp)
	jmp	.L559
.L560:
	movaps	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm7, %xmm2
	movss	-68(%rbp), %xmm8
	mulss	%xmm8, %xmm2
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm6, %xmm5
	movss	-72(%rbp), %xmm9
	mulss	%xmm9, %xmm5
	movss	-56(%rbp), %xmm6
	movaps	%xmm6, %xmm1
	mulss	%xmm6, %xmm1
	movss	-76(%rbp), %xmm10
	mulss	%xmm10, %xmm1
	movss	-60(%rbp), %xmm7
	movaps	%xmm7, %xmm4
	mulss	%xmm7, %xmm4
	movss	-80(%rbp), %xmm11
	mulss	%xmm11, %xmm4
	movss	-64(%rbp), %xmm6
	movaps	%xmm6, %xmm0
	mulss	%xmm6, %xmm0
	movss	-84(%rbp), %xmm12
	mulss	%xmm12, %xmm0
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm8, %xmm2
	movaps	%xmm5, %xmm7
	mulss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	%xmm10, %xmm1
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	%xmm12, %xmm0
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm8, %xmm2
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	%xmm10, %xmm1
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	mulss	%xmm12, %xmm0
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm8, %xmm2
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	%xmm10, %xmm1
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	%xmm12, %xmm0
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	mulss	%xmm8, %xmm2
	movaps	%xmm4, %xmm7
	mulss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm5, %xmm3
	mulss	%xmm5, %xmm3
	movaps	%xmm3, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	mulss	%xmm12, %xmm0
	movaps	%xmm2, %xmm7
	mulss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	movaps	%xmm8, %xmm1
	mulss	%xmm1, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm10, %xmm7
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	%xmm12, %xmm0
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	movaps	%xmm8, %xmm1
	mulss	%xmm1, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm10, %xmm7
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	%xmm12, %xmm0
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm1, %xmm2
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm11, %xmm4
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	mulss	%xmm12, %xmm0
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm1, %xmm2
	movaps	%xmm3, %xmm7
	mulss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm10, %xmm7
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	%xmm11, %xmm4
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	%xmm12, %xmm0
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	%xmm9, %xmm5
	movss	%xmm5, -52(%rbp)
	movaps	%xmm7, %xmm5
	mulss	%xmm7, %xmm5
	mulss	%xmm10, %xmm5
	movss	%xmm5, -56(%rbp)
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm11, %xmm3
	movss	%xmm3, -60(%rbp)
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	mulss	%xmm12, %xmm7
	movss	%xmm7, -64(%rbp)
.L559:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L560
	cvttss2sil	%xmm2, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE189:
	.size	float_mul_4, .-float_mul_4
	.globl	float_mul_5
	.type	float_mul_5, @function
float_mul_5:
.LFB190:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm2
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm0, %xmm5
	movss	%xmm5, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm3, %xmm3
	cvtsd2ss	%xmm0, %xmm3
	movss	%xmm3, -76(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm0, %xmm5
	movss	%xmm5, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm0, %xmm6
	movss	%xmm6, -84(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm3, %xmm3
	cvtsd2ss	%xmm0, %xmm3
	movss	%xmm3, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm5, %xmm5
	cvtsd2ss	%xmm0, %xmm5
	movss	%xmm5, -92(%rbp)
	jmp	.L562
.L563:
	movaps	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm7, %xmm2
	movss	-72(%rbp), %xmm9
	mulss	%xmm9, %xmm2
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm6, %xmm5
	movss	-76(%rbp), %xmm10
	mulss	%xmm10, %xmm5
	movss	-56(%rbp), %xmm3
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movss	-80(%rbp), %xmm11
	mulss	%xmm11, %xmm6
	movss	-60(%rbp), %xmm3
	movaps	%xmm3, %xmm1
	mulss	%xmm3, %xmm1
	movss	-84(%rbp), %xmm12
	mulss	%xmm12, %xmm1
	movss	-64(%rbp), %xmm3
	movaps	%xmm3, %xmm0
	mulss	%xmm3, %xmm0
	movss	-88(%rbp), %xmm13
	mulss	%xmm13, %xmm0
	movss	-68(%rbp), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm7, %xmm8
	movss	-92(%rbp), %xmm14
	mulss	%xmm14, %xmm8
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm9, %xmm2
	movaps	%xmm5, %xmm7
	mulss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm11, %xmm7
	movaps	%xmm1, %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm1
	mulss	%xmm12, %xmm1
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	%xmm13, %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	%xmm14, %xmm8
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm9, %xmm2
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm11, %xmm7
	movaps	%xmm1, %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm1
	mulss	%xmm12, %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	%xmm13, %xmm0
	movaps	%xmm8, %xmm5
	mulss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	mulss	%xmm14, %xmm8
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm9, %xmm2
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm7, %xmm5
	mulss	%xmm7, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm1, %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm1
	mulss	%xmm12, %xmm1
	movaps	%xmm0, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm0
	mulss	%xmm13, %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	%xmm14, %xmm8
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	mulss	%xmm9, %xmm2
	movaps	%xmm4, %xmm7
	mulss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm1, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm1
	mulss	%xmm12, %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	%xmm13, %xmm0
	movaps	%xmm8, %xmm5
	mulss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	mulss	%xmm14, %xmm8
	movaps	%xmm2, %xmm7
	mulss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm9, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm11, %xmm7
	movaps	%xmm1, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm1
	mulss	%xmm12, %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	%xmm13, %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	%xmm14, %xmm8
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm9, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm11, %xmm7
	movaps	%xmm1, %xmm4
	mulss	%xmm1, %xmm4
	movaps	%xmm4, %xmm1
	mulss	%xmm12, %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	%xmm13, %xmm0
	movaps	%xmm8, %xmm5
	mulss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	mulss	%xmm14, %xmm8
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm9, %xmm2
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm1, %xmm4
	mulss	%xmm1, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	%xmm13, %xmm0
	movaps	%xmm8, %xmm7
	mulss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm14, %xmm8
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	movaps	%xmm9, %xmm1
	mulss	%xmm1, %xmm2
	movaps	%xmm3, %xmm7
	mulss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm11, %xmm7
	movaps	%xmm4, %xmm5
	mulss	%xmm4, %xmm5
	movaps	%xmm5, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	%xmm13, %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	%xmm14, %xmm8
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	movaps	%xmm9, %xmm1
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	%xmm10, %xmm5
	movss	%xmm5, -52(%rbp)
	movaps	%xmm7, %xmm1
	mulss	%xmm7, %xmm1
	mulss	%xmm11, %xmm1
	movss	%xmm1, -56(%rbp)
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm12, %xmm3
	movss	%xmm3, -60(%rbp)
	movaps	%xmm0, %xmm3
	mulss	%xmm0, %xmm3
	mulss	%xmm13, %xmm3
	movss	%xmm3, -64(%rbp)
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	mulss	%xmm14, %xmm4
	movss	%xmm4, -68(%rbp)
.L562:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L563
	cvttss2sil	%xmm2, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE190:
	.size	float_mul_5, .-float_mul_5
	.globl	float_mul_6
	.type	float_mul_6, @function
float_mul_6:
.LFB191:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm2
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm0, %xmm6
	movss	%xmm6, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm0, %xmm7
	movss	%xmm7, -84(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm0, %xmm6
	movss	%xmm6, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm0, %xmm7
	movss	%xmm7, -92(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm0, %xmm6
	movss	%xmm6, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm7, %xmm7
	cvtsd2ss	%xmm0, %xmm7
	movss	%xmm7, -100(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm6, %xmm6
	cvtsd2ss	%xmm0, %xmm6
	movss	%xmm6, -52(%rbp)
	jmp	.L565
.L566:
	movaps	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm7, %xmm2
	movss	-80(%rbp), %xmm10
	mulss	%xmm10, %xmm2
	movss	-56(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm6, %xmm5
	movss	-84(%rbp), %xmm11
	mulss	%xmm11, %xmm5
	movss	-60(%rbp), %xmm7
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movss	-88(%rbp), %xmm12
	mulss	%xmm12, %xmm6
	movss	-64(%rbp), %xmm3
	movaps	%xmm3, %xmm1
	mulss	%xmm3, %xmm1
	movss	-92(%rbp), %xmm13
	mulss	%xmm13, %xmm1
	movss	-68(%rbp), %xmm3
	movaps	%xmm3, %xmm0
	mulss	%xmm3, %xmm0
	movss	-96(%rbp), %xmm14
	mulss	%xmm14, %xmm0
	movss	-72(%rbp), %xmm4
	movaps	%xmm4, %xmm8
	mulss	%xmm4, %xmm8
	movss	-100(%rbp), %xmm15
	mulss	%xmm15, %xmm8
	movss	-76(%rbp), %xmm4
	movaps	%xmm4, %xmm9
	mulss	%xmm4, %xmm9
	movss	-52(%rbp), %xmm4
	mulss	%xmm4, %xmm9
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm10, %xmm2
	movaps	%xmm5, %xmm7
	mulss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm12, %xmm7
	movaps	%xmm1, %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm1
	mulss	%xmm13, %xmm1
	movaps	%xmm0, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm0
	mulss	%xmm14, %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	%xmm15, %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movaps	%xmm4, %xmm3
	mulss	%xmm3, %xmm9
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm10, %xmm2
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm11, %xmm4
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm12, %xmm7
	movaps	%xmm1, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm1
	mulss	%xmm13, %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	%xmm14, %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	%xmm15, %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	movss	%xmm3, -52(%rbp)
	mulss	%xmm3, %xmm9
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm10, %xmm2
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	%xmm11, %xmm4
	movaps	%xmm7, %xmm5
	mulss	%xmm7, %xmm5
	mulss	%xmm12, %xmm5
	movaps	%xmm1, %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm1
	mulss	%xmm13, %xmm1
	movaps	%xmm0, %xmm3
	mulss	%xmm0, %xmm3
	movaps	%xmm3, %xmm0
	mulss	%xmm14, %xmm0
	movaps	%xmm8, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	mulss	%xmm15, %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	movss	-52(%rbp), %xmm3
	mulss	%xmm3, %xmm9
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	mulss	%xmm10, %xmm2
	movaps	%xmm4, %xmm7
	mulss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	mulss	%xmm11, %xmm4
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	mulss	%xmm12, %xmm6
	movaps	%xmm1, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm1
	mulss	%xmm13, %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	%xmm14, %xmm0
	movaps	%xmm8, %xmm5
	mulss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	mulss	%xmm15, %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	movss	%xmm3, -52(%rbp)
	movaps	%xmm3, %xmm5
	mulss	%xmm5, %xmm9
	movaps	%xmm2, %xmm7
	mulss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm10, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm11, %xmm4
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm12, %xmm7
	movaps	%xmm1, %xmm3
	mulss	%xmm1, %xmm3
	movaps	%xmm3, %xmm1
	mulss	%xmm13, %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	%xmm14, %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	%xmm15, %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	movss	-52(%rbp), %xmm5
	mulss	%xmm5, %xmm9
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm10, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm12, %xmm7
	movaps	%xmm1, %xmm4
	mulss	%xmm1, %xmm4
	movaps	%xmm4, %xmm1
	mulss	%xmm13, %xmm1
	movaps	%xmm0, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	mulss	%xmm14, %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	%xmm15, %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	%xmm5, %xmm9
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm10, %xmm2
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	%xmm12, %xmm6
	movaps	%xmm1, %xmm4
	mulss	%xmm1, %xmm4
	movaps	%xmm4, %xmm1
	mulss	%xmm13, %xmm1
	movaps	%xmm0, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	mulss	%xmm14, %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	%xmm15, %xmm8
	movaps	%xmm9, %xmm7
	mulss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	movss	%xmm5, -52(%rbp)
	mulss	%xmm5, %xmm9
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm10, %xmm2
	movaps	%xmm3, %xmm7
	mulss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm12, %xmm7
	movaps	%xmm1, %xmm5
	mulss	%xmm1, %xmm5
	movaps	%xmm5, %xmm1
	mulss	%xmm13, %xmm1
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	%xmm14, %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	%xmm15, %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	mulss	-52(%rbp), %xmm9
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm2
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	%xmm11, %xmm5
	movss	%xmm5, -56(%rbp)
	movaps	%xmm7, %xmm3
	mulss	%xmm7, %xmm3
	mulss	%xmm12, %xmm3
	movss	%xmm3, -60(%rbp)
	movaps	%xmm1, %xmm4
	mulss	%xmm1, %xmm4
	mulss	%xmm13, %xmm4
	movss	%xmm4, -64(%rbp)
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	mulss	%xmm14, %xmm5
	movss	%xmm5, -68(%rbp)
	movaps	%xmm8, %xmm3
	mulss	%xmm8, %xmm3
	mulss	%xmm15, %xmm3
	movss	%xmm3, -72(%rbp)
	movaps	%xmm9, %xmm3
	mulss	%xmm9, %xmm3
	mulss	-52(%rbp), %xmm3
	movss	%xmm3, -76(%rbp)
.L565:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L566
	cvttss2sil	%xmm2, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE191:
	.size	float_mul_6, .-float_mul_6
	.globl	float_mul_7
	.type	float_mul_7, @function
float_mul_7:
.LFB192:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm11
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -92(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -100(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm4, %xmm4
	cvtsd2ss	%xmm0, %xmm4
	movss	%xmm4, -108(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -52(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm4, %xmm4
	cvtsd2ss	%xmm0, %xmm4
	movss	%xmm4, -56(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -60(%rbp)
	jmp	.L568
.L569:
	movaps	%xmm11, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm7, %xmm2
	movss	-92(%rbp), %xmm11
	mulss	%xmm11, %xmm2
	movss	-64(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm6, %xmm5
	movss	-96(%rbp), %xmm12
	mulss	%xmm12, %xmm5
	movss	-68(%rbp), %xmm7
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movss	-100(%rbp), %xmm13
	mulss	%xmm13, %xmm6
	movss	-72(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm3, %xmm4
	movss	-104(%rbp), %xmm14
	mulss	%xmm14, %xmm4
	movss	-76(%rbp), %xmm1
	movaps	%xmm1, %xmm0
	mulss	%xmm1, %xmm0
	movss	-108(%rbp), %xmm15
	mulss	%xmm15, %xmm0
	movss	-80(%rbp), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm7, %xmm8
	mulss	-52(%rbp), %xmm8
	movss	-84(%rbp), %xmm7
	movaps	%xmm7, %xmm9
	mulss	%xmm7, %xmm9
	mulss	-56(%rbp), %xmm9
	movss	-88(%rbp), %xmm3
	movaps	%xmm3, %xmm10
	mulss	%xmm3, %xmm10
	mulss	-60(%rbp), %xmm10
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm11, %xmm2
	movaps	%xmm5, %xmm7
	mulss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	mulss	%xmm12, %xmm5
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm13, %xmm7
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm14, %xmm3
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	%xmm15, %xmm0
	movaps	%xmm8, %xmm1
	mulss	%xmm8, %xmm1
	movaps	%xmm1, %xmm8
	mulss	-52(%rbp), %xmm8
	movaps	%xmm9, %xmm1
	mulss	%xmm9, %xmm1
	movaps	%xmm1, %xmm9
	mulss	-56(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movss	-60(%rbp), %xmm1
	mulss	%xmm1, %xmm10
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm11, %xmm2
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm13, %xmm7
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	mulss	%xmm14, %xmm3
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	%xmm15, %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-52(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-56(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	movss	%xmm1, -60(%rbp)
	mulss	%xmm1, %xmm10
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm11, %xmm2
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm7, %xmm5
	mulss	%xmm7, %xmm5
	mulss	%xmm13, %xmm5
	movaps	%xmm3, %xmm1
	mulss	%xmm3, %xmm1
	movaps	%xmm1, %xmm3
	mulss	%xmm14, %xmm3
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	%xmm15, %xmm0
	movaps	%xmm8, %xmm7
	mulss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	mulss	-52(%rbp), %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	mulss	-56(%rbp), %xmm9
	movaps	%xmm10, %xmm1
	mulss	%xmm10, %xmm1
	movaps	%xmm1, %xmm10
	mulss	-60(%rbp), %xmm10
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	mulss	%xmm11, %xmm2
	movaps	%xmm4, %xmm7
	mulss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	mulss	%xmm13, %xmm6
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	%xmm14, %xmm5
	movaps	%xmm0, %xmm1
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	mulss	%xmm15, %xmm0
	movaps	%xmm8, %xmm7
	mulss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	mulss	-52(%rbp), %xmm8
	movaps	%xmm9, %xmm1
	mulss	%xmm9, %xmm1
	movaps	%xmm1, %xmm9
	mulss	-56(%rbp), %xmm9
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	-60(%rbp), %xmm10
	movaps	%xmm2, %xmm7
	mulss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm11, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm13, %xmm7
	movaps	%xmm5, %xmm1
	mulss	%xmm5, %xmm1
	movaps	%xmm1, %xmm5
	mulss	%xmm14, %xmm5
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	%xmm15, %xmm0
	movaps	%xmm8, %xmm1
	mulss	%xmm8, %xmm1
	movaps	%xmm1, %xmm8
	mulss	-52(%rbp), %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	mulss	-56(%rbp), %xmm9
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	-60(%rbp), %xmm10
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm11, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm12, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm13, %xmm7
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm14, %xmm5
	movaps	%xmm0, %xmm1
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	mulss	%xmm15, %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-52(%rbp), %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	-56(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	mulss	-60(%rbp), %xmm10
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm11, %xmm2
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	%xmm12, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	%xmm13, %xmm6
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm14, %xmm4
	movaps	%xmm0, %xmm1
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	mulss	%xmm15, %xmm0
	movaps	%xmm8, %xmm7
	mulss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	mulss	-52(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-56(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	mulss	-60(%rbp), %xmm10
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm11, %xmm2
	movaps	%xmm3, %xmm7
	mulss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	mulss	%xmm12, %xmm3
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm13, %xmm7
	movaps	%xmm4, %xmm1
	mulss	%xmm4, %xmm1
	movaps	%xmm1, %xmm4
	mulss	%xmm14, %xmm4
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	%xmm15, %xmm0
	movaps	%xmm8, %xmm5
	mulss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	mulss	-52(%rbp), %xmm8
	movaps	%xmm9, %xmm1
	mulss	%xmm9, %xmm1
	movaps	%xmm1, %xmm9
	mulss	-56(%rbp), %xmm9
	movaps	%xmm10, %xmm1
	mulss	%xmm10, %xmm1
	movaps	%xmm1, %xmm10
	mulss	-60(%rbp), %xmm10
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	%xmm12, %xmm5
	movss	%xmm5, -64(%rbp)
	movaps	%xmm7, %xmm3
	mulss	%xmm7, %xmm3
	mulss	%xmm13, %xmm3
	movss	%xmm3, -68(%rbp)
	movaps	%xmm4, %xmm1
	mulss	%xmm4, %xmm1
	mulss	%xmm14, %xmm1
	movss	%xmm1, -72(%rbp)
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	mulss	%xmm15, %xmm6
	movss	%xmm6, -76(%rbp)
	movaps	%xmm8, %xmm3
	mulss	%xmm8, %xmm3
	mulss	-52(%rbp), %xmm3
	movss	%xmm3, -80(%rbp)
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	mulss	-56(%rbp), %xmm4
	movss	%xmm4, -84(%rbp)
	movaps	%xmm10, %xmm7
	mulss	%xmm10, %xmm7
	mulss	-60(%rbp), %xmm7
	movss	%xmm7, -88(%rbp)
.L568:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L569
	cvttss2sil	%xmm11, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE192:
	.size	float_mul_7, .-float_mul_7
	.globl	float_mul_8
	.type	float_mul_8, @function
float_mul_8:
.LFB193:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm12
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm4, %xmm4
	cvtsd2ss	%xmm0, %xmm4
	movss	%xmm4, -108(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm4, %xmm4
	cvtsd2ss	%xmm0, %xmm4
	movss	%xmm4, -116(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -52(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm4, %xmm4
	cvtsd2ss	%xmm0, %xmm4
	movss	%xmm4, -68(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -56(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm4, %xmm4
	cvtsd2ss	%xmm0, %xmm4
	movss	%xmm4, -60(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -64(%rbp)
	jmp	.L571
.L572:
	movaps	%xmm12, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm7, %xmm2
	movss	-104(%rbp), %xmm12
	mulss	%xmm12, %xmm2
	movss	-72(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm6, %xmm5
	movss	-108(%rbp), %xmm13
	mulss	%xmm13, %xmm5
	movss	-76(%rbp), %xmm7
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movss	-112(%rbp), %xmm14
	mulss	%xmm14, %xmm6
	movss	-80(%rbp), %xmm1
	movaps	%xmm1, %xmm4
	mulss	%xmm1, %xmm4
	movss	-116(%rbp), %xmm15
	mulss	%xmm15, %xmm4
	movss	-84(%rbp), %xmm7
	movaps	%xmm7, %xmm0
	mulss	%xmm7, %xmm0
	movss	-52(%rbp), %xmm1
	mulss	%xmm1, %xmm0
	movss	-88(%rbp), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm7, %xmm8
	mulss	-68(%rbp), %xmm8
	movss	-92(%rbp), %xmm3
	movaps	%xmm3, %xmm9
	mulss	%xmm3, %xmm9
	mulss	-56(%rbp), %xmm9
	movss	-96(%rbp), %xmm7
	movaps	%xmm7, %xmm10
	mulss	%xmm7, %xmm10
	mulss	-60(%rbp), %xmm10
	movss	-100(%rbp), %xmm3
	movaps	%xmm3, %xmm11
	mulss	%xmm3, %xmm11
	mulss	-64(%rbp), %xmm11
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm12, %xmm2
	movaps	%xmm5, %xmm7
	mulss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	mulss	%xmm13, %xmm5
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm14, %xmm7
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm15, %xmm3
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movss	%xmm1, -52(%rbp)
	mulss	%xmm1, %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-68(%rbp), %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	-56(%rbp), %xmm9
	movaps	%xmm10, %xmm1
	mulss	%xmm10, %xmm1
	movaps	%xmm1, %xmm10
	movss	-60(%rbp), %xmm1
	mulss	%xmm1, %xmm10
	movaps	%xmm11, %xmm4
	mulss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	mulss	-64(%rbp), %xmm11
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm12, %xmm2
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm14, %xmm7
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	mulss	%xmm15, %xmm3
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	-52(%rbp), %xmm0
	movaps	%xmm8, %xmm5
	mulss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	mulss	-68(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-56(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	movss	%xmm1, -60(%rbp)
	mulss	%xmm1, %xmm10
	movaps	%xmm11, %xmm1
	mulss	%xmm11, %xmm1
	movaps	%xmm1, %xmm11
	mulss	-64(%rbp), %xmm11
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm12, %xmm2
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm7, %xmm5
	mulss	%xmm7, %xmm5
	mulss	%xmm14, %xmm5
	movaps	%xmm3, %xmm1
	mulss	%xmm3, %xmm1
	movaps	%xmm1, %xmm3
	mulss	%xmm15, %xmm3
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	-52(%rbp), %xmm0
	movaps	%xmm8, %xmm7
	mulss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	mulss	-68(%rbp), %xmm8
	movaps	%xmm9, %xmm1
	mulss	%xmm9, %xmm1
	movaps	%xmm1, %xmm9
	mulss	-56(%rbp), %xmm9
	movaps	%xmm10, %xmm1
	mulss	%xmm10, %xmm1
	movaps	%xmm1, %xmm10
	mulss	-60(%rbp), %xmm10
	movaps	%xmm11, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	mulss	-64(%rbp), %xmm11
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	mulss	%xmm12, %xmm2
	movaps	%xmm4, %xmm7
	mulss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	mulss	%xmm14, %xmm6
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	%xmm15, %xmm5
	movaps	%xmm0, %xmm1
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	movss	-52(%rbp), %xmm1
	mulss	%xmm1, %xmm0
	movaps	%xmm8, %xmm7
	mulss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	mulss	-68(%rbp), %xmm8
	movaps	%xmm9, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	mulss	-56(%rbp), %xmm9
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	-60(%rbp), %xmm10
	movaps	%xmm11, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	mulss	-64(%rbp), %xmm11
	movaps	%xmm2, %xmm7
	mulss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm12, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm14, %xmm7
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm15, %xmm5
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	movss	%xmm1, -52(%rbp)
	mulss	%xmm1, %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-68(%rbp), %xmm8
	movaps	%xmm9, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	mulss	-56(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	mulss	-60(%rbp), %xmm10
	movaps	%xmm11, %xmm1
	mulss	%xmm11, %xmm1
	movaps	%xmm1, %xmm11
	mulss	-64(%rbp), %xmm11
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm12, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm13, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm14, %xmm7
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm15, %xmm5
	movaps	%xmm0, %xmm1
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	mulss	-52(%rbp), %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	-68(%rbp), %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	-56(%rbp), %xmm9
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	mulss	-60(%rbp), %xmm10
	movaps	%xmm11, %xmm4
	mulss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	mulss	-64(%rbp), %xmm11
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm12, %xmm2
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	%xmm13, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	%xmm14, %xmm6
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm15, %xmm4
	movaps	%xmm0, %xmm1
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	mulss	-52(%rbp), %xmm0
	movaps	%xmm8, %xmm5
	mulss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	mulss	-68(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-56(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	mulss	-60(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-64(%rbp), %xmm11
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm12, %xmm2
	movaps	%xmm3, %xmm7
	mulss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	mulss	%xmm13, %xmm3
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm14, %xmm7
	movaps	%xmm4, %xmm1
	mulss	%xmm4, %xmm1
	movaps	%xmm1, %xmm4
	mulss	%xmm15, %xmm4
	movaps	%xmm0, %xmm1
	mulss	%xmm0, %xmm1
	mulss	-52(%rbp), %xmm1
	movaps	%xmm8, %xmm0
	mulss	%xmm8, %xmm0
	movss	-68(%rbp), %xmm8
	mulss	%xmm8, %xmm0
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-56(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	mulss	-60(%rbp), %xmm10
	movaps	%xmm11, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	mulss	-64(%rbp), %xmm11
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	mulss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	%xmm13, %xmm5
	movss	%xmm5, -72(%rbp)
	movaps	%xmm7, %xmm3
	mulss	%xmm7, %xmm3
	mulss	%xmm14, %xmm3
	movss	%xmm3, -76(%rbp)
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm15, %xmm3
	movss	%xmm3, -80(%rbp)
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	mulss	-52(%rbp), %xmm6
	movss	%xmm6, -84(%rbp)
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	mulss	%xmm8, %xmm6
	movss	%xmm6, -88(%rbp)
	movaps	%xmm9, %xmm7
	mulss	%xmm9, %xmm7
	mulss	-56(%rbp), %xmm7
	movss	%xmm7, -92(%rbp)
	movaps	%xmm10, %xmm2
	mulss	%xmm10, %xmm2
	mulss	-60(%rbp), %xmm2
	movss	%xmm2, -96(%rbp)
	movaps	%xmm11, %xmm2
	mulss	%xmm11, %xmm2
	mulss	-64(%rbp), %xmm2
	movss	%xmm2, -100(%rbp)
.L571:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L572
	cvttss2sil	%xmm12, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE193:
	.size	float_mul_8, .-float_mul_8
	.globl	float_mul_9
	.type	float_mul_9, @function
float_mul_9:
.LFB194:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm12
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -116(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -124(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -52(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -56(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -60(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm4, %xmm4
	cvtsd2ss	%xmm0, %xmm4
	movss	%xmm4, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -68(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -108(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm4, %xmm4
	cvtsd2ss	%xmm0, %xmm4
	movss	%xmm4, -72(%rbp)
	jmp	.L574
.L575:
	movaps	%xmm12, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm7, %xmm2
	movss	-112(%rbp), %xmm12
	mulss	%xmm12, %xmm2
	movss	-76(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm6, %xmm5
	movss	-116(%rbp), %xmm13
	mulss	%xmm13, %xmm5
	movss	-80(%rbp), %xmm7
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movss	-120(%rbp), %xmm14
	mulss	%xmm14, %xmm6
	movss	-84(%rbp), %xmm1
	movaps	%xmm1, %xmm4
	mulss	%xmm1, %xmm4
	movss	-124(%rbp), %xmm15
	mulss	%xmm15, %xmm4
	movss	-88(%rbp), %xmm7
	movaps	%xmm7, %xmm1
	mulss	%xmm7, %xmm1
	mulss	-52(%rbp), %xmm1
	movss	-92(%rbp), %xmm7
	movaps	%xmm7, %xmm0
	mulss	%xmm7, %xmm0
	mulss	-56(%rbp), %xmm0
	movss	-96(%rbp), %xmm3
	movaps	%xmm3, %xmm8
	mulss	%xmm3, %xmm8
	mulss	-60(%rbp), %xmm8
	movss	-100(%rbp), %xmm7
	movaps	%xmm7, %xmm9
	mulss	%xmm7, %xmm9
	mulss	-64(%rbp), %xmm9
	movss	-104(%rbp), %xmm3
	movaps	%xmm3, %xmm10
	mulss	%xmm3, %xmm10
	mulss	-68(%rbp), %xmm10
	movss	-108(%rbp), %xmm3
	movaps	%xmm3, %xmm11
	mulss	%xmm3, %xmm11
	mulss	-72(%rbp), %xmm11
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm12, %xmm2
	movaps	%xmm5, %xmm7
	mulss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	mulss	%xmm13, %xmm5
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm14, %xmm7
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm15, %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-52(%rbp), %xmm1
	movaps	%xmm0, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	mulss	-56(%rbp), %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	-60(%rbp), %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	-64(%rbp), %xmm9
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	mulss	-68(%rbp), %xmm10
	movaps	%xmm11, %xmm4
	mulss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	mulss	-72(%rbp), %xmm11
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm12, %xmm2
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm14, %xmm7
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	mulss	%xmm15, %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-52(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-56(%rbp), %xmm0
	movaps	%xmm8, %xmm5
	mulss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	mulss	-60(%rbp), %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	mulss	-64(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	mulss	-68(%rbp), %xmm10
	movaps	%xmm11, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	mulss	-72(%rbp), %xmm11
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm12, %xmm2
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm7, %xmm5
	mulss	%xmm7, %xmm5
	mulss	%xmm14, %xmm5
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	%xmm15, %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-52(%rbp), %xmm1
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	mulss	-56(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-60(%rbp), %xmm8
	movaps	%xmm9, %xmm7
	mulss	%xmm9, %xmm7
	movaps	%xmm7, %xmm9
	mulss	-64(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	mulss	-68(%rbp), %xmm10
	movaps	%xmm11, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	mulss	-72(%rbp), %xmm11
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	mulss	%xmm12, %xmm2
	movaps	%xmm4, %xmm7
	mulss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	mulss	%xmm14, %xmm6
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	%xmm15, %xmm5
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	-52(%rbp), %xmm1
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	mulss	-56(%rbp), %xmm0
	movaps	%xmm8, %xmm7
	mulss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	mulss	-60(%rbp), %xmm8
	movaps	%xmm9, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	mulss	-64(%rbp), %xmm9
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	-68(%rbp), %xmm10
	movaps	%xmm11, %xmm7
	mulss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	mulss	-72(%rbp), %xmm11
	movaps	%xmm2, %xmm7
	mulss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm12, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm14, %xmm7
	movaps	%xmm5, %xmm3
	mulss	%xmm5, %xmm3
	movaps	%xmm3, %xmm5
	mulss	%xmm15, %xmm5
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-52(%rbp), %xmm1
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	-56(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-60(%rbp), %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	mulss	-64(%rbp), %xmm9
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	-68(%rbp), %xmm10
	movaps	%xmm11, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	mulss	-72(%rbp), %xmm11
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm12, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm13, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm14, %xmm7
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	mulss	%xmm15, %xmm5
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-52(%rbp), %xmm1
	movaps	%xmm0, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	mulss	-56(%rbp), %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	-60(%rbp), %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	-64(%rbp), %xmm9
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	mulss	-68(%rbp), %xmm10
	movaps	%xmm11, %xmm4
	mulss	%xmm11, %xmm4
	movaps	%xmm4, %xmm11
	mulss	-72(%rbp), %xmm11
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm12, %xmm2
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	%xmm13, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	%xmm14, %xmm6
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm15, %xmm4
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	-52(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-56(%rbp), %xmm0
	movaps	%xmm8, %xmm5
	mulss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	mulss	-60(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-64(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	mulss	-68(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-72(%rbp), %xmm11
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm12, %xmm2
	movaps	%xmm3, %xmm7
	mulss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	mulss	%xmm13, %xmm3
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm14, %xmm7
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	%xmm15, %xmm4
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-52(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-56(%rbp), %xmm0
	movaps	%xmm8, %xmm5
	mulss	%xmm8, %xmm5
	movaps	%xmm5, %xmm8
	mulss	-60(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-64(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	mulss	-68(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-72(%rbp), %xmm11
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	mulss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	%xmm13, %xmm5
	movss	%xmm5, -76(%rbp)
	movaps	%xmm7, %xmm3
	mulss	%xmm7, %xmm3
	mulss	%xmm14, %xmm3
	movss	%xmm3, -80(%rbp)
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm15, %xmm3
	movss	%xmm3, -84(%rbp)
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	mulss	-52(%rbp), %xmm6
	movss	%xmm6, -88(%rbp)
	movaps	%xmm0, %xmm3
	mulss	%xmm0, %xmm3
	mulss	-56(%rbp), %xmm3
	movss	%xmm3, -92(%rbp)
	movaps	%xmm8, %xmm2
	mulss	%xmm8, %xmm2
	mulss	-60(%rbp), %xmm2
	movss	%xmm2, -96(%rbp)
	movaps	%xmm9, %xmm2
	mulss	%xmm9, %xmm2
	mulss	-64(%rbp), %xmm2
	movss	%xmm2, -100(%rbp)
	movaps	%xmm10, %xmm7
	mulss	%xmm10, %xmm7
	mulss	-68(%rbp), %xmm7
	movss	%xmm7, -104(%rbp)
	movaps	%xmm11, %xmm2
	mulss	%xmm11, %xmm2
	mulss	-72(%rbp), %xmm2
	movss	%xmm2, -108(%rbp)
.L574:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L575
	cvttss2sil	%xmm12, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE194:
	.size	float_mul_9, .-float_mul_9
	.globl	float_mul_10
	.type	float_mul_10, @function
float_mul_10:
.LFB195:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm13
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -124(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -132(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -52(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -56(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -60(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -108(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -68(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -116(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm4, %xmm4
	cvtsd2ss	%xmm0, %xmm4
	movss	%xmm4, -76(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -80(%rbp)
	jmp	.L577
.L578:
	movaps	%xmm13, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm7, %xmm2
	movss	-124(%rbp), %xmm13
	mulss	%xmm13, %xmm2
	movss	-84(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm6, %xmm5
	movss	-128(%rbp), %xmm14
	mulss	%xmm14, %xmm5
	movss	-88(%rbp), %xmm7
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movss	-132(%rbp), %xmm15
	mulss	%xmm15, %xmm6
	movss	-92(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm3, %xmm4
	mulss	-52(%rbp), %xmm4
	movss	-96(%rbp), %xmm7
	movaps	%xmm7, %xmm1
	mulss	%xmm7, %xmm1
	mulss	-56(%rbp), %xmm1
	movss	-100(%rbp), %xmm7
	movaps	%xmm7, %xmm0
	mulss	%xmm7, %xmm0
	mulss	-60(%rbp), %xmm0
	movss	-104(%rbp), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm7, %xmm8
	mulss	-64(%rbp), %xmm8
	movss	-108(%rbp), %xmm7
	movaps	%xmm7, %xmm9
	mulss	%xmm7, %xmm9
	mulss	-68(%rbp), %xmm9
	movss	-112(%rbp), %xmm7
	movaps	%xmm7, %xmm10
	mulss	%xmm7, %xmm10
	mulss	-72(%rbp), %xmm10
	movss	-116(%rbp), %xmm7
	movaps	%xmm7, %xmm11
	mulss	%xmm7, %xmm11
	mulss	-76(%rbp), %xmm11
	movss	-120(%rbp), %xmm7
	movaps	%xmm7, %xmm12
	mulss	%xmm7, %xmm12
	mulss	-80(%rbp), %xmm12
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm13, %xmm2
	movaps	%xmm5, %xmm7
	mulss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	mulss	%xmm14, %xmm5
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm15, %xmm7
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	-52(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-56(%rbp), %xmm1
	movaps	%xmm0, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	mulss	-60(%rbp), %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	-64(%rbp), %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	-68(%rbp), %xmm9
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	mulss	-72(%rbp), %xmm10
	movaps	%xmm11, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	mulss	-76(%rbp), %xmm11
	movaps	%xmm12, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	mulss	-80(%rbp), %xmm12
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm13, %xmm2
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm14, %xmm4
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm15, %xmm7
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	mulss	-52(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-56(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-60(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-64(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-68(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	mulss	-72(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-76(%rbp), %xmm11
	movaps	%xmm12, %xmm6
	mulss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	mulss	-80(%rbp), %xmm12
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm13, %xmm2
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	%xmm14, %xmm4
	movaps	%xmm7, %xmm5
	mulss	%xmm7, %xmm5
	mulss	%xmm15, %xmm5
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	-52(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-56(%rbp), %xmm1
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	mulss	-60(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-64(%rbp), %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	mulss	-68(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	mulss	-72(%rbp), %xmm10
	movaps	%xmm11, %xmm7
	mulss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	mulss	-76(%rbp), %xmm11
	movaps	%xmm12, %xmm6
	mulss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	mulss	-80(%rbp), %xmm12
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	mulss	%xmm13, %xmm2
	movaps	%xmm4, %xmm7
	mulss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	mulss	%xmm14, %xmm4
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	mulss	%xmm15, %xmm6
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	-52(%rbp), %xmm5
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	-56(%rbp), %xmm1
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	mulss	-60(%rbp), %xmm0
	movaps	%xmm8, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	mulss	-64(%rbp), %xmm8
	movaps	%xmm9, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	mulss	-68(%rbp), %xmm9
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	-72(%rbp), %xmm10
	movaps	%xmm11, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	mulss	-76(%rbp), %xmm11
	movaps	%xmm12, %xmm3
	mulss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	mulss	-80(%rbp), %xmm12
	movaps	%xmm2, %xmm7
	mulss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm13, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm14, %xmm4
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm15, %xmm7
	movaps	%xmm5, %xmm3
	mulss	%xmm5, %xmm3
	movaps	%xmm3, %xmm5
	mulss	-52(%rbp), %xmm5
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-56(%rbp), %xmm1
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	-60(%rbp), %xmm0
	movaps	%xmm8, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	mulss	-64(%rbp), %xmm8
	movaps	%xmm9, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	mulss	-68(%rbp), %xmm9
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	-72(%rbp), %xmm10
	movaps	%xmm11, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	mulss	-76(%rbp), %xmm11
	movaps	%xmm12, %xmm3
	mulss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	mulss	-80(%rbp), %xmm12
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm13, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm14, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm15, %xmm7
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	mulss	-52(%rbp), %xmm5
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-56(%rbp), %xmm1
	movaps	%xmm0, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	mulss	-60(%rbp), %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	-64(%rbp), %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	-68(%rbp), %xmm9
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	mulss	-72(%rbp), %xmm10
	movaps	%xmm11, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	mulss	-76(%rbp), %xmm11
	movaps	%xmm12, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	mulss	-80(%rbp), %xmm12
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm13, %xmm2
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	%xmm14, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	%xmm15, %xmm6
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	-52(%rbp), %xmm4
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	-56(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-60(%rbp), %xmm0
	movaps	%xmm8, %xmm7
	mulss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	mulss	-64(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-68(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	mulss	-72(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-76(%rbp), %xmm11
	movaps	%xmm12, %xmm5
	mulss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	mulss	-80(%rbp), %xmm12
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm13, %xmm2
	movaps	%xmm3, %xmm7
	mulss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	mulss	%xmm14, %xmm3
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	%xmm15, %xmm7
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	-52(%rbp), %xmm4
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-56(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-60(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-64(%rbp), %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	mulss	-68(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	mulss	-72(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-76(%rbp), %xmm11
	movaps	%xmm12, %xmm5
	mulss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	mulss	-80(%rbp), %xmm12
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	mulss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	%xmm14, %xmm5
	movss	%xmm5, -84(%rbp)
	movaps	%xmm7, %xmm3
	mulss	%xmm7, %xmm3
	mulss	%xmm15, %xmm3
	movss	%xmm3, -88(%rbp)
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	-52(%rbp), %xmm3
	movss	%xmm3, -92(%rbp)
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	mulss	-56(%rbp), %xmm6
	movss	%xmm6, -96(%rbp)
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	mulss	-60(%rbp), %xmm5
	movss	%xmm5, -100(%rbp)
	movaps	%xmm8, %xmm2
	mulss	%xmm8, %xmm2
	mulss	-64(%rbp), %xmm2
	movss	%xmm2, -104(%rbp)
	movaps	%xmm9, %xmm2
	mulss	%xmm9, %xmm2
	mulss	-68(%rbp), %xmm2
	movss	%xmm2, -108(%rbp)
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	mulss	-72(%rbp), %xmm4
	movss	%xmm4, -112(%rbp)
	movaps	%xmm11, %xmm2
	mulss	%xmm11, %xmm2
	mulss	-76(%rbp), %xmm2
	movss	%xmm2, -116(%rbp)
	movaps	%xmm12, %xmm2
	mulss	%xmm12, %xmm2
	mulss	-80(%rbp), %xmm2
	movss	%xmm2, -120(%rbp)
.L577:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L578
	cvttss2sil	%xmm13, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE195:
	.size	float_mul_10, .-float_mul_10
	.globl	float_mul_11
	.type	float_mul_11, @function
float_mul_11:
.LFB196:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm14
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -140(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -52(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -56(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -60(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -108(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -68(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -116(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -76(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -124(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -84(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -132(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -88(%rbp)
	jmp	.L580
.L581:
	movaps	%xmm14, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm7, %xmm2
	movss	-136(%rbp), %xmm14
	mulss	%xmm14, %xmm2
	movss	-92(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm6, %xmm5
	movss	-140(%rbp), %xmm15
	mulss	%xmm15, %xmm5
	movss	-96(%rbp), %xmm7
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	-52(%rbp), %xmm6
	movss	-100(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm3, %xmm4
	mulss	-56(%rbp), %xmm4
	movss	-104(%rbp), %xmm7
	movaps	%xmm7, %xmm1
	mulss	%xmm7, %xmm1
	mulss	-60(%rbp), %xmm1
	movss	-108(%rbp), %xmm7
	movaps	%xmm7, %xmm0
	mulss	%xmm7, %xmm0
	mulss	-64(%rbp), %xmm0
	movss	-112(%rbp), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm7, %xmm8
	mulss	-68(%rbp), %xmm8
	movss	-116(%rbp), %xmm7
	movaps	%xmm7, %xmm9
	mulss	%xmm7, %xmm9
	mulss	-72(%rbp), %xmm9
	movss	-120(%rbp), %xmm7
	movaps	%xmm7, %xmm10
	mulss	%xmm7, %xmm10
	mulss	-76(%rbp), %xmm10
	movss	-124(%rbp), %xmm7
	movaps	%xmm7, %xmm11
	mulss	%xmm7, %xmm11
	mulss	-80(%rbp), %xmm11
	movss	-128(%rbp), %xmm7
	movaps	%xmm7, %xmm12
	mulss	%xmm7, %xmm12
	mulss	-84(%rbp), %xmm12
	movss	-132(%rbp), %xmm7
	movaps	%xmm7, %xmm13
	mulss	%xmm7, %xmm13
	mulss	-88(%rbp), %xmm13
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm14, %xmm2
	movaps	%xmm5, %xmm7
	mulss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	mulss	%xmm15, %xmm5
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	-52(%rbp), %xmm7
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	-56(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-60(%rbp), %xmm1
	movaps	%xmm0, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	mulss	-64(%rbp), %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	-68(%rbp), %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	-72(%rbp), %xmm9
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	mulss	-76(%rbp), %xmm10
	movaps	%xmm11, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	mulss	-80(%rbp), %xmm11
	movaps	%xmm12, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	mulss	-84(%rbp), %xmm12
	movaps	%xmm13, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	mulss	-88(%rbp), %xmm13
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm14, %xmm2
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	%xmm15, %xmm4
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	-52(%rbp), %xmm7
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	mulss	-56(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-60(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-64(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-68(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-72(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	mulss	-76(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-80(%rbp), %xmm11
	movaps	%xmm12, %xmm6
	mulss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	mulss	-84(%rbp), %xmm12
	movaps	%xmm13, %xmm6
	mulss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	mulss	-88(%rbp), %xmm13
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm14, %xmm2
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	%xmm15, %xmm4
	movaps	%xmm7, %xmm5
	mulss	%xmm7, %xmm5
	mulss	-52(%rbp), %xmm5
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	-56(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-60(%rbp), %xmm1
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	mulss	-64(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-68(%rbp), %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	mulss	-72(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	mulss	-76(%rbp), %xmm10
	movaps	%xmm11, %xmm7
	mulss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	mulss	-80(%rbp), %xmm11
	movaps	%xmm12, %xmm6
	mulss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	mulss	-84(%rbp), %xmm12
	movaps	%xmm13, %xmm6
	mulss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	mulss	-88(%rbp), %xmm13
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	mulss	%xmm14, %xmm2
	movaps	%xmm4, %xmm7
	mulss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	mulss	%xmm15, %xmm4
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	mulss	-52(%rbp), %xmm6
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	-56(%rbp), %xmm5
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	-60(%rbp), %xmm1
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	mulss	-64(%rbp), %xmm0
	movaps	%xmm8, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	mulss	-68(%rbp), %xmm8
	movaps	%xmm9, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	mulss	-72(%rbp), %xmm9
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	-76(%rbp), %xmm10
	movaps	%xmm11, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	mulss	-80(%rbp), %xmm11
	movaps	%xmm12, %xmm3
	mulss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	mulss	-84(%rbp), %xmm12
	movaps	%xmm13, %xmm3
	mulss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	mulss	-88(%rbp), %xmm13
	movaps	%xmm2, %xmm7
	mulss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm14, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm15, %xmm4
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	-52(%rbp), %xmm7
	movaps	%xmm5, %xmm3
	mulss	%xmm5, %xmm3
	movaps	%xmm3, %xmm5
	mulss	-56(%rbp), %xmm5
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-60(%rbp), %xmm1
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	-64(%rbp), %xmm0
	movaps	%xmm8, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	mulss	-68(%rbp), %xmm8
	movaps	%xmm9, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	mulss	-72(%rbp), %xmm9
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	-76(%rbp), %xmm10
	movaps	%xmm11, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	mulss	-80(%rbp), %xmm11
	movaps	%xmm12, %xmm3
	mulss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	mulss	-84(%rbp), %xmm12
	movaps	%xmm13, %xmm3
	mulss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	mulss	-88(%rbp), %xmm13
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm14, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	%xmm15, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	-52(%rbp), %xmm7
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	mulss	-56(%rbp), %xmm5
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-60(%rbp), %xmm1
	movaps	%xmm0, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	mulss	-64(%rbp), %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	-68(%rbp), %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	-72(%rbp), %xmm9
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	mulss	-76(%rbp), %xmm10
	movaps	%xmm11, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	mulss	-80(%rbp), %xmm11
	movaps	%xmm12, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	mulss	-84(%rbp), %xmm12
	movaps	%xmm13, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	mulss	-88(%rbp), %xmm13
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm14, %xmm2
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	%xmm15, %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	-52(%rbp), %xmm6
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	-56(%rbp), %xmm4
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	-60(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-64(%rbp), %xmm0
	movaps	%xmm8, %xmm7
	mulss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	mulss	-68(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-72(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	mulss	-76(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-80(%rbp), %xmm11
	movaps	%xmm12, %xmm5
	mulss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	mulss	-84(%rbp), %xmm12
	movaps	%xmm13, %xmm5
	mulss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	mulss	-88(%rbp), %xmm13
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm14, %xmm2
	movaps	%xmm3, %xmm7
	mulss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	mulss	%xmm15, %xmm3
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	-52(%rbp), %xmm7
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	-56(%rbp), %xmm4
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-60(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-64(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-68(%rbp), %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	mulss	-72(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	mulss	-76(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-80(%rbp), %xmm11
	movaps	%xmm12, %xmm5
	mulss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	mulss	-84(%rbp), %xmm12
	movaps	%xmm13, %xmm5
	mulss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	mulss	-88(%rbp), %xmm13
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	mulss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	%xmm15, %xmm5
	movss	%xmm5, -92(%rbp)
	movaps	%xmm7, %xmm3
	mulss	%xmm7, %xmm3
	mulss	-52(%rbp), %xmm3
	movss	%xmm3, -96(%rbp)
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	-56(%rbp), %xmm3
	movss	%xmm3, -100(%rbp)
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	mulss	-60(%rbp), %xmm6
	movss	%xmm6, -104(%rbp)
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	mulss	-64(%rbp), %xmm5
	movss	%xmm5, -108(%rbp)
	movaps	%xmm8, %xmm2
	mulss	%xmm8, %xmm2
	mulss	-68(%rbp), %xmm2
	movss	%xmm2, -112(%rbp)
	movaps	%xmm9, %xmm2
	mulss	%xmm9, %xmm2
	mulss	-72(%rbp), %xmm2
	movss	%xmm2, -116(%rbp)
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	mulss	-76(%rbp), %xmm4
	movss	%xmm4, -120(%rbp)
	movaps	%xmm11, %xmm2
	mulss	%xmm11, %xmm2
	mulss	-80(%rbp), %xmm2
	movss	%xmm2, -124(%rbp)
	movaps	%xmm12, %xmm2
	mulss	%xmm12, %xmm2
	mulss	-84(%rbp), %xmm2
	movss	%xmm2, -128(%rbp)
	movaps	%xmm13, %xmm2
	mulss	%xmm13, %xmm2
	mulss	-88(%rbp), %xmm2
	movss	%xmm2, -132(%rbp)
.L580:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L581
	cvttss2sil	%xmm14, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-140(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE196:
	.size	float_mul_11, .-float_mul_11
	.globl	float_mul_12
	.type	float_mul_12, @function
float_mul_12:
.LFB197:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm15
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -148(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -52(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -56(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -108(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -60(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -116(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -68(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -124(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -76(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -132(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -84(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -140(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -92(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -96(%rbp)
	jmp	.L583
.L584:
	movaps	%xmm15, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm7, %xmm2
	movss	-148(%rbp), %xmm15
	mulss	%xmm15, %xmm2
	movss	-100(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm6, %xmm5
	mulss	-52(%rbp), %xmm5
	movss	-104(%rbp), %xmm7
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	-56(%rbp), %xmm6
	movss	-108(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm3, %xmm4
	mulss	-60(%rbp), %xmm4
	movss	-112(%rbp), %xmm7
	movaps	%xmm7, %xmm1
	mulss	%xmm7, %xmm1
	mulss	-64(%rbp), %xmm1
	movss	-116(%rbp), %xmm7
	movaps	%xmm7, %xmm0
	mulss	%xmm7, %xmm0
	mulss	-68(%rbp), %xmm0
	movss	-120(%rbp), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm7, %xmm8
	mulss	-72(%rbp), %xmm8
	movss	-124(%rbp), %xmm7
	movaps	%xmm7, %xmm9
	mulss	%xmm7, %xmm9
	mulss	-76(%rbp), %xmm9
	movss	-128(%rbp), %xmm7
	movaps	%xmm7, %xmm10
	mulss	%xmm7, %xmm10
	mulss	-80(%rbp), %xmm10
	movss	-132(%rbp), %xmm7
	movaps	%xmm7, %xmm11
	mulss	%xmm7, %xmm11
	mulss	-84(%rbp), %xmm11
	movss	-136(%rbp), %xmm7
	movaps	%xmm7, %xmm12
	mulss	%xmm7, %xmm12
	mulss	-88(%rbp), %xmm12
	movss	-140(%rbp), %xmm7
	movaps	%xmm7, %xmm13
	mulss	%xmm7, %xmm13
	mulss	-92(%rbp), %xmm13
	movss	-144(%rbp), %xmm7
	movaps	%xmm7, %xmm14
	mulss	%xmm7, %xmm14
	mulss	-96(%rbp), %xmm14
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm15, %xmm2
	movaps	%xmm5, %xmm7
	mulss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	mulss	-52(%rbp), %xmm5
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	-56(%rbp), %xmm7
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	-60(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-64(%rbp), %xmm1
	movaps	%xmm0, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	mulss	-68(%rbp), %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	-72(%rbp), %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	-76(%rbp), %xmm9
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	mulss	-80(%rbp), %xmm10
	movaps	%xmm11, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	mulss	-84(%rbp), %xmm11
	movaps	%xmm12, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	mulss	-88(%rbp), %xmm12
	movaps	%xmm13, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	mulss	-92(%rbp), %xmm13
	movaps	%xmm14, %xmm4
	mulss	%xmm14, %xmm4
	movaps	%xmm4, %xmm14
	mulss	-96(%rbp), %xmm14
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm15, %xmm2
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	-52(%rbp), %xmm4
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	-56(%rbp), %xmm7
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	mulss	-60(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-64(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-68(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-72(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-76(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	mulss	-80(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-84(%rbp), %xmm11
	movaps	%xmm12, %xmm6
	mulss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	mulss	-88(%rbp), %xmm12
	movaps	%xmm13, %xmm6
	mulss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	mulss	-92(%rbp), %xmm13
	movaps	%xmm14, %xmm6
	mulss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	mulss	-96(%rbp), %xmm14
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm15, %xmm2
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	-52(%rbp), %xmm4
	movaps	%xmm7, %xmm5
	mulss	%xmm7, %xmm5
	mulss	-56(%rbp), %xmm5
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	-60(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-64(%rbp), %xmm1
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	mulss	-68(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-72(%rbp), %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	mulss	-76(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	mulss	-80(%rbp), %xmm10
	movaps	%xmm11, %xmm7
	mulss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	mulss	-84(%rbp), %xmm11
	movaps	%xmm12, %xmm6
	mulss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	mulss	-88(%rbp), %xmm12
	movaps	%xmm13, %xmm6
	mulss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	mulss	-92(%rbp), %xmm13
	movaps	%xmm14, %xmm7
	mulss	%xmm14, %xmm7
	movaps	%xmm7, %xmm14
	mulss	-96(%rbp), %xmm14
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	mulss	%xmm15, %xmm2
	movaps	%xmm4, %xmm7
	mulss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	mulss	-52(%rbp), %xmm4
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	mulss	-56(%rbp), %xmm6
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	-60(%rbp), %xmm5
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	-64(%rbp), %xmm1
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	mulss	-68(%rbp), %xmm0
	movaps	%xmm8, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	mulss	-72(%rbp), %xmm8
	movaps	%xmm9, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	mulss	-76(%rbp), %xmm9
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	-80(%rbp), %xmm10
	movaps	%xmm11, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	mulss	-84(%rbp), %xmm11
	movaps	%xmm12, %xmm3
	mulss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	mulss	-88(%rbp), %xmm12
	movaps	%xmm13, %xmm3
	mulss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	mulss	-92(%rbp), %xmm13
	movaps	%xmm14, %xmm3
	mulss	%xmm14, %xmm3
	movaps	%xmm3, %xmm14
	mulss	-96(%rbp), %xmm14
	movaps	%xmm2, %xmm7
	mulss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm15, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	mulss	-52(%rbp), %xmm4
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	-56(%rbp), %xmm7
	movaps	%xmm5, %xmm3
	mulss	%xmm5, %xmm3
	movaps	%xmm3, %xmm5
	mulss	-60(%rbp), %xmm5
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-64(%rbp), %xmm1
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	-68(%rbp), %xmm0
	movaps	%xmm8, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	mulss	-72(%rbp), %xmm8
	movaps	%xmm9, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	mulss	-76(%rbp), %xmm9
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	-80(%rbp), %xmm10
	movaps	%xmm11, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	mulss	-84(%rbp), %xmm11
	movaps	%xmm12, %xmm3
	mulss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	mulss	-88(%rbp), %xmm12
	movaps	%xmm13, %xmm3
	mulss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	mulss	-92(%rbp), %xmm13
	movaps	%xmm14, %xmm3
	mulss	%xmm14, %xmm3
	movaps	%xmm3, %xmm14
	mulss	-96(%rbp), %xmm14
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	%xmm15, %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	-52(%rbp), %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	-56(%rbp), %xmm7
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	mulss	-60(%rbp), %xmm5
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-64(%rbp), %xmm1
	movaps	%xmm0, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	mulss	-68(%rbp), %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	-72(%rbp), %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	-76(%rbp), %xmm9
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	mulss	-80(%rbp), %xmm10
	movaps	%xmm11, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	mulss	-84(%rbp), %xmm11
	movaps	%xmm12, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	mulss	-88(%rbp), %xmm12
	movaps	%xmm13, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	mulss	-92(%rbp), %xmm13
	movaps	%xmm14, %xmm4
	mulss	%xmm14, %xmm4
	movaps	%xmm4, %xmm14
	mulss	-96(%rbp), %xmm14
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	%xmm15, %xmm2
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	-52(%rbp), %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	-56(%rbp), %xmm6
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	-60(%rbp), %xmm4
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	-64(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-68(%rbp), %xmm0
	movaps	%xmm8, %xmm7
	mulss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	mulss	-72(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-76(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	mulss	-80(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-84(%rbp), %xmm11
	movaps	%xmm12, %xmm5
	mulss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	mulss	-88(%rbp), %xmm12
	movaps	%xmm13, %xmm5
	mulss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	mulss	-92(%rbp), %xmm13
	movaps	%xmm14, %xmm5
	mulss	%xmm14, %xmm5
	movaps	%xmm5, %xmm14
	mulss	-96(%rbp), %xmm14
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm15, %xmm2
	movaps	%xmm3, %xmm7
	mulss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	mulss	-52(%rbp), %xmm3
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	-56(%rbp), %xmm7
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	-60(%rbp), %xmm4
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-64(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-68(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-72(%rbp), %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	mulss	-76(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	mulss	-80(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-84(%rbp), %xmm11
	movaps	%xmm12, %xmm5
	mulss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	mulss	-88(%rbp), %xmm12
	movaps	%xmm13, %xmm5
	mulss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	mulss	-92(%rbp), %xmm13
	movaps	%xmm14, %xmm6
	mulss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	mulss	-96(%rbp), %xmm14
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	mulss	%xmm15, %xmm6
	movaps	%xmm6, %xmm15
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	-52(%rbp), %xmm5
	movss	%xmm5, -100(%rbp)
	movaps	%xmm7, %xmm3
	mulss	%xmm7, %xmm3
	mulss	-56(%rbp), %xmm3
	movss	%xmm3, -104(%rbp)
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	-60(%rbp), %xmm3
	movss	%xmm3, -108(%rbp)
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	mulss	-64(%rbp), %xmm6
	movss	%xmm6, -112(%rbp)
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	mulss	-68(%rbp), %xmm5
	movss	%xmm5, -116(%rbp)
	movaps	%xmm8, %xmm2
	mulss	%xmm8, %xmm2
	mulss	-72(%rbp), %xmm2
	movss	%xmm2, -120(%rbp)
	movaps	%xmm9, %xmm2
	mulss	%xmm9, %xmm2
	mulss	-76(%rbp), %xmm2
	movss	%xmm2, -124(%rbp)
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	mulss	-80(%rbp), %xmm4
	movss	%xmm4, -128(%rbp)
	movaps	%xmm11, %xmm2
	mulss	%xmm11, %xmm2
	mulss	-84(%rbp), %xmm2
	movss	%xmm2, -132(%rbp)
	movaps	%xmm12, %xmm2
	mulss	%xmm12, %xmm2
	mulss	-88(%rbp), %xmm2
	movss	%xmm2, -136(%rbp)
	movaps	%xmm13, %xmm2
	mulss	%xmm13, %xmm2
	mulss	-92(%rbp), %xmm2
	movss	%xmm2, -140(%rbp)
	movaps	%xmm14, %xmm2
	mulss	%xmm14, %xmm2
	mulss	-96(%rbp), %xmm2
	movss	%xmm2, -144(%rbp)
.L583:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L584
	cvttss2sil	%xmm15, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-148(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-140(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE197:
	.size	float_mul_12, .-float_mul_12
	.globl	float_mul_13
	.type	float_mul_13, @function
float_mul_13:
.LFB198:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -108(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -52(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -56(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -116(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -60(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -124(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -68(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -132(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -76(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -140(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -84(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -148(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -92(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -156(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -100(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -104(%rbp)
	jmp	.L586
.L587:
	movss	-108(%rbp), %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm7, %xmm2
	mulss	-52(%rbp), %xmm2
	movss	-112(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm6, %xmm5
	mulss	-56(%rbp), %xmm5
	movss	-116(%rbp), %xmm7
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	-60(%rbp), %xmm6
	movss	-120(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm3, %xmm4
	mulss	-64(%rbp), %xmm4
	movss	-124(%rbp), %xmm7
	movaps	%xmm7, %xmm1
	mulss	%xmm7, %xmm1
	mulss	-68(%rbp), %xmm1
	movss	-128(%rbp), %xmm7
	movaps	%xmm7, %xmm0
	mulss	%xmm7, %xmm0
	mulss	-72(%rbp), %xmm0
	movss	-132(%rbp), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm7, %xmm8
	mulss	-76(%rbp), %xmm8
	movss	-136(%rbp), %xmm7
	movaps	%xmm7, %xmm9
	mulss	%xmm7, %xmm9
	mulss	-80(%rbp), %xmm9
	movss	-140(%rbp), %xmm7
	movaps	%xmm7, %xmm10
	mulss	%xmm7, %xmm10
	mulss	-84(%rbp), %xmm10
	movss	-144(%rbp), %xmm7
	movaps	%xmm7, %xmm11
	mulss	%xmm7, %xmm11
	mulss	-88(%rbp), %xmm11
	movss	-148(%rbp), %xmm7
	movaps	%xmm7, %xmm12
	mulss	%xmm7, %xmm12
	mulss	-92(%rbp), %xmm12
	movss	-152(%rbp), %xmm7
	movaps	%xmm7, %xmm13
	mulss	%xmm7, %xmm13
	mulss	-96(%rbp), %xmm13
	movss	-156(%rbp), %xmm7
	movaps	%xmm7, %xmm14
	mulss	%xmm7, %xmm14
	mulss	-100(%rbp), %xmm14
	movss	-160(%rbp), %xmm7
	movaps	%xmm7, %xmm15
	mulss	%xmm7, %xmm15
	mulss	-104(%rbp), %xmm15
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	-52(%rbp), %xmm2
	movaps	%xmm5, %xmm7
	mulss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	mulss	-56(%rbp), %xmm5
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	-60(%rbp), %xmm7
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	-64(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-68(%rbp), %xmm1
	movaps	%xmm0, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	mulss	-72(%rbp), %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	-76(%rbp), %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	-80(%rbp), %xmm9
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	mulss	-84(%rbp), %xmm10
	movaps	%xmm11, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	mulss	-88(%rbp), %xmm11
	movaps	%xmm12, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	mulss	-92(%rbp), %xmm12
	movaps	%xmm13, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	mulss	-96(%rbp), %xmm13
	movaps	%xmm14, %xmm4
	mulss	%xmm14, %xmm4
	movaps	%xmm4, %xmm14
	mulss	-100(%rbp), %xmm14
	movaps	%xmm15, %xmm4
	mulss	%xmm15, %xmm4
	movaps	%xmm4, %xmm15
	mulss	-104(%rbp), %xmm15
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	-52(%rbp), %xmm2
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	-56(%rbp), %xmm4
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	-60(%rbp), %xmm7
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	mulss	-64(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-68(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-72(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-76(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-80(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	mulss	-84(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-88(%rbp), %xmm11
	movaps	%xmm12, %xmm6
	mulss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	mulss	-92(%rbp), %xmm12
	movaps	%xmm13, %xmm6
	mulss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	mulss	-96(%rbp), %xmm13
	movaps	%xmm14, %xmm6
	mulss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	mulss	-100(%rbp), %xmm14
	movaps	%xmm15, %xmm5
	mulss	%xmm15, %xmm5
	movaps	%xmm5, %xmm15
	mulss	-104(%rbp), %xmm15
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	-52(%rbp), %xmm2
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	-56(%rbp), %xmm4
	movaps	%xmm7, %xmm5
	mulss	%xmm7, %xmm5
	mulss	-60(%rbp), %xmm5
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	-64(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-68(%rbp), %xmm1
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	mulss	-72(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-76(%rbp), %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	mulss	-80(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	mulss	-84(%rbp), %xmm10
	movaps	%xmm11, %xmm7
	mulss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	mulss	-88(%rbp), %xmm11
	movaps	%xmm12, %xmm6
	mulss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	mulss	-92(%rbp), %xmm12
	movaps	%xmm13, %xmm6
	mulss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	mulss	-96(%rbp), %xmm13
	movaps	%xmm14, %xmm7
	mulss	%xmm14, %xmm7
	movaps	%xmm7, %xmm14
	mulss	-100(%rbp), %xmm14
	movaps	%xmm15, %xmm6
	mulss	%xmm15, %xmm6
	movaps	%xmm6, %xmm15
	mulss	-104(%rbp), %xmm15
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	mulss	-52(%rbp), %xmm2
	movaps	%xmm4, %xmm7
	mulss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	mulss	-56(%rbp), %xmm4
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	mulss	-60(%rbp), %xmm6
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	-64(%rbp), %xmm5
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	-68(%rbp), %xmm1
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	mulss	-72(%rbp), %xmm0
	movaps	%xmm8, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	mulss	-76(%rbp), %xmm8
	movaps	%xmm9, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	mulss	-80(%rbp), %xmm9
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	-84(%rbp), %xmm10
	movaps	%xmm11, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	mulss	-88(%rbp), %xmm11
	movaps	%xmm12, %xmm3
	mulss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	mulss	-92(%rbp), %xmm12
	movaps	%xmm13, %xmm3
	mulss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	mulss	-96(%rbp), %xmm13
	movaps	%xmm14, %xmm3
	mulss	%xmm14, %xmm3
	movaps	%xmm3, %xmm14
	mulss	-100(%rbp), %xmm14
	movaps	%xmm15, %xmm3
	mulss	%xmm15, %xmm3
	movaps	%xmm3, %xmm15
	mulss	-104(%rbp), %xmm15
	movaps	%xmm2, %xmm7
	mulss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	mulss	-52(%rbp), %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	mulss	-56(%rbp), %xmm4
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	-60(%rbp), %xmm7
	movaps	%xmm5, %xmm3
	mulss	%xmm5, %xmm3
	movaps	%xmm3, %xmm5
	mulss	-64(%rbp), %xmm5
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-68(%rbp), %xmm1
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	-72(%rbp), %xmm0
	movaps	%xmm8, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	mulss	-76(%rbp), %xmm8
	movaps	%xmm9, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	mulss	-80(%rbp), %xmm9
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	-84(%rbp), %xmm10
	movaps	%xmm11, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	mulss	-88(%rbp), %xmm11
	movaps	%xmm12, %xmm3
	mulss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	mulss	-92(%rbp), %xmm12
	movaps	%xmm13, %xmm3
	mulss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	mulss	-96(%rbp), %xmm13
	movaps	%xmm14, %xmm3
	mulss	%xmm14, %xmm3
	movaps	%xmm3, %xmm14
	mulss	-100(%rbp), %xmm14
	movaps	%xmm15, %xmm3
	mulss	%xmm15, %xmm3
	movaps	%xmm3, %xmm15
	mulss	-104(%rbp), %xmm15
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	-52(%rbp), %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	-56(%rbp), %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	-60(%rbp), %xmm7
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	mulss	-64(%rbp), %xmm5
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-68(%rbp), %xmm1
	movaps	%xmm0, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	mulss	-72(%rbp), %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	-76(%rbp), %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	-80(%rbp), %xmm9
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	mulss	-84(%rbp), %xmm10
	movaps	%xmm11, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	mulss	-88(%rbp), %xmm11
	movaps	%xmm12, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	mulss	-92(%rbp), %xmm12
	movaps	%xmm13, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	mulss	-96(%rbp), %xmm13
	movaps	%xmm14, %xmm4
	mulss	%xmm14, %xmm4
	movaps	%xmm4, %xmm14
	mulss	-100(%rbp), %xmm14
	movaps	%xmm15, %xmm4
	mulss	%xmm15, %xmm4
	movaps	%xmm4, %xmm15
	mulss	-104(%rbp), %xmm15
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	-52(%rbp), %xmm2
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	-56(%rbp), %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	-60(%rbp), %xmm6
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	-64(%rbp), %xmm4
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	-68(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-72(%rbp), %xmm0
	movaps	%xmm8, %xmm7
	mulss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	mulss	-76(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-80(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	mulss	-84(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-88(%rbp), %xmm11
	movaps	%xmm12, %xmm5
	mulss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	mulss	-92(%rbp), %xmm12
	movaps	%xmm13, %xmm5
	mulss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	mulss	-96(%rbp), %xmm13
	movaps	%xmm14, %xmm5
	mulss	%xmm14, %xmm5
	movaps	%xmm5, %xmm14
	mulss	-100(%rbp), %xmm14
	movaps	%xmm15, %xmm5
	mulss	%xmm15, %xmm5
	movaps	%xmm5, %xmm15
	mulss	-104(%rbp), %xmm15
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	-52(%rbp), %xmm2
	movaps	%xmm3, %xmm7
	mulss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	mulss	-56(%rbp), %xmm3
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	-60(%rbp), %xmm7
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	-64(%rbp), %xmm4
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-68(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-72(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-76(%rbp), %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	mulss	-80(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	mulss	-84(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-88(%rbp), %xmm11
	movaps	%xmm12, %xmm5
	mulss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	mulss	-92(%rbp), %xmm12
	movaps	%xmm13, %xmm5
	mulss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	mulss	-96(%rbp), %xmm13
	movaps	%xmm14, %xmm6
	mulss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	mulss	-100(%rbp), %xmm14
	movaps	%xmm15, %xmm6
	mulss	%xmm15, %xmm6
	movaps	%xmm6, %xmm15
	mulss	-104(%rbp), %xmm15
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	mulss	-52(%rbp), %xmm6
	movss	%xmm6, -108(%rbp)
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	-56(%rbp), %xmm5
	movss	%xmm5, -112(%rbp)
	movaps	%xmm7, %xmm3
	mulss	%xmm7, %xmm3
	mulss	-60(%rbp), %xmm3
	movss	%xmm3, -116(%rbp)
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	-64(%rbp), %xmm3
	movss	%xmm3, -120(%rbp)
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	mulss	-68(%rbp), %xmm6
	movss	%xmm6, -124(%rbp)
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	mulss	-72(%rbp), %xmm5
	movss	%xmm5, -128(%rbp)
	movaps	%xmm8, %xmm2
	mulss	%xmm8, %xmm2
	mulss	-76(%rbp), %xmm2
	movss	%xmm2, -132(%rbp)
	movaps	%xmm9, %xmm2
	mulss	%xmm9, %xmm2
	mulss	-80(%rbp), %xmm2
	movss	%xmm2, -136(%rbp)
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	mulss	-84(%rbp), %xmm4
	movss	%xmm4, -140(%rbp)
	movaps	%xmm11, %xmm2
	mulss	%xmm11, %xmm2
	mulss	-88(%rbp), %xmm2
	movss	%xmm2, -144(%rbp)
	movaps	%xmm12, %xmm2
	mulss	%xmm12, %xmm2
	mulss	-92(%rbp), %xmm2
	movss	%xmm2, -148(%rbp)
	movaps	%xmm13, %xmm2
	mulss	%xmm13, %xmm2
	mulss	-96(%rbp), %xmm2
	movss	%xmm2, -152(%rbp)
	movaps	%xmm14, %xmm2
	mulss	%xmm14, %xmm2
	mulss	-100(%rbp), %xmm2
	movss	%xmm2, -156(%rbp)
	movaps	%xmm15, %xmm5
	mulss	%xmm15, %xmm5
	mulss	-104(%rbp), %xmm5
	movss	%xmm5, -160(%rbp)
.L586:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L587
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-140(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-148(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-156(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE198:
	.size	float_mul_13, .-float_mul_13
	.globl	float_mul_14
	.type	float_mul_14, @function
float_mul_14:
.LFB199:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -116(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -56(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -60(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -124(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -68(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -132(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -76(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -140(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -84(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -148(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -92(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -156(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -100(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -164(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -108(%rbp)
	movq	-24(%rbp), %rax
	movsd	192(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	192(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -112(%rbp)
	jmp	.L589
.L590:
	movss	-116(%rbp), %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm7, %xmm2
	mulss	-56(%rbp), %xmm2
	movss	-120(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm6, %xmm5
	mulss	-60(%rbp), %xmm5
	movss	-124(%rbp), %xmm7
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	-64(%rbp), %xmm6
	movss	-128(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm3, %xmm4
	mulss	-68(%rbp), %xmm4
	movss	-132(%rbp), %xmm7
	movaps	%xmm7, %xmm1
	mulss	%xmm7, %xmm1
	mulss	-72(%rbp), %xmm1
	movss	-136(%rbp), %xmm7
	movaps	%xmm7, %xmm0
	mulss	%xmm7, %xmm0
	mulss	-76(%rbp), %xmm0
	movss	-140(%rbp), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm7, %xmm8
	mulss	-80(%rbp), %xmm8
	movss	-144(%rbp), %xmm7
	movaps	%xmm7, %xmm9
	mulss	%xmm7, %xmm9
	mulss	-84(%rbp), %xmm9
	movss	-148(%rbp), %xmm7
	movaps	%xmm7, %xmm10
	mulss	%xmm7, %xmm10
	mulss	-88(%rbp), %xmm10
	movss	-152(%rbp), %xmm7
	movaps	%xmm7, %xmm11
	mulss	%xmm7, %xmm11
	mulss	-92(%rbp), %xmm11
	movss	-156(%rbp), %xmm7
	movaps	%xmm7, %xmm12
	mulss	%xmm7, %xmm12
	mulss	-96(%rbp), %xmm12
	movss	-160(%rbp), %xmm7
	movaps	%xmm7, %xmm13
	mulss	%xmm7, %xmm13
	mulss	-100(%rbp), %xmm13
	movss	-164(%rbp), %xmm7
	movaps	%xmm7, %xmm14
	mulss	%xmm7, %xmm14
	mulss	-104(%rbp), %xmm14
	movss	-168(%rbp), %xmm7
	movaps	%xmm7, %xmm15
	mulss	%xmm7, %xmm15
	mulss	-108(%rbp), %xmm15
	movss	-52(%rbp), %xmm7
	movaps	%xmm7, %xmm3
	mulss	%xmm7, %xmm3
	mulss	-112(%rbp), %xmm3
	movss	%xmm3, -52(%rbp)
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	-56(%rbp), %xmm2
	movaps	%xmm5, %xmm7
	mulss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	mulss	-60(%rbp), %xmm5
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	-64(%rbp), %xmm7
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	-68(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-72(%rbp), %xmm1
	movaps	%xmm0, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	mulss	-76(%rbp), %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	-80(%rbp), %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	-84(%rbp), %xmm9
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	mulss	-88(%rbp), %xmm10
	movaps	%xmm11, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	mulss	-92(%rbp), %xmm11
	movaps	%xmm12, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	mulss	-96(%rbp), %xmm12
	movaps	%xmm13, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	mulss	-100(%rbp), %xmm13
	movaps	%xmm14, %xmm4
	mulss	%xmm14, %xmm4
	movaps	%xmm4, %xmm14
	mulss	-104(%rbp), %xmm14
	movaps	%xmm15, %xmm4
	mulss	%xmm15, %xmm4
	movaps	%xmm4, %xmm15
	mulss	-108(%rbp), %xmm15
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm4
	mulss	%xmm6, %xmm4
	mulss	-112(%rbp), %xmm4
	movss	%xmm4, -52(%rbp)
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	-56(%rbp), %xmm2
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	-60(%rbp), %xmm4
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	-64(%rbp), %xmm7
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	mulss	-68(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-72(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-76(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-80(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-84(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	mulss	-88(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-92(%rbp), %xmm11
	movaps	%xmm12, %xmm6
	mulss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	mulss	-96(%rbp), %xmm12
	movaps	%xmm13, %xmm6
	mulss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	mulss	-100(%rbp), %xmm13
	movaps	%xmm14, %xmm6
	mulss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	mulss	-104(%rbp), %xmm14
	movaps	%xmm15, %xmm5
	mulss	%xmm15, %xmm5
	movaps	%xmm5, %xmm15
	mulss	-108(%rbp), %xmm15
	movss	-52(%rbp), %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	mulss	-112(%rbp), %xmm6
	movss	%xmm6, -52(%rbp)
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	-56(%rbp), %xmm2
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	-60(%rbp), %xmm4
	movaps	%xmm7, %xmm5
	mulss	%xmm7, %xmm5
	mulss	-64(%rbp), %xmm5
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	-68(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-72(%rbp), %xmm1
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	mulss	-76(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-80(%rbp), %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	mulss	-84(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	mulss	-88(%rbp), %xmm10
	movaps	%xmm11, %xmm7
	mulss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	mulss	-92(%rbp), %xmm11
	movaps	%xmm12, %xmm6
	mulss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	mulss	-96(%rbp), %xmm12
	movaps	%xmm13, %xmm6
	mulss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	mulss	-100(%rbp), %xmm13
	movaps	%xmm14, %xmm7
	mulss	%xmm14, %xmm7
	movaps	%xmm7, %xmm14
	mulss	-104(%rbp), %xmm14
	movaps	%xmm15, %xmm6
	mulss	%xmm15, %xmm6
	movaps	%xmm6, %xmm15
	mulss	-108(%rbp), %xmm15
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	-112(%rbp), %xmm7
	movss	%xmm7, -52(%rbp)
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	mulss	-56(%rbp), %xmm2
	movaps	%xmm4, %xmm7
	mulss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	mulss	-60(%rbp), %xmm4
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	mulss	-64(%rbp), %xmm6
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	-68(%rbp), %xmm5
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	-72(%rbp), %xmm1
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	mulss	-76(%rbp), %xmm0
	movaps	%xmm8, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	mulss	-80(%rbp), %xmm8
	movaps	%xmm9, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	mulss	-84(%rbp), %xmm9
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	-88(%rbp), %xmm10
	movaps	%xmm11, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	mulss	-92(%rbp), %xmm11
	movaps	%xmm12, %xmm3
	mulss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	mulss	-96(%rbp), %xmm12
	movaps	%xmm13, %xmm3
	mulss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	mulss	-100(%rbp), %xmm13
	movaps	%xmm14, %xmm3
	mulss	%xmm14, %xmm3
	movaps	%xmm3, %xmm14
	mulss	-104(%rbp), %xmm14
	movaps	%xmm15, %xmm3
	mulss	%xmm15, %xmm3
	movaps	%xmm3, %xmm15
	mulss	-108(%rbp), %xmm15
	movss	-52(%rbp), %xmm7
	movaps	%xmm7, %xmm3
	mulss	%xmm7, %xmm3
	mulss	-112(%rbp), %xmm3
	movss	%xmm3, -52(%rbp)
	movaps	%xmm2, %xmm7
	mulss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	mulss	-56(%rbp), %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	mulss	-60(%rbp), %xmm4
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	-64(%rbp), %xmm7
	movaps	%xmm5, %xmm3
	mulss	%xmm5, %xmm3
	movaps	%xmm3, %xmm5
	mulss	-68(%rbp), %xmm5
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-72(%rbp), %xmm1
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	-76(%rbp), %xmm0
	movaps	%xmm8, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	mulss	-80(%rbp), %xmm8
	movaps	%xmm9, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	mulss	-84(%rbp), %xmm9
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	-88(%rbp), %xmm10
	movaps	%xmm11, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	mulss	-92(%rbp), %xmm11
	movaps	%xmm12, %xmm3
	mulss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	mulss	-96(%rbp), %xmm12
	movaps	%xmm13, %xmm3
	mulss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	mulss	-100(%rbp), %xmm13
	movaps	%xmm14, %xmm3
	mulss	%xmm14, %xmm3
	movaps	%xmm3, %xmm14
	mulss	-104(%rbp), %xmm14
	movaps	%xmm15, %xmm3
	mulss	%xmm15, %xmm3
	movaps	%xmm3, %xmm15
	mulss	-108(%rbp), %xmm15
	movss	-52(%rbp), %xmm3
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	mulss	-112(%rbp), %xmm6
	movss	%xmm6, -52(%rbp)
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	-56(%rbp), %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	-60(%rbp), %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	-64(%rbp), %xmm7
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	mulss	-68(%rbp), %xmm5
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-72(%rbp), %xmm1
	movaps	%xmm0, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	mulss	-76(%rbp), %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	-80(%rbp), %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	-84(%rbp), %xmm9
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	mulss	-88(%rbp), %xmm10
	movaps	%xmm11, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	mulss	-92(%rbp), %xmm11
	movaps	%xmm12, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	mulss	-96(%rbp), %xmm12
	movaps	%xmm13, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	mulss	-100(%rbp), %xmm13
	movaps	%xmm14, %xmm4
	mulss	%xmm14, %xmm4
	movaps	%xmm4, %xmm14
	mulss	-104(%rbp), %xmm14
	movaps	%xmm15, %xmm4
	mulss	%xmm15, %xmm4
	movaps	%xmm4, %xmm15
	mulss	-108(%rbp), %xmm15
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm4
	mulss	%xmm6, %xmm4
	mulss	-112(%rbp), %xmm4
	movss	%xmm4, -52(%rbp)
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	-56(%rbp), %xmm2
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	-60(%rbp), %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	-64(%rbp), %xmm6
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	-68(%rbp), %xmm4
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	-72(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-76(%rbp), %xmm0
	movaps	%xmm8, %xmm7
	mulss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	mulss	-80(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-84(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	mulss	-88(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-92(%rbp), %xmm11
	movaps	%xmm12, %xmm5
	mulss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	mulss	-96(%rbp), %xmm12
	movaps	%xmm13, %xmm5
	mulss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	mulss	-100(%rbp), %xmm13
	movaps	%xmm14, %xmm5
	mulss	%xmm14, %xmm5
	movaps	%xmm5, %xmm14
	mulss	-104(%rbp), %xmm14
	movaps	%xmm15, %xmm5
	mulss	%xmm15, %xmm5
	movaps	%xmm5, %xmm15
	mulss	-108(%rbp), %xmm15
	movss	-52(%rbp), %xmm7
	movaps	%xmm7, %xmm5
	mulss	%xmm7, %xmm5
	mulss	-112(%rbp), %xmm5
	movss	%xmm5, -52(%rbp)
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	-56(%rbp), %xmm2
	movaps	%xmm3, %xmm7
	mulss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	mulss	-60(%rbp), %xmm3
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	-64(%rbp), %xmm7
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	-68(%rbp), %xmm4
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-72(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-76(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-80(%rbp), %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	mulss	-84(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	mulss	-88(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-92(%rbp), %xmm11
	movaps	%xmm12, %xmm5
	mulss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	mulss	-96(%rbp), %xmm12
	movaps	%xmm13, %xmm5
	mulss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	mulss	-100(%rbp), %xmm13
	movaps	%xmm14, %xmm6
	mulss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	mulss	-104(%rbp), %xmm14
	movaps	%xmm15, %xmm6
	mulss	%xmm15, %xmm6
	movaps	%xmm6, %xmm15
	mulss	-108(%rbp), %xmm15
	movss	-52(%rbp), %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	mulss	-112(%rbp), %xmm6
	movss	%xmm6, -52(%rbp)
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	mulss	-56(%rbp), %xmm6
	movss	%xmm6, -116(%rbp)
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	-60(%rbp), %xmm5
	movss	%xmm5, -120(%rbp)
	movaps	%xmm7, %xmm3
	mulss	%xmm7, %xmm3
	mulss	-64(%rbp), %xmm3
	movss	%xmm3, -124(%rbp)
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	-68(%rbp), %xmm3
	movss	%xmm3, -128(%rbp)
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	mulss	-72(%rbp), %xmm6
	movss	%xmm6, -132(%rbp)
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	mulss	-76(%rbp), %xmm5
	movss	%xmm5, -136(%rbp)
	movaps	%xmm8, %xmm2
	mulss	%xmm8, %xmm2
	mulss	-80(%rbp), %xmm2
	movss	%xmm2, -140(%rbp)
	movaps	%xmm9, %xmm2
	mulss	%xmm9, %xmm2
	mulss	-84(%rbp), %xmm2
	movss	%xmm2, -144(%rbp)
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	mulss	-88(%rbp), %xmm4
	movss	%xmm4, -148(%rbp)
	movaps	%xmm11, %xmm2
	mulss	%xmm11, %xmm2
	mulss	-92(%rbp), %xmm2
	movss	%xmm2, -152(%rbp)
	movaps	%xmm12, %xmm2
	mulss	%xmm12, %xmm2
	mulss	-96(%rbp), %xmm2
	movss	%xmm2, -156(%rbp)
	movaps	%xmm13, %xmm2
	mulss	%xmm13, %xmm2
	mulss	-100(%rbp), %xmm2
	movss	%xmm2, -160(%rbp)
	movaps	%xmm14, %xmm2
	mulss	%xmm14, %xmm2
	mulss	-104(%rbp), %xmm2
	movss	%xmm2, -164(%rbp)
	movaps	%xmm15, %xmm5
	mulss	%xmm15, %xmm5
	mulss	-108(%rbp), %xmm5
	movss	%xmm5, -168(%rbp)
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm2
	mulss	%xmm6, %xmm2
	mulss	-112(%rbp), %xmm2
	movss	%xmm2, -52(%rbp)
.L589:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L590
	cvttss2sil	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-140(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-148(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-156(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-164(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE199:
	.size	float_mul_14, .-float_mul_14
	.globl	float_mul_15
	.type	float_mul_15, @function
float_mul_15:
.LFB200:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -124(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -60(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -132(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -68(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -140(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -76(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -148(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -84(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -156(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -92(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -164(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -100(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -172(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -108(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -176(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	192(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	192(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -116(%rbp)
	movq	-24(%rbp), %rax
	movsd	200(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC4(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2ssl	%eax, %xmm0
	pxor	%xmm1, %xmm1
	cvtss2sd	%xmm0, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	200(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	pxor	%xmm2, %xmm2
	cvtsd2ss	%xmm0, %xmm2
	movss	%xmm2, -120(%rbp)
	jmp	.L592
.L593:
	movss	-124(%rbp), %xmm7
	movaps	%xmm7, %xmm2
	mulss	%xmm7, %xmm2
	mulss	-60(%rbp), %xmm2
	movss	-128(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm6, %xmm5
	mulss	-64(%rbp), %xmm5
	movss	-132(%rbp), %xmm7
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	-68(%rbp), %xmm6
	movss	-136(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	mulss	%xmm3, %xmm4
	mulss	-72(%rbp), %xmm4
	movss	-140(%rbp), %xmm7
	movaps	%xmm7, %xmm1
	mulss	%xmm7, %xmm1
	mulss	-76(%rbp), %xmm1
	movss	-144(%rbp), %xmm7
	movaps	%xmm7, %xmm0
	mulss	%xmm7, %xmm0
	mulss	-80(%rbp), %xmm0
	movss	-148(%rbp), %xmm7
	movaps	%xmm7, %xmm8
	mulss	%xmm7, %xmm8
	mulss	-84(%rbp), %xmm8
	movss	-152(%rbp), %xmm7
	movaps	%xmm7, %xmm9
	mulss	%xmm7, %xmm9
	mulss	-88(%rbp), %xmm9
	movss	-156(%rbp), %xmm7
	movaps	%xmm7, %xmm10
	mulss	%xmm7, %xmm10
	mulss	-92(%rbp), %xmm10
	movss	-160(%rbp), %xmm7
	movaps	%xmm7, %xmm11
	mulss	%xmm7, %xmm11
	mulss	-96(%rbp), %xmm11
	movss	-164(%rbp), %xmm7
	movaps	%xmm7, %xmm12
	mulss	%xmm7, %xmm12
	mulss	-100(%rbp), %xmm12
	movss	-168(%rbp), %xmm7
	movaps	%xmm7, %xmm13
	mulss	%xmm7, %xmm13
	mulss	-104(%rbp), %xmm13
	movss	-172(%rbp), %xmm7
	movaps	%xmm7, %xmm14
	mulss	%xmm7, %xmm14
	mulss	-108(%rbp), %xmm14
	movss	-176(%rbp), %xmm7
	movaps	%xmm7, %xmm15
	mulss	%xmm7, %xmm15
	mulss	-112(%rbp), %xmm15
	movss	-52(%rbp), %xmm7
	movaps	%xmm7, %xmm3
	mulss	%xmm7, %xmm3
	mulss	-116(%rbp), %xmm3
	movss	%xmm3, -52(%rbp)
	movss	-56(%rbp), %xmm3
	movaps	%xmm3, %xmm7
	mulss	%xmm3, %xmm7
	mulss	-120(%rbp), %xmm7
	movss	%xmm7, -56(%rbp)
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	-60(%rbp), %xmm2
	movaps	%xmm5, %xmm7
	mulss	%xmm5, %xmm7
	movaps	%xmm7, %xmm5
	mulss	-64(%rbp), %xmm5
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	-68(%rbp), %xmm7
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	-72(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-76(%rbp), %xmm1
	movaps	%xmm0, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	mulss	-80(%rbp), %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	-84(%rbp), %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	-88(%rbp), %xmm9
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	mulss	-92(%rbp), %xmm10
	movaps	%xmm11, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	mulss	-96(%rbp), %xmm11
	movaps	%xmm12, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	mulss	-100(%rbp), %xmm12
	movaps	%xmm13, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	mulss	-104(%rbp), %xmm13
	movaps	%xmm14, %xmm4
	mulss	%xmm14, %xmm4
	movaps	%xmm4, %xmm14
	mulss	-108(%rbp), %xmm14
	movaps	%xmm15, %xmm4
	mulss	%xmm15, %xmm4
	movaps	%xmm4, %xmm15
	mulss	-112(%rbp), %xmm15
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm4
	mulss	%xmm6, %xmm4
	mulss	-116(%rbp), %xmm4
	movss	%xmm4, -52(%rbp)
	movss	-56(%rbp), %xmm4
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	mulss	-120(%rbp), %xmm6
	movss	%xmm6, -56(%rbp)
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	-60(%rbp), %xmm2
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	-64(%rbp), %xmm4
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	-68(%rbp), %xmm7
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	movaps	%xmm5, %xmm3
	mulss	-72(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-76(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-80(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-84(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-88(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	mulss	-92(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-96(%rbp), %xmm11
	movaps	%xmm12, %xmm6
	mulss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	mulss	-100(%rbp), %xmm12
	movaps	%xmm13, %xmm6
	mulss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	mulss	-104(%rbp), %xmm13
	movaps	%xmm14, %xmm6
	mulss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	mulss	-108(%rbp), %xmm14
	movaps	%xmm15, %xmm5
	mulss	%xmm15, %xmm5
	movaps	%xmm5, %xmm15
	mulss	-112(%rbp), %xmm15
	movss	-52(%rbp), %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	mulss	-116(%rbp), %xmm6
	movss	%xmm6, -52(%rbp)
	movss	-56(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm6, %xmm5
	mulss	-120(%rbp), %xmm5
	movss	%xmm5, -56(%rbp)
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	-60(%rbp), %xmm2
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	-64(%rbp), %xmm4
	movaps	%xmm7, %xmm5
	mulss	%xmm7, %xmm5
	mulss	-68(%rbp), %xmm5
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	-72(%rbp), %xmm3
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-76(%rbp), %xmm1
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	mulss	-80(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-84(%rbp), %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	mulss	-88(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	mulss	-92(%rbp), %xmm10
	movaps	%xmm11, %xmm7
	mulss	%xmm11, %xmm7
	movaps	%xmm7, %xmm11
	mulss	-96(%rbp), %xmm11
	movaps	%xmm12, %xmm6
	mulss	%xmm12, %xmm6
	movaps	%xmm6, %xmm12
	mulss	-100(%rbp), %xmm12
	movaps	%xmm13, %xmm6
	mulss	%xmm13, %xmm6
	movaps	%xmm6, %xmm13
	mulss	-104(%rbp), %xmm13
	movaps	%xmm14, %xmm7
	mulss	%xmm14, %xmm7
	movaps	%xmm7, %xmm14
	mulss	-108(%rbp), %xmm14
	movaps	%xmm15, %xmm6
	mulss	%xmm15, %xmm6
	movaps	%xmm6, %xmm15
	mulss	-112(%rbp), %xmm15
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	-116(%rbp), %xmm7
	movss	%xmm7, -52(%rbp)
	movss	-56(%rbp), %xmm7
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	-120(%rbp), %xmm6
	movss	%xmm6, -56(%rbp)
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	movaps	%xmm6, %xmm2
	mulss	-60(%rbp), %xmm2
	movaps	%xmm4, %xmm7
	mulss	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	mulss	-64(%rbp), %xmm4
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	mulss	-68(%rbp), %xmm6
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	-72(%rbp), %xmm5
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	-76(%rbp), %xmm1
	movaps	%xmm0, %xmm7
	mulss	%xmm0, %xmm7
	movaps	%xmm7, %xmm0
	mulss	-80(%rbp), %xmm0
	movaps	%xmm8, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	mulss	-84(%rbp), %xmm8
	movaps	%xmm9, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	mulss	-88(%rbp), %xmm9
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	-92(%rbp), %xmm10
	movaps	%xmm11, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	mulss	-96(%rbp), %xmm11
	movaps	%xmm12, %xmm3
	mulss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	mulss	-100(%rbp), %xmm12
	movaps	%xmm13, %xmm3
	mulss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	mulss	-104(%rbp), %xmm13
	movaps	%xmm14, %xmm3
	mulss	%xmm14, %xmm3
	movaps	%xmm3, %xmm14
	mulss	-108(%rbp), %xmm14
	movaps	%xmm15, %xmm3
	mulss	%xmm15, %xmm3
	movaps	%xmm3, %xmm15
	mulss	-112(%rbp), %xmm15
	movss	-52(%rbp), %xmm7
	movaps	%xmm7, %xmm3
	mulss	%xmm7, %xmm3
	mulss	-116(%rbp), %xmm3
	movss	%xmm3, -52(%rbp)
	movss	-56(%rbp), %xmm3
	movaps	%xmm3, %xmm7
	mulss	%xmm3, %xmm7
	mulss	-120(%rbp), %xmm7
	movss	%xmm7, -56(%rbp)
	movaps	%xmm2, %xmm7
	mulss	%xmm2, %xmm7
	movaps	%xmm7, %xmm2
	mulss	-60(%rbp), %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	movaps	%xmm3, %xmm4
	mulss	-64(%rbp), %xmm4
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	-68(%rbp), %xmm7
	movaps	%xmm5, %xmm3
	mulss	%xmm5, %xmm3
	movaps	%xmm3, %xmm5
	mulss	-72(%rbp), %xmm5
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-76(%rbp), %xmm1
	movaps	%xmm0, %xmm6
	mulss	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	mulss	-80(%rbp), %xmm0
	movaps	%xmm8, %xmm3
	mulss	%xmm8, %xmm3
	movaps	%xmm3, %xmm8
	mulss	-84(%rbp), %xmm8
	movaps	%xmm9, %xmm3
	mulss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	mulss	-88(%rbp), %xmm9
	movaps	%xmm10, %xmm3
	mulss	%xmm10, %xmm3
	movaps	%xmm3, %xmm10
	mulss	-92(%rbp), %xmm10
	movaps	%xmm11, %xmm3
	mulss	%xmm11, %xmm3
	movaps	%xmm3, %xmm11
	mulss	-96(%rbp), %xmm11
	movaps	%xmm12, %xmm3
	mulss	%xmm12, %xmm3
	movaps	%xmm3, %xmm12
	mulss	-100(%rbp), %xmm12
	movaps	%xmm13, %xmm3
	mulss	%xmm13, %xmm3
	movaps	%xmm3, %xmm13
	mulss	-104(%rbp), %xmm13
	movaps	%xmm14, %xmm3
	mulss	%xmm14, %xmm3
	movaps	%xmm3, %xmm14
	mulss	-108(%rbp), %xmm14
	movaps	%xmm15, %xmm3
	mulss	%xmm15, %xmm3
	movaps	%xmm3, %xmm15
	mulss	-112(%rbp), %xmm15
	movss	-52(%rbp), %xmm3
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	mulss	-116(%rbp), %xmm6
	movss	%xmm6, -52(%rbp)
	movss	-56(%rbp), %xmm6
	movaps	%xmm6, %xmm3
	mulss	%xmm6, %xmm3
	mulss	-120(%rbp), %xmm3
	movss	%xmm3, -56(%rbp)
	movaps	%xmm2, %xmm3
	mulss	%xmm2, %xmm3
	movaps	%xmm3, %xmm2
	mulss	-60(%rbp), %xmm2
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	-64(%rbp), %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	movaps	%xmm6, %xmm7
	mulss	-68(%rbp), %xmm7
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	movaps	%xmm4, %xmm5
	mulss	-72(%rbp), %xmm5
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-76(%rbp), %xmm1
	movaps	%xmm0, %xmm4
	mulss	%xmm0, %xmm4
	movaps	%xmm4, %xmm0
	mulss	-80(%rbp), %xmm0
	movaps	%xmm8, %xmm4
	mulss	%xmm8, %xmm4
	movaps	%xmm4, %xmm8
	mulss	-84(%rbp), %xmm8
	movaps	%xmm9, %xmm4
	mulss	%xmm9, %xmm4
	movaps	%xmm4, %xmm9
	mulss	-88(%rbp), %xmm9
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	movaps	%xmm4, %xmm10
	mulss	-92(%rbp), %xmm10
	movaps	%xmm11, %xmm6
	mulss	%xmm11, %xmm6
	movaps	%xmm6, %xmm11
	mulss	-96(%rbp), %xmm11
	movaps	%xmm12, %xmm4
	mulss	%xmm12, %xmm4
	movaps	%xmm4, %xmm12
	mulss	-100(%rbp), %xmm12
	movaps	%xmm13, %xmm4
	mulss	%xmm13, %xmm4
	movaps	%xmm4, %xmm13
	mulss	-104(%rbp), %xmm13
	movaps	%xmm14, %xmm4
	mulss	%xmm14, %xmm4
	movaps	%xmm4, %xmm14
	mulss	-108(%rbp), %xmm14
	movaps	%xmm15, %xmm4
	mulss	%xmm15, %xmm4
	movaps	%xmm4, %xmm15
	mulss	-112(%rbp), %xmm15
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm4
	mulss	%xmm6, %xmm4
	mulss	-116(%rbp), %xmm4
	movss	%xmm4, -52(%rbp)
	movss	-56(%rbp), %xmm4
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	mulss	-120(%rbp), %xmm6
	movss	%xmm6, -56(%rbp)
	movaps	%xmm2, %xmm4
	mulss	%xmm2, %xmm4
	movaps	%xmm4, %xmm2
	mulss	-60(%rbp), %xmm2
	movaps	%xmm3, %xmm6
	mulss	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	mulss	-64(%rbp), %xmm3
	movaps	%xmm7, %xmm6
	mulss	%xmm7, %xmm6
	mulss	-68(%rbp), %xmm6
	movaps	%xmm5, %xmm4
	mulss	%xmm5, %xmm4
	mulss	-72(%rbp), %xmm4
	movaps	%xmm1, %xmm7
	mulss	%xmm1, %xmm7
	movaps	%xmm7, %xmm1
	mulss	-76(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-80(%rbp), %xmm0
	movaps	%xmm8, %xmm7
	mulss	%xmm8, %xmm7
	movaps	%xmm7, %xmm8
	mulss	-84(%rbp), %xmm8
	movaps	%xmm9, %xmm5
	mulss	%xmm9, %xmm5
	movaps	%xmm5, %xmm9
	mulss	-88(%rbp), %xmm9
	movaps	%xmm10, %xmm5
	mulss	%xmm10, %xmm5
	movaps	%xmm5, %xmm10
	mulss	-92(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-96(%rbp), %xmm11
	movaps	%xmm12, %xmm5
	mulss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	mulss	-100(%rbp), %xmm12
	movaps	%xmm13, %xmm5
	mulss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	mulss	-104(%rbp), %xmm13
	movaps	%xmm14, %xmm5
	mulss	%xmm14, %xmm5
	movaps	%xmm5, %xmm14
	mulss	-108(%rbp), %xmm14
	movaps	%xmm15, %xmm5
	mulss	%xmm15, %xmm5
	movaps	%xmm5, %xmm15
	mulss	-112(%rbp), %xmm15
	movss	-52(%rbp), %xmm7
	movaps	%xmm7, %xmm5
	mulss	%xmm7, %xmm5
	mulss	-116(%rbp), %xmm5
	movss	%xmm5, -52(%rbp)
	movss	-56(%rbp), %xmm5
	movaps	%xmm5, %xmm7
	mulss	%xmm5, %xmm7
	mulss	-120(%rbp), %xmm7
	movss	%xmm7, -56(%rbp)
	movaps	%xmm2, %xmm5
	mulss	%xmm2, %xmm5
	movaps	%xmm5, %xmm2
	mulss	-60(%rbp), %xmm2
	movaps	%xmm3, %xmm7
	mulss	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	mulss	-64(%rbp), %xmm3
	movaps	%xmm6, %xmm7
	mulss	%xmm6, %xmm7
	mulss	-68(%rbp), %xmm7
	movaps	%xmm4, %xmm6
	mulss	%xmm4, %xmm6
	movaps	%xmm6, %xmm4
	mulss	-72(%rbp), %xmm4
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	mulss	-76(%rbp), %xmm1
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	movaps	%xmm5, %xmm0
	mulss	-80(%rbp), %xmm0
	movaps	%xmm8, %xmm6
	mulss	%xmm8, %xmm6
	movaps	%xmm6, %xmm8
	mulss	-84(%rbp), %xmm8
	movaps	%xmm9, %xmm6
	mulss	%xmm9, %xmm6
	movaps	%xmm6, %xmm9
	mulss	-88(%rbp), %xmm9
	movaps	%xmm10, %xmm6
	mulss	%xmm10, %xmm6
	movaps	%xmm6, %xmm10
	mulss	-92(%rbp), %xmm10
	movaps	%xmm11, %xmm5
	mulss	%xmm11, %xmm5
	movaps	%xmm5, %xmm11
	mulss	-96(%rbp), %xmm11
	movaps	%xmm12, %xmm5
	mulss	%xmm12, %xmm5
	movaps	%xmm5, %xmm12
	mulss	-100(%rbp), %xmm12
	movaps	%xmm13, %xmm5
	mulss	%xmm13, %xmm5
	movaps	%xmm5, %xmm13
	mulss	-104(%rbp), %xmm13
	movaps	%xmm14, %xmm6
	mulss	%xmm14, %xmm6
	movaps	%xmm6, %xmm14
	mulss	-108(%rbp), %xmm14
	movaps	%xmm15, %xmm6
	mulss	%xmm15, %xmm6
	movaps	%xmm6, %xmm15
	mulss	-112(%rbp), %xmm15
	movss	-52(%rbp), %xmm5
	movaps	%xmm5, %xmm6
	mulss	%xmm5, %xmm6
	mulss	-116(%rbp), %xmm6
	movss	%xmm6, -52(%rbp)
	movss	-56(%rbp), %xmm6
	movaps	%xmm6, %xmm5
	mulss	%xmm6, %xmm5
	mulss	-120(%rbp), %xmm5
	movss	%xmm5, -56(%rbp)
	movaps	%xmm2, %xmm6
	mulss	%xmm2, %xmm6
	mulss	-60(%rbp), %xmm6
	movss	%xmm6, -124(%rbp)
	movaps	%xmm3, %xmm5
	mulss	%xmm3, %xmm5
	mulss	-64(%rbp), %xmm5
	movss	%xmm5, -128(%rbp)
	movaps	%xmm7, %xmm3
	mulss	%xmm7, %xmm3
	mulss	-68(%rbp), %xmm3
	movss	%xmm3, -132(%rbp)
	movaps	%xmm4, %xmm3
	mulss	%xmm4, %xmm3
	mulss	-72(%rbp), %xmm3
	movss	%xmm3, -136(%rbp)
	movaps	%xmm1, %xmm6
	mulss	%xmm1, %xmm6
	mulss	-76(%rbp), %xmm6
	movss	%xmm6, -140(%rbp)
	movaps	%xmm0, %xmm5
	mulss	%xmm0, %xmm5
	mulss	-80(%rbp), %xmm5
	movss	%xmm5, -144(%rbp)
	movaps	%xmm8, %xmm2
	mulss	%xmm8, %xmm2
	mulss	-84(%rbp), %xmm2
	movss	%xmm2, -148(%rbp)
	movaps	%xmm9, %xmm2
	mulss	%xmm9, %xmm2
	mulss	-88(%rbp), %xmm2
	movss	%xmm2, -152(%rbp)
	movaps	%xmm10, %xmm4
	mulss	%xmm10, %xmm4
	mulss	-92(%rbp), %xmm4
	movss	%xmm4, -156(%rbp)
	movaps	%xmm11, %xmm2
	mulss	%xmm11, %xmm2
	mulss	-96(%rbp), %xmm2
	movss	%xmm2, -160(%rbp)
	movaps	%xmm12, %xmm2
	mulss	%xmm12, %xmm2
	mulss	-100(%rbp), %xmm2
	movss	%xmm2, -164(%rbp)
	movaps	%xmm13, %xmm2
	mulss	%xmm13, %xmm2
	mulss	-104(%rbp), %xmm2
	movss	%xmm2, -168(%rbp)
	movaps	%xmm14, %xmm2
	mulss	%xmm14, %xmm2
	mulss	-108(%rbp), %xmm2
	movss	%xmm2, -172(%rbp)
	movaps	%xmm15, %xmm5
	mulss	%xmm15, %xmm5
	mulss	-112(%rbp), %xmm5
	movss	%xmm5, -176(%rbp)
	movss	-52(%rbp), %xmm6
	movaps	%xmm6, %xmm2
	mulss	%xmm6, %xmm2
	mulss	-116(%rbp), %xmm2
	movss	%xmm2, -52(%rbp)
	movss	-56(%rbp), %xmm5
	movaps	%xmm5, %xmm2
	mulss	%xmm5, %xmm2
	mulss	-120(%rbp), %xmm2
	movss	%xmm2, -56(%rbp)
.L592:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L593
	cvttss2sil	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-140(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-148(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-156(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-164(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-172(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE200:
	.size	float_mul_15, .-float_mul_15
	.globl	float_mul_benchmarks
	.section	.data.rel.local
	.align 32
	.type	float_mul_benchmarks, @object
	.size	float_mul_benchmarks, 128
float_mul_benchmarks:
	.quad	float_mul_0
	.quad	float_mul_1
	.quad	float_mul_2
	.quad	float_mul_3
	.quad	float_mul_4
	.quad	float_mul_5
	.quad	float_mul_6
	.quad	float_mul_7
	.quad	float_mul_8
	.quad	float_mul_9
	.quad	float_mul_10
	.quad	float_mul_11
	.quad	float_mul_12
	.quad	float_mul_13
	.quad	float_mul_14
	.quad	float_mul_15
	.text
	.globl	float_div_0
	.type	float_div_0, @function
float_div_0:
.LFB201:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm7
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	jmp	.L595
.L596:
	movss	-52(%rbp), %xmm2
	movaps	%xmm2, %xmm3
	movaps	%xmm7, %xmm0
	divss	%xmm0, %xmm3
	movaps	%xmm2, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm2, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm2, %xmm6
	divss	%xmm5, %xmm6
	movaps	%xmm2, %xmm7
	divss	%xmm6, %xmm7
	movaps	%xmm2, %xmm3
	divss	%xmm7, %xmm3
	movaps	%xmm2, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm2, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm2, %xmm6
	divss	%xmm5, %xmm6
	divss	%xmm6, %xmm2
	movaps	%xmm2, %xmm7
.L595:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L596
	cvttss2sil	%xmm7, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE201:
	.size	float_div_0, .-float_div_0
	.globl	float_div_1
	.type	float_div_1, @function
float_div_1:
.LFB202:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm5
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	jmp	.L598
.L599:
	movss	-56(%rbp), %xmm2
	movaps	%xmm2, %xmm6
	movaps	%xmm5, %xmm3
	divss	%xmm3, %xmm6
	movss	-60(%rbp), %xmm7
	movaps	%xmm7, %xmm0
	divss	-52(%rbp), %xmm0
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm7, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm7, %xmm6
	divss	%xmm1, %xmm6
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm1
	movaps	%xmm1, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm2, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm1, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm2, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm1, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm1, %xmm7
	movaps	%xmm7, %xmm0
	divss	%xmm3, %xmm0
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm7, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm0
	divss	%xmm1, %xmm0
	divss	%xmm3, %xmm2
	movaps	%xmm2, %xmm5
	divss	%xmm0, %xmm7
	movss	%xmm7, -52(%rbp)
.L598:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L599
	cvttss2sil	%xmm5, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE202:
	.size	float_div_1, .-float_div_1
	.globl	float_div_2
	.type	float_div_2, @function
float_div_2:
.LFB203:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm5
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	jmp	.L601
.L602:
	movss	-60(%rbp), %xmm2
	movaps	%xmm2, %xmm6
	movaps	%xmm5, %xmm3
	divss	%xmm3, %xmm6
	movss	-64(%rbp), %xmm7
	movaps	%xmm7, %xmm0
	divss	-52(%rbp), %xmm0
	movss	-68(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	divss	-56(%rbp), %xmm4
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm7, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm3, %xmm6
	divss	%xmm4, %xmm6
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm7, %xmm5
	divss	%xmm1, %xmm5
	movaps	%xmm3, %xmm0
	movaps	%xmm0, %xmm9
	divss	%xmm6, %xmm9
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm1
	movaps	%xmm1, %xmm6
	divss	%xmm5, %xmm6
	movaps	%xmm0, %xmm4
	divss	%xmm9, %xmm4
	movaps	%xmm2, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm1, %xmm3
	divss	%xmm6, %xmm3
	movaps	%xmm0, %xmm5
	divss	%xmm4, %xmm5
	movaps	%xmm2, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm1, %xmm4
	divss	%xmm3, %xmm4
	movaps	%xmm0, %xmm8
	divss	%xmm5, %xmm8
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm1, %xmm7
	movaps	%xmm7, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm0, %xmm1
	divss	%xmm8, %xmm1
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm7, %xmm5
	divss	%xmm3, %xmm5
	movaps	%xmm0, %xmm6
	movaps	%xmm6, %xmm0
	divss	%xmm1, %xmm0
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm1
	divss	%xmm0, %xmm1
	divss	%xmm3, %xmm2
	movaps	%xmm2, %xmm5
	divss	%xmm4, %xmm7
	movss	%xmm7, -52(%rbp)
	divss	%xmm1, %xmm6
	movss	%xmm6, -56(%rbp)
.L601:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L602
	cvttss2sil	%xmm5, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE203:
	.size	float_div_2, .-float_div_2
	.globl	float_div_3
	.type	float_div_3, @function
float_div_3:
.LFB204:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm3
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	jmp	.L604
.L605:
	movss	-64(%rbp), %xmm2
	movaps	%xmm2, %xmm6
	divss	%xmm3, %xmm6
	movss	-68(%rbp), %xmm4
	movaps	%xmm4, %xmm1
	divss	-52(%rbp), %xmm1
	movss	-72(%rbp), %xmm0
	movaps	%xmm0, %xmm7
	divss	-56(%rbp), %xmm7
	movss	-76(%rbp), %xmm11
	movaps	%xmm11, %xmm15
	divss	-60(%rbp), %xmm15
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm4, %xmm6
	movaps	%xmm6, %xmm3
	divss	%xmm1, %xmm3
	movaps	%xmm0, %xmm10
	divss	%xmm7, %xmm10
	movaps	%xmm11, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm0, %xmm1
	divss	%xmm10, %xmm1
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm6, %xmm4
	movaps	%xmm4, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm0, %xmm9
	divss	%xmm1, %xmm9
	movaps	%xmm11, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm2, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm4, %xmm1
	divss	%xmm6, %xmm1
	movaps	%xmm0, %xmm5
	divss	%xmm9, %xmm5
	movaps	%xmm11, %xmm15
	divss	%xmm12, %xmm15
	movaps	%xmm2, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	divss	%xmm1, %xmm4
	movaps	%xmm0, %xmm8
	divss	%xmm5, %xmm8
	movaps	%xmm11, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm7, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm0, %xmm1
	divss	%xmm8, %xmm1
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm7, %xmm5
	divss	%xmm3, %xmm5
	movaps	%xmm0, %xmm6
	divss	%xmm1, %xmm6
	movaps	%xmm11, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm1
	divss	%xmm5, %xmm1
	movaps	%xmm0, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm11, %xmm6
	divss	%xmm12, %xmm6
	divss	%xmm3, %xmm2
	movaps	%xmm2, %xmm3
	divss	%xmm1, %xmm7
	movss	%xmm7, -52(%rbp)
	divss	%xmm5, %xmm0
	movss	%xmm0, -56(%rbp)
	movaps	%xmm11, %xmm4
	divss	%xmm6, %xmm4
	movss	%xmm4, -60(%rbp)
.L604:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L605
	cvttss2sil	%xmm3, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE204:
	.size	float_div_3, .-float_div_3
	.globl	float_div_4
	.type	float_div_4, @function
float_div_4:
.LFB205:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm3
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	jmp	.L607
.L608:
	movss	-68(%rbp), %xmm2
	movaps	%xmm2, %xmm6
	divss	%xmm3, %xmm6
	movss	-72(%rbp), %xmm1
	movaps	%xmm1, %xmm4
	divss	-52(%rbp), %xmm4
	movss	-76(%rbp), %xmm7
	movaps	%xmm7, %xmm10
	divss	-56(%rbp), %xmm10
	movss	-80(%rbp), %xmm11
	movaps	%xmm11, %xmm15
	divss	-60(%rbp), %xmm15
	movss	-84(%rbp), %xmm8
	movaps	%xmm8, %xmm9
	divss	-64(%rbp), %xmm9
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm1, %xmm6
	movaps	%xmm6, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm1
	movaps	%xmm1, %xmm0
	divss	%xmm10, %xmm0
	movaps	%xmm11, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm8, %xmm15
	divss	%xmm9, %xmm15
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm1, %xmm9
	divss	%xmm0, %xmm9
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm6, %xmm4
	movaps	%xmm4, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm1, %xmm0
	divss	%xmm9, %xmm0
	movaps	%xmm11, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm9
	movaps	%xmm9, %xmm13
	divss	%xmm10, %xmm13
	movaps	%xmm2, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm4, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm1, %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm11, %xmm15
	divss	%xmm12, %xmm15
	movaps	%xmm9, %xmm10
	divss	%xmm13, %xmm10
	movaps	%xmm2, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm4, %xmm7
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm1, %xmm0
	divss	%xmm8, %xmm0
	movaps	%xmm11, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm9, %xmm12
	divss	%xmm10, %xmm12
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm7, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	divss	%xmm0, %xmm6
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm9, %xmm8
	movaps	%xmm8, %xmm10
	divss	%xmm12, %xmm10
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm7, %xmm5
	divss	%xmm3, %xmm5
	movaps	%xmm1, %xmm0
	divss	%xmm6, %xmm0
	movaps	%xmm11, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm9
	divss	%xmm10, %xmm9
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm1, %xmm6
	divss	%xmm0, %xmm6
	movaps	%xmm11, %xmm0
	divss	%xmm12, %xmm0
	movaps	%xmm8, %xmm10
	divss	%xmm9, %xmm10
	divss	%xmm3, %xmm2
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm7
	movss	%xmm7, -52(%rbp)
	divss	%xmm6, %xmm1
	movss	%xmm1, -56(%rbp)
	movaps	%xmm11, %xmm5
	divss	%xmm0, %xmm5
	movss	%xmm5, -60(%rbp)
	divss	%xmm10, %xmm8
	movss	%xmm8, -64(%rbp)
.L607:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L608
	cvttss2sil	%xmm3, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE205:
	.size	float_div_4, .-float_div_4
	.globl	float_div_5
	.type	float_div_5, @function
float_div_5:
.LFB206:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm3
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	jmp	.L610
.L611:
	movss	-72(%rbp), %xmm2
	movaps	%xmm2, %xmm6
	divss	%xmm3, %xmm6
	movss	-76(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	divss	-56(%rbp), %xmm4
	movss	-80(%rbp), %xmm1
	movaps	%xmm1, %xmm7
	divss	-60(%rbp), %xmm7
	movss	-84(%rbp), %xmm10
	movaps	%xmm10, %xmm13
	divss	-64(%rbp), %xmm13
	movss	-88(%rbp), %xmm8
	movaps	%xmm8, %xmm9
	divss	-68(%rbp), %xmm9
	movss	-92(%rbp), %xmm11
	movaps	%xmm11, %xmm14
	divss	-52(%rbp), %xmm14
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm0
	divss	%xmm7, %xmm0
	movaps	%xmm10, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm15
	divss	%xmm9, %xmm15
	movaps	%xmm11, %xmm13
	movaps	%xmm13, %xmm7
	divss	%xmm14, %xmm7
	movss	%xmm7, -52(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm1, %xmm9
	divss	%xmm0, %xmm9
	movaps	%xmm10, %xmm11
	divss	%xmm12, %xmm11
	movaps	%xmm8, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm13, %xmm12
	divss	-52(%rbp), %xmm12
	movss	%xmm12, -52(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm6, %xmm4
	movaps	%xmm4, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm1, %xmm0
	divss	%xmm9, %xmm0
	movaps	%xmm10, %xmm15
	divss	%xmm11, %xmm15
	movaps	%xmm8, %xmm9
	movaps	%xmm9, %xmm12
	divss	%xmm14, %xmm12
	movaps	%xmm13, %xmm5
	divss	-52(%rbp), %xmm5
	movss	%xmm5, -52(%rbp)
	movaps	%xmm2, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm4, %xmm3
	movaps	%xmm3, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm1, %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm10, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm9, %xmm11
	divss	%xmm12, %xmm11
	movaps	%xmm13, %xmm12
	movaps	%xmm12, %xmm4
	divss	-52(%rbp), %xmm4
	movss	%xmm4, -52(%rbp)
	movaps	%xmm2, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm3, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm1, %xmm0
	divss	%xmm8, %xmm0
	movaps	%xmm10, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm9, %xmm8
	movaps	%xmm8, %xmm15
	divss	%xmm11, %xmm15
	movaps	%xmm12, %xmm9
	movaps	%xmm9, %xmm11
	divss	-52(%rbp), %xmm11
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm3, %xmm7
	movaps	%xmm7, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	divss	%xmm0, %xmm6
	movaps	%xmm10, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm9, %xmm13
	movaps	%xmm13, %xmm9
	divss	%xmm11, %xmm9
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm7, %xmm5
	divss	%xmm3, %xmm5
	movaps	%xmm1, %xmm0
	divss	%xmm6, %xmm0
	movaps	%xmm10, %xmm11
	divss	%xmm12, %xmm11
	movaps	%xmm8, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm13, %xmm12
	divss	%xmm9, %xmm12
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm1, %xmm6
	divss	%xmm0, %xmm6
	movaps	%xmm10, %xmm0
	divss	%xmm11, %xmm0
	movaps	%xmm8, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm13, %xmm9
	divss	%xmm12, %xmm9
	divss	%xmm3, %xmm2
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm7
	movss	%xmm7, -56(%rbp)
	divss	%xmm6, %xmm1
	movss	%xmm1, -60(%rbp)
	movaps	%xmm10, %xmm5
	divss	%xmm0, %xmm5
	movss	%xmm5, -64(%rbp)
	divss	%xmm14, %xmm8
	movss	%xmm8, -68(%rbp)
	divss	%xmm9, %xmm13
	movss	%xmm13, -52(%rbp)
.L610:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L611
	cvttss2sil	%xmm3, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE206:
	.size	float_div_5, .-float_div_5
	.globl	float_div_6
	.type	float_div_6, @function
float_div_6:
.LFB207:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm3
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	jmp	.L613
.L614:
	movss	-80(%rbp), %xmm2
	movaps	%xmm2, %xmm6
	movaps	%xmm3, %xmm7
	divss	%xmm7, %xmm6
	movss	-84(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	divss	-64(%rbp), %xmm4
	movss	-88(%rbp), %xmm1
	movaps	%xmm1, %xmm7
	divss	-68(%rbp), %xmm7
	movss	-92(%rbp), %xmm10
	movaps	%xmm10, %xmm13
	divss	-72(%rbp), %xmm13
	movss	-96(%rbp), %xmm8
	movaps	%xmm8, %xmm9
	divss	-76(%rbp), %xmm9
	movss	-100(%rbp), %xmm11
	movaps	%xmm11, %xmm14
	divss	-56(%rbp), %xmm14
	movss	-60(%rbp), %xmm5
	divss	-52(%rbp), %xmm5
	movss	%xmm5, -52(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm0
	divss	%xmm7, %xmm0
	movaps	%xmm10, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm15
	divss	%xmm9, %xmm15
	movaps	%xmm11, %xmm13
	movaps	%xmm13, %xmm7
	divss	%xmm14, %xmm7
	movss	%xmm7, -56(%rbp)
	movss	-60(%rbp), %xmm4
	divss	-52(%rbp), %xmm4
	movss	%xmm4, -52(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	movaps	%xmm5, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm1, %xmm9
	divss	%xmm0, %xmm9
	movaps	%xmm10, %xmm11
	divss	%xmm12, %xmm11
	movaps	%xmm8, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm13, %xmm12
	divss	-56(%rbp), %xmm12
	movss	%xmm12, -56(%rbp)
	movss	-60(%rbp), %xmm3
	divss	-52(%rbp), %xmm3
	movss	%xmm3, -52(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm5, %xmm4
	movaps	%xmm4, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm1, %xmm0
	divss	%xmm9, %xmm0
	movaps	%xmm10, %xmm15
	divss	%xmm11, %xmm15
	movaps	%xmm8, %xmm9
	movaps	%xmm9, %xmm12
	divss	%xmm14, %xmm12
	movaps	%xmm13, %xmm5
	divss	-56(%rbp), %xmm5
	movss	%xmm5, -56(%rbp)
	movss	-60(%rbp), %xmm7
	divss	-52(%rbp), %xmm7
	movss	%xmm7, -52(%rbp)
	movaps	%xmm2, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm4, %xmm3
	movaps	%xmm3, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm1, %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm10, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm9, %xmm11
	divss	%xmm12, %xmm11
	movaps	%xmm13, %xmm12
	movaps	%xmm12, %xmm6
	divss	-56(%rbp), %xmm6
	movss	%xmm6, -56(%rbp)
	movss	-60(%rbp), %xmm4
	divss	-52(%rbp), %xmm4
	movss	%xmm4, -52(%rbp)
	movaps	%xmm2, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm3, %xmm7
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm1, %xmm0
	divss	%xmm8, %xmm0
	movaps	%xmm10, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm9, %xmm8
	movaps	%xmm8, %xmm15
	divss	%xmm11, %xmm15
	movaps	%xmm12, %xmm9
	movaps	%xmm9, %xmm11
	divss	-56(%rbp), %xmm11
	movss	-60(%rbp), %xmm3
	divss	-52(%rbp), %xmm3
	movss	%xmm3, -52(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm7, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	movaps	%xmm6, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm10, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm9, %xmm13
	movaps	%xmm13, %xmm9
	divss	%xmm11, %xmm9
	movss	-60(%rbp), %xmm4
	divss	-52(%rbp), %xmm4
	movss	%xmm4, -52(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm7, %xmm5
	divss	%xmm3, %xmm5
	movaps	%xmm6, %xmm0
	divss	%xmm1, %xmm0
	movaps	%xmm10, %xmm1
	movaps	%xmm1, %xmm11
	divss	%xmm12, %xmm11
	movaps	%xmm8, %xmm15
	divss	%xmm14, %xmm15
	movaps	%xmm13, %xmm3
	divss	%xmm9, %xmm3
	movaps	%xmm3, %xmm9
	movss	-60(%rbp), %xmm12
	movaps	%xmm12, %xmm3
	divss	-52(%rbp), %xmm3
	movss	%xmm3, -52(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	divss	%xmm0, %xmm5
	movaps	%xmm1, %xmm0
	divss	%xmm11, %xmm0
	movaps	%xmm8, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm13, %xmm10
	divss	%xmm9, %xmm10
	movaps	%xmm12, %xmm15
	movaps	%xmm15, %xmm11
	divss	-52(%rbp), %xmm11
	divss	%xmm3, %xmm2
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm7
	movss	%xmm7, -64(%rbp)
	divss	%xmm5, %xmm6
	movss	%xmm6, -68(%rbp)
	divss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	divss	%xmm14, %xmm8
	movss	%xmm8, -76(%rbp)
	divss	%xmm10, %xmm13
	movss	%xmm13, -56(%rbp)
	divss	%xmm11, %xmm15
	movss	%xmm15, -52(%rbp)
.L613:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L614
	cvttss2sil	%xmm3, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE207:
	.size	float_div_6, .-float_div_6
	.globl	float_div_7
	.type	float_div_7, @function
float_div_7:
.LFB208:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm3
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -108(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	jmp	.L616
.L617:
	movss	-88(%rbp), %xmm2
	movaps	%xmm2, %xmm6
	movaps	%xmm3, %xmm7
	divss	%xmm7, %xmm6
	movss	-92(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	divss	-72(%rbp), %xmm4
	movss	-96(%rbp), %xmm1
	movaps	%xmm1, %xmm7
	divss	-76(%rbp), %xmm7
	movss	-100(%rbp), %xmm11
	movaps	%xmm11, %xmm0
	divss	-80(%rbp), %xmm0
	movss	-104(%rbp), %xmm8
	movaps	%xmm8, %xmm15
	divss	-84(%rbp), %xmm15
	movss	-108(%rbp), %xmm12
	movaps	%xmm12, %xmm13
	divss	-64(%rbp), %xmm13
	movss	-68(%rbp), %xmm5
	divss	-52(%rbp), %xmm5
	movss	%xmm5, -52(%rbp)
	movss	-60(%rbp), %xmm10
	divss	-56(%rbp), %xmm10
	movss	%xmm10, -56(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm10
	divss	%xmm7, %xmm10
	movaps	%xmm11, %xmm14
	divss	%xmm0, %xmm14
	movaps	%xmm8, %xmm9
	divss	%xmm15, %xmm9
	movaps	%xmm12, %xmm7
	divss	%xmm13, %xmm7
	movss	%xmm7, -64(%rbp)
	movss	-68(%rbp), %xmm4
	divss	-52(%rbp), %xmm4
	movss	%xmm4, -52(%rbp)
	movss	-60(%rbp), %xmm0
	divss	-56(%rbp), %xmm0
	movss	%xmm0, -56(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	movaps	%xmm5, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm1, %xmm0
	divss	%xmm10, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm8, %xmm15
	divss	%xmm9, %xmm15
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm10
	divss	-64(%rbp), %xmm10
	movss	-68(%rbp), %xmm6
	divss	-52(%rbp), %xmm6
	movss	%xmm6, -52(%rbp)
	movss	-60(%rbp), %xmm3
	divss	-56(%rbp), %xmm3
	movss	%xmm3, -56(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm5, %xmm4
	movaps	%xmm4, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm1, %xmm9
	divss	%xmm0, %xmm9
	movaps	%xmm11, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm13
	divss	%xmm15, %xmm13
	movaps	%xmm14, %xmm5
	divss	%xmm10, %xmm5
	movss	%xmm5, -64(%rbp)
	movss	-68(%rbp), %xmm7
	divss	-52(%rbp), %xmm7
	movss	%xmm7, -52(%rbp)
	movss	-60(%rbp), %xmm10
	divss	-56(%rbp), %xmm10
	movss	%xmm10, -56(%rbp)
	movaps	%xmm2, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm4, %xmm3
	movaps	%xmm3, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm1, %xmm0
	divss	%xmm9, %xmm0
	movaps	%xmm11, %xmm15
	divss	%xmm12, %xmm15
	movaps	%xmm8, %xmm9
	movaps	%xmm9, %xmm10
	divss	%xmm13, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm13
	divss	-64(%rbp), %xmm13
	movss	-68(%rbp), %xmm4
	divss	-52(%rbp), %xmm4
	movss	%xmm4, -52(%rbp)
	movss	-60(%rbp), %xmm6
	divss	-56(%rbp), %xmm6
	movss	%xmm6, -56(%rbp)
	movaps	%xmm2, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm3, %xmm7
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm1, %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm11, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm9, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm12, %xmm3
	divss	%xmm13, %xmm3
	movss	%xmm3, -64(%rbp)
	movss	-68(%rbp), %xmm5
	divss	-52(%rbp), %xmm5
	movss	%xmm5, -52(%rbp)
	movss	-60(%rbp), %xmm0
	divss	-56(%rbp), %xmm0
	movss	%xmm0, -56(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm7, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	movaps	%xmm6, %xmm0
	divss	%xmm8, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm9, %xmm8
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm9
	divss	-64(%rbp), %xmm9
	movss	-68(%rbp), %xmm4
	divss	-52(%rbp), %xmm4
	movss	%xmm4, -52(%rbp)
	movss	-60(%rbp), %xmm1
	divss	-56(%rbp), %xmm1
	movss	%xmm1, -56(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm7, %xmm5
	divss	%xmm3, %xmm5
	movaps	%xmm6, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm11, %xmm0
	movaps	%xmm0, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm14, %xmm11
	divss	%xmm9, %xmm11
	movss	-68(%rbp), %xmm13
	movaps	%xmm13, %xmm3
	divss	-52(%rbp), %xmm3
	movss	%xmm3, -52(%rbp)
	movss	-60(%rbp), %xmm9
	divss	-56(%rbp), %xmm9
	movss	%xmm9, -56(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	divss	%xmm1, %xmm5
	movaps	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	divss	%xmm12, %xmm0
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm9
	divss	%xmm11, %xmm9
	movaps	%xmm13, %xmm11
	movaps	%xmm11, %xmm14
	divss	-52(%rbp), %xmm14
	movss	-60(%rbp), %xmm15
	movaps	%xmm15, %xmm13
	divss	-56(%rbp), %xmm13
	divss	%xmm3, %xmm2
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm7
	movss	%xmm7, -72(%rbp)
	divss	%xmm5, %xmm6
	movss	%xmm6, -76(%rbp)
	divss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	divss	%xmm10, %xmm8
	movss	%xmm8, -84(%rbp)
	divss	%xmm9, %xmm12
	movss	%xmm12, -64(%rbp)
	divss	%xmm14, %xmm11
	movss	%xmm11, -52(%rbp)
	divss	%xmm13, %xmm15
	movss	%xmm15, -56(%rbp)
.L616:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L617
	cvttss2sil	%xmm3, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE208:
	.size	float_div_7, .-float_div_7
	.globl	float_div_8
	.type	float_div_8, @function
float_div_8:
.LFB209:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movaps	%xmm1, %xmm3
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -108(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -116(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	jmp	.L619
.L620:
	movss	-96(%rbp), %xmm2
	movaps	%xmm2, %xmm6
	movaps	%xmm3, %xmm7
	divss	%xmm7, %xmm6
	movss	-100(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	divss	-80(%rbp), %xmm4
	movss	-104(%rbp), %xmm1
	movaps	%xmm1, %xmm7
	divss	-84(%rbp), %xmm7
	movss	-108(%rbp), %xmm11
	movaps	%xmm11, %xmm0
	divss	-88(%rbp), %xmm0
	movss	-112(%rbp), %xmm8
	movaps	%xmm8, %xmm15
	divss	-92(%rbp), %xmm15
	movss	-116(%rbp), %xmm12
	movaps	%xmm12, %xmm13
	divss	-64(%rbp), %xmm13
	movss	-76(%rbp), %xmm5
	divss	-52(%rbp), %xmm5
	movss	%xmm5, -52(%rbp)
	movss	-72(%rbp), %xmm10
	divss	-56(%rbp), %xmm10
	movss	%xmm10, -56(%rbp)
	movss	-68(%rbp), %xmm14
	divss	-60(%rbp), %xmm14
	movss	%xmm14, -60(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm10
	divss	%xmm7, %xmm10
	movaps	%xmm11, %xmm14
	divss	%xmm0, %xmm14
	movaps	%xmm8, %xmm9
	divss	%xmm15, %xmm9
	movaps	%xmm12, %xmm7
	divss	%xmm13, %xmm7
	movss	%xmm7, -64(%rbp)
	movss	-76(%rbp), %xmm4
	divss	-52(%rbp), %xmm4
	movss	%xmm4, -52(%rbp)
	movss	-72(%rbp), %xmm0
	divss	-56(%rbp), %xmm0
	movss	%xmm0, -56(%rbp)
	movss	-68(%rbp), %xmm15
	divss	-60(%rbp), %xmm15
	movss	%xmm15, -60(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	movaps	%xmm5, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm1, %xmm0
	divss	%xmm10, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm8, %xmm15
	divss	%xmm9, %xmm15
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm10
	divss	-64(%rbp), %xmm10
	movss	-76(%rbp), %xmm6
	divss	-52(%rbp), %xmm6
	movss	%xmm6, -52(%rbp)
	movss	-72(%rbp), %xmm3
	divss	-56(%rbp), %xmm3
	movss	%xmm3, -56(%rbp)
	movss	-68(%rbp), %xmm9
	divss	-60(%rbp), %xmm9
	movss	%xmm9, -60(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm5, %xmm4
	movaps	%xmm4, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm1, %xmm9
	divss	%xmm0, %xmm9
	movaps	%xmm11, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm13
	divss	%xmm15, %xmm13
	movaps	%xmm14, %xmm5
	divss	%xmm10, %xmm5
	movss	%xmm5, -64(%rbp)
	movss	-76(%rbp), %xmm7
	divss	-52(%rbp), %xmm7
	movss	%xmm7, -52(%rbp)
	movss	-72(%rbp), %xmm10
	divss	-56(%rbp), %xmm10
	movss	%xmm10, -56(%rbp)
	movss	-68(%rbp), %xmm0
	divss	-60(%rbp), %xmm0
	movss	%xmm0, -60(%rbp)
	movaps	%xmm2, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm4, %xmm3
	movaps	%xmm3, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm1, %xmm0
	divss	%xmm9, %xmm0
	movaps	%xmm11, %xmm15
	divss	%xmm12, %xmm15
	movaps	%xmm8, %xmm9
	movaps	%xmm9, %xmm10
	divss	%xmm13, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm13
	divss	-64(%rbp), %xmm13
	movss	-76(%rbp), %xmm4
	divss	-52(%rbp), %xmm4
	movss	%xmm4, -52(%rbp)
	movss	-72(%rbp), %xmm6
	divss	-56(%rbp), %xmm6
	movss	%xmm6, -56(%rbp)
	movss	-68(%rbp), %xmm8
	divss	-60(%rbp), %xmm8
	movss	%xmm8, -60(%rbp)
	movaps	%xmm2, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm3, %xmm7
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm1, %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm11, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm9, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm12, %xmm3
	divss	%xmm13, %xmm3
	movss	%xmm3, -64(%rbp)
	movss	-76(%rbp), %xmm5
	divss	-52(%rbp), %xmm5
	movss	%xmm5, -52(%rbp)
	movss	-72(%rbp), %xmm0
	divss	-56(%rbp), %xmm0
	movss	%xmm0, -56(%rbp)
	movss	-68(%rbp), %xmm10
	divss	-60(%rbp), %xmm10
	movss	%xmm10, -60(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm7, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	movaps	%xmm6, %xmm0
	divss	%xmm8, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm9, %xmm8
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm9
	divss	-64(%rbp), %xmm9
	movss	-76(%rbp), %xmm4
	divss	-52(%rbp), %xmm4
	movss	%xmm4, -52(%rbp)
	movss	-72(%rbp), %xmm1
	divss	-56(%rbp), %xmm1
	movss	%xmm1, -56(%rbp)
	movss	-68(%rbp), %xmm12
	divss	-60(%rbp), %xmm12
	movss	%xmm12, -60(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm7, %xmm5
	divss	%xmm3, %xmm5
	movaps	%xmm6, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm11, %xmm0
	movaps	%xmm0, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm14, %xmm11
	divss	%xmm9, %xmm11
	movss	-76(%rbp), %xmm13
	movaps	%xmm13, %xmm3
	divss	-52(%rbp), %xmm3
	movss	%xmm3, -52(%rbp)
	movss	-72(%rbp), %xmm9
	divss	-56(%rbp), %xmm9
	movss	%xmm9, -56(%rbp)
	movss	-68(%rbp), %xmm10
	divss	-60(%rbp), %xmm10
	movss	%xmm10, -60(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	divss	%xmm1, %xmm5
	movaps	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	divss	%xmm12, %xmm0
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm9
	divss	%xmm11, %xmm9
	movss	%xmm9, -64(%rbp)
	movaps	%xmm13, %xmm11
	movaps	%xmm11, %xmm14
	divss	-52(%rbp), %xmm14
	movss	-72(%rbp), %xmm15
	movaps	%xmm15, %xmm13
	divss	-56(%rbp), %xmm13
	movss	-68(%rbp), %xmm9
	divss	-60(%rbp), %xmm9
	divss	%xmm3, %xmm2
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm7
	movss	%xmm7, -80(%rbp)
	divss	%xmm5, %xmm6
	movss	%xmm6, -84(%rbp)
	divss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	divss	%xmm10, %xmm8
	movss	%xmm8, -92(%rbp)
	divss	-64(%rbp), %xmm12
	movss	%xmm12, -64(%rbp)
	divss	%xmm14, %xmm11
	movss	%xmm11, -52(%rbp)
	divss	%xmm13, %xmm15
	movss	%xmm15, -56(%rbp)
	movss	-68(%rbp), %xmm10
	divss	%xmm9, %xmm10
	movss	%xmm10, -60(%rbp)
.L619:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L620
	cvttss2sil	%xmm3, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE209:
	.size	float_div_8, .-float_div_8
	.globl	float_div_9
	.type	float_div_9, @function
float_div_9:
.LFB210:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -108(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -116(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -124(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	jmp	.L622
.L623:
	movss	-108(%rbp), %xmm2
	movaps	%xmm2, %xmm6
	divss	-88(%rbp), %xmm6
	movss	-112(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	divss	-92(%rbp), %xmm4
	movss	-116(%rbp), %xmm1
	movaps	%xmm1, %xmm7
	divss	-96(%rbp), %xmm7
	movss	-120(%rbp), %xmm11
	movaps	%xmm11, %xmm0
	divss	-100(%rbp), %xmm0
	movss	-124(%rbp), %xmm8
	movaps	%xmm8, %xmm15
	divss	-104(%rbp), %xmm15
	movss	-128(%rbp), %xmm12
	movaps	%xmm12, %xmm13
	divss	-68(%rbp), %xmm13
	movss	-84(%rbp), %xmm5
	divss	-56(%rbp), %xmm5
	movss	%xmm5, -56(%rbp)
	movss	-80(%rbp), %xmm10
	divss	-60(%rbp), %xmm10
	movss	%xmm10, -60(%rbp)
	movss	-72(%rbp), %xmm14
	divss	-52(%rbp), %xmm14
	movss	%xmm14, -52(%rbp)
	movss	-76(%rbp), %xmm9
	divss	-64(%rbp), %xmm9
	movss	%xmm9, -64(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm10
	divss	%xmm7, %xmm10
	movaps	%xmm11, %xmm14
	divss	%xmm0, %xmm14
	movaps	%xmm8, %xmm9
	divss	%xmm15, %xmm9
	movaps	%xmm12, %xmm7
	divss	%xmm13, %xmm7
	movss	%xmm7, -68(%rbp)
	movss	-84(%rbp), %xmm4
	divss	-56(%rbp), %xmm4
	movss	%xmm4, -56(%rbp)
	movss	-80(%rbp), %xmm0
	divss	-60(%rbp), %xmm0
	movss	%xmm0, -60(%rbp)
	movss	-72(%rbp), %xmm15
	divss	-52(%rbp), %xmm15
	movss	%xmm15, -52(%rbp)
	movss	-76(%rbp), %xmm13
	divss	-64(%rbp), %xmm13
	movss	%xmm13, -64(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	movaps	%xmm5, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm1, %xmm0
	divss	%xmm10, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm8, %xmm15
	divss	%xmm9, %xmm15
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm10
	divss	-68(%rbp), %xmm10
	movss	-84(%rbp), %xmm6
	divss	-56(%rbp), %xmm6
	movss	%xmm6, -56(%rbp)
	movss	-80(%rbp), %xmm3
	divss	-60(%rbp), %xmm3
	movss	%xmm3, -60(%rbp)
	movss	-72(%rbp), %xmm9
	divss	-52(%rbp), %xmm9
	movss	%xmm9, -52(%rbp)
	movss	-76(%rbp), %xmm12
	divss	-64(%rbp), %xmm12
	movss	%xmm12, -64(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm5, %xmm4
	movaps	%xmm4, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm1, %xmm9
	divss	%xmm0, %xmm9
	movaps	%xmm11, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm13
	divss	%xmm15, %xmm13
	movaps	%xmm14, %xmm5
	divss	%xmm10, %xmm5
	movss	%xmm5, -68(%rbp)
	movss	-84(%rbp), %xmm7
	divss	-56(%rbp), %xmm7
	movss	%xmm7, -56(%rbp)
	movss	-80(%rbp), %xmm10
	divss	-60(%rbp), %xmm10
	movss	%xmm10, -60(%rbp)
	movss	-72(%rbp), %xmm0
	divss	-52(%rbp), %xmm0
	movss	%xmm0, -52(%rbp)
	movss	-76(%rbp), %xmm15
	divss	-64(%rbp), %xmm15
	movss	%xmm15, -64(%rbp)
	movaps	%xmm2, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm4, %xmm3
	movaps	%xmm3, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm1, %xmm0
	divss	%xmm9, %xmm0
	movaps	%xmm11, %xmm15
	divss	%xmm12, %xmm15
	movaps	%xmm8, %xmm9
	movaps	%xmm9, %xmm10
	divss	%xmm13, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm13
	divss	-68(%rbp), %xmm13
	movss	-84(%rbp), %xmm4
	divss	-56(%rbp), %xmm4
	movss	%xmm4, -56(%rbp)
	movss	-80(%rbp), %xmm6
	divss	-60(%rbp), %xmm6
	movss	%xmm6, -60(%rbp)
	movss	-72(%rbp), %xmm8
	divss	-52(%rbp), %xmm8
	movss	%xmm8, -52(%rbp)
	movss	-76(%rbp), %xmm14
	divss	-64(%rbp), %xmm14
	movss	%xmm14, -64(%rbp)
	movaps	%xmm2, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm3, %xmm7
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm1, %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm11, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm9, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm12, %xmm3
	divss	%xmm13, %xmm3
	movss	%xmm3, -68(%rbp)
	movss	-84(%rbp), %xmm5
	divss	-56(%rbp), %xmm5
	movss	%xmm5, -56(%rbp)
	movss	-80(%rbp), %xmm0
	divss	-60(%rbp), %xmm0
	movss	%xmm0, -60(%rbp)
	movss	-72(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-76(%rbp), %xmm13
	divss	-64(%rbp), %xmm13
	movss	%xmm13, -64(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm7, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	movaps	%xmm6, %xmm0
	divss	%xmm8, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm9, %xmm8
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm9
	divss	-68(%rbp), %xmm9
	movss	-84(%rbp), %xmm4
	divss	-56(%rbp), %xmm4
	movss	%xmm4, -56(%rbp)
	movss	-80(%rbp), %xmm1
	divss	-60(%rbp), %xmm1
	movss	%xmm1, -60(%rbp)
	movss	-72(%rbp), %xmm12
	divss	-52(%rbp), %xmm12
	movss	%xmm12, -52(%rbp)
	movss	-76(%rbp), %xmm15
	divss	-64(%rbp), %xmm15
	movss	%xmm15, -64(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm7, %xmm5
	divss	%xmm3, %xmm5
	movaps	%xmm6, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm11, %xmm0
	movaps	%xmm0, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm14, %xmm11
	divss	%xmm9, %xmm11
	movss	-84(%rbp), %xmm13
	movaps	%xmm13, %xmm3
	divss	-56(%rbp), %xmm3
	movss	%xmm3, -56(%rbp)
	movss	-80(%rbp), %xmm9
	divss	-60(%rbp), %xmm9
	movss	%xmm9, -60(%rbp)
	movss	-72(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-76(%rbp), %xmm3
	divss	-64(%rbp), %xmm3
	movss	%xmm3, -64(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	divss	%xmm1, %xmm5
	movaps	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	divss	%xmm12, %xmm0
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm9
	divss	%xmm11, %xmm9
	movss	%xmm9, -68(%rbp)
	movaps	%xmm13, %xmm11
	movaps	%xmm11, %xmm14
	divss	-56(%rbp), %xmm14
	movss	-80(%rbp), %xmm15
	movaps	%xmm15, %xmm13
	divss	-60(%rbp), %xmm13
	movss	-72(%rbp), %xmm9
	divss	-52(%rbp), %xmm9
	movss	%xmm9, -52(%rbp)
	movss	-76(%rbp), %xmm9
	divss	-64(%rbp), %xmm9
	divss	%xmm3, %xmm2
	movss	%xmm2, -88(%rbp)
	divss	%xmm4, %xmm7
	movss	%xmm7, -92(%rbp)
	divss	%xmm5, %xmm6
	movss	%xmm6, -96(%rbp)
	divss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	divss	%xmm10, %xmm8
	movss	%xmm8, -104(%rbp)
	divss	-68(%rbp), %xmm12
	movss	%xmm12, -68(%rbp)
	divss	%xmm14, %xmm11
	movss	%xmm11, -56(%rbp)
	divss	%xmm13, %xmm15
	movss	%xmm15, -60(%rbp)
	movss	-72(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-76(%rbp), %xmm3
	divss	%xmm9, %xmm3
	movss	%xmm3, -64(%rbp)
.L622:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L623
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE210:
	.size	float_div_9, .-float_div_9
	.globl	float_div_10
	.type	float_div_10, @function
float_div_10:
.LFB211:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -116(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -124(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -108(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -132(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	jmp	.L625
.L626:
	movss	-116(%rbp), %xmm2
	movaps	%xmm2, %xmm6
	divss	-96(%rbp), %xmm6
	movss	-120(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	divss	-100(%rbp), %xmm4
	movss	-124(%rbp), %xmm1
	movaps	%xmm1, %xmm7
	divss	-104(%rbp), %xmm7
	movss	-128(%rbp), %xmm11
	movaps	%xmm11, %xmm0
	divss	-108(%rbp), %xmm0
	movss	-132(%rbp), %xmm8
	movaps	%xmm8, %xmm15
	divss	-112(%rbp), %xmm15
	movss	-136(%rbp), %xmm12
	movaps	%xmm12, %xmm13
	divss	-72(%rbp), %xmm13
	movss	-92(%rbp), %xmm5
	divss	-60(%rbp), %xmm5
	movss	%xmm5, -60(%rbp)
	movss	-88(%rbp), %xmm10
	divss	-64(%rbp), %xmm10
	movss	%xmm10, -64(%rbp)
	movss	-76(%rbp), %xmm14
	divss	-52(%rbp), %xmm14
	movss	%xmm14, -52(%rbp)
	movss	-80(%rbp), %xmm9
	divss	-56(%rbp), %xmm9
	movss	%xmm9, -56(%rbp)
	movss	-84(%rbp), %xmm14
	divss	-68(%rbp), %xmm14
	movss	%xmm14, -68(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm10
	divss	%xmm7, %xmm10
	movaps	%xmm11, %xmm14
	divss	%xmm0, %xmm14
	movaps	%xmm8, %xmm9
	divss	%xmm15, %xmm9
	movaps	%xmm12, %xmm7
	divss	%xmm13, %xmm7
	movss	%xmm7, -72(%rbp)
	movss	-92(%rbp), %xmm4
	divss	-60(%rbp), %xmm4
	movss	%xmm4, -60(%rbp)
	movss	-88(%rbp), %xmm0
	divss	-64(%rbp), %xmm0
	movss	%xmm0, -64(%rbp)
	movss	-76(%rbp), %xmm15
	divss	-52(%rbp), %xmm15
	movss	%xmm15, -52(%rbp)
	movss	-80(%rbp), %xmm13
	divss	-56(%rbp), %xmm13
	movss	%xmm13, -56(%rbp)
	movss	-84(%rbp), %xmm7
	divss	-68(%rbp), %xmm7
	movss	%xmm7, -68(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	movaps	%xmm5, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm1, %xmm0
	divss	%xmm10, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm8, %xmm15
	divss	%xmm9, %xmm15
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm10
	divss	-72(%rbp), %xmm10
	movss	-92(%rbp), %xmm6
	divss	-60(%rbp), %xmm6
	movss	%xmm6, -60(%rbp)
	movss	-88(%rbp), %xmm3
	divss	-64(%rbp), %xmm3
	movss	%xmm3, -64(%rbp)
	movss	-76(%rbp), %xmm9
	divss	-52(%rbp), %xmm9
	movss	%xmm9, -52(%rbp)
	movss	-80(%rbp), %xmm12
	divss	-56(%rbp), %xmm12
	movss	%xmm12, -56(%rbp)
	movss	-84(%rbp), %xmm6
	divss	-68(%rbp), %xmm6
	movss	%xmm6, -68(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm5, %xmm4
	movaps	%xmm4, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm1, %xmm9
	divss	%xmm0, %xmm9
	movaps	%xmm11, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm13
	divss	%xmm15, %xmm13
	movaps	%xmm14, %xmm5
	divss	%xmm10, %xmm5
	movss	%xmm5, -72(%rbp)
	movss	-92(%rbp), %xmm7
	divss	-60(%rbp), %xmm7
	movss	%xmm7, -60(%rbp)
	movss	-88(%rbp), %xmm10
	divss	-64(%rbp), %xmm10
	movss	%xmm10, -64(%rbp)
	movss	-76(%rbp), %xmm0
	divss	-52(%rbp), %xmm0
	movss	%xmm0, -52(%rbp)
	movss	-80(%rbp), %xmm15
	divss	-56(%rbp), %xmm15
	movss	%xmm15, -56(%rbp)
	movss	-84(%rbp), %xmm5
	divss	-68(%rbp), %xmm5
	movss	%xmm5, -68(%rbp)
	movaps	%xmm2, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm4, %xmm3
	movaps	%xmm3, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm1, %xmm0
	divss	%xmm9, %xmm0
	movaps	%xmm11, %xmm15
	divss	%xmm12, %xmm15
	movaps	%xmm8, %xmm9
	movaps	%xmm9, %xmm10
	divss	%xmm13, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm13
	divss	-72(%rbp), %xmm13
	movss	-92(%rbp), %xmm4
	divss	-60(%rbp), %xmm4
	movss	%xmm4, -60(%rbp)
	movss	-88(%rbp), %xmm6
	divss	-64(%rbp), %xmm6
	movss	%xmm6, -64(%rbp)
	movss	-76(%rbp), %xmm8
	divss	-52(%rbp), %xmm8
	movss	%xmm8, -52(%rbp)
	movss	-80(%rbp), %xmm14
	divss	-56(%rbp), %xmm14
	movss	%xmm14, -56(%rbp)
	movss	-84(%rbp), %xmm8
	divss	-68(%rbp), %xmm8
	movss	%xmm8, -68(%rbp)
	movaps	%xmm2, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm3, %xmm7
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm1, %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm11, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm9, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm12, %xmm3
	divss	%xmm13, %xmm3
	movss	%xmm3, -72(%rbp)
	movss	-92(%rbp), %xmm5
	divss	-60(%rbp), %xmm5
	movss	%xmm5, -60(%rbp)
	movss	-88(%rbp), %xmm0
	divss	-64(%rbp), %xmm0
	movss	%xmm0, -64(%rbp)
	movss	-76(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-80(%rbp), %xmm13
	divss	-56(%rbp), %xmm13
	movss	%xmm13, -56(%rbp)
	movss	-84(%rbp), %xmm3
	divss	-68(%rbp), %xmm3
	movss	%xmm3, -68(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm7, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	movaps	%xmm6, %xmm0
	divss	%xmm8, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm9, %xmm8
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm9
	divss	-72(%rbp), %xmm9
	movss	-92(%rbp), %xmm4
	divss	-60(%rbp), %xmm4
	movss	%xmm4, -60(%rbp)
	movss	-88(%rbp), %xmm1
	divss	-64(%rbp), %xmm1
	movss	%xmm1, -64(%rbp)
	movss	-76(%rbp), %xmm12
	divss	-52(%rbp), %xmm12
	movss	%xmm12, -52(%rbp)
	movss	-80(%rbp), %xmm15
	divss	-56(%rbp), %xmm15
	movss	%xmm15, -56(%rbp)
	movss	-84(%rbp), %xmm4
	divss	-68(%rbp), %xmm4
	movss	%xmm4, -68(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm7, %xmm5
	divss	%xmm3, %xmm5
	movaps	%xmm6, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm11, %xmm0
	movaps	%xmm0, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm14, %xmm11
	divss	%xmm9, %xmm11
	movss	-92(%rbp), %xmm13
	movaps	%xmm13, %xmm3
	divss	-60(%rbp), %xmm3
	movss	%xmm3, -60(%rbp)
	movss	-88(%rbp), %xmm9
	divss	-64(%rbp), %xmm9
	movss	%xmm9, -64(%rbp)
	movss	-76(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-80(%rbp), %xmm3
	divss	-56(%rbp), %xmm3
	movss	%xmm3, -56(%rbp)
	movss	-84(%rbp), %xmm9
	divss	-68(%rbp), %xmm9
	movss	%xmm9, -68(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	divss	%xmm1, %xmm5
	movaps	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	divss	%xmm12, %xmm0
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm9
	divss	%xmm11, %xmm9
	movss	%xmm9, -72(%rbp)
	movaps	%xmm13, %xmm11
	movaps	%xmm11, %xmm14
	divss	-60(%rbp), %xmm14
	movss	-88(%rbp), %xmm15
	movaps	%xmm15, %xmm13
	divss	-64(%rbp), %xmm13
	movss	-76(%rbp), %xmm9
	divss	-52(%rbp), %xmm9
	movss	%xmm9, -52(%rbp)
	movss	-80(%rbp), %xmm9
	divss	-56(%rbp), %xmm9
	movss	%xmm9, -56(%rbp)
	movss	-84(%rbp), %xmm9
	divss	-68(%rbp), %xmm9
	divss	%xmm3, %xmm2
	movss	%xmm2, -96(%rbp)
	divss	%xmm4, %xmm7
	movss	%xmm7, -100(%rbp)
	divss	%xmm5, %xmm6
	movss	%xmm6, -104(%rbp)
	divss	%xmm0, %xmm1
	movss	%xmm1, -108(%rbp)
	divss	%xmm10, %xmm8
	movss	%xmm8, -112(%rbp)
	divss	-72(%rbp), %xmm12
	movss	%xmm12, -72(%rbp)
	divss	%xmm14, %xmm11
	movss	%xmm11, -60(%rbp)
	divss	%xmm13, %xmm15
	movss	%xmm15, -64(%rbp)
	movss	-76(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-80(%rbp), %xmm3
	divss	-56(%rbp), %xmm3
	movss	%xmm3, -56(%rbp)
	movss	-84(%rbp), %xmm4
	divss	%xmm9, %xmm4
	movss	%xmm4, -68(%rbp)
.L625:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L626
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE211:
	.size	float_div_10, .-float_div_10
	.globl	float_div_11
	.type	float_div_11, @function
float_div_11:
.LFB212:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -124(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -108(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -132(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -116(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -140(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	56(%rax), %eax
	subl	$10, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	jmp	.L628
.L629:
	movss	-124(%rbp), %xmm2
	movaps	%xmm2, %xmm6
	divss	-104(%rbp), %xmm6
	movss	-128(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	divss	-108(%rbp), %xmm4
	movss	-132(%rbp), %xmm1
	movaps	%xmm1, %xmm7
	divss	-112(%rbp), %xmm7
	movss	-136(%rbp), %xmm11
	movaps	%xmm11, %xmm0
	divss	-116(%rbp), %xmm0
	movss	-140(%rbp), %xmm8
	movaps	%xmm8, %xmm15
	divss	-120(%rbp), %xmm15
	movss	-144(%rbp), %xmm12
	movaps	%xmm12, %xmm13
	divss	-76(%rbp), %xmm13
	movss	-100(%rbp), %xmm5
	divss	-64(%rbp), %xmm5
	movss	%xmm5, -64(%rbp)
	movss	-96(%rbp), %xmm10
	divss	-68(%rbp), %xmm10
	movss	%xmm10, -68(%rbp)
	movss	-80(%rbp), %xmm14
	divss	-52(%rbp), %xmm14
	movss	%xmm14, -52(%rbp)
	movss	-84(%rbp), %xmm9
	divss	-56(%rbp), %xmm9
	movss	%xmm9, -56(%rbp)
	movss	-88(%rbp), %xmm14
	divss	-60(%rbp), %xmm14
	movss	%xmm14, -60(%rbp)
	movss	-92(%rbp), %xmm9
	divss	-72(%rbp), %xmm9
	movss	%xmm9, -72(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm10
	divss	%xmm7, %xmm10
	movaps	%xmm11, %xmm14
	divss	%xmm0, %xmm14
	movaps	%xmm8, %xmm9
	divss	%xmm15, %xmm9
	movaps	%xmm12, %xmm7
	divss	%xmm13, %xmm7
	movss	%xmm7, -76(%rbp)
	movss	-100(%rbp), %xmm4
	divss	-64(%rbp), %xmm4
	movss	%xmm4, -64(%rbp)
	movss	-96(%rbp), %xmm0
	divss	-68(%rbp), %xmm0
	movss	%xmm0, -68(%rbp)
	movss	-80(%rbp), %xmm15
	divss	-52(%rbp), %xmm15
	movss	%xmm15, -52(%rbp)
	movss	-84(%rbp), %xmm13
	divss	-56(%rbp), %xmm13
	movss	%xmm13, -56(%rbp)
	movss	-88(%rbp), %xmm7
	divss	-60(%rbp), %xmm7
	movss	%xmm7, -60(%rbp)
	movss	-92(%rbp), %xmm15
	divss	-72(%rbp), %xmm15
	movss	%xmm15, -72(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	movaps	%xmm5, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm1, %xmm0
	divss	%xmm10, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm8, %xmm15
	divss	%xmm9, %xmm15
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm10
	divss	-76(%rbp), %xmm10
	movss	-100(%rbp), %xmm6
	divss	-64(%rbp), %xmm6
	movss	%xmm6, -64(%rbp)
	movss	-96(%rbp), %xmm3
	divss	-68(%rbp), %xmm3
	movss	%xmm3, -68(%rbp)
	movss	-80(%rbp), %xmm9
	divss	-52(%rbp), %xmm9
	movss	%xmm9, -52(%rbp)
	movss	-84(%rbp), %xmm12
	divss	-56(%rbp), %xmm12
	movss	%xmm12, -56(%rbp)
	movss	-88(%rbp), %xmm6
	divss	-60(%rbp), %xmm6
	movss	%xmm6, -60(%rbp)
	movss	-92(%rbp), %xmm3
	divss	-72(%rbp), %xmm3
	movss	%xmm3, -72(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm5, %xmm4
	movaps	%xmm4, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm1, %xmm9
	divss	%xmm0, %xmm9
	movaps	%xmm11, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm13
	divss	%xmm15, %xmm13
	movaps	%xmm14, %xmm5
	divss	%xmm10, %xmm5
	movss	%xmm5, -76(%rbp)
	movss	-100(%rbp), %xmm7
	divss	-64(%rbp), %xmm7
	movss	%xmm7, -64(%rbp)
	movss	-96(%rbp), %xmm10
	divss	-68(%rbp), %xmm10
	movss	%xmm10, -68(%rbp)
	movss	-80(%rbp), %xmm0
	divss	-52(%rbp), %xmm0
	movss	%xmm0, -52(%rbp)
	movss	-84(%rbp), %xmm15
	divss	-56(%rbp), %xmm15
	movss	%xmm15, -56(%rbp)
	movss	-88(%rbp), %xmm5
	divss	-60(%rbp), %xmm5
	movss	%xmm5, -60(%rbp)
	movss	-92(%rbp), %xmm7
	divss	-72(%rbp), %xmm7
	movss	%xmm7, -72(%rbp)
	movaps	%xmm2, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm4, %xmm3
	movaps	%xmm3, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm1, %xmm0
	divss	%xmm9, %xmm0
	movaps	%xmm11, %xmm15
	divss	%xmm12, %xmm15
	movaps	%xmm8, %xmm9
	movaps	%xmm9, %xmm10
	divss	%xmm13, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm13
	divss	-76(%rbp), %xmm13
	movss	-100(%rbp), %xmm4
	divss	-64(%rbp), %xmm4
	movss	%xmm4, -64(%rbp)
	movss	-96(%rbp), %xmm6
	divss	-68(%rbp), %xmm6
	movss	%xmm6, -68(%rbp)
	movss	-80(%rbp), %xmm8
	divss	-52(%rbp), %xmm8
	movss	%xmm8, -52(%rbp)
	movss	-84(%rbp), %xmm14
	divss	-56(%rbp), %xmm14
	movss	%xmm14, -56(%rbp)
	movss	-88(%rbp), %xmm8
	divss	-60(%rbp), %xmm8
	movss	%xmm8, -60(%rbp)
	movss	-92(%rbp), %xmm6
	divss	-72(%rbp), %xmm6
	movss	%xmm6, -72(%rbp)
	movaps	%xmm2, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm3, %xmm7
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm1, %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm11, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm9, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm12, %xmm3
	divss	%xmm13, %xmm3
	movss	%xmm3, -76(%rbp)
	movss	-100(%rbp), %xmm5
	divss	-64(%rbp), %xmm5
	movss	%xmm5, -64(%rbp)
	movss	-96(%rbp), %xmm0
	divss	-68(%rbp), %xmm0
	movss	%xmm0, -68(%rbp)
	movss	-80(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-84(%rbp), %xmm13
	divss	-56(%rbp), %xmm13
	movss	%xmm13, -56(%rbp)
	movss	-88(%rbp), %xmm3
	divss	-60(%rbp), %xmm3
	movss	%xmm3, -60(%rbp)
	movss	-92(%rbp), %xmm5
	divss	-72(%rbp), %xmm5
	movss	%xmm5, -72(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm7, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	movaps	%xmm6, %xmm0
	divss	%xmm8, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm9, %xmm8
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm9
	divss	-76(%rbp), %xmm9
	movss	-100(%rbp), %xmm4
	divss	-64(%rbp), %xmm4
	movss	%xmm4, -64(%rbp)
	movss	-96(%rbp), %xmm1
	divss	-68(%rbp), %xmm1
	movss	%xmm1, -68(%rbp)
	movss	-80(%rbp), %xmm12
	divss	-52(%rbp), %xmm12
	movss	%xmm12, -52(%rbp)
	movss	-84(%rbp), %xmm15
	divss	-56(%rbp), %xmm15
	movss	%xmm15, -56(%rbp)
	movss	-88(%rbp), %xmm4
	divss	-60(%rbp), %xmm4
	movss	%xmm4, -60(%rbp)
	movss	-92(%rbp), %xmm1
	divss	-72(%rbp), %xmm1
	movss	%xmm1, -72(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm7, %xmm5
	divss	%xmm3, %xmm5
	movaps	%xmm6, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm11, %xmm0
	movaps	%xmm0, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm14, %xmm11
	divss	%xmm9, %xmm11
	movss	-100(%rbp), %xmm13
	movaps	%xmm13, %xmm3
	divss	-64(%rbp), %xmm3
	movss	%xmm3, -64(%rbp)
	movss	-96(%rbp), %xmm9
	divss	-68(%rbp), %xmm9
	movss	%xmm9, -68(%rbp)
	movss	-80(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-84(%rbp), %xmm3
	divss	-56(%rbp), %xmm3
	movss	%xmm3, -56(%rbp)
	movss	-88(%rbp), %xmm9
	divss	-60(%rbp), %xmm9
	movss	%xmm9, -60(%rbp)
	movss	-92(%rbp), %xmm10
	divss	-72(%rbp), %xmm10
	movss	%xmm10, -72(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	divss	%xmm1, %xmm5
	movaps	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	divss	%xmm12, %xmm0
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm9
	divss	%xmm11, %xmm9
	movss	%xmm9, -76(%rbp)
	movaps	%xmm13, %xmm11
	movaps	%xmm11, %xmm14
	divss	-64(%rbp), %xmm14
	movss	-96(%rbp), %xmm15
	movaps	%xmm15, %xmm13
	divss	-68(%rbp), %xmm13
	movss	-80(%rbp), %xmm9
	divss	-52(%rbp), %xmm9
	movss	%xmm9, -52(%rbp)
	movss	-84(%rbp), %xmm9
	divss	-56(%rbp), %xmm9
	movss	%xmm9, -56(%rbp)
	movss	-88(%rbp), %xmm9
	divss	-60(%rbp), %xmm9
	movss	%xmm9, -60(%rbp)
	movss	-92(%rbp), %xmm9
	divss	-72(%rbp), %xmm9
	divss	%xmm3, %xmm2
	movss	%xmm2, -104(%rbp)
	divss	%xmm4, %xmm7
	movss	%xmm7, -108(%rbp)
	divss	%xmm5, %xmm6
	movss	%xmm6, -112(%rbp)
	divss	%xmm0, %xmm1
	movss	%xmm1, -116(%rbp)
	divss	%xmm10, %xmm8
	movss	%xmm8, -120(%rbp)
	divss	-76(%rbp), %xmm12
	movss	%xmm12, -76(%rbp)
	divss	%xmm14, %xmm11
	movss	%xmm11, -64(%rbp)
	divss	%xmm13, %xmm15
	movss	%xmm15, -68(%rbp)
	movss	-80(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-84(%rbp), %xmm3
	divss	-56(%rbp), %xmm3
	movss	%xmm3, -56(%rbp)
	movss	-88(%rbp), %xmm4
	divss	-60(%rbp), %xmm4
	movss	%xmm4, -60(%rbp)
	movss	-92(%rbp), %xmm10
	divss	%xmm9, %xmm10
	movss	%xmm10, -72(%rbp)
.L628:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L629
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-140(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE212:
	.size	float_div_11, .-float_div_11
	.globl	float_div_12
	.type	float_div_12, @function
float_div_12:
.LFB213:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -132(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -116(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -140(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -124(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -148(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -108(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	56(%rax), %eax
	subl	$10, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	60(%rax), %eax
	subl	$11, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	jmp	.L631
.L632:
	movss	-132(%rbp), %xmm2
	movaps	%xmm2, %xmm6
	divss	-112(%rbp), %xmm6
	movss	-136(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	divss	-116(%rbp), %xmm4
	movss	-140(%rbp), %xmm1
	movaps	%xmm1, %xmm7
	divss	-120(%rbp), %xmm7
	movss	-144(%rbp), %xmm11
	movaps	%xmm11, %xmm0
	divss	-124(%rbp), %xmm0
	movss	-148(%rbp), %xmm8
	movaps	%xmm8, %xmm15
	divss	-128(%rbp), %xmm15
	movss	-152(%rbp), %xmm12
	movaps	%xmm12, %xmm13
	divss	-80(%rbp), %xmm13
	movss	-108(%rbp), %xmm5
	divss	-68(%rbp), %xmm5
	movss	%xmm5, -68(%rbp)
	movss	-104(%rbp), %xmm10
	divss	-72(%rbp), %xmm10
	movss	%xmm10, -72(%rbp)
	movss	-84(%rbp), %xmm14
	divss	-52(%rbp), %xmm14
	movss	%xmm14, -52(%rbp)
	movss	-88(%rbp), %xmm9
	divss	-56(%rbp), %xmm9
	movss	%xmm9, -56(%rbp)
	movss	-92(%rbp), %xmm14
	divss	-60(%rbp), %xmm14
	movss	%xmm14, -60(%rbp)
	movss	-96(%rbp), %xmm9
	divss	-64(%rbp), %xmm9
	movss	%xmm9, -64(%rbp)
	movss	-100(%rbp), %xmm5
	divss	-76(%rbp), %xmm5
	movss	%xmm5, -76(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm10
	divss	%xmm7, %xmm10
	movaps	%xmm11, %xmm14
	divss	%xmm0, %xmm14
	movaps	%xmm8, %xmm9
	divss	%xmm15, %xmm9
	movaps	%xmm12, %xmm7
	divss	%xmm13, %xmm7
	movss	%xmm7, -80(%rbp)
	movss	-108(%rbp), %xmm4
	divss	-68(%rbp), %xmm4
	movss	%xmm4, -68(%rbp)
	movss	-104(%rbp), %xmm0
	divss	-72(%rbp), %xmm0
	movss	%xmm0, -72(%rbp)
	movss	-84(%rbp), %xmm15
	divss	-52(%rbp), %xmm15
	movss	%xmm15, -52(%rbp)
	movss	-88(%rbp), %xmm13
	divss	-56(%rbp), %xmm13
	movss	%xmm13, -56(%rbp)
	movss	-92(%rbp), %xmm7
	divss	-60(%rbp), %xmm7
	movss	%xmm7, -60(%rbp)
	movss	-96(%rbp), %xmm15
	divss	-64(%rbp), %xmm15
	movss	%xmm15, -64(%rbp)
	movss	-100(%rbp), %xmm0
	divss	-76(%rbp), %xmm0
	movss	%xmm0, -76(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	movaps	%xmm5, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm1, %xmm0
	divss	%xmm10, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm8, %xmm15
	divss	%xmm9, %xmm15
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm10
	divss	-80(%rbp), %xmm10
	movss	-108(%rbp), %xmm6
	divss	-68(%rbp), %xmm6
	movss	%xmm6, -68(%rbp)
	movss	-104(%rbp), %xmm3
	divss	-72(%rbp), %xmm3
	movss	%xmm3, -72(%rbp)
	movss	-84(%rbp), %xmm9
	divss	-52(%rbp), %xmm9
	movss	%xmm9, -52(%rbp)
	movss	-88(%rbp), %xmm12
	divss	-56(%rbp), %xmm12
	movss	%xmm12, -56(%rbp)
	movss	-92(%rbp), %xmm6
	divss	-60(%rbp), %xmm6
	movss	%xmm6, -60(%rbp)
	movss	-96(%rbp), %xmm3
	divss	-64(%rbp), %xmm3
	movss	%xmm3, -64(%rbp)
	movss	-100(%rbp), %xmm12
	divss	-76(%rbp), %xmm12
	movss	%xmm12, -76(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm5, %xmm4
	movaps	%xmm4, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm1, %xmm9
	divss	%xmm0, %xmm9
	movaps	%xmm11, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm13
	divss	%xmm15, %xmm13
	movaps	%xmm14, %xmm5
	divss	%xmm10, %xmm5
	movss	%xmm5, -80(%rbp)
	movss	-108(%rbp), %xmm7
	divss	-68(%rbp), %xmm7
	movss	%xmm7, -68(%rbp)
	movss	-104(%rbp), %xmm10
	divss	-72(%rbp), %xmm10
	movss	%xmm10, -72(%rbp)
	movss	-84(%rbp), %xmm0
	divss	-52(%rbp), %xmm0
	movss	%xmm0, -52(%rbp)
	movss	-88(%rbp), %xmm15
	divss	-56(%rbp), %xmm15
	movss	%xmm15, -56(%rbp)
	movss	-92(%rbp), %xmm5
	divss	-60(%rbp), %xmm5
	movss	%xmm5, -60(%rbp)
	movss	-96(%rbp), %xmm7
	divss	-64(%rbp), %xmm7
	movss	%xmm7, -64(%rbp)
	movss	-100(%rbp), %xmm10
	divss	-76(%rbp), %xmm10
	movss	%xmm10, -76(%rbp)
	movaps	%xmm2, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm4, %xmm3
	movaps	%xmm3, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm1, %xmm0
	divss	%xmm9, %xmm0
	movaps	%xmm11, %xmm15
	divss	%xmm12, %xmm15
	movaps	%xmm8, %xmm9
	movaps	%xmm9, %xmm10
	divss	%xmm13, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm13
	divss	-80(%rbp), %xmm13
	movss	-108(%rbp), %xmm4
	divss	-68(%rbp), %xmm4
	movss	%xmm4, -68(%rbp)
	movss	-104(%rbp), %xmm6
	divss	-72(%rbp), %xmm6
	movss	%xmm6, -72(%rbp)
	movss	-84(%rbp), %xmm8
	divss	-52(%rbp), %xmm8
	movss	%xmm8, -52(%rbp)
	movss	-88(%rbp), %xmm14
	divss	-56(%rbp), %xmm14
	movss	%xmm14, -56(%rbp)
	movss	-92(%rbp), %xmm8
	divss	-60(%rbp), %xmm8
	movss	%xmm8, -60(%rbp)
	movss	-96(%rbp), %xmm6
	divss	-64(%rbp), %xmm6
	movss	%xmm6, -64(%rbp)
	movss	-100(%rbp), %xmm14
	divss	-76(%rbp), %xmm14
	movss	%xmm14, -76(%rbp)
	movaps	%xmm2, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm3, %xmm7
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm1, %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm11, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm9, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm12, %xmm3
	divss	%xmm13, %xmm3
	movss	%xmm3, -80(%rbp)
	movss	-108(%rbp), %xmm5
	divss	-68(%rbp), %xmm5
	movss	%xmm5, -68(%rbp)
	movss	-104(%rbp), %xmm0
	divss	-72(%rbp), %xmm0
	movss	%xmm0, -72(%rbp)
	movss	-84(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-88(%rbp), %xmm13
	divss	-56(%rbp), %xmm13
	movss	%xmm13, -56(%rbp)
	movss	-92(%rbp), %xmm3
	divss	-60(%rbp), %xmm3
	movss	%xmm3, -60(%rbp)
	movss	-96(%rbp), %xmm5
	divss	-64(%rbp), %xmm5
	movss	%xmm5, -64(%rbp)
	movss	-100(%rbp), %xmm0
	divss	-76(%rbp), %xmm0
	movss	%xmm0, -76(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm7, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	movaps	%xmm6, %xmm0
	divss	%xmm8, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm9, %xmm8
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm9
	divss	-80(%rbp), %xmm9
	movss	-108(%rbp), %xmm4
	divss	-68(%rbp), %xmm4
	movss	%xmm4, -68(%rbp)
	movss	-104(%rbp), %xmm1
	divss	-72(%rbp), %xmm1
	movss	%xmm1, -72(%rbp)
	movss	-84(%rbp), %xmm12
	divss	-52(%rbp), %xmm12
	movss	%xmm12, -52(%rbp)
	movss	-88(%rbp), %xmm15
	divss	-56(%rbp), %xmm15
	movss	%xmm15, -56(%rbp)
	movss	-92(%rbp), %xmm4
	divss	-60(%rbp), %xmm4
	movss	%xmm4, -60(%rbp)
	movss	-96(%rbp), %xmm1
	divss	-64(%rbp), %xmm1
	movss	%xmm1, -64(%rbp)
	movss	-100(%rbp), %xmm12
	divss	-76(%rbp), %xmm12
	movss	%xmm12, -76(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm7, %xmm5
	divss	%xmm3, %xmm5
	movaps	%xmm6, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm11, %xmm0
	movaps	%xmm0, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm14, %xmm11
	divss	%xmm9, %xmm11
	movss	-108(%rbp), %xmm13
	movaps	%xmm13, %xmm3
	divss	-68(%rbp), %xmm3
	movss	%xmm3, -68(%rbp)
	movss	-104(%rbp), %xmm9
	divss	-72(%rbp), %xmm9
	movss	%xmm9, -72(%rbp)
	movss	-84(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-88(%rbp), %xmm3
	divss	-56(%rbp), %xmm3
	movss	%xmm3, -56(%rbp)
	movss	-92(%rbp), %xmm9
	divss	-60(%rbp), %xmm9
	movss	%xmm9, -60(%rbp)
	movss	-96(%rbp), %xmm10
	divss	-64(%rbp), %xmm10
	movss	%xmm10, -64(%rbp)
	movss	-100(%rbp), %xmm3
	divss	-76(%rbp), %xmm3
	movss	%xmm3, -76(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	divss	%xmm1, %xmm5
	movaps	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	divss	%xmm12, %xmm0
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm9
	divss	%xmm11, %xmm9
	movss	%xmm9, -80(%rbp)
	movaps	%xmm13, %xmm11
	movaps	%xmm11, %xmm14
	divss	-68(%rbp), %xmm14
	movss	-104(%rbp), %xmm15
	movaps	%xmm15, %xmm13
	divss	-72(%rbp), %xmm13
	movss	-84(%rbp), %xmm9
	divss	-52(%rbp), %xmm9
	movss	%xmm9, -52(%rbp)
	movss	-88(%rbp), %xmm9
	divss	-56(%rbp), %xmm9
	movss	%xmm9, -56(%rbp)
	movss	-92(%rbp), %xmm9
	divss	-60(%rbp), %xmm9
	movss	%xmm9, -60(%rbp)
	movss	-96(%rbp), %xmm9
	divss	-64(%rbp), %xmm9
	movss	%xmm9, -64(%rbp)
	movss	-100(%rbp), %xmm9
	divss	-76(%rbp), %xmm9
	divss	%xmm3, %xmm2
	movss	%xmm2, -112(%rbp)
	divss	%xmm4, %xmm7
	movss	%xmm7, -116(%rbp)
	divss	%xmm5, %xmm6
	movss	%xmm6, -120(%rbp)
	divss	%xmm0, %xmm1
	movss	%xmm1, -124(%rbp)
	divss	%xmm10, %xmm8
	movss	%xmm8, -128(%rbp)
	divss	-80(%rbp), %xmm12
	movss	%xmm12, -80(%rbp)
	divss	%xmm14, %xmm11
	movss	%xmm11, -68(%rbp)
	divss	%xmm13, %xmm15
	movss	%xmm15, -72(%rbp)
	movss	-84(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-88(%rbp), %xmm3
	divss	-56(%rbp), %xmm3
	movss	%xmm3, -56(%rbp)
	movss	-92(%rbp), %xmm4
	divss	-60(%rbp), %xmm4
	movss	%xmm4, -60(%rbp)
	movss	-96(%rbp), %xmm10
	divss	-64(%rbp), %xmm10
	movss	%xmm10, -64(%rbp)
	movss	-100(%rbp), %xmm3
	divss	%xmm9, %xmm3
	movss	%xmm3, -76(%rbp)
.L631:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L632
	cvttss2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-140(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-148(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE213:
	.size	float_div_12, .-float_div_12
	.globl	float_div_13
	.type	float_div_13, @function
float_div_13:
.LFB214:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -140(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -124(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -148(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -132(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -156(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -116(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	56(%rax), %eax
	subl	$10, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	60(%rax), %eax
	subl	$11, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	64(%rax), %eax
	subl	$12, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -108(%rbp)
	jmp	.L634
.L635:
	movss	-140(%rbp), %xmm2
	movaps	%xmm2, %xmm6
	divss	-120(%rbp), %xmm6
	movss	-144(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	divss	-124(%rbp), %xmm4
	movss	-148(%rbp), %xmm1
	movaps	%xmm1, %xmm7
	divss	-128(%rbp), %xmm7
	movss	-152(%rbp), %xmm11
	movaps	%xmm11, %xmm0
	divss	-132(%rbp), %xmm0
	movss	-156(%rbp), %xmm8
	movaps	%xmm8, %xmm15
	divss	-136(%rbp), %xmm15
	movss	-160(%rbp), %xmm12
	movaps	%xmm12, %xmm13
	divss	-84(%rbp), %xmm13
	movss	-116(%rbp), %xmm5
	divss	-72(%rbp), %xmm5
	movss	%xmm5, -72(%rbp)
	movss	-112(%rbp), %xmm10
	divss	-76(%rbp), %xmm10
	movss	%xmm10, -76(%rbp)
	movss	-88(%rbp), %xmm14
	divss	-52(%rbp), %xmm14
	movss	%xmm14, -52(%rbp)
	movss	-92(%rbp), %xmm9
	divss	-56(%rbp), %xmm9
	movss	%xmm9, -56(%rbp)
	movss	-96(%rbp), %xmm14
	divss	-60(%rbp), %xmm14
	movss	%xmm14, -60(%rbp)
	movss	-100(%rbp), %xmm9
	divss	-64(%rbp), %xmm9
	movss	%xmm9, -64(%rbp)
	movss	-104(%rbp), %xmm5
	divss	-68(%rbp), %xmm5
	movss	%xmm5, -68(%rbp)
	movss	-108(%rbp), %xmm10
	divss	-80(%rbp), %xmm10
	movss	%xmm10, -80(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm10
	divss	%xmm7, %xmm10
	movaps	%xmm11, %xmm14
	divss	%xmm0, %xmm14
	movaps	%xmm8, %xmm9
	divss	%xmm15, %xmm9
	movaps	%xmm12, %xmm7
	divss	%xmm13, %xmm7
	movss	%xmm7, -84(%rbp)
	movss	-116(%rbp), %xmm4
	divss	-72(%rbp), %xmm4
	movss	%xmm4, -72(%rbp)
	movss	-112(%rbp), %xmm0
	divss	-76(%rbp), %xmm0
	movss	%xmm0, -76(%rbp)
	movss	-88(%rbp), %xmm15
	divss	-52(%rbp), %xmm15
	movss	%xmm15, -52(%rbp)
	movss	-92(%rbp), %xmm13
	divss	-56(%rbp), %xmm13
	movss	%xmm13, -56(%rbp)
	movss	-96(%rbp), %xmm7
	divss	-60(%rbp), %xmm7
	movss	%xmm7, -60(%rbp)
	movss	-100(%rbp), %xmm15
	divss	-64(%rbp), %xmm15
	movss	%xmm15, -64(%rbp)
	movss	-104(%rbp), %xmm0
	divss	-68(%rbp), %xmm0
	movss	%xmm0, -68(%rbp)
	movss	-108(%rbp), %xmm13
	divss	-80(%rbp), %xmm13
	movss	%xmm13, -80(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	movaps	%xmm5, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm1, %xmm0
	divss	%xmm10, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm8, %xmm15
	divss	%xmm9, %xmm15
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm10
	divss	-84(%rbp), %xmm10
	movss	-116(%rbp), %xmm6
	divss	-72(%rbp), %xmm6
	movss	%xmm6, -72(%rbp)
	movss	-112(%rbp), %xmm3
	divss	-76(%rbp), %xmm3
	movss	%xmm3, -76(%rbp)
	movss	-88(%rbp), %xmm9
	divss	-52(%rbp), %xmm9
	movss	%xmm9, -52(%rbp)
	movss	-92(%rbp), %xmm12
	divss	-56(%rbp), %xmm12
	movss	%xmm12, -56(%rbp)
	movss	-96(%rbp), %xmm6
	divss	-60(%rbp), %xmm6
	movss	%xmm6, -60(%rbp)
	movss	-100(%rbp), %xmm3
	divss	-64(%rbp), %xmm3
	movss	%xmm3, -64(%rbp)
	movss	-104(%rbp), %xmm12
	divss	-68(%rbp), %xmm12
	movss	%xmm12, -68(%rbp)
	movss	-108(%rbp), %xmm9
	divss	-80(%rbp), %xmm9
	movss	%xmm9, -80(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm5, %xmm4
	movaps	%xmm4, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm1, %xmm9
	divss	%xmm0, %xmm9
	movaps	%xmm11, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm13
	divss	%xmm15, %xmm13
	movaps	%xmm14, %xmm5
	divss	%xmm10, %xmm5
	movss	%xmm5, -84(%rbp)
	movss	-116(%rbp), %xmm7
	divss	-72(%rbp), %xmm7
	movss	%xmm7, -72(%rbp)
	movss	-112(%rbp), %xmm10
	divss	-76(%rbp), %xmm10
	movss	%xmm10, -76(%rbp)
	movss	-88(%rbp), %xmm0
	divss	-52(%rbp), %xmm0
	movss	%xmm0, -52(%rbp)
	movss	-92(%rbp), %xmm15
	divss	-56(%rbp), %xmm15
	movss	%xmm15, -56(%rbp)
	movss	-96(%rbp), %xmm5
	divss	-60(%rbp), %xmm5
	movss	%xmm5, -60(%rbp)
	movss	-100(%rbp), %xmm7
	divss	-64(%rbp), %xmm7
	movss	%xmm7, -64(%rbp)
	movss	-104(%rbp), %xmm10
	divss	-68(%rbp), %xmm10
	movss	%xmm10, -68(%rbp)
	movss	-108(%rbp), %xmm0
	divss	-80(%rbp), %xmm0
	movss	%xmm0, -80(%rbp)
	movaps	%xmm2, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm4, %xmm3
	movaps	%xmm3, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm1, %xmm0
	divss	%xmm9, %xmm0
	movaps	%xmm11, %xmm15
	divss	%xmm12, %xmm15
	movaps	%xmm8, %xmm9
	movaps	%xmm9, %xmm10
	divss	%xmm13, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm13
	divss	-84(%rbp), %xmm13
	movss	-116(%rbp), %xmm4
	divss	-72(%rbp), %xmm4
	movss	%xmm4, -72(%rbp)
	movss	-112(%rbp), %xmm6
	divss	-76(%rbp), %xmm6
	movss	%xmm6, -76(%rbp)
	movss	-88(%rbp), %xmm8
	divss	-52(%rbp), %xmm8
	movss	%xmm8, -52(%rbp)
	movss	-92(%rbp), %xmm14
	divss	-56(%rbp), %xmm14
	movss	%xmm14, -56(%rbp)
	movss	-96(%rbp), %xmm8
	divss	-60(%rbp), %xmm8
	movss	%xmm8, -60(%rbp)
	movss	-100(%rbp), %xmm6
	divss	-64(%rbp), %xmm6
	movss	%xmm6, -64(%rbp)
	movss	-104(%rbp), %xmm14
	divss	-68(%rbp), %xmm14
	movss	%xmm14, -68(%rbp)
	movss	-108(%rbp), %xmm4
	divss	-80(%rbp), %xmm4
	movss	%xmm4, -80(%rbp)
	movaps	%xmm2, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm3, %xmm7
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm1, %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm11, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm9, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm12, %xmm3
	divss	%xmm13, %xmm3
	movss	%xmm3, -84(%rbp)
	movss	-116(%rbp), %xmm5
	divss	-72(%rbp), %xmm5
	movss	%xmm5, -72(%rbp)
	movss	-112(%rbp), %xmm0
	divss	-76(%rbp), %xmm0
	movss	%xmm0, -76(%rbp)
	movss	-88(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-92(%rbp), %xmm13
	divss	-56(%rbp), %xmm13
	movss	%xmm13, -56(%rbp)
	movss	-96(%rbp), %xmm3
	divss	-60(%rbp), %xmm3
	movss	%xmm3, -60(%rbp)
	movss	-100(%rbp), %xmm5
	divss	-64(%rbp), %xmm5
	movss	%xmm5, -64(%rbp)
	movss	-104(%rbp), %xmm0
	divss	-68(%rbp), %xmm0
	movss	%xmm0, -68(%rbp)
	movss	-108(%rbp), %xmm10
	divss	-80(%rbp), %xmm10
	movss	%xmm10, -80(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm7, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	movaps	%xmm6, %xmm0
	divss	%xmm8, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm9, %xmm8
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm9
	divss	-84(%rbp), %xmm9
	movss	-116(%rbp), %xmm4
	divss	-72(%rbp), %xmm4
	movss	%xmm4, -72(%rbp)
	movss	-112(%rbp), %xmm1
	divss	-76(%rbp), %xmm1
	movss	%xmm1, -76(%rbp)
	movss	-88(%rbp), %xmm12
	divss	-52(%rbp), %xmm12
	movss	%xmm12, -52(%rbp)
	movss	-92(%rbp), %xmm15
	divss	-56(%rbp), %xmm15
	movss	%xmm15, -56(%rbp)
	movss	-96(%rbp), %xmm4
	divss	-60(%rbp), %xmm4
	movss	%xmm4, -60(%rbp)
	movss	-100(%rbp), %xmm1
	divss	-64(%rbp), %xmm1
	movss	%xmm1, -64(%rbp)
	movss	-104(%rbp), %xmm12
	divss	-68(%rbp), %xmm12
	movss	%xmm12, -68(%rbp)
	movss	-108(%rbp), %xmm15
	divss	-80(%rbp), %xmm15
	movss	%xmm15, -80(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm7, %xmm5
	divss	%xmm3, %xmm5
	movaps	%xmm6, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm11, %xmm0
	movaps	%xmm0, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm14, %xmm11
	divss	%xmm9, %xmm11
	movss	-116(%rbp), %xmm13
	movaps	%xmm13, %xmm3
	divss	-72(%rbp), %xmm3
	movss	%xmm3, -72(%rbp)
	movss	-112(%rbp), %xmm9
	divss	-76(%rbp), %xmm9
	movss	%xmm9, -76(%rbp)
	movss	-88(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-92(%rbp), %xmm3
	divss	-56(%rbp), %xmm3
	movss	%xmm3, -56(%rbp)
	movss	-96(%rbp), %xmm9
	divss	-60(%rbp), %xmm9
	movss	%xmm9, -60(%rbp)
	movss	-100(%rbp), %xmm10
	divss	-64(%rbp), %xmm10
	movss	%xmm10, -64(%rbp)
	movss	-104(%rbp), %xmm3
	divss	-68(%rbp), %xmm3
	movss	%xmm3, -68(%rbp)
	movss	-108(%rbp), %xmm9
	divss	-80(%rbp), %xmm9
	movss	%xmm9, -80(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	divss	%xmm1, %xmm5
	movaps	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	divss	%xmm12, %xmm0
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm9
	divss	%xmm11, %xmm9
	movss	%xmm9, -84(%rbp)
	movaps	%xmm13, %xmm11
	movaps	%xmm11, %xmm14
	divss	-72(%rbp), %xmm14
	movss	-112(%rbp), %xmm15
	movaps	%xmm15, %xmm13
	divss	-76(%rbp), %xmm13
	movss	-88(%rbp), %xmm9
	divss	-52(%rbp), %xmm9
	movss	%xmm9, -52(%rbp)
	movss	-92(%rbp), %xmm9
	divss	-56(%rbp), %xmm9
	movss	%xmm9, -56(%rbp)
	movss	-96(%rbp), %xmm9
	divss	-60(%rbp), %xmm9
	movss	%xmm9, -60(%rbp)
	movss	-100(%rbp), %xmm9
	divss	-64(%rbp), %xmm9
	movss	%xmm9, -64(%rbp)
	movss	-104(%rbp), %xmm9
	divss	-68(%rbp), %xmm9
	movss	%xmm9, -68(%rbp)
	movss	-108(%rbp), %xmm9
	divss	-80(%rbp), %xmm9
	divss	%xmm3, %xmm2
	movss	%xmm2, -120(%rbp)
	divss	%xmm4, %xmm7
	movss	%xmm7, -124(%rbp)
	divss	%xmm5, %xmm6
	movss	%xmm6, -128(%rbp)
	divss	%xmm0, %xmm1
	movss	%xmm1, -132(%rbp)
	divss	%xmm10, %xmm8
	movss	%xmm8, -136(%rbp)
	divss	-84(%rbp), %xmm12
	movss	%xmm12, -84(%rbp)
	divss	%xmm14, %xmm11
	movss	%xmm11, -72(%rbp)
	divss	%xmm13, %xmm15
	movss	%xmm15, -76(%rbp)
	movss	-88(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-92(%rbp), %xmm3
	divss	-56(%rbp), %xmm3
	movss	%xmm3, -56(%rbp)
	movss	-96(%rbp), %xmm4
	divss	-60(%rbp), %xmm4
	movss	%xmm4, -60(%rbp)
	movss	-100(%rbp), %xmm10
	divss	-64(%rbp), %xmm10
	movss	%xmm10, -64(%rbp)
	movss	-104(%rbp), %xmm3
	divss	-68(%rbp), %xmm3
	movss	%xmm3, -68(%rbp)
	movss	-108(%rbp), %xmm15
	divss	%xmm9, %xmm15
	movss	%xmm15, -80(%rbp)
.L634:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L635
	cvttss2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-140(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-148(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-156(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE214:
	.size	float_div_13, .-float_div_13
	.globl	float_div_14
	.type	float_div_14, @function
float_div_14:
.LFB215:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -148(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -132(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -156(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -140(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -164(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -124(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	56(%rax), %eax
	subl	$10, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	60(%rax), %eax
	subl	$11, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -108(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	64(%rax), %eax
	subl	$12, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	192(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movl	68(%rax), %eax
	subl	$13, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -116(%rbp)
	jmp	.L637
.L638:
	movss	-148(%rbp), %xmm2
	movaps	%xmm2, %xmm6
	divss	-128(%rbp), %xmm6
	movss	-152(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	divss	-132(%rbp), %xmm4
	movss	-156(%rbp), %xmm1
	movaps	%xmm1, %xmm7
	divss	-136(%rbp), %xmm7
	movss	-160(%rbp), %xmm11
	movaps	%xmm11, %xmm0
	divss	-140(%rbp), %xmm0
	movss	-164(%rbp), %xmm8
	movaps	%xmm8, %xmm15
	divss	-144(%rbp), %xmm15
	movss	-168(%rbp), %xmm12
	movaps	%xmm12, %xmm13
	divss	-88(%rbp), %xmm13
	movss	-124(%rbp), %xmm5
	divss	-76(%rbp), %xmm5
	movss	%xmm5, -76(%rbp)
	movss	-120(%rbp), %xmm10
	divss	-80(%rbp), %xmm10
	movss	%xmm10, -80(%rbp)
	movss	-92(%rbp), %xmm14
	divss	-52(%rbp), %xmm14
	movss	%xmm14, -52(%rbp)
	movss	-96(%rbp), %xmm9
	divss	-56(%rbp), %xmm9
	movss	%xmm9, -56(%rbp)
	movss	-100(%rbp), %xmm14
	divss	-60(%rbp), %xmm14
	movss	%xmm14, -60(%rbp)
	movss	-104(%rbp), %xmm9
	divss	-64(%rbp), %xmm9
	movss	%xmm9, -64(%rbp)
	movss	-108(%rbp), %xmm5
	divss	-68(%rbp), %xmm5
	movss	%xmm5, -68(%rbp)
	movss	-112(%rbp), %xmm10
	divss	-72(%rbp), %xmm10
	movss	%xmm10, -72(%rbp)
	movss	-116(%rbp), %xmm14
	divss	-84(%rbp), %xmm14
	movss	%xmm14, -84(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm10
	divss	%xmm7, %xmm10
	movaps	%xmm11, %xmm14
	divss	%xmm0, %xmm14
	movaps	%xmm8, %xmm9
	divss	%xmm15, %xmm9
	movaps	%xmm12, %xmm7
	divss	%xmm13, %xmm7
	movss	%xmm7, -88(%rbp)
	movss	-124(%rbp), %xmm4
	divss	-76(%rbp), %xmm4
	movss	%xmm4, -76(%rbp)
	movss	-120(%rbp), %xmm0
	divss	-80(%rbp), %xmm0
	movss	%xmm0, -80(%rbp)
	movss	-92(%rbp), %xmm15
	divss	-52(%rbp), %xmm15
	movss	%xmm15, -52(%rbp)
	movss	-96(%rbp), %xmm13
	divss	-56(%rbp), %xmm13
	movss	%xmm13, -56(%rbp)
	movss	-100(%rbp), %xmm7
	divss	-60(%rbp), %xmm7
	movss	%xmm7, -60(%rbp)
	movss	-104(%rbp), %xmm15
	divss	-64(%rbp), %xmm15
	movss	%xmm15, -64(%rbp)
	movss	-108(%rbp), %xmm0
	divss	-68(%rbp), %xmm0
	movss	%xmm0, -68(%rbp)
	movss	-112(%rbp), %xmm13
	divss	-72(%rbp), %xmm13
	movss	%xmm13, -72(%rbp)
	movss	-116(%rbp), %xmm4
	divss	-84(%rbp), %xmm4
	movss	%xmm4, -84(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	movaps	%xmm5, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm1, %xmm0
	divss	%xmm10, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm8, %xmm15
	divss	%xmm9, %xmm15
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm10
	divss	-88(%rbp), %xmm10
	movss	-124(%rbp), %xmm6
	divss	-76(%rbp), %xmm6
	movss	%xmm6, -76(%rbp)
	movss	-120(%rbp), %xmm3
	divss	-80(%rbp), %xmm3
	movss	%xmm3, -80(%rbp)
	movss	-92(%rbp), %xmm9
	divss	-52(%rbp), %xmm9
	movss	%xmm9, -52(%rbp)
	movss	-96(%rbp), %xmm12
	divss	-56(%rbp), %xmm12
	movss	%xmm12, -56(%rbp)
	movss	-100(%rbp), %xmm6
	divss	-60(%rbp), %xmm6
	movss	%xmm6, -60(%rbp)
	movss	-104(%rbp), %xmm3
	divss	-64(%rbp), %xmm3
	movss	%xmm3, -64(%rbp)
	movss	-108(%rbp), %xmm12
	divss	-68(%rbp), %xmm12
	movss	%xmm12, -68(%rbp)
	movss	-112(%rbp), %xmm9
	divss	-72(%rbp), %xmm9
	movss	%xmm9, -72(%rbp)
	movss	-116(%rbp), %xmm6
	divss	-84(%rbp), %xmm6
	movss	%xmm6, -84(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm5, %xmm4
	movaps	%xmm4, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm1, %xmm9
	divss	%xmm0, %xmm9
	movaps	%xmm11, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm13
	divss	%xmm15, %xmm13
	movaps	%xmm14, %xmm5
	divss	%xmm10, %xmm5
	movss	%xmm5, -88(%rbp)
	movss	-124(%rbp), %xmm7
	divss	-76(%rbp), %xmm7
	movss	%xmm7, -76(%rbp)
	movss	-120(%rbp), %xmm10
	divss	-80(%rbp), %xmm10
	movss	%xmm10, -80(%rbp)
	movss	-92(%rbp), %xmm0
	divss	-52(%rbp), %xmm0
	movss	%xmm0, -52(%rbp)
	movss	-96(%rbp), %xmm15
	divss	-56(%rbp), %xmm15
	movss	%xmm15, -56(%rbp)
	movss	-100(%rbp), %xmm5
	divss	-60(%rbp), %xmm5
	movss	%xmm5, -60(%rbp)
	movss	-104(%rbp), %xmm7
	divss	-64(%rbp), %xmm7
	movss	%xmm7, -64(%rbp)
	movss	-108(%rbp), %xmm10
	divss	-68(%rbp), %xmm10
	movss	%xmm10, -68(%rbp)
	movss	-112(%rbp), %xmm0
	divss	-72(%rbp), %xmm0
	movss	%xmm0, -72(%rbp)
	movss	-116(%rbp), %xmm15
	divss	-84(%rbp), %xmm15
	movss	%xmm15, -84(%rbp)
	movaps	%xmm2, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm4, %xmm3
	movaps	%xmm3, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm1, %xmm0
	divss	%xmm9, %xmm0
	movaps	%xmm11, %xmm15
	divss	%xmm12, %xmm15
	movaps	%xmm8, %xmm9
	movaps	%xmm9, %xmm10
	divss	%xmm13, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm13
	divss	-88(%rbp), %xmm13
	movss	-124(%rbp), %xmm4
	divss	-76(%rbp), %xmm4
	movss	%xmm4, -76(%rbp)
	movss	-120(%rbp), %xmm6
	divss	-80(%rbp), %xmm6
	movss	%xmm6, -80(%rbp)
	movss	-92(%rbp), %xmm8
	divss	-52(%rbp), %xmm8
	movss	%xmm8, -52(%rbp)
	movss	-96(%rbp), %xmm14
	divss	-56(%rbp), %xmm14
	movss	%xmm14, -56(%rbp)
	movss	-100(%rbp), %xmm8
	divss	-60(%rbp), %xmm8
	movss	%xmm8, -60(%rbp)
	movss	-104(%rbp), %xmm6
	divss	-64(%rbp), %xmm6
	movss	%xmm6, -64(%rbp)
	movss	-108(%rbp), %xmm14
	divss	-68(%rbp), %xmm14
	movss	%xmm14, -68(%rbp)
	movss	-112(%rbp), %xmm4
	divss	-72(%rbp), %xmm4
	movss	%xmm4, -72(%rbp)
	movss	-116(%rbp), %xmm8
	divss	-84(%rbp), %xmm8
	movss	%xmm8, -84(%rbp)
	movaps	%xmm2, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm3, %xmm7
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm1, %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm11, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm9, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm12, %xmm3
	divss	%xmm13, %xmm3
	movss	%xmm3, -88(%rbp)
	movss	-124(%rbp), %xmm5
	divss	-76(%rbp), %xmm5
	movss	%xmm5, -76(%rbp)
	movss	-120(%rbp), %xmm0
	divss	-80(%rbp), %xmm0
	movss	%xmm0, -80(%rbp)
	movss	-92(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-96(%rbp), %xmm13
	divss	-56(%rbp), %xmm13
	movss	%xmm13, -56(%rbp)
	movss	-100(%rbp), %xmm3
	divss	-60(%rbp), %xmm3
	movss	%xmm3, -60(%rbp)
	movss	-104(%rbp), %xmm5
	divss	-64(%rbp), %xmm5
	movss	%xmm5, -64(%rbp)
	movss	-108(%rbp), %xmm0
	divss	-68(%rbp), %xmm0
	movss	%xmm0, -68(%rbp)
	movss	-112(%rbp), %xmm10
	divss	-72(%rbp), %xmm10
	movss	%xmm10, -72(%rbp)
	movss	-116(%rbp), %xmm13
	divss	-84(%rbp), %xmm13
	movss	%xmm13, -84(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm7, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	movaps	%xmm6, %xmm0
	divss	%xmm8, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm9, %xmm8
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm9
	divss	-88(%rbp), %xmm9
	movss	-124(%rbp), %xmm4
	divss	-76(%rbp), %xmm4
	movss	%xmm4, -76(%rbp)
	movss	-120(%rbp), %xmm1
	divss	-80(%rbp), %xmm1
	movss	%xmm1, -80(%rbp)
	movss	-92(%rbp), %xmm12
	divss	-52(%rbp), %xmm12
	movss	%xmm12, -52(%rbp)
	movss	-96(%rbp), %xmm15
	divss	-56(%rbp), %xmm15
	movss	%xmm15, -56(%rbp)
	movss	-100(%rbp), %xmm4
	divss	-60(%rbp), %xmm4
	movss	%xmm4, -60(%rbp)
	movss	-104(%rbp), %xmm1
	divss	-64(%rbp), %xmm1
	movss	%xmm1, -64(%rbp)
	movss	-108(%rbp), %xmm12
	divss	-68(%rbp), %xmm12
	movss	%xmm12, -68(%rbp)
	movss	-112(%rbp), %xmm15
	divss	-72(%rbp), %xmm15
	movss	%xmm15, -72(%rbp)
	movss	-116(%rbp), %xmm1
	divss	-84(%rbp), %xmm1
	movss	%xmm1, -84(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm7, %xmm5
	divss	%xmm3, %xmm5
	movaps	%xmm6, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm11, %xmm0
	movaps	%xmm0, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm14, %xmm11
	divss	%xmm9, %xmm11
	movss	-124(%rbp), %xmm13
	movaps	%xmm13, %xmm3
	divss	-76(%rbp), %xmm3
	movss	%xmm3, -76(%rbp)
	movss	-120(%rbp), %xmm9
	divss	-80(%rbp), %xmm9
	movss	%xmm9, -80(%rbp)
	movss	-92(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-96(%rbp), %xmm3
	divss	-56(%rbp), %xmm3
	movss	%xmm3, -56(%rbp)
	movss	-100(%rbp), %xmm9
	divss	-60(%rbp), %xmm9
	movss	%xmm9, -60(%rbp)
	movss	-104(%rbp), %xmm10
	divss	-64(%rbp), %xmm10
	movss	%xmm10, -64(%rbp)
	movss	-108(%rbp), %xmm3
	divss	-68(%rbp), %xmm3
	movss	%xmm3, -68(%rbp)
	movss	-112(%rbp), %xmm9
	divss	-72(%rbp), %xmm9
	movss	%xmm9, -72(%rbp)
	movss	-116(%rbp), %xmm10
	divss	-84(%rbp), %xmm10
	movss	%xmm10, -84(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	divss	%xmm1, %xmm5
	movaps	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	divss	%xmm12, %xmm0
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm9
	divss	%xmm11, %xmm9
	movss	%xmm9, -88(%rbp)
	movaps	%xmm13, %xmm11
	movaps	%xmm11, %xmm14
	divss	-76(%rbp), %xmm14
	movss	-120(%rbp), %xmm15
	movaps	%xmm15, %xmm13
	divss	-80(%rbp), %xmm13
	movss	-92(%rbp), %xmm9
	divss	-52(%rbp), %xmm9
	movss	%xmm9, -52(%rbp)
	movss	-96(%rbp), %xmm9
	divss	-56(%rbp), %xmm9
	movss	%xmm9, -56(%rbp)
	movss	-100(%rbp), %xmm9
	divss	-60(%rbp), %xmm9
	movss	%xmm9, -60(%rbp)
	movss	-104(%rbp), %xmm9
	divss	-64(%rbp), %xmm9
	movss	%xmm9, -64(%rbp)
	movss	-108(%rbp), %xmm9
	divss	-68(%rbp), %xmm9
	movss	%xmm9, -68(%rbp)
	movss	-112(%rbp), %xmm9
	divss	-72(%rbp), %xmm9
	movss	%xmm9, -72(%rbp)
	movss	-116(%rbp), %xmm9
	divss	-84(%rbp), %xmm9
	divss	%xmm3, %xmm2
	movss	%xmm2, -128(%rbp)
	divss	%xmm4, %xmm7
	movss	%xmm7, -132(%rbp)
	divss	%xmm5, %xmm6
	movss	%xmm6, -136(%rbp)
	divss	%xmm0, %xmm1
	movss	%xmm1, -140(%rbp)
	divss	%xmm10, %xmm8
	movss	%xmm8, -144(%rbp)
	divss	-88(%rbp), %xmm12
	movss	%xmm12, -88(%rbp)
	divss	%xmm14, %xmm11
	movss	%xmm11, -76(%rbp)
	divss	%xmm13, %xmm15
	movss	%xmm15, -80(%rbp)
	movss	-92(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-96(%rbp), %xmm3
	divss	-56(%rbp), %xmm3
	movss	%xmm3, -56(%rbp)
	movss	-100(%rbp), %xmm4
	divss	-60(%rbp), %xmm4
	movss	%xmm4, -60(%rbp)
	movss	-104(%rbp), %xmm10
	divss	-64(%rbp), %xmm10
	movss	%xmm10, -64(%rbp)
	movss	-108(%rbp), %xmm3
	divss	-68(%rbp), %xmm3
	movss	%xmm3, -68(%rbp)
	movss	-112(%rbp), %xmm15
	divss	-72(%rbp), %xmm15
	movss	%xmm15, -72(%rbp)
	movss	-116(%rbp), %xmm10
	divss	%xmm9, %xmm10
	movss	%xmm10, -84(%rbp)
.L637:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L638
	cvttss2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-148(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-156(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-140(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-164(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE215:
	.size	float_div_14, .-float_div_14
	.globl	float_div_15
	.type	float_div_15, @function
float_div_15:
.LFB216:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -156(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -140(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -164(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -148(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -172(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -92(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -176(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -132(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -84(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -52(%rbp)
	movq	-24(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -100(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -60(%rbp)
	movq	-24(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	56(%rax), %eax
	subl	$10, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -108(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -68(%rbp)
	movq	-24(%rbp), %rax
	movl	60(%rax), %eax
	subl	$11, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	64(%rax), %eax
	subl	$12, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -116(%rbp)
	movq	-24(%rbp), %rax
	movsd	192(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -76(%rbp)
	movq	-24(%rbp), %rax
	movl	68(%rax), %eax
	subl	$13, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	200(%rax), %xmm0
	pxor	%xmm1, %xmm1
	cvtsd2ss	%xmm0, %xmm1
	movss	.LC7(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	72(%rax), %eax
	subl	$14, %eax
	pxor	%xmm1, %xmm1
	cvtsi2ssl	%eax, %xmm1
	movss	.LC8(%rip), %xmm0
	mulss	%xmm0, %xmm1
	movss	%xmm1, -124(%rbp)
	jmp	.L640
.L641:
	movss	-156(%rbp), %xmm2
	movaps	%xmm2, %xmm6
	divss	-136(%rbp), %xmm6
	movss	-160(%rbp), %xmm3
	movaps	%xmm3, %xmm4
	divss	-140(%rbp), %xmm4
	movss	-164(%rbp), %xmm1
	movaps	%xmm1, %xmm7
	divss	-144(%rbp), %xmm7
	movss	-168(%rbp), %xmm11
	movaps	%xmm11, %xmm0
	divss	-148(%rbp), %xmm0
	movss	-172(%rbp), %xmm8
	movaps	%xmm8, %xmm15
	divss	-152(%rbp), %xmm15
	movss	-176(%rbp), %xmm12
	movaps	%xmm12, %xmm13
	divss	-92(%rbp), %xmm13
	movss	-132(%rbp), %xmm5
	divss	-80(%rbp), %xmm5
	movss	%xmm5, -80(%rbp)
	movss	-128(%rbp), %xmm10
	divss	-84(%rbp), %xmm10
	movss	%xmm10, -84(%rbp)
	movss	-96(%rbp), %xmm14
	divss	-52(%rbp), %xmm14
	movss	%xmm14, -52(%rbp)
	movss	-100(%rbp), %xmm9
	divss	-56(%rbp), %xmm9
	movss	%xmm9, -56(%rbp)
	movss	-104(%rbp), %xmm14
	divss	-60(%rbp), %xmm14
	movss	%xmm14, -60(%rbp)
	movss	-108(%rbp), %xmm9
	divss	-64(%rbp), %xmm9
	movss	%xmm9, -64(%rbp)
	movss	-112(%rbp), %xmm5
	divss	-68(%rbp), %xmm5
	movss	%xmm5, -68(%rbp)
	movss	-116(%rbp), %xmm10
	divss	-72(%rbp), %xmm10
	movss	%xmm10, -72(%rbp)
	movss	-120(%rbp), %xmm14
	divss	-76(%rbp), %xmm14
	movss	%xmm14, -76(%rbp)
	movss	-124(%rbp), %xmm9
	divss	-88(%rbp), %xmm9
	movss	%xmm9, -88(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm3, %xmm6
	movaps	%xmm6, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm10
	divss	%xmm7, %xmm10
	movaps	%xmm11, %xmm14
	divss	%xmm0, %xmm14
	movaps	%xmm8, %xmm9
	divss	%xmm15, %xmm9
	movaps	%xmm12, %xmm7
	divss	%xmm13, %xmm7
	movss	%xmm7, -92(%rbp)
	movss	-132(%rbp), %xmm4
	divss	-80(%rbp), %xmm4
	movss	%xmm4, -80(%rbp)
	movss	-128(%rbp), %xmm0
	divss	-84(%rbp), %xmm0
	movss	%xmm0, -84(%rbp)
	movss	-96(%rbp), %xmm15
	divss	-52(%rbp), %xmm15
	movss	%xmm15, -52(%rbp)
	movss	-100(%rbp), %xmm13
	divss	-56(%rbp), %xmm13
	movss	%xmm13, -56(%rbp)
	movss	-104(%rbp), %xmm7
	divss	-60(%rbp), %xmm7
	movss	%xmm7, -60(%rbp)
	movss	-108(%rbp), %xmm15
	divss	-64(%rbp), %xmm15
	movss	%xmm15, -64(%rbp)
	movss	-112(%rbp), %xmm0
	divss	-68(%rbp), %xmm0
	movss	%xmm0, -68(%rbp)
	movss	-116(%rbp), %xmm13
	divss	-72(%rbp), %xmm13
	movss	%xmm13, -72(%rbp)
	movss	-120(%rbp), %xmm4
	divss	-76(%rbp), %xmm4
	movss	%xmm4, -76(%rbp)
	movss	-124(%rbp), %xmm7
	divss	-88(%rbp), %xmm7
	movss	%xmm7, -88(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	movaps	%xmm5, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm1, %xmm0
	divss	%xmm10, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm8, %xmm15
	divss	%xmm9, %xmm15
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm10
	divss	-92(%rbp), %xmm10
	movss	-132(%rbp), %xmm6
	divss	-80(%rbp), %xmm6
	movss	%xmm6, -80(%rbp)
	movss	-128(%rbp), %xmm3
	divss	-84(%rbp), %xmm3
	movss	%xmm3, -84(%rbp)
	movss	-96(%rbp), %xmm9
	divss	-52(%rbp), %xmm9
	movss	%xmm9, -52(%rbp)
	movss	-100(%rbp), %xmm12
	divss	-56(%rbp), %xmm12
	movss	%xmm12, -56(%rbp)
	movss	-104(%rbp), %xmm6
	divss	-60(%rbp), %xmm6
	movss	%xmm6, -60(%rbp)
	movss	-108(%rbp), %xmm3
	divss	-64(%rbp), %xmm3
	movss	%xmm3, -64(%rbp)
	movss	-112(%rbp), %xmm12
	divss	-68(%rbp), %xmm12
	movss	%xmm12, -68(%rbp)
	movss	-116(%rbp), %xmm9
	divss	-72(%rbp), %xmm9
	movss	%xmm9, -72(%rbp)
	movss	-120(%rbp), %xmm6
	divss	-76(%rbp), %xmm6
	movss	%xmm6, -76(%rbp)
	movss	-124(%rbp), %xmm3
	divss	-88(%rbp), %xmm3
	movss	%xmm3, -88(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm5, %xmm4
	movaps	%xmm4, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm1, %xmm9
	divss	%xmm0, %xmm9
	movaps	%xmm11, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm13
	divss	%xmm15, %xmm13
	movaps	%xmm14, %xmm5
	divss	%xmm10, %xmm5
	movss	%xmm5, -92(%rbp)
	movss	-132(%rbp), %xmm7
	divss	-80(%rbp), %xmm7
	movss	%xmm7, -80(%rbp)
	movss	-128(%rbp), %xmm10
	divss	-84(%rbp), %xmm10
	movss	%xmm10, -84(%rbp)
	movss	-96(%rbp), %xmm0
	divss	-52(%rbp), %xmm0
	movss	%xmm0, -52(%rbp)
	movss	-100(%rbp), %xmm15
	divss	-56(%rbp), %xmm15
	movss	%xmm15, -56(%rbp)
	movss	-104(%rbp), %xmm5
	divss	-60(%rbp), %xmm5
	movss	%xmm5, -60(%rbp)
	movss	-108(%rbp), %xmm7
	divss	-64(%rbp), %xmm7
	movss	%xmm7, -64(%rbp)
	movss	-112(%rbp), %xmm10
	divss	-68(%rbp), %xmm10
	movss	%xmm10, -68(%rbp)
	movss	-116(%rbp), %xmm0
	divss	-72(%rbp), %xmm0
	movss	%xmm0, -72(%rbp)
	movss	-120(%rbp), %xmm15
	divss	-76(%rbp), %xmm15
	movss	%xmm15, -76(%rbp)
	movss	-124(%rbp), %xmm5
	divss	-88(%rbp), %xmm5
	movss	%xmm5, -88(%rbp)
	movaps	%xmm2, %xmm7
	divss	%xmm3, %xmm7
	movaps	%xmm4, %xmm3
	movaps	%xmm3, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm1, %xmm0
	divss	%xmm9, %xmm0
	movaps	%xmm11, %xmm15
	divss	%xmm12, %xmm15
	movaps	%xmm8, %xmm9
	movaps	%xmm9, %xmm10
	divss	%xmm13, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm13
	divss	-92(%rbp), %xmm13
	movss	-132(%rbp), %xmm4
	divss	-80(%rbp), %xmm4
	movss	%xmm4, -80(%rbp)
	movss	-128(%rbp), %xmm6
	divss	-84(%rbp), %xmm6
	movss	%xmm6, -84(%rbp)
	movss	-96(%rbp), %xmm8
	divss	-52(%rbp), %xmm8
	movss	%xmm8, -52(%rbp)
	movss	-100(%rbp), %xmm14
	divss	-56(%rbp), %xmm14
	movss	%xmm14, -56(%rbp)
	movss	-104(%rbp), %xmm8
	divss	-60(%rbp), %xmm8
	movss	%xmm8, -60(%rbp)
	movss	-108(%rbp), %xmm6
	divss	-64(%rbp), %xmm6
	movss	%xmm6, -64(%rbp)
	movss	-112(%rbp), %xmm14
	divss	-68(%rbp), %xmm14
	movss	%xmm14, -68(%rbp)
	movss	-116(%rbp), %xmm4
	divss	-72(%rbp), %xmm4
	movss	%xmm4, -72(%rbp)
	movss	-120(%rbp), %xmm8
	divss	-76(%rbp), %xmm8
	movss	%xmm8, -76(%rbp)
	movss	-124(%rbp), %xmm6
	divss	-88(%rbp), %xmm6
	movss	%xmm6, -88(%rbp)
	movaps	%xmm2, %xmm6
	divss	%xmm7, %xmm6
	movaps	%xmm3, %xmm7
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm1, %xmm8
	divss	%xmm0, %xmm8
	movaps	%xmm11, %xmm14
	divss	%xmm15, %xmm14
	movaps	%xmm9, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm12, %xmm3
	divss	%xmm13, %xmm3
	movss	%xmm3, -92(%rbp)
	movss	-132(%rbp), %xmm5
	divss	-80(%rbp), %xmm5
	movss	%xmm5, -80(%rbp)
	movss	-128(%rbp), %xmm0
	divss	-84(%rbp), %xmm0
	movss	%xmm0, -84(%rbp)
	movss	-96(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-100(%rbp), %xmm13
	divss	-56(%rbp), %xmm13
	movss	%xmm13, -56(%rbp)
	movss	-104(%rbp), %xmm3
	divss	-60(%rbp), %xmm3
	movss	%xmm3, -60(%rbp)
	movss	-108(%rbp), %xmm5
	divss	-64(%rbp), %xmm5
	movss	%xmm5, -64(%rbp)
	movss	-112(%rbp), %xmm0
	divss	-68(%rbp), %xmm0
	movss	%xmm0, -68(%rbp)
	movss	-116(%rbp), %xmm10
	divss	-72(%rbp), %xmm10
	movss	%xmm10, -72(%rbp)
	movss	-120(%rbp), %xmm13
	divss	-76(%rbp), %xmm13
	movss	%xmm13, -76(%rbp)
	movss	-124(%rbp), %xmm3
	divss	-88(%rbp), %xmm3
	movss	%xmm3, -88(%rbp)
	movaps	%xmm2, %xmm5
	divss	%xmm6, %xmm5
	movaps	%xmm7, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm1, %xmm6
	movaps	%xmm6, %xmm0
	divss	%xmm8, %xmm0
	movaps	%xmm11, %xmm13
	divss	%xmm14, %xmm13
	movaps	%xmm9, %xmm8
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm12, %xmm14
	movaps	%xmm14, %xmm9
	divss	-92(%rbp), %xmm9
	movss	-132(%rbp), %xmm4
	divss	-80(%rbp), %xmm4
	movss	%xmm4, -80(%rbp)
	movss	-128(%rbp), %xmm1
	divss	-84(%rbp), %xmm1
	movss	%xmm1, -84(%rbp)
	movss	-96(%rbp), %xmm12
	divss	-52(%rbp), %xmm12
	movss	%xmm12, -52(%rbp)
	movss	-100(%rbp), %xmm15
	divss	-56(%rbp), %xmm15
	movss	%xmm15, -56(%rbp)
	movss	-104(%rbp), %xmm4
	divss	-60(%rbp), %xmm4
	movss	%xmm4, -60(%rbp)
	movss	-108(%rbp), %xmm1
	divss	-64(%rbp), %xmm1
	movss	%xmm1, -64(%rbp)
	movss	-112(%rbp), %xmm12
	divss	-68(%rbp), %xmm12
	movss	%xmm12, -68(%rbp)
	movss	-116(%rbp), %xmm15
	divss	-72(%rbp), %xmm15
	movss	%xmm15, -72(%rbp)
	movss	-120(%rbp), %xmm1
	divss	-76(%rbp), %xmm1
	movss	%xmm1, -76(%rbp)
	movss	-124(%rbp), %xmm12
	divss	-88(%rbp), %xmm12
	movss	%xmm12, -88(%rbp)
	movaps	%xmm2, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm7, %xmm5
	divss	%xmm3, %xmm5
	movaps	%xmm6, %xmm1
	divss	%xmm0, %xmm1
	movaps	%xmm11, %xmm0
	movaps	%xmm0, %xmm12
	divss	%xmm13, %xmm12
	movaps	%xmm8, %xmm15
	divss	%xmm10, %xmm15
	movaps	%xmm14, %xmm11
	divss	%xmm9, %xmm11
	movss	-132(%rbp), %xmm13
	movaps	%xmm13, %xmm3
	divss	-80(%rbp), %xmm3
	movss	%xmm3, -80(%rbp)
	movss	-128(%rbp), %xmm9
	divss	-84(%rbp), %xmm9
	movss	%xmm9, -84(%rbp)
	movss	-96(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-100(%rbp), %xmm3
	divss	-56(%rbp), %xmm3
	movss	%xmm3, -56(%rbp)
	movss	-104(%rbp), %xmm9
	divss	-60(%rbp), %xmm9
	movss	%xmm9, -60(%rbp)
	movss	-108(%rbp), %xmm10
	divss	-64(%rbp), %xmm10
	movss	%xmm10, -64(%rbp)
	movss	-112(%rbp), %xmm3
	divss	-68(%rbp), %xmm3
	movss	%xmm3, -68(%rbp)
	movss	-116(%rbp), %xmm9
	divss	-72(%rbp), %xmm9
	movss	%xmm9, -72(%rbp)
	movss	-120(%rbp), %xmm10
	divss	-76(%rbp), %xmm10
	movss	%xmm10, -76(%rbp)
	movss	-124(%rbp), %xmm3
	divss	-88(%rbp), %xmm3
	movss	%xmm3, -88(%rbp)
	movaps	%xmm2, %xmm3
	divss	%xmm4, %xmm3
	movaps	%xmm7, %xmm4
	divss	%xmm5, %xmm4
	movaps	%xmm6, %xmm5
	divss	%xmm1, %xmm5
	movaps	%xmm0, %xmm1
	movaps	%xmm1, %xmm0
	divss	%xmm12, %xmm0
	movaps	%xmm8, %xmm10
	divss	%xmm15, %xmm10
	movaps	%xmm14, %xmm12
	movaps	%xmm12, %xmm9
	divss	%xmm11, %xmm9
	movss	%xmm9, -92(%rbp)
	movaps	%xmm13, %xmm11
	movaps	%xmm11, %xmm14
	divss	-80(%rbp), %xmm14
	movss	-128(%rbp), %xmm15
	movaps	%xmm15, %xmm13
	divss	-84(%rbp), %xmm13
	movss	-96(%rbp), %xmm9
	divss	-52(%rbp), %xmm9
	movss	%xmm9, -52(%rbp)
	movss	-100(%rbp), %xmm9
	divss	-56(%rbp), %xmm9
	movss	%xmm9, -56(%rbp)
	movss	-104(%rbp), %xmm9
	divss	-60(%rbp), %xmm9
	movss	%xmm9, -60(%rbp)
	movss	-108(%rbp), %xmm9
	divss	-64(%rbp), %xmm9
	movss	%xmm9, -64(%rbp)
	movss	-112(%rbp), %xmm9
	divss	-68(%rbp), %xmm9
	movss	%xmm9, -68(%rbp)
	movss	-116(%rbp), %xmm9
	divss	-72(%rbp), %xmm9
	movss	%xmm9, -72(%rbp)
	movss	-120(%rbp), %xmm9
	divss	-76(%rbp), %xmm9
	movss	%xmm9, -76(%rbp)
	movss	-124(%rbp), %xmm9
	divss	-88(%rbp), %xmm9
	divss	%xmm3, %xmm2
	movss	%xmm2, -136(%rbp)
	divss	%xmm4, %xmm7
	movss	%xmm7, -140(%rbp)
	divss	%xmm5, %xmm6
	movss	%xmm6, -144(%rbp)
	divss	%xmm0, %xmm1
	movss	%xmm1, -148(%rbp)
	divss	%xmm10, %xmm8
	movss	%xmm8, -152(%rbp)
	divss	-92(%rbp), %xmm12
	movss	%xmm12, -92(%rbp)
	divss	%xmm14, %xmm11
	movss	%xmm11, -80(%rbp)
	divss	%xmm13, %xmm15
	movss	%xmm15, -84(%rbp)
	movss	-96(%rbp), %xmm10
	divss	-52(%rbp), %xmm10
	movss	%xmm10, -52(%rbp)
	movss	-100(%rbp), %xmm3
	divss	-56(%rbp), %xmm3
	movss	%xmm3, -56(%rbp)
	movss	-104(%rbp), %xmm4
	divss	-60(%rbp), %xmm4
	movss	%xmm4, -60(%rbp)
	movss	-108(%rbp), %xmm10
	divss	-64(%rbp), %xmm10
	movss	%xmm10, -64(%rbp)
	movss	-112(%rbp), %xmm3
	divss	-68(%rbp), %xmm3
	movss	%xmm3, -68(%rbp)
	movss	-116(%rbp), %xmm15
	divss	-72(%rbp), %xmm15
	movss	%xmm15, -72(%rbp)
	movss	-120(%rbp), %xmm10
	divss	-76(%rbp), %xmm10
	movss	%xmm10, -76(%rbp)
	movss	-124(%rbp), %xmm3
	divss	%xmm9, %xmm3
	movss	%xmm3, -88(%rbp)
.L640:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L641
	cvttss2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-156(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-140(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-164(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-148(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-172(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-92(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-132(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-84(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-52(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-100(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-60(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-108(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-68(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-116(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-76(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttss2sil	-124(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE216:
	.size	float_div_15, .-float_div_15
	.globl	float_div_benchmarks
	.section	.data.rel.local
	.align 32
	.type	float_div_benchmarks, @object
	.size	float_div_benchmarks, 128
float_div_benchmarks:
	.quad	float_div_0
	.quad	float_div_1
	.quad	float_div_2
	.quad	float_div_3
	.quad	float_div_4
	.quad	float_div_5
	.quad	float_div_6
	.quad	float_div_7
	.quad	float_div_8
	.quad	float_div_9
	.quad	float_div_10
	.quad	float_div_11
	.quad	float_div_12
	.quad	float_div_13
	.quad	float_div_14
	.quad	float_div_15
	.text
	.globl	double_add_0
	.type	double_add_0, @function
double_add_0:
.LFB217:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$40, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm7
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2sdl	%eax, %xmm5
	movapd	%xmm5, %xmm8
	jmp	.L643
.L644:
	movapd	%xmm7, %xmm2
	movapd	%xmm8, %xmm6
	addsd	%xmm6, %xmm2
	movapd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	addsd	%xmm4, %xmm2
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm4
	addsd	%xmm3, %xmm4
	movapd	%xmm4, %xmm5
	addsd	%xmm4, %xmm5
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movapd	%xmm6, %xmm7
	addsd	%xmm6, %xmm7
	movapd	%xmm7, %xmm0
	addsd	%xmm7, %xmm0
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm2
	addsd	%xmm1, %xmm2
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm7
.L643:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L644
	cvttsd2sil	%xmm7, %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE217:
	.size	double_add_0, .-double_add_0
	.globl	double_add_1
	.type	double_add_1, @function
double_add_1:
.LFB218:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm3
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm6, %xmm6
	cvtsi2sdl	%eax, %xmm6
	movapd	%xmm6, %xmm8
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2sdl	%eax, %xmm7
	movapd	%xmm7, %xmm9
	jmp	.L646
.L647:
	movapd	%xmm3, %xmm2
	movapd	%xmm8, %xmm6
	addsd	%xmm6, %xmm2
	movsd	-56(%rbp), %xmm0
	movapd	%xmm9, %xmm7
	addsd	%xmm7, %xmm0
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm0, %xmm3
	addsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm0
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm0, %xmm4
	addsd	%xmm0, %xmm4
	movapd	%xmm4, %xmm0
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm0, %xmm5
	addsd	%xmm0, %xmm5
	movapd	%xmm5, %xmm0
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm0, %xmm3
	addsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm0
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm0, %xmm4
	addsd	%xmm0, %xmm4
	movapd	%xmm4, %xmm0
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm3
	movapd	%xmm0, %xmm5
	addsd	%xmm0, %xmm5
	movsd	%xmm5, -56(%rbp)
.L646:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L647
	cvttsd2sil	%xmm3, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE218:
	.size	double_add_1, .-double_add_1
	.globl	double_add_2
	.type	double_add_2, @function
double_add_2:
.LFB219:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm4
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2sdl	%eax, %xmm5
	movapd	%xmm5, %xmm8
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2sdl	%eax, %xmm5
	movapd	%xmm5, %xmm9
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movapd	%xmm3, %xmm10
	jmp	.L649
.L650:
	movapd	%xmm4, %xmm2
	movapd	%xmm8, %xmm5
	addsd	%xmm5, %xmm2
	movsd	-56(%rbp), %xmm0
	movapd	%xmm9, %xmm3
	addsd	%xmm3, %xmm0
	movsd	-64(%rbp), %xmm4
	movapd	%xmm10, %xmm5
	addsd	%xmm5, %xmm4
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm4, %xmm1
	addsd	%xmm4, %xmm1
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm0, %xmm3
	addsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm0
	movapd	%xmm1, %xmm3
	addsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm1
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm0, %xmm4
	addsd	%xmm0, %xmm4
	movapd	%xmm4, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm0, %xmm5
	addsd	%xmm0, %xmm5
	movapd	%xmm5, %xmm0
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm6, %xmm7
	addsd	%xmm6, %xmm7
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm7, %xmm4
	addsd	%xmm7, %xmm4
	movapd	%xmm4, %xmm7
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm0, %xmm3
	addsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm0
	movapd	%xmm7, %xmm6
	addsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm0, %xmm4
	addsd	%xmm0, %xmm4
	movapd	%xmm4, %xmm0
	movapd	%xmm7, %xmm6
	addsd	%xmm7, %xmm6
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm0, %xmm5
	addsd	%xmm0, %xmm5
	movsd	%xmm5, -56(%rbp)
	movapd	%xmm6, %xmm3
	addsd	%xmm6, %xmm3
	movsd	%xmm3, -64(%rbp)
.L649:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L650
	cvttsd2sil	%xmm4, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE219:
	.size	double_add_2, .-double_add_2
	.globl	double_add_3
	.type	double_add_3, @function
double_add_3:
.LFB220:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm2
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movapd	%xmm3, %xmm8
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2sdl	%eax, %xmm4
	movapd	%xmm4, %xmm9
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movapd	%xmm0, %xmm10
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movapd	%xmm3, %xmm11
	jmp	.L652
.L653:
	movapd	%xmm8, %xmm4
	addsd	%xmm4, %xmm2
	movsd	-56(%rbp), %xmm5
	movapd	%xmm9, %xmm0
	addsd	%xmm0, %xmm5
	movsd	-64(%rbp), %xmm1
	movapd	%xmm10, %xmm3
	addsd	%xmm3, %xmm1
	movsd	-72(%rbp), %xmm7
	movapd	%xmm11, %xmm4
	addsd	%xmm4, %xmm7
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm7, %xmm6
	addsd	%xmm7, %xmm6
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	movapd	%xmm6, %xmm7
	addsd	%xmm6, %xmm7
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm7, %xmm5
	addsd	%xmm7, %xmm5
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	movapd	%xmm1, %xmm0
	addsd	%xmm1, %xmm0
	movapd	%xmm0, %xmm1
	movapd	%xmm6, %xmm7
	addsd	%xmm6, %xmm7
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm1, %xmm0
	addsd	%xmm1, %xmm0
	movapd	%xmm0, %xmm1
	movapd	%xmm7, %xmm5
	addsd	%xmm7, %xmm5
	movapd	%xmm5, %xmm7
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm3, %xmm0
	addsd	%xmm3, %xmm0
	movapd	%xmm0, %xmm3
	movapd	%xmm1, %xmm0
	addsd	%xmm1, %xmm0
	movapd	%xmm7, %xmm5
	addsd	%xmm7, %xmm5
	movapd	%xmm5, %xmm7
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm3, %xmm1
	addsd	%xmm3, %xmm1
	movapd	%xmm1, %xmm3
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm7, %xmm5
	addsd	%xmm7, %xmm5
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movapd	%xmm6, %xmm5
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm3, %xmm7
	addsd	%xmm3, %xmm7
	movsd	%xmm7, -56(%rbp)
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movsd	%xmm6, -64(%rbp)
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movsd	%xmm6, -72(%rbp)
.L652:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L653
	cvttsd2sil	%xmm2, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE220:
	.size	double_add_3, .-double_add_3
	.globl	double_add_4
	.type	double_add_4, @function
double_add_4:
.LFB221:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm2
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2sdl	%eax, %xmm5
	movapd	%xmm5, %xmm8
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2sdl	%eax, %xmm5
	movapd	%xmm5, %xmm9
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2sdl	%eax, %xmm5
	movapd	%xmm5, %xmm10
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2sdl	%eax, %xmm5
	movapd	%xmm5, %xmm11
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2sdl	%eax, %xmm7
	movapd	%xmm7, %xmm12
	jmp	.L655
.L656:
	movapd	%xmm8, %xmm5
	addsd	%xmm5, %xmm2
	movsd	-56(%rbp), %xmm5
	movapd	%xmm9, %xmm7
	addsd	%xmm7, %xmm5
	movsd	-64(%rbp), %xmm0
	movapd	%xmm10, %xmm7
	addsd	%xmm7, %xmm0
	movsd	-72(%rbp), %xmm6
	movapd	%xmm11, %xmm3
	addsd	%xmm3, %xmm6
	movsd	-80(%rbp), %xmm7
	movapd	%xmm12, %xmm4
	addsd	%xmm4, %xmm7
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm6, %xmm1
	addsd	%xmm6, %xmm1
	movapd	%xmm1, %xmm6
	movapd	%xmm7, %xmm3
	addsd	%xmm7, %xmm3
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm6, %xmm1
	addsd	%xmm6, %xmm1
	movapd	%xmm3, %xmm7
	addsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm6, %xmm0
	addsd	%xmm6, %xmm0
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm6
	addsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm0, %xmm4
	addsd	%xmm0, %xmm4
	movapd	%xmm4, %xmm0
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm7, %xmm6
	addsd	%xmm7, %xmm6
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm0, %xmm4
	addsd	%xmm0, %xmm4
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm3, %xmm7
	addsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	movapd	%xmm6, %xmm7
	addsd	%xmm6, %xmm7
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm4, %xmm0
	addsd	%xmm4, %xmm0
	movapd	%xmm0, %xmm4
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm3, %xmm0
	addsd	%xmm3, %xmm0
	movapd	%xmm0, %xmm3
	movapd	%xmm7, %xmm6
	addsd	%xmm7, %xmm6
	movapd	%xmm5, %xmm1
	addsd	%xmm5, %xmm1
	movapd	%xmm1, %xmm5
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm3, %xmm1
	addsd	%xmm3, %xmm1
	movsd	%xmm1, -56(%rbp)
	movapd	%xmm6, %xmm1
	addsd	%xmm6, %xmm1
	movsd	%xmm1, -64(%rbp)
	movapd	%xmm5, %xmm3
	addsd	%xmm5, %xmm3
	movsd	%xmm3, -72(%rbp)
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movsd	%xmm7, -80(%rbp)
.L655:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L656
	cvttsd2sil	%xmm2, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE221:
	.size	double_add_4, .-double_add_4
	.globl	double_add_5
	.type	double_add_5, @function
double_add_5:
.LFB222:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm2
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movapd	%xmm3, %xmm10
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movapd	%xmm3, %xmm11
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2sdl	%eax, %xmm7
	movapd	%xmm7, %xmm12
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movapd	%xmm3, %xmm13
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2sdl	%eax, %xmm7
	movapd	%xmm7, %xmm14
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movapd	%xmm3, %xmm15
	jmp	.L658
.L659:
	movapd	%xmm10, %xmm6
	addsd	%xmm6, %xmm2
	movsd	-56(%rbp), %xmm5
	movapd	%xmm11, %xmm4
	addsd	%xmm4, %xmm5
	movsd	-64(%rbp), %xmm1
	movapd	%xmm12, %xmm7
	addsd	%xmm7, %xmm1
	movsd	-72(%rbp), %xmm0
	movapd	%xmm13, %xmm3
	addsd	%xmm3, %xmm0
	movsd	-80(%rbp), %xmm8
	movapd	%xmm14, %xmm6
	addsd	%xmm6, %xmm8
	movsd	-88(%rbp), %xmm9
	movapd	%xmm15, %xmm4
	addsd	%xmm4, %xmm9
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm8, %xmm3
	addsd	%xmm8, %xmm3
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm1, %xmm0
	addsd	%xmm1, %xmm0
	movapd	%xmm0, %xmm1
	movapd	%xmm7, %xmm0
	addsd	%xmm7, %xmm0
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movapd	%xmm6, %xmm5
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movapd	%xmm6, %xmm5
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm1, %xmm0
	addsd	%xmm1, %xmm0
	movapd	%xmm0, %xmm1
	movapd	%xmm7, %xmm0
	addsd	%xmm7, %xmm0
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm3, %xmm7
	addsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm1, %xmm0
	addsd	%xmm1, %xmm0
	movapd	%xmm0, %xmm1
	movapd	%xmm7, %xmm5
	addsd	%xmm7, %xmm5
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm3, %xmm0
	addsd	%xmm3, %xmm0
	movsd	%xmm0, -56(%rbp)
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movsd	%xmm6, -64(%rbp)
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movsd	%xmm6, -72(%rbp)
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, -80(%rbp)
	movapd	%xmm9, %xmm3
	addsd	%xmm9, %xmm3
	movsd	%xmm3, -88(%rbp)
.L658:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L659
	cvttsd2sil	%xmm2, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE222:
	.size	double_add_5, .-double_add_5
	.globl	double_add_6
	.type	double_add_6, @function
double_add_6:
.LFB223:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm10
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movapd	%xmm2, %xmm11
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movapd	%xmm3, %xmm12
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2sdl	%eax, %xmm4
	movapd	%xmm4, %xmm13
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2sdl	%eax, %xmm7
	movapd	%xmm7, %xmm14
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movapd	%xmm2, %xmm15
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	%xmm3, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2sdl	%eax, %xmm4
	movsd	%xmm4, -112(%rbp)
	jmp	.L661
.L662:
	movapd	%xmm10, %xmm2
	movapd	%xmm11, %xmm6
	addsd	%xmm6, %xmm2
	movsd	-56(%rbp), %xmm5
	movapd	%xmm12, %xmm7
	addsd	%xmm7, %xmm5
	movsd	-64(%rbp), %xmm0
	movapd	%xmm13, %xmm3
	addsd	%xmm3, %xmm0
	movsd	-72(%rbp), %xmm6
	movapd	%xmm14, %xmm4
	addsd	%xmm4, %xmm6
	movsd	-80(%rbp), %xmm1
	movapd	%xmm15, %xmm7
	addsd	%xmm7, %xmm1
	movsd	-88(%rbp), %xmm8
	addsd	-104(%rbp), %xmm8
	movsd	-96(%rbp), %xmm9
	addsd	-112(%rbp), %xmm9
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm6, %xmm7
	addsd	%xmm6, %xmm7
	movapd	%xmm1, %xmm3
	addsd	%xmm1, %xmm3
	movapd	%xmm8, %xmm6
	addsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	movapd	%xmm9, %xmm4
	addsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm7, %xmm5
	addsd	%xmm7, %xmm5
	movapd	%xmm5, %xmm7
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm8, %xmm6
	addsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm7, %xmm5
	addsd	%xmm7, %xmm5
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm5, %xmm1
	addsd	%xmm5, %xmm1
	movapd	%xmm1, %xmm5
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm3
	addsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm5, %xmm1
	addsd	%xmm5, %xmm1
	movapd	%xmm6, %xmm5
	addsd	%xmm6, %xmm5
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm3
	addsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm4
	addsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm1
	addsd	%xmm9, %xmm1
	movapd	%xmm1, %xmm9
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm3, %xmm7
	addsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm6, %xmm1
	addsd	%xmm6, %xmm1
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm8, %xmm5
	addsd	%xmm8, %xmm5
	movapd	%xmm5, %xmm8
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm8, %xmm5
	addsd	%xmm8, %xmm5
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movsd	%xmm6, -56(%rbp)
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movsd	%xmm7, -64(%rbp)
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movsd	%xmm6, -72(%rbp)
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movsd	%xmm3, -80(%rbp)
	movapd	%xmm5, %xmm7
	addsd	%xmm5, %xmm7
	movsd	%xmm7, -88(%rbp)
	movapd	%xmm9, %xmm1
	addsd	%xmm9, %xmm1
	movsd	%xmm1, -96(%rbp)
.L661:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L662
	cvttsd2sil	%xmm10, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE223:
	.size	double_add_6, .-double_add_6
	.globl	double_add_7
	.type	double_add_7, @function
double_add_7:
.LFB224:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm12
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movapd	%xmm3, %xmm13
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2sdl	%eax, %xmm4
	movapd	%xmm4, %xmm14
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movapd	%xmm3, %xmm15
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2sdl	%eax, %xmm4
	movsd	%xmm4, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2sdl	%eax, %xmm5
	movsd	%xmm5, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	%xmm3, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2sdl	%eax, %xmm4
	movsd	%xmm4, -144(%rbp)
	jmp	.L664
.L665:
	movapd	%xmm12, %xmm2
	movapd	%xmm13, %xmm5
	addsd	%xmm5, %xmm2
	movsd	-56(%rbp), %xmm5
	movapd	%xmm14, %xmm6
	addsd	%xmm6, %xmm5
	movsd	-64(%rbp), %xmm0
	movapd	%xmm15, %xmm3
	addsd	%xmm3, %xmm0
	movsd	-72(%rbp), %xmm1
	addsd	-112(%rbp), %xmm1
	movsd	-80(%rbp), %xmm8
	addsd	-120(%rbp), %xmm8
	movsd	-88(%rbp), %xmm9
	addsd	-128(%rbp), %xmm9
	movsd	-96(%rbp), %xmm10
	addsd	-136(%rbp), %xmm10
	movsd	-104(%rbp), %xmm11
	addsd	-144(%rbp), %xmm11
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm8, %xmm3
	addsd	%xmm8, %xmm3
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm4
	addsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm6
	addsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm5, %xmm1
	addsd	%xmm5, %xmm1
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm3
	addsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movapd	%xmm6, %xmm5
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm3
	addsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm4
	addsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm7, %xmm1
	addsd	%xmm7, %xmm1
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm3, %xmm7
	addsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm6
	addsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm12
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movsd	%xmm6, -56(%rbp)
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movsd	%xmm6, -72(%rbp)
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movsd	%xmm7, -80(%rbp)
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movsd	%xmm7, -88(%rbp)
	movapd	%xmm10, %xmm3
	addsd	%xmm10, %xmm3
	movsd	%xmm3, -96(%rbp)
	movapd	%xmm11, %xmm2
	addsd	%xmm11, %xmm2
	movsd	%xmm2, -104(%rbp)
.L664:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L665
	cvttsd2sil	%xmm12, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE224:
	.size	double_add_7, .-double_add_7
	.globl	double_add_8
	.type	double_add_8, @function
double_add_8:
.LFB225:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm13
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movapd	%xmm2, %xmm14
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movapd	%xmm3, %xmm15
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2sdl	%eax, %xmm4
	movsd	%xmm4, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	%xmm3, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2sdl	%eax, %xmm4
	movsd	%xmm4, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2sdl	%eax, %xmm5
	movsd	%xmm5, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2sdl	%eax, %xmm7
	movsd	%xmm7, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -168(%rbp)
	jmp	.L667
.L668:
	movapd	%xmm13, %xmm2
	movapd	%xmm14, %xmm3
	addsd	%xmm3, %xmm2
	movsd	-56(%rbp), %xmm5
	movapd	%xmm15, %xmm4
	addsd	%xmm4, %xmm5
	movsd	-64(%rbp), %xmm0
	addsd	-120(%rbp), %xmm0
	movsd	-72(%rbp), %xmm1
	addsd	-128(%rbp), %xmm1
	movsd	-80(%rbp), %xmm8
	addsd	-136(%rbp), %xmm8
	movsd	-88(%rbp), %xmm9
	addsd	-144(%rbp), %xmm9
	movsd	-96(%rbp), %xmm10
	addsd	-152(%rbp), %xmm10
	movsd	-104(%rbp), %xmm11
	addsd	-160(%rbp), %xmm11
	movsd	-112(%rbp), %xmm12
	addsd	-168(%rbp), %xmm12
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm8, %xmm3
	addsd	%xmm8, %xmm3
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm4
	addsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	movapd	%xmm12, %xmm4
	addsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	movapd	%xmm12, %xmm5
	addsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm6
	addsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm12, %xmm6
	addsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm5, %xmm1
	addsd	%xmm5, %xmm1
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm3
	addsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	movapd	%xmm12, %xmm3
	addsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movapd	%xmm6, %xmm5
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm3
	addsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	movapd	%xmm12, %xmm3
	addsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm4
	addsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	movapd	%xmm12, %xmm4
	addsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm7, %xmm1
	addsd	%xmm7, %xmm1
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	movapd	%xmm12, %xmm5
	addsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm3, %xmm7
	addsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm6
	addsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm12, %xmm7
	addsd	%xmm12, %xmm7
	movapd	%xmm7, %xmm12
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	movapd	%xmm12, %xmm6
	addsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm13
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movsd	%xmm6, -56(%rbp)
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movsd	%xmm6, -72(%rbp)
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movsd	%xmm7, -80(%rbp)
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movsd	%xmm7, -88(%rbp)
	movapd	%xmm10, %xmm3
	addsd	%xmm10, %xmm3
	movsd	%xmm3, -96(%rbp)
	movapd	%xmm11, %xmm2
	addsd	%xmm11, %xmm2
	movsd	%xmm2, -104(%rbp)
	movapd	%xmm12, %xmm2
	addsd	%xmm12, %xmm2
	movsd	%xmm2, -112(%rbp)
.L667:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L668
	cvttsd2sil	%xmm13, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE225:
	.size	double_add_8, .-double_add_8
	.globl	double_add_9
	.type	double_add_9, @function
double_add_9:
.LFB226:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$184, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm13
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movapd	%xmm2, %xmm14
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movapd	%xmm2, %xmm15
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	%xmm3, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2sdl	%eax, %xmm4
	movsd	%xmm4, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2sdl	%eax, %xmm5
	movsd	%xmm5, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	%xmm3, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2sdl	%eax, %xmm4
	movsd	%xmm4, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2sdl	%eax, %xmm5
	movsd	%xmm5, -176(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm6, %xmm6
	cvtsi2sdl	%eax, %xmm6
	movsd	%xmm6, -184(%rbp)
	jmp	.L670
.L671:
	movapd	%xmm13, %xmm2
	movapd	%xmm14, %xmm7
	addsd	%xmm7, %xmm2
	movsd	-56(%rbp), %xmm5
	movapd	%xmm15, %xmm3
	addsd	%xmm3, %xmm5
	movsd	-64(%rbp), %xmm0
	addsd	-128(%rbp), %xmm0
	movsd	-72(%rbp), %xmm1
	addsd	-136(%rbp), %xmm1
	movsd	-80(%rbp), %xmm6
	addsd	-144(%rbp), %xmm6
	movsd	-88(%rbp), %xmm8
	addsd	-152(%rbp), %xmm8
	movsd	-96(%rbp), %xmm9
	addsd	-160(%rbp), %xmm9
	movsd	-104(%rbp), %xmm10
	addsd	-168(%rbp), %xmm10
	movsd	-112(%rbp), %xmm11
	addsd	-176(%rbp), %xmm11
	movsd	-120(%rbp), %xmm12
	addsd	-184(%rbp), %xmm12
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	movapd	%xmm6, %xmm3
	addsd	%xmm6, %xmm3
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm4
	addsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	movapd	%xmm10, %xmm4
	addsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	movapd	%xmm11, %xmm4
	addsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	movapd	%xmm12, %xmm4
	addsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm5
	addsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	movapd	%xmm12, %xmm6
	addsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm1
	addsd	%xmm11, %xmm1
	movapd	%xmm1, %xmm11
	movapd	%xmm12, %xmm7
	addsd	%xmm12, %xmm7
	movapd	%xmm7, %xmm12
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movapd	%xmm6, %xmm5
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm3
	addsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movapd	%xmm10, %xmm3
	addsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	movapd	%xmm11, %xmm3
	addsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	movapd	%xmm12, %xmm3
	addsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm5, %xmm1
	addsd	%xmm5, %xmm1
	movapd	%xmm1, %xmm5
	movapd	%xmm6, %xmm7
	addsd	%xmm6, %xmm7
	movapd	%xmm7, %xmm6
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm3
	addsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movapd	%xmm10, %xmm3
	addsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	movapd	%xmm11, %xmm3
	addsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	movapd	%xmm12, %xmm1
	addsd	%xmm12, %xmm1
	movapd	%xmm1, %xmm12
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm5, %xmm1
	addsd	%xmm5, %xmm1
	movapd	%xmm6, %xmm5
	addsd	%xmm6, %xmm5
	movapd	%xmm8, %xmm4
	addsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	movapd	%xmm9, %xmm4
	addsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	movapd	%xmm10, %xmm4
	addsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	movapd	%xmm11, %xmm6
	addsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm12, %xmm4
	addsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm5
	addsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	movapd	%xmm11, %xmm5
	addsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	movapd	%xmm12, %xmm5
	addsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm3, %xmm7
	addsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm8, %xmm5
	addsd	%xmm8, %xmm5
	movapd	%xmm5, %xmm8
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm1
	addsd	%xmm11, %xmm1
	movapd	%xmm1, %xmm11
	movapd	%xmm12, %xmm6
	addsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm7, %xmm1
	addsd	%xmm7, %xmm1
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm8, %xmm5
	addsd	%xmm8, %xmm5
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	movapd	%xmm12, %xmm7
	addsd	%xmm12, %xmm7
	movapd	%xmm7, %xmm12
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm13
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movsd	%xmm6, -56(%rbp)
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movsd	%xmm6, -64(%rbp)
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movsd	%xmm6, -72(%rbp)
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movsd	%xmm7, -80(%rbp)
	movapd	%xmm5, %xmm3
	addsd	%xmm5, %xmm3
	movsd	%xmm3, -88(%rbp)
	movapd	%xmm9, %xmm2
	addsd	%xmm9, %xmm2
	movsd	%xmm2, -96(%rbp)
	movapd	%xmm10, %xmm2
	addsd	%xmm10, %xmm2
	movsd	%xmm2, -104(%rbp)
	movapd	%xmm11, %xmm2
	addsd	%xmm11, %xmm2
	movsd	%xmm2, -112(%rbp)
	movapd	%xmm12, %xmm2
	addsd	%xmm12, %xmm2
	movsd	%xmm2, -120(%rbp)
.L670:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L671
	cvttsd2sil	%xmm13, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE226:
	.size	double_add_9, .-double_add_9
	.globl	double_add_10
	.type	double_add_10, @function
double_add_10:
.LFB227:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$200, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm14
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movapd	%xmm2, %xmm15
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	%xmm3, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2sdl	%eax, %xmm4
	movsd	%xmm4, -176(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movsd	%xmm0, -184(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -192(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	%xmm3, -200(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm6, %xmm6
	cvtsi2sdl	%eax, %xmm6
	movsd	%xmm6, -208(%rbp)
	jmp	.L673
.L674:
	movapd	%xmm14, %xmm2
	movapd	%xmm15, %xmm4
	addsd	%xmm4, %xmm2
	movsd	-56(%rbp), %xmm5
	addsd	-136(%rbp), %xmm5
	movsd	-64(%rbp), %xmm0
	addsd	-144(%rbp), %xmm0
	movsd	-72(%rbp), %xmm1
	addsd	-152(%rbp), %xmm1
	movsd	-80(%rbp), %xmm6
	addsd	-160(%rbp), %xmm6
	movsd	-88(%rbp), %xmm8
	addsd	-168(%rbp), %xmm8
	movsd	-96(%rbp), %xmm9
	addsd	-176(%rbp), %xmm9
	movsd	-104(%rbp), %xmm10
	addsd	-184(%rbp), %xmm10
	movsd	-112(%rbp), %xmm11
	addsd	-192(%rbp), %xmm11
	movsd	-120(%rbp), %xmm12
	addsd	-200(%rbp), %xmm12
	movsd	-128(%rbp), %xmm13
	addsd	-208(%rbp), %xmm13
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	movapd	%xmm6, %xmm3
	addsd	%xmm6, %xmm3
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm4
	addsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	movapd	%xmm11, %xmm4
	addsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	movapd	%xmm12, %xmm4
	addsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	movapd	%xmm13, %xmm4
	addsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm5
	addsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	movapd	%xmm12, %xmm5
	addsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	movapd	%xmm13, %xmm5
	addsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm8, %xmm6
	addsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	movapd	%xmm9, %xmm3
	addsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movapd	%xmm10, %xmm3
	addsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	movapd	%xmm11, %xmm3
	addsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	movapd	%xmm12, %xmm3
	addsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	movapd	%xmm13, %xmm3
	addsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm5, %xmm7
	addsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	movapd	%xmm8, %xmm3
	addsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm6
	addsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm12, %xmm7
	addsd	%xmm12, %xmm7
	movapd	%xmm7, %xmm12
	movapd	%xmm13, %xmm3
	addsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	movapd	%xmm5, %xmm7
	addsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm3
	addsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movapd	%xmm10, %xmm3
	addsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	movapd	%xmm11, %xmm3
	addsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	movapd	%xmm12, %xmm6
	addsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	movapd	%xmm13, %xmm7
	addsd	%xmm13, %xmm7
	movapd	%xmm7, %xmm13
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm8, %xmm4
	addsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm4
	addsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	movapd	%xmm11, %xmm4
	addsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	movapd	%xmm12, %xmm4
	addsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	movapd	%xmm13, %xmm6
	addsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm5
	addsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	movapd	%xmm12, %xmm5
	addsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	movapd	%xmm13, %xmm1
	addsd	%xmm13, %xmm1
	movapd	%xmm1, %xmm13
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm3, %xmm7
	addsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm6, %xmm1
	addsd	%xmm6, %xmm1
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm8, %xmm4
	addsd	%xmm8, %xmm4
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm6
	addsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm12, %xmm6
	addsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	movapd	%xmm13, %xmm5
	addsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm7, %xmm5
	addsd	%xmm7, %xmm5
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	movapd	%xmm12, %xmm7
	addsd	%xmm12, %xmm7
	movapd	%xmm7, %xmm12
	movapd	%xmm13, %xmm7
	addsd	%xmm13, %xmm7
	movapd	%xmm7, %xmm13
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm14
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movsd	%xmm6, -56(%rbp)
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movsd	%xmm7, -64(%rbp)
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movsd	%xmm7, -72(%rbp)
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movsd	%xmm6, -80(%rbp)
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movsd	%xmm3, -88(%rbp)
	movapd	%xmm9, %xmm3
	addsd	%xmm9, %xmm3
	movsd	%xmm3, -96(%rbp)
	movapd	%xmm10, %xmm2
	addsd	%xmm10, %xmm2
	movsd	%xmm2, -104(%rbp)
	movapd	%xmm11, %xmm2
	addsd	%xmm11, %xmm2
	movsd	%xmm2, -112(%rbp)
	movapd	%xmm12, %xmm2
	addsd	%xmm12, %xmm2
	movsd	%xmm2, -120(%rbp)
	movapd	%xmm13, %xmm2
	addsd	%xmm13, %xmm2
	movsd	%xmm2, -128(%rbp)
.L673:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L674
	cvttsd2sil	%xmm14, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE227:
	.size	double_add_10, .-double_add_10
	.globl	double_add_11
	.type	double_add_11, @function
double_add_11:
.LFB228:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$232, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm15
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -176(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movsd	%xmm0, -184(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2sdl	%eax, %xmm4
	movsd	%xmm4, -192(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -200(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movsd	%xmm0, -208(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	%xmm3, -216(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm4, %xmm4
	cvtsi2sdl	%eax, %xmm4
	movsd	%xmm4, -224(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2sdl	%eax, %xmm5
	movsd	%xmm5, -232(%rbp)
	jmp	.L676
.L677:
	movapd	%xmm15, %xmm2
	addsd	-144(%rbp), %xmm2
	movsd	-56(%rbp), %xmm5
	addsd	-152(%rbp), %xmm5
	movsd	-64(%rbp), %xmm0
	addsd	-160(%rbp), %xmm0
	movsd	-72(%rbp), %xmm1
	addsd	-168(%rbp), %xmm1
	movsd	-80(%rbp), %xmm6
	addsd	-176(%rbp), %xmm6
	movsd	-88(%rbp), %xmm8
	addsd	-184(%rbp), %xmm8
	movsd	-96(%rbp), %xmm9
	addsd	-192(%rbp), %xmm9
	movsd	-104(%rbp), %xmm10
	addsd	-200(%rbp), %xmm10
	movsd	-112(%rbp), %xmm11
	addsd	-208(%rbp), %xmm11
	movsd	-120(%rbp), %xmm12
	addsd	-216(%rbp), %xmm12
	movsd	-128(%rbp), %xmm13
	addsd	-224(%rbp), %xmm13
	movsd	-136(%rbp), %xmm14
	addsd	-232(%rbp), %xmm14
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	movapd	%xmm6, %xmm3
	addsd	%xmm6, %xmm3
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm4
	addsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	movapd	%xmm11, %xmm4
	addsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	movapd	%xmm12, %xmm4
	addsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	movapd	%xmm13, %xmm4
	addsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	movapd	%xmm14, %xmm4
	addsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm5
	addsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	movapd	%xmm11, %xmm5
	addsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	movapd	%xmm12, %xmm5
	addsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	movapd	%xmm13, %xmm5
	addsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	movapd	%xmm14, %xmm7
	addsd	%xmm14, %xmm7
	movapd	%xmm7, %xmm14
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm3
	addsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movapd	%xmm10, %xmm3
	addsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	movapd	%xmm11, %xmm3
	addsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	movapd	%xmm12, %xmm3
	addsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	movapd	%xmm13, %xmm3
	addsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	movapd	%xmm14, %xmm3
	addsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm14
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm3
	addsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	movapd	%xmm12, %xmm3
	addsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	movapd	%xmm13, %xmm3
	addsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	movapd	%xmm14, %xmm3
	addsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm14
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm6, %xmm5
	addsd	%xmm6, %xmm5
	movapd	%xmm8, %xmm3
	addsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm3
	addsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	movapd	%xmm11, %xmm6
	addsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm12, %xmm6
	addsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	movapd	%xmm13, %xmm6
	addsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	movapd	%xmm14, %xmm6
	addsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm4
	addsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	movapd	%xmm10, %xmm4
	addsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	movapd	%xmm11, %xmm4
	addsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	movapd	%xmm12, %xmm7
	addsd	%xmm12, %xmm7
	movapd	%xmm7, %xmm12
	movapd	%xmm13, %xmm4
	addsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	movapd	%xmm14, %xmm4
	addsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm5
	addsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	movapd	%xmm11, %xmm5
	addsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	movapd	%xmm12, %xmm5
	addsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	movapd	%xmm13, %xmm7
	addsd	%xmm13, %xmm7
	movapd	%xmm7, %xmm13
	movapd	%xmm14, %xmm5
	addsd	%xmm14, %xmm5
	movapd	%xmm5, %xmm14
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm3, %xmm7
	addsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm6, %xmm1
	addsd	%xmm6, %xmm1
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm8, %xmm6
	addsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm6
	addsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm12, %xmm6
	addsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	movapd	%xmm13, %xmm5
	addsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	movapd	%xmm14, %xmm7
	addsd	%xmm14, %xmm7
	movapd	%xmm7, %xmm14
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm8, %xmm1
	addsd	%xmm8, %xmm1
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	movapd	%xmm12, %xmm7
	addsd	%xmm12, %xmm7
	movapd	%xmm7, %xmm12
	movapd	%xmm13, %xmm6
	addsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	movapd	%xmm14, %xmm6
	addsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm15
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movsd	%xmm6, -56(%rbp)
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movsd	%xmm7, -64(%rbp)
	movapd	%xmm5, %xmm7
	addsd	%xmm5, %xmm7
	movsd	%xmm7, -72(%rbp)
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movsd	%xmm7, -80(%rbp)
	movapd	%xmm1, %xmm3
	addsd	%xmm1, %xmm3
	movsd	%xmm3, -88(%rbp)
	movapd	%xmm9, %xmm3
	addsd	%xmm9, %xmm3
	movsd	%xmm3, -96(%rbp)
	movapd	%xmm10, %xmm2
	addsd	%xmm10, %xmm2
	movsd	%xmm2, -104(%rbp)
	movapd	%xmm11, %xmm2
	addsd	%xmm11, %xmm2
	movsd	%xmm2, -112(%rbp)
	movapd	%xmm12, %xmm2
	addsd	%xmm12, %xmm2
	movsd	%xmm2, -120(%rbp)
	movapd	%xmm13, %xmm2
	addsd	%xmm13, %xmm2
	movsd	%xmm2, -128(%rbp)
	movapd	%xmm14, %xmm2
	addsd	%xmm14, %xmm2
	movsd	%xmm2, -136(%rbp)
.L676:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L677
	cvttsd2sil	%xmm15, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE228:
	.size	double_add_11, .-double_add_11
	.globl	double_add_12
	.type	double_add_12, @function
double_add_12:
.LFB229:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$248, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -176(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -184(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movsd	%xmm0, -192(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -200(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2sdl	%eax, %xmm7
	movsd	%xmm7, -208(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movsd	%xmm0, -216(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -224(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	%xmm3, -232(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2sdl	%eax, %xmm5
	movsd	%xmm5, -240(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm6, %xmm6
	cvtsi2sdl	%eax, %xmm6
	movsd	%xmm6, -248(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm7, %xmm7
	cvtsi2sdl	%eax, %xmm7
	movsd	%xmm7, -256(%rbp)
	jmp	.L679
.L680:
	movsd	-64(%rbp), %xmm2
	addsd	-160(%rbp), %xmm2
	movsd	-72(%rbp), %xmm5
	addsd	-168(%rbp), %xmm5
	movsd	-80(%rbp), %xmm0
	addsd	-176(%rbp), %xmm0
	movsd	-88(%rbp), %xmm1
	addsd	-184(%rbp), %xmm1
	movsd	-96(%rbp), %xmm8
	addsd	-192(%rbp), %xmm8
	movsd	-104(%rbp), %xmm9
	addsd	-200(%rbp), %xmm9
	movsd	-112(%rbp), %xmm10
	addsd	-208(%rbp), %xmm10
	movsd	-120(%rbp), %xmm11
	addsd	-216(%rbp), %xmm11
	movsd	-128(%rbp), %xmm12
	addsd	-224(%rbp), %xmm12
	movsd	-136(%rbp), %xmm13
	addsd	-232(%rbp), %xmm13
	movsd	-144(%rbp), %xmm14
	addsd	-240(%rbp), %xmm14
	movsd	-152(%rbp), %xmm15
	addsd	-248(%rbp), %xmm15
	movsd	-56(%rbp), %xmm4
	addsd	-256(%rbp), %xmm4
	movsd	%xmm4, -56(%rbp)
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm8, %xmm3
	addsd	%xmm8, %xmm3
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm4
	addsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	movapd	%xmm12, %xmm4
	addsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	movapd	%xmm13, %xmm4
	addsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	movapd	%xmm14, %xmm4
	addsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	movapd	%xmm15, %xmm4
	addsd	%xmm15, %xmm4
	movapd	%xmm4, %xmm15
	movsd	-56(%rbp), %xmm4
	movapd	%xmm4, %xmm8
	addsd	%xmm4, %xmm8
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm6
	addsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm12, %xmm5
	addsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	movapd	%xmm13, %xmm5
	addsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	movapd	%xmm14, %xmm7
	addsd	%xmm14, %xmm7
	movapd	%xmm7, %xmm14
	movapd	%xmm15, %xmm7
	addsd	%xmm15, %xmm7
	movapd	%xmm7, %xmm15
	movapd	%xmm8, %xmm6
	addsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm3
	addsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	movapd	%xmm11, %xmm3
	addsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	movapd	%xmm12, %xmm3
	addsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	movapd	%xmm13, %xmm3
	addsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	movapd	%xmm14, %xmm3
	addsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm14
	movapd	%xmm15, %xmm3
	addsd	%xmm15, %xmm3
	movapd	%xmm3, %xmm15
	movapd	%xmm8, %xmm3
	addsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm5, %xmm7
	addsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm3
	addsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	movapd	%xmm12, %xmm7
	addsd	%xmm12, %xmm7
	movapd	%xmm7, %xmm12
	movapd	%xmm13, %xmm6
	addsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	movapd	%xmm14, %xmm6
	addsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	movapd	%xmm15, %xmm3
	addsd	%xmm15, %xmm3
	movapd	%xmm3, %xmm15
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	movapd	%xmm5, %xmm7
	addsd	%xmm5, %xmm7
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm3
	addsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	movapd	%xmm12, %xmm3
	addsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	movapd	%xmm13, %xmm3
	addsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	movapd	%xmm14, %xmm5
	addsd	%xmm14, %xmm5
	movapd	%xmm5, %xmm14
	movapd	%xmm15, %xmm5
	addsd	%xmm15, %xmm5
	movapd	%xmm5, %xmm15
	movapd	%xmm8, %xmm3
	addsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm7, %xmm5
	addsd	%xmm7, %xmm5
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm4
	addsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	movapd	%xmm11, %xmm4
	addsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	movapd	%xmm12, %xmm4
	addsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	movapd	%xmm13, %xmm4
	addsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	movapd	%xmm14, %xmm4
	addsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	movapd	%xmm15, %xmm6
	addsd	%xmm15, %xmm6
	movapd	%xmm6, %xmm15
	movapd	%xmm8, %xmm4
	addsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm6
	addsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm12, %xmm5
	addsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	movapd	%xmm13, %xmm7
	addsd	%xmm13, %xmm7
	movapd	%xmm7, %xmm13
	movapd	%xmm14, %xmm7
	addsd	%xmm14, %xmm7
	movapd	%xmm7, %xmm14
	movapd	%xmm15, %xmm7
	addsd	%xmm15, %xmm7
	movapd	%xmm7, %xmm15
	movapd	%xmm8, %xmm5
	addsd	%xmm8, %xmm5
	movapd	%xmm5, %xmm8
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm3, %xmm7
	addsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	movapd	%xmm12, %xmm6
	addsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	movapd	%xmm13, %xmm5
	addsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	movapd	%xmm14, %xmm5
	addsd	%xmm14, %xmm5
	movapd	%xmm5, %xmm14
	movapd	%xmm15, %xmm5
	addsd	%xmm15, %xmm5
	movapd	%xmm5, %xmm15
	movapd	%xmm8, %xmm6
	addsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm6
	addsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm12, %xmm7
	addsd	%xmm12, %xmm7
	movapd	%xmm7, %xmm12
	movapd	%xmm13, %xmm6
	addsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	movapd	%xmm14, %xmm6
	addsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	movapd	%xmm15, %xmm1
	addsd	%xmm15, %xmm1
	movapd	%xmm1, %xmm15
	movapd	%xmm8, %xmm1
	addsd	%xmm8, %xmm1
	movapd	%xmm1, %xmm8
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movsd	%xmm7, -64(%rbp)
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movsd	%xmm6, -72(%rbp)
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movapd	%xmm5, %xmm7
	addsd	%xmm5, %xmm7
	movsd	%xmm7, -88(%rbp)
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, -96(%rbp)
	movapd	%xmm9, %xmm3
	addsd	%xmm9, %xmm3
	movsd	%xmm3, -104(%rbp)
	movapd	%xmm10, %xmm3
	addsd	%xmm10, %xmm3
	movsd	%xmm3, -112(%rbp)
	movapd	%xmm11, %xmm2
	addsd	%xmm11, %xmm2
	movsd	%xmm2, -120(%rbp)
	movapd	%xmm12, %xmm2
	addsd	%xmm12, %xmm2
	movsd	%xmm2, -128(%rbp)
	movapd	%xmm13, %xmm2
	addsd	%xmm13, %xmm2
	movsd	%xmm2, -136(%rbp)
	movapd	%xmm14, %xmm2
	addsd	%xmm14, %xmm2
	movsd	%xmm2, -144(%rbp)
	movapd	%xmm15, %xmm2
	addsd	%xmm15, %xmm2
	movsd	%xmm2, -152(%rbp)
	movapd	%xmm8, %xmm2
	addsd	%xmm8, %xmm2
	movsd	%xmm2, -56(%rbp)
.L679:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L680
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE229:
	.size	double_add_12, .-double_add_12
	.globl	double_add_13
	.type	double_add_13, @function
double_add_13:
.LFB230:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$264, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -176(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -184(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -192(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movsd	%xmm0, -200(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -208(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movsd	%xmm0, -216(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -224(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movsd	%xmm0, -232(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -240(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movsd	%xmm0, -248(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -256(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movsd	%xmm0, -264(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	%xmm3, -272(%rbp)
	jmp	.L682
.L683:
	movsd	-64(%rbp), %xmm2
	addsd	-168(%rbp), %xmm2
	movsd	-72(%rbp), %xmm5
	addsd	-176(%rbp), %xmm5
	movsd	-80(%rbp), %xmm0
	addsd	-184(%rbp), %xmm0
	movsd	-88(%rbp), %xmm1
	addsd	-192(%rbp), %xmm1
	movsd	-96(%rbp), %xmm6
	addsd	-200(%rbp), %xmm6
	movsd	-104(%rbp), %xmm8
	addsd	-208(%rbp), %xmm8
	movsd	-112(%rbp), %xmm9
	addsd	-216(%rbp), %xmm9
	movsd	-120(%rbp), %xmm10
	addsd	-224(%rbp), %xmm10
	movsd	-128(%rbp), %xmm11
	addsd	-232(%rbp), %xmm11
	movsd	-136(%rbp), %xmm12
	addsd	-240(%rbp), %xmm12
	movsd	-144(%rbp), %xmm13
	addsd	-248(%rbp), %xmm13
	movsd	-152(%rbp), %xmm14
	addsd	-256(%rbp), %xmm14
	movsd	-160(%rbp), %xmm15
	addsd	-264(%rbp), %xmm15
	movsd	-56(%rbp), %xmm4
	addsd	-272(%rbp), %xmm4
	movsd	%xmm4, -56(%rbp)
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm1, %xmm3
	addsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm1
	movapd	%xmm6, %xmm3
	addsd	%xmm6, %xmm3
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm4
	addsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	movapd	%xmm11, %xmm4
	addsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	movapd	%xmm12, %xmm4
	addsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	movapd	%xmm13, %xmm7
	addsd	%xmm13, %xmm7
	movapd	%xmm7, %xmm13
	movapd	%xmm14, %xmm4
	addsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	movapd	%xmm15, %xmm4
	addsd	%xmm15, %xmm4
	movapd	%xmm4, %xmm15
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm4
	addsd	%xmm6, %xmm4
	movsd	%xmm4, -56(%rbp)
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm3, %xmm7
	addsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm9, %xmm5
	addsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm5
	addsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	movapd	%xmm12, %xmm5
	addsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	movapd	%xmm13, %xmm5
	addsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	movapd	%xmm14, %xmm7
	addsd	%xmm14, %xmm7
	movapd	%xmm7, %xmm14
	movapd	%xmm15, %xmm7
	addsd	%xmm15, %xmm7
	movapd	%xmm7, %xmm15
	movsd	-56(%rbp), %xmm7
	movapd	%xmm7, %xmm5
	addsd	%xmm7, %xmm5
	movsd	%xmm5, -56(%rbp)
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm8, %xmm6
	addsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm5
	addsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	movapd	%xmm12, %xmm6
	addsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	movapd	%xmm13, %xmm6
	addsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	movapd	%xmm14, %xmm5
	addsd	%xmm14, %xmm5
	movapd	%xmm5, %xmm14
	movapd	%xmm15, %xmm5
	addsd	%xmm15, %xmm5
	movapd	%xmm5, %xmm15
	movsd	-56(%rbp), %xmm5
	movapd	%xmm5, %xmm7
	addsd	%xmm5, %xmm7
	movsd	%xmm7, -56(%rbp)
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm8, %xmm0
	addsd	%xmm8, %xmm0
	movapd	%xmm9, %xmm3
	addsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movapd	%xmm10, %xmm3
	addsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	movapd	%xmm11, %xmm3
	addsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	movapd	%xmm12, %xmm3
	addsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	movapd	%xmm13, %xmm3
	addsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	movapd	%xmm14, %xmm3
	addsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm14
	movapd	%xmm15, %xmm3
	addsd	%xmm15, %xmm3
	movapd	%xmm3, %xmm15
	movsd	-56(%rbp), %xmm7
	movapd	%xmm7, %xmm8
	addsd	%xmm7, %xmm8
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	movapd	%xmm6, %xmm7
	addsd	%xmm6, %xmm7
	movapd	%xmm1, %xmm3
	addsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm1
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movapd	%xmm6, %xmm5
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm9, %xmm3
	addsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movapd	%xmm10, %xmm3
	addsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	movapd	%xmm11, %xmm3
	addsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	movapd	%xmm12, %xmm3
	addsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	movapd	%xmm13, %xmm3
	addsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	movapd	%xmm14, %xmm6
	addsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	movapd	%xmm15, %xmm3
	addsd	%xmm15, %xmm3
	movapd	%xmm3, %xmm15
	movapd	%xmm8, %xmm3
	addsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm6
	addsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	movapd	%xmm1, %xmm4
	addsd	%xmm1, %xmm4
	movapd	%xmm4, %xmm1
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm9, %xmm4
	addsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	movapd	%xmm10, %xmm4
	addsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	movapd	%xmm11, %xmm4
	addsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	movapd	%xmm12, %xmm4
	addsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	movapd	%xmm13, %xmm4
	addsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	movapd	%xmm14, %xmm4
	addsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	movapd	%xmm15, %xmm4
	addsd	%xmm15, %xmm4
	movapd	%xmm4, %xmm15
	movapd	%xmm8, %xmm4
	addsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm7, %xmm6
	addsd	%xmm7, %xmm6
	movapd	%xmm1, %xmm4
	addsd	%xmm1, %xmm4
	movapd	%xmm4, %xmm1
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm5
	addsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	movapd	%xmm12, %xmm7
	addsd	%xmm12, %xmm7
	movapd	%xmm7, %xmm12
	movapd	%xmm13, %xmm7
	addsd	%xmm13, %xmm7
	movapd	%xmm7, %xmm13
	movapd	%xmm14, %xmm7
	addsd	%xmm14, %xmm7
	movapd	%xmm7, %xmm14
	movapd	%xmm15, %xmm7
	addsd	%xmm15, %xmm7
	movapd	%xmm7, %xmm15
	movapd	%xmm8, %xmm5
	addsd	%xmm8, %xmm5
	movapd	%xmm5, %xmm8
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm3, %xmm7
	addsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	movapd	%xmm6, %xmm7
	addsd	%xmm6, %xmm7
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm9, %xmm5
	addsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm6
	addsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm12, %xmm5
	addsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	movapd	%xmm13, %xmm5
	addsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	movapd	%xmm14, %xmm5
	addsd	%xmm14, %xmm5
	movapd	%xmm5, %xmm14
	movapd	%xmm15, %xmm6
	addsd	%xmm15, %xmm6
	movapd	%xmm6, %xmm15
	movapd	%xmm8, %xmm6
	addsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm7, %xmm5
	addsd	%xmm7, %xmm5
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	movapd	%xmm12, %xmm6
	addsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	movapd	%xmm13, %xmm6
	addsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	movapd	%xmm14, %xmm6
	addsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	movapd	%xmm15, %xmm6
	addsd	%xmm15, %xmm6
	movapd	%xmm6, %xmm15
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movsd	%xmm7, -64(%rbp)
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movsd	%xmm6, -72(%rbp)
	movapd	%xmm5, %xmm7
	addsd	%xmm5, %xmm7
	movsd	%xmm7, -80(%rbp)
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movsd	%xmm5, -88(%rbp)
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movsd	%xmm7, -96(%rbp)
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movsd	%xmm7, -104(%rbp)
	movapd	%xmm9, %xmm2
	addsd	%xmm9, %xmm2
	movsd	%xmm2, -112(%rbp)
	movapd	%xmm10, %xmm2
	addsd	%xmm10, %xmm2
	movsd	%xmm2, -120(%rbp)
	movapd	%xmm11, %xmm2
	addsd	%xmm11, %xmm2
	movsd	%xmm2, -128(%rbp)
	movapd	%xmm12, %xmm2
	addsd	%xmm12, %xmm2
	movsd	%xmm2, -136(%rbp)
	movapd	%xmm13, %xmm2
	addsd	%xmm13, %xmm2
	movsd	%xmm2, -144(%rbp)
	movapd	%xmm14, %xmm2
	addsd	%xmm14, %xmm2
	movsd	%xmm2, -152(%rbp)
	movapd	%xmm15, %xmm2
	addsd	%xmm15, %xmm2
	movsd	%xmm2, -160(%rbp)
	movapd	%xmm8, %xmm2
	addsd	%xmm8, %xmm2
	movsd	%xmm2, -56(%rbp)
.L682:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L683
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE230:
	.size	double_add_13, .-double_add_13
	.globl	double_add_14
	.type	double_add_14, @function
double_add_14:
.LFB231:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$280, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -176(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -184(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -192(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -200(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -208(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -216(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -224(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -232(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -240(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -248(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -256(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	%xmm3, -264(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -272(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm5, %xmm5
	cvtsi2sdl	%eax, %xmm5
	movsd	%xmm5, -280(%rbp)
	movq	-24(%rbp), %rax
	movsd	192(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm3, %xmm3
	cvtsi2sdl	%eax, %xmm3
	movsd	%xmm3, -288(%rbp)
	jmp	.L685
.L686:
	movsd	-80(%rbp), %xmm2
	addsd	-176(%rbp), %xmm2
	movsd	-88(%rbp), %xmm5
	addsd	-184(%rbp), %xmm5
	movsd	-96(%rbp), %xmm7
	addsd	-192(%rbp), %xmm7
	movsd	-104(%rbp), %xmm1
	addsd	-200(%rbp), %xmm1
	movsd	-112(%rbp), %xmm8
	addsd	-208(%rbp), %xmm8
	movsd	-120(%rbp), %xmm9
	addsd	-216(%rbp), %xmm9
	movsd	-128(%rbp), %xmm10
	addsd	-224(%rbp), %xmm10
	movsd	-136(%rbp), %xmm11
	addsd	-232(%rbp), %xmm11
	movsd	-144(%rbp), %xmm12
	addsd	-240(%rbp), %xmm12
	movsd	-152(%rbp), %xmm13
	addsd	-248(%rbp), %xmm13
	movsd	-160(%rbp), %xmm14
	addsd	-256(%rbp), %xmm14
	movsd	-168(%rbp), %xmm15
	addsd	-264(%rbp), %xmm15
	movsd	-72(%rbp), %xmm3
	addsd	-272(%rbp), %xmm3
	movsd	%xmm3, -72(%rbp)
	movsd	-56(%rbp), %xmm6
	addsd	-280(%rbp), %xmm6
	movsd	%xmm6, -56(%rbp)
	movsd	-64(%rbp), %xmm0
	addsd	-288(%rbp), %xmm0
	movsd	%xmm0, -64(%rbp)
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm7, %xmm6
	addsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	movapd	%xmm1, %xmm0
	addsd	%xmm1, %xmm0
	movapd	%xmm0, %xmm1
	movapd	%xmm8, %xmm3
	addsd	%xmm8, %xmm3
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm0
	addsd	%xmm11, %xmm0
	movapd	%xmm0, %xmm11
	movapd	%xmm12, %xmm4
	addsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	movapd	%xmm13, %xmm6
	addsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	movapd	%xmm14, %xmm4
	addsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	movapd	%xmm15, %xmm6
	addsd	%xmm15, %xmm6
	movapd	%xmm6, %xmm15
	movsd	-72(%rbp), %xmm0
	movapd	%xmm0, %xmm8
	addsd	%xmm0, %xmm8
	movsd	-56(%rbp), %xmm4
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, -56(%rbp)
	movsd	-64(%rbp), %xmm4
	movapd	%xmm4, %xmm0
	addsd	%xmm4, %xmm0
	movsd	%xmm0, -64(%rbp)
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm5
	addsd	%xmm7, %xmm5
	movapd	%xmm5, %xmm7
	movapd	%xmm1, %xmm0
	addsd	%xmm1, %xmm0
	movapd	%xmm0, %xmm1
	movapd	%xmm3, %xmm0
	addsd	%xmm3, %xmm0
	movapd	%xmm0, %xmm3
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm0
	addsd	%xmm10, %xmm0
	movapd	%xmm0, %xmm10
	movapd	%xmm11, %xmm6
	addsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm12, %xmm5
	addsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	movapd	%xmm13, %xmm5
	addsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	movapd	%xmm14, %xmm5
	addsd	%xmm14, %xmm5
	movapd	%xmm5, %xmm14
	movapd	%xmm15, %xmm5
	addsd	%xmm15, %xmm5
	movapd	%xmm5, %xmm15
	movapd	%xmm8, %xmm5
	addsd	%xmm8, %xmm5
	movapd	%xmm5, %xmm8
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	addsd	%xmm6, %xmm5
	movsd	%xmm5, -56(%rbp)
	movsd	-64(%rbp), %xmm0
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movsd	%xmm6, -64(%rbp)
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm7, %xmm5
	addsd	%xmm7, %xmm5
	movapd	%xmm1, %xmm0
	addsd	%xmm1, %xmm0
	movapd	%xmm0, %xmm1
	movapd	%xmm3, %xmm0
	addsd	%xmm3, %xmm0
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm3
	addsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	movapd	%xmm11, %xmm3
	addsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	movapd	%xmm12, %xmm3
	addsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	movapd	%xmm13, %xmm3
	addsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	movapd	%xmm14, %xmm3
	addsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm14
	movapd	%xmm15, %xmm3
	addsd	%xmm15, %xmm3
	movapd	%xmm3, %xmm15
	movapd	%xmm8, %xmm3
	addsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	movsd	-56(%rbp), %xmm7
	movapd	%xmm7, %xmm3
	addsd	%xmm7, %xmm3
	movsd	%xmm3, -56(%rbp)
	movsd	-64(%rbp), %xmm6
	movapd	%xmm6, %xmm7
	addsd	%xmm6, %xmm7
	movsd	%xmm7, -64(%rbp)
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movapd	%xmm1, %xmm3
	addsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm1
	movapd	%xmm0, %xmm5
	addsd	%xmm0, %xmm5
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	movapd	%xmm12, %xmm0
	addsd	%xmm12, %xmm0
	movapd	%xmm0, %xmm12
	movapd	%xmm13, %xmm3
	addsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	movapd	%xmm14, %xmm0
	addsd	%xmm14, %xmm0
	movapd	%xmm0, %xmm14
	movapd	%xmm15, %xmm3
	addsd	%xmm15, %xmm3
	movapd	%xmm3, %xmm15
	movapd	%xmm8, %xmm0
	addsd	%xmm8, %xmm0
	movapd	%xmm0, %xmm8
	movsd	-56(%rbp), %xmm3
	movapd	%xmm3, %xmm7
	addsd	%xmm3, %xmm7
	movsd	%xmm7, -56(%rbp)
	movsd	-64(%rbp), %xmm7
	movapd	%xmm7, %xmm0
	addsd	%xmm7, %xmm0
	movsd	%xmm0, -64(%rbp)
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	movapd	%xmm6, %xmm7
	addsd	%xmm6, %xmm7
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movapd	%xmm6, %xmm5
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm0
	addsd	%xmm11, %xmm0
	movapd	%xmm0, %xmm11
	movapd	%xmm12, %xmm6
	addsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	movapd	%xmm13, %xmm0
	addsd	%xmm13, %xmm0
	movapd	%xmm0, %xmm13
	movapd	%xmm14, %xmm6
	addsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	movapd	%xmm15, %xmm0
	addsd	%xmm15, %xmm0
	movapd	%xmm0, %xmm15
	movapd	%xmm8, %xmm6
	addsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	movsd	-56(%rbp), %xmm3
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movsd	%xmm6, -56(%rbp)
	movsd	-64(%rbp), %xmm0
	movapd	%xmm0, %xmm3
	addsd	%xmm0, %xmm3
	movsd	%xmm3, -64(%rbp)
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm6
	addsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	movapd	%xmm1, %xmm4
	addsd	%xmm1, %xmm4
	movapd	%xmm4, %xmm1
	movapd	%xmm5, %xmm0
	addsd	%xmm5, %xmm0
	movapd	%xmm0, %xmm5
	movapd	%xmm9, %xmm4
	addsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	movapd	%xmm10, %xmm4
	addsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	movapd	%xmm11, %xmm4
	addsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	movapd	%xmm12, %xmm4
	addsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	movapd	%xmm13, %xmm4
	addsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	movapd	%xmm14, %xmm4
	addsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	movapd	%xmm15, %xmm4
	addsd	%xmm15, %xmm4
	movapd	%xmm4, %xmm15
	movapd	%xmm8, %xmm4
	addsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm4
	addsd	%xmm6, %xmm4
	movsd	%xmm4, -56(%rbp)
	movsd	-64(%rbp), %xmm0
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movsd	%xmm6, -64(%rbp)
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm7, %xmm6
	addsd	%xmm7, %xmm6
	movapd	%xmm1, %xmm0
	addsd	%xmm1, %xmm0
	movapd	%xmm0, %xmm1
	movapd	%xmm5, %xmm0
	addsd	%xmm5, %xmm0
	movapd	%xmm9, %xmm4
	addsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	movapd	%xmm10, %xmm4
	addsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	movapd	%xmm11, %xmm5
	addsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	movapd	%xmm12, %xmm7
	addsd	%xmm12, %xmm7
	movapd	%xmm7, %xmm12
	movapd	%xmm13, %xmm7
	addsd	%xmm13, %xmm7
	movapd	%xmm7, %xmm13
	movapd	%xmm14, %xmm7
	addsd	%xmm14, %xmm7
	movapd	%xmm7, %xmm14
	movapd	%xmm15, %xmm7
	addsd	%xmm15, %xmm7
	movapd	%xmm7, %xmm15
	movapd	%xmm8, %xmm4
	addsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	movsd	-56(%rbp), %xmm4
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movsd	%xmm7, -56(%rbp)
	movsd	-64(%rbp), %xmm5
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movsd	%xmm4, -64(%rbp)
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm3, %xmm7
	addsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	movapd	%xmm6, %xmm7
	addsd	%xmm6, %xmm7
	movapd	%xmm1, %xmm4
	addsd	%xmm1, %xmm4
	movapd	%xmm4, %xmm1
	movapd	%xmm0, %xmm4
	addsd	%xmm0, %xmm4
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm5
	addsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	movapd	%xmm11, %xmm6
	addsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm12, %xmm5
	addsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	movapd	%xmm13, %xmm6
	addsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	movapd	%xmm14, %xmm5
	addsd	%xmm14, %xmm5
	movapd	%xmm5, %xmm14
	movapd	%xmm15, %xmm6
	addsd	%xmm15, %xmm6
	movapd	%xmm6, %xmm15
	movapd	%xmm8, %xmm5
	addsd	%xmm8, %xmm5
	movapd	%xmm5, %xmm8
	movsd	-56(%rbp), %xmm5
	movapd	%xmm5, %xmm0
	addsd	%xmm5, %xmm0
	movsd	%xmm0, -56(%rbp)
	movsd	-64(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	addsd	%xmm6, %xmm5
	movsd	%xmm5, -64(%rbp)
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm7, %xmm5
	addsd	%xmm7, %xmm5
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm9, %xmm7
	addsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	movapd	%xmm12, %xmm0
	addsd	%xmm12, %xmm0
	movapd	%xmm0, %xmm12
	movapd	%xmm13, %xmm7
	addsd	%xmm13, %xmm7
	movapd	%xmm7, %xmm13
	movapd	%xmm14, %xmm0
	addsd	%xmm14, %xmm0
	movapd	%xmm0, %xmm14
	movapd	%xmm15, %xmm7
	addsd	%xmm15, %xmm7
	movapd	%xmm7, %xmm15
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movsd	-56(%rbp), %xmm0
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movsd	%xmm6, -56(%rbp)
	movsd	-64(%rbp), %xmm0
	movapd	%xmm0, %xmm7
	addsd	%xmm0, %xmm7
	movsd	%xmm7, -64(%rbp)
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movsd	%xmm7, -80(%rbp)
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movsd	%xmm6, -88(%rbp)
	movapd	%xmm5, %xmm0
	addsd	%xmm5, %xmm0
	movsd	%xmm0, -96(%rbp)
	movapd	%xmm1, %xmm3
	addsd	%xmm1, %xmm3
	movsd	%xmm3, -104(%rbp)
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movsd	%xmm3, -112(%rbp)
	movapd	%xmm9, %xmm3
	addsd	%xmm9, %xmm3
	movsd	%xmm3, -120(%rbp)
	movapd	%xmm10, %xmm3
	addsd	%xmm10, %xmm3
	movsd	%xmm3, -128(%rbp)
	movapd	%xmm11, %xmm2
	addsd	%xmm11, %xmm2
	movsd	%xmm2, -136(%rbp)
	movapd	%xmm12, %xmm2
	addsd	%xmm12, %xmm2
	movsd	%xmm2, -144(%rbp)
	movapd	%xmm13, %xmm2
	addsd	%xmm13, %xmm2
	movsd	%xmm2, -152(%rbp)
	movapd	%xmm14, %xmm2
	addsd	%xmm14, %xmm2
	movsd	%xmm2, -160(%rbp)
	movapd	%xmm15, %xmm2
	addsd	%xmm15, %xmm2
	movsd	%xmm2, -168(%rbp)
	movapd	%xmm8, %xmm2
	addsd	%xmm8, %xmm2
	movsd	%xmm2, -72(%rbp)
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm2
	addsd	%xmm6, %xmm2
	movsd	%xmm2, -56(%rbp)
	movsd	-64(%rbp), %xmm7
	movapd	%xmm7, %xmm2
	addsd	%xmm7, %xmm2
	movsd	%xmm2, -64(%rbp)
.L685:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L686
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE231:
	.size	double_add_14, .-double_add_14
	.globl	double_add_15
	.type	double_add_15, @function
double_add_15:
.LFB232:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$296, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -184(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -192(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -200(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movsd	%xmm0, -208(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -216(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movsd	%xmm0, -224(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -232(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movsd	%xmm0, -240(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -248(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movsd	%xmm0, -256(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -264(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -176(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movsd	%xmm0, -272(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -280(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movsd	%xmm0, -288(%rbp)
	movq	-24(%rbp), %rax
	movsd	192(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm2, %xmm2
	cvtsi2sdl	%eax, %xmm2
	movsd	%xmm2, -296(%rbp)
	movq	-24(%rbp), %rax
	movsd	200(%rax), %xmm1
	movsd	.LC9(%rip), %xmm0
	addsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	8(%rax), %eax
	pxor	%xmm0, %xmm0
	cvtsi2sdl	%eax, %xmm0
	movsd	%xmm0, -304(%rbp)
	jmp	.L688
.L689:
	movsd	-88(%rbp), %xmm2
	addsd	-184(%rbp), %xmm2
	movsd	-96(%rbp), %xmm5
	addsd	-192(%rbp), %xmm5
	movsd	-104(%rbp), %xmm7
	addsd	-200(%rbp), %xmm7
	movsd	-112(%rbp), %xmm0
	addsd	-208(%rbp), %xmm0
	movsd	-120(%rbp), %xmm8
	addsd	-216(%rbp), %xmm8
	movsd	-128(%rbp), %xmm9
	addsd	-224(%rbp), %xmm9
	movsd	-136(%rbp), %xmm10
	addsd	-232(%rbp), %xmm10
	movsd	-144(%rbp), %xmm11
	addsd	-240(%rbp), %xmm11
	movsd	-152(%rbp), %xmm12
	addsd	-248(%rbp), %xmm12
	movsd	-160(%rbp), %xmm13
	addsd	-256(%rbp), %xmm13
	movsd	-168(%rbp), %xmm14
	addsd	-264(%rbp), %xmm14
	movsd	-176(%rbp), %xmm15
	addsd	-272(%rbp), %xmm15
	movsd	-80(%rbp), %xmm4
	addsd	-280(%rbp), %xmm4
	movsd	%xmm4, -80(%rbp)
	movsd	-72(%rbp), %xmm1
	addsd	-288(%rbp), %xmm1
	movsd	%xmm1, -72(%rbp)
	movsd	-56(%rbp), %xmm3
	addsd	-296(%rbp), %xmm3
	movsd	%xmm3, -56(%rbp)
	movsd	-64(%rbp), %xmm3
	addsd	-304(%rbp), %xmm3
	movsd	%xmm3, -64(%rbp)
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	movapd	%xmm7, %xmm6
	addsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm8, %xmm3
	addsd	%xmm8, %xmm3
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm4
	addsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	movapd	%xmm12, %xmm4
	addsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	movapd	%xmm13, %xmm4
	addsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	movapd	%xmm14, %xmm6
	addsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	movapd	%xmm15, %xmm1
	addsd	%xmm15, %xmm1
	movapd	%xmm1, %xmm15
	movsd	-80(%rbp), %xmm4
	movapd	%xmm4, %xmm8
	addsd	%xmm4, %xmm8
	movsd	-72(%rbp), %xmm4
	movapd	%xmm4, %xmm1
	addsd	%xmm4, %xmm1
	movsd	%xmm1, -72(%rbp)
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm4
	addsd	%xmm6, %xmm4
	movsd	%xmm4, -56(%rbp)
	movsd	-64(%rbp), %xmm6
	movapd	%xmm6, %xmm4
	addsd	%xmm6, %xmm4
	movsd	%xmm4, -64(%rbp)
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm5, %xmm4
	addsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm5
	addsd	%xmm7, %xmm5
	movapd	%xmm5, %xmm7
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm3, %xmm1
	addsd	%xmm3, %xmm1
	movapd	%xmm1, %xmm3
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm5
	addsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	movapd	%xmm11, %xmm5
	addsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	movapd	%xmm12, %xmm5
	addsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	movapd	%xmm13, %xmm5
	addsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	movapd	%xmm14, %xmm1
	addsd	%xmm14, %xmm1
	movapd	%xmm1, %xmm14
	movapd	%xmm15, %xmm5
	addsd	%xmm15, %xmm5
	movapd	%xmm5, %xmm15
	movapd	%xmm8, %xmm1
	addsd	%xmm8, %xmm1
	movapd	%xmm1, %xmm8
	movsd	-72(%rbp), %xmm1
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movsd	%xmm5, -72(%rbp)
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm1
	addsd	%xmm6, %xmm1
	movsd	%xmm1, -56(%rbp)
	movsd	-64(%rbp), %xmm1
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movsd	%xmm5, -64(%rbp)
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm7, %xmm5
	addsd	%xmm7, %xmm5
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm3, %xmm1
	addsd	%xmm3, %xmm1
	movapd	%xmm9, %xmm6
	addsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movapd	%xmm10, %xmm3
	addsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	movapd	%xmm11, %xmm3
	addsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	movapd	%xmm12, %xmm3
	addsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	movapd	%xmm13, %xmm3
	addsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	movapd	%xmm14, %xmm3
	addsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm14
	movapd	%xmm15, %xmm3
	addsd	%xmm15, %xmm3
	movapd	%xmm3, %xmm15
	movapd	%xmm8, %xmm3
	addsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	movsd	-72(%rbp), %xmm7
	movapd	%xmm7, %xmm3
	addsd	%xmm7, %xmm3
	movsd	%xmm3, -72(%rbp)
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm7
	addsd	%xmm6, %xmm7
	movsd	%xmm7, -56(%rbp)
	movsd	-64(%rbp), %xmm7
	movapd	%xmm7, %xmm6
	addsd	%xmm7, %xmm6
	movsd	%xmm6, -64(%rbp)
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm4, %xmm7
	addsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movapd	%xmm0, %xmm3
	addsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm0
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movapd	%xmm9, %xmm3
	addsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm3
	addsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	movapd	%xmm12, %xmm7
	addsd	%xmm12, %xmm7
	movapd	%xmm7, %xmm12
	movapd	%xmm13, %xmm3
	addsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	movapd	%xmm14, %xmm3
	addsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm14
	movapd	%xmm15, %xmm1
	addsd	%xmm15, %xmm1
	movapd	%xmm1, %xmm15
	movapd	%xmm8, %xmm3
	addsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	movsd	-72(%rbp), %xmm3
	movapd	%xmm3, %xmm1
	addsd	%xmm3, %xmm1
	movsd	%xmm1, -72(%rbp)
	movsd	-56(%rbp), %xmm1
	movapd	%xmm1, %xmm7
	addsd	%xmm1, %xmm7
	movsd	%xmm7, -56(%rbp)
	movsd	-64(%rbp), %xmm1
	movapd	%xmm1, %xmm3
	addsd	%xmm1, %xmm3
	movsd	%xmm3, -64(%rbp)
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	movapd	%xmm6, %xmm7
	addsd	%xmm6, %xmm7
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm5, %xmm1
	addsd	%xmm5, %xmm1
	movapd	%xmm9, %xmm5
	addsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm6
	addsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm12, %xmm6
	addsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	movapd	%xmm13, %xmm6
	addsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	movapd	%xmm14, %xmm5
	addsd	%xmm14, %xmm5
	movapd	%xmm5, %xmm14
	movapd	%xmm15, %xmm6
	addsd	%xmm15, %xmm6
	movapd	%xmm6, %xmm15
	movapd	%xmm8, %xmm5
	addsd	%xmm8, %xmm5
	movapd	%xmm5, %xmm8
	movsd	-72(%rbp), %xmm3
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movsd	%xmm6, -72(%rbp)
	movsd	-56(%rbp), %xmm5
	movapd	%xmm5, %xmm3
	addsd	%xmm5, %xmm3
	movsd	%xmm3, -56(%rbp)
	movsd	-64(%rbp), %xmm3
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movsd	%xmm5, -64(%rbp)
	movapd	%xmm2, %xmm3
	addsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm4, %xmm3
	addsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm6
	addsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	movapd	%xmm0, %xmm4
	addsd	%xmm0, %xmm4
	movapd	%xmm4, %xmm0
	movapd	%xmm1, %xmm4
	addsd	%xmm1, %xmm4
	movapd	%xmm4, %xmm1
	movapd	%xmm9, %xmm5
	addsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	movapd	%xmm10, %xmm5
	addsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	movapd	%xmm11, %xmm4
	addsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	movapd	%xmm12, %xmm4
	addsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	movapd	%xmm13, %xmm4
	addsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	movapd	%xmm14, %xmm4
	addsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	movapd	%xmm15, %xmm4
	addsd	%xmm15, %xmm4
	movapd	%xmm4, %xmm15
	movapd	%xmm8, %xmm4
	addsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	movsd	-72(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	addsd	%xmm6, %xmm5
	movsd	%xmm5, -72(%rbp)
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm4
	addsd	%xmm6, %xmm4
	movsd	%xmm4, -56(%rbp)
	movsd	-64(%rbp), %xmm4
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, -64(%rbp)
	movapd	%xmm2, %xmm4
	addsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm7, %xmm6
	addsd	%xmm7, %xmm6
	movapd	%xmm0, %xmm4
	addsd	%xmm0, %xmm4
	movapd	%xmm4, %xmm0
	movapd	%xmm1, %xmm4
	addsd	%xmm1, %xmm4
	movapd	%xmm9, %xmm5
	addsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm7
	addsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	movapd	%xmm12, %xmm7
	addsd	%xmm12, %xmm7
	movapd	%xmm7, %xmm12
	movapd	%xmm13, %xmm7
	addsd	%xmm13, %xmm7
	movapd	%xmm7, %xmm13
	movapd	%xmm14, %xmm7
	addsd	%xmm14, %xmm7
	movapd	%xmm7, %xmm14
	movapd	%xmm15, %xmm7
	addsd	%xmm15, %xmm7
	movapd	%xmm7, %xmm15
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movsd	-72(%rbp), %xmm5
	movapd	%xmm5, %xmm7
	addsd	%xmm5, %xmm7
	movsd	%xmm7, -72(%rbp)
	movsd	-56(%rbp), %xmm7
	movapd	%xmm7, %xmm1
	addsd	%xmm7, %xmm1
	movsd	%xmm1, -56(%rbp)
	movsd	-64(%rbp), %xmm5
	movapd	%xmm5, %xmm7
	addsd	%xmm5, %xmm7
	movsd	%xmm7, -64(%rbp)
	movapd	%xmm2, %xmm5
	addsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm3, %xmm7
	addsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	movapd	%xmm6, %xmm7
	addsd	%xmm6, %xmm7
	movapd	%xmm0, %xmm6
	addsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm9, %xmm1
	addsd	%xmm9, %xmm1
	movapd	%xmm1, %xmm9
	movapd	%xmm10, %xmm6
	addsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	movapd	%xmm11, %xmm5
	addsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	movapd	%xmm12, %xmm5
	addsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	movapd	%xmm13, %xmm5
	addsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	movapd	%xmm14, %xmm6
	addsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	movapd	%xmm15, %xmm5
	addsd	%xmm15, %xmm5
	movapd	%xmm5, %xmm15
	movapd	%xmm8, %xmm6
	addsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	movsd	-72(%rbp), %xmm5
	movapd	%xmm5, %xmm6
	addsd	%xmm5, %xmm6
	movsd	%xmm6, -72(%rbp)
	movsd	-56(%rbp), %xmm1
	movapd	%xmm1, %xmm5
	addsd	%xmm1, %xmm5
	movsd	%xmm5, -56(%rbp)
	movsd	-64(%rbp), %xmm5
	movapd	%xmm5, %xmm1
	addsd	%xmm5, %xmm1
	movsd	%xmm1, -64(%rbp)
	movapd	%xmm2, %xmm6
	addsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm3, %xmm5
	addsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movapd	%xmm7, %xmm5
	addsd	%xmm7, %xmm5
	movapd	%xmm0, %xmm1
	addsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm9, %xmm1
	addsd	%xmm9, %xmm1
	movapd	%xmm10, %xmm7
	addsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	movapd	%xmm11, %xmm6
	addsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm12, %xmm6
	addsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	movapd	%xmm13, %xmm7
	addsd	%xmm13, %xmm7
	movapd	%xmm7, %xmm13
	movapd	%xmm14, %xmm7
	addsd	%xmm14, %xmm7
	movapd	%xmm7, %xmm14
	movapd	%xmm15, %xmm6
	addsd	%xmm15, %xmm6
	movapd	%xmm6, %xmm15
	movapd	%xmm8, %xmm7
	addsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	movsd	-72(%rbp), %xmm6
	movapd	%xmm6, %xmm9
	addsd	%xmm6, %xmm9
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm7
	addsd	%xmm6, %xmm7
	movsd	%xmm7, -56(%rbp)
	movsd	-64(%rbp), %xmm7
	movapd	%xmm7, %xmm6
	addsd	%xmm7, %xmm6
	movsd	%xmm6, -64(%rbp)
	movapd	%xmm2, %xmm7
	addsd	%xmm2, %xmm7
	movsd	%xmm7, -88(%rbp)
	movapd	%xmm3, %xmm6
	addsd	%xmm3, %xmm6
	movsd	%xmm6, -96(%rbp)
	movapd	%xmm5, %xmm7
	addsd	%xmm5, %xmm7
	movsd	%xmm7, -104(%rbp)
	movapd	%xmm0, %xmm3
	addsd	%xmm0, %xmm3
	movsd	%xmm3, -112(%rbp)
	movapd	%xmm4, %xmm6
	addsd	%xmm4, %xmm6
	movsd	%xmm6, -120(%rbp)
	movapd	%xmm1, %xmm6
	addsd	%xmm1, %xmm6
	movsd	%xmm6, -128(%rbp)
	movapd	%xmm10, %xmm3
	addsd	%xmm10, %xmm3
	movsd	%xmm3, -136(%rbp)
	movapd	%xmm11, %xmm2
	addsd	%xmm11, %xmm2
	movsd	%xmm2, -144(%rbp)
	movapd	%xmm12, %xmm2
	addsd	%xmm12, %xmm2
	movsd	%xmm2, -152(%rbp)
	movapd	%xmm13, %xmm2
	addsd	%xmm13, %xmm2
	movsd	%xmm2, -160(%rbp)
	movapd	%xmm14, %xmm2
	addsd	%xmm14, %xmm2
	movsd	%xmm2, -168(%rbp)
	movapd	%xmm15, %xmm2
	addsd	%xmm15, %xmm2
	movsd	%xmm2, -176(%rbp)
	movapd	%xmm8, %xmm2
	addsd	%xmm8, %xmm2
	movsd	%xmm2, -80(%rbp)
	movapd	%xmm9, %xmm2
	addsd	%xmm9, %xmm2
	movsd	%xmm2, -72(%rbp)
	movsd	-56(%rbp), %xmm7
	movapd	%xmm7, %xmm2
	addsd	%xmm7, %xmm2
	movsd	%xmm2, -56(%rbp)
	movsd	-64(%rbp), %xmm6
	movapd	%xmm6, %xmm2
	addsd	%xmm6, %xmm2
	movsd	%xmm2, -64(%rbp)
.L688:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L689
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE232:
	.size	double_add_15, .-double_add_15
	.globl	double_add_benchmarks
	.section	.data.rel.local
	.align 32
	.type	double_add_benchmarks, @object
	.size	double_add_benchmarks, 128
double_add_benchmarks:
	.quad	double_add_0
	.quad	double_add_1
	.quad	double_add_2
	.quad	double_add_3
	.quad	double_add_4
	.quad	double_add_5
	.quad	double_add_6
	.quad	double_add_7
	.quad	double_add_8
	.quad	double_add_9
	.quad	double_add_10
	.quad	double_add_11
	.quad	double_add_12
	.quad	double_add_13
	.quad	double_add_14
	.quad	double_add_15
	.text
	.globl	double_mul_0
	.type	double_mul_0, @function
double_mul_0:
.LFB233:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm7
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -56(%rbp)
	jmp	.L691
.L692:
	movapd	%xmm7, %xmm6
	movapd	%xmm6, %xmm2
	mulsd	%xmm6, %xmm2
	movsd	-56(%rbp), %xmm0
	mulsd	%xmm0, %xmm2
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm3, %xmm4
	mulsd	%xmm0, %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm4, %xmm5
	mulsd	%xmm0, %xmm5
	movapd	%xmm5, %xmm6
	mulsd	%xmm5, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm7, %xmm2
	mulsd	%xmm0, %xmm2
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	mulsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm3, %xmm4
	mulsd	%xmm0, %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm4, %xmm5
	mulsd	%xmm0, %xmm5
	movapd	%xmm5, %xmm7
.L691:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L692
	cvttsd2sil	%xmm7, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE233:
	.size	double_mul_0, .-double_mul_0
	.globl	double_mul_1
	.type	double_mul_1, @function
double_mul_1:
.LFB234:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm5
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -72(%rbp)
	jmp	.L694
.L695:
	movapd	%xmm5, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm7, %xmm2
	movsd	-64(%rbp), %xmm6
	mulsd	%xmm6, %xmm2
	movsd	-56(%rbp), %xmm1
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movsd	-72(%rbp), %xmm0
	mulsd	%xmm0, %xmm5
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm6, %xmm1
	mulsd	%xmm1, %xmm2
	movapd	%xmm5, %xmm7
	mulsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	mulsd	%xmm0, %xmm5
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm6, %xmm1
	mulsd	%xmm1, %xmm2
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	%xmm0, %xmm4
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm1, %xmm2
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	%xmm0, %xmm4
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	mulsd	%xmm1, %xmm2
	movapd	%xmm4, %xmm7
	mulsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm0, %xmm6
	mulsd	%xmm6, %xmm4
	movapd	%xmm2, %xmm7
	mulsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm1, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	movapd	%xmm0, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm7, %xmm4
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm1, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm6, %xmm7
	mulsd	%xmm7, %xmm3
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm1, %xmm2
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm6, %xmm3
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm1, %xmm2
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	movapd	%xmm6, %xmm7
	mulsd	%xmm7, %xmm3
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm5
	movapd	%xmm3, %xmm0
	mulsd	%xmm3, %xmm0
	mulsd	%xmm7, %xmm0
	movsd	%xmm0, -56(%rbp)
.L694:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L695
	cvttsd2sil	%xmm5, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE234:
	.size	double_mul_1, .-double_mul_1
	.globl	double_mul_2
	.type	double_mul_2, @function
double_mul_2:
.LFB235:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm6
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -88(%rbp)
	jmp	.L697
.L698:
	movapd	%xmm6, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm7, %xmm2
	movsd	-72(%rbp), %xmm1
	mulsd	%xmm1, %xmm2
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	mulsd	%xmm6, %xmm5
	movsd	-80(%rbp), %xmm4
	mulsd	%xmm4, %xmm5
	movsd	-64(%rbp), %xmm6
	movapd	%xmm6, %xmm0
	mulsd	%xmm6, %xmm0
	movsd	-88(%rbp), %xmm8
	mulsd	%xmm8, %xmm0
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm1, %xmm6
	mulsd	%xmm6, %xmm2
	movapd	%xmm5, %xmm7
	mulsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	movapd	%xmm4, %xmm3
	mulsd	%xmm3, %xmm5
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	%xmm8, %xmm0
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm1, %xmm6
	mulsd	%xmm6, %xmm2
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	%xmm3, %xmm4
	movapd	%xmm0, %xmm1
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	mulsd	%xmm8, %xmm0
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm6, %xmm5
	mulsd	%xmm5, %xmm2
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movapd	%xmm3, %xmm1
	mulsd	%xmm1, %xmm4
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	%xmm8, %xmm0
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	mulsd	%xmm5, %xmm2
	movapd	%xmm4, %xmm7
	mulsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	movapd	%xmm3, %xmm1
	mulsd	%xmm1, %xmm4
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	%xmm8, %xmm0
	movapd	%xmm2, %xmm7
	mulsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm5, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm1, %xmm4
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	mulsd	%xmm8, %xmm7
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm5, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm1, %xmm0
	mulsd	%xmm0, %xmm3
	movapd	%xmm7, %xmm1
	mulsd	%xmm7, %xmm1
	movapd	%xmm1, %xmm7
	mulsd	%xmm8, %xmm7
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm5, %xmm1
	mulsd	%xmm1, %xmm2
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm0, %xmm4
	mulsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm8, %xmm0
	mulsd	%xmm0, %xmm6
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm1, %xmm2
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	movapd	%xmm8, %xmm0
	mulsd	%xmm0, %xmm7
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	%xmm4, %xmm5
	movsd	%xmm5, -56(%rbp)
	movapd	%xmm7, %xmm1
	mulsd	%xmm7, %xmm1
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
.L697:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L698
	cvttsd2sil	%xmm6, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE235:
	.size	double_mul_2, .-double_mul_2
	.globl	double_mul_3
	.type	double_mul_3, @function
double_mul_3:
.LFB236:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm6
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -104(%rbp)
	jmp	.L700
.L701:
	movapd	%xmm6, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm7, %xmm2
	movsd	-80(%rbp), %xmm7
	mulsd	%xmm7, %xmm2
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	mulsd	%xmm6, %xmm5
	movsd	-88(%rbp), %xmm8
	mulsd	%xmm8, %xmm5
	movsd	-64(%rbp), %xmm1
	movapd	%xmm1, %xmm0
	mulsd	%xmm1, %xmm0
	movsd	-96(%rbp), %xmm9
	mulsd	%xmm9, %xmm0
	movsd	-72(%rbp), %xmm6
	movapd	%xmm6, %xmm1
	mulsd	%xmm6, %xmm1
	movsd	-104(%rbp), %xmm10
	mulsd	%xmm10, %xmm1
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm7, %xmm6
	mulsd	%xmm6, %xmm2
	movapd	%xmm5, %xmm7
	mulsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	mulsd	%xmm8, %xmm5
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	%xmm9, %xmm0
	movapd	%xmm1, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm1
	mulsd	%xmm10, %xmm1
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm6, %xmm3
	mulsd	%xmm3, %xmm2
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	%xmm8, %xmm4
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	%xmm9, %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	%xmm10, %xmm1
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm3, %xmm2
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	%xmm8, %xmm4
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	%xmm9, %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm3, %xmm1
	mulsd	%xmm1, %xmm2
	movapd	%xmm4, %xmm7
	mulsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	mulsd	%xmm8, %xmm4
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm5, %xmm3
	mulsd	%xmm5, %xmm3
	movapd	%xmm3, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm2, %xmm7
	mulsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm1, %xmm0
	mulsd	%xmm0, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm8, %xmm4
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm9, %xmm7
	movapd	%xmm5, %xmm3
	mulsd	%xmm5, %xmm3
	movapd	%xmm3, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm1, %xmm0
	mulsd	%xmm0, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm8, %xmm3
	movapd	%xmm7, %xmm1
	mulsd	%xmm7, %xmm1
	movapd	%xmm1, %xmm7
	mulsd	%xmm9, %xmm7
	movapd	%xmm5, %xmm6
	mulsd	%xmm5, %xmm6
	movapd	%xmm6, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm0, %xmm2
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	movapd	%xmm8, %xmm1
	mulsd	%xmm1, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm0, %xmm2
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	movapd	%xmm8, %xmm1
	mulsd	%xmm1, %xmm3
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm9, %xmm7
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	%xmm1, %xmm5
	movsd	%xmm5, -56(%rbp)
	movapd	%xmm7, %xmm1
	mulsd	%xmm7, %xmm1
	mulsd	%xmm9, %xmm1
	movsd	%xmm1, -64(%rbp)
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm10, %xmm3
	movsd	%xmm3, -72(%rbp)
.L700:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L701
	cvttsd2sil	%xmm6, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE236:
	.size	double_mul_3, .-double_mul_3
	.globl	double_mul_4
	.type	double_mul_4, @function
double_mul_4:
.LFB237:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm6
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -120(%rbp)
	jmp	.L703
.L704:
	movapd	%xmm6, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm7, %xmm2
	movsd	-88(%rbp), %xmm8
	mulsd	%xmm8, %xmm2
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	mulsd	%xmm6, %xmm5
	movsd	-96(%rbp), %xmm9
	mulsd	%xmm9, %xmm5
	movsd	-64(%rbp), %xmm6
	movapd	%xmm6, %xmm0
	mulsd	%xmm6, %xmm0
	movsd	-104(%rbp), %xmm10
	mulsd	%xmm10, %xmm0
	movsd	-72(%rbp), %xmm3
	movapd	%xmm3, %xmm1
	mulsd	%xmm3, %xmm1
	movsd	-112(%rbp), %xmm11
	mulsd	%xmm11, %xmm1
	movsd	-80(%rbp), %xmm7
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movsd	-120(%rbp), %xmm12
	mulsd	%xmm12, %xmm6
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm8, %xmm2
	movapd	%xmm5, %xmm7
	mulsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	%xmm10, %xmm0
	movapd	%xmm1, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm1
	mulsd	%xmm11, %xmm1
	movapd	%xmm6, %xmm3
	mulsd	%xmm6, %xmm3
	mulsd	%xmm12, %xmm3
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm8, %xmm2
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	%xmm10, %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	%xmm11, %xmm1
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	mulsd	%xmm12, %xmm3
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm8, %xmm2
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	%xmm10, %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm3, %xmm1
	mulsd	%xmm3, %xmm1
	mulsd	%xmm12, %xmm1
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	mulsd	%xmm8, %xmm2
	movapd	%xmm4, %xmm7
	mulsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm5, %xmm3
	mulsd	%xmm5, %xmm3
	movapd	%xmm3, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm1, %xmm0
	mulsd	%xmm1, %xmm0
	mulsd	%xmm12, %xmm0
	movapd	%xmm2, %xmm7
	mulsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	movapd	%xmm8, %xmm1
	mulsd	%xmm1, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm10, %xmm7
	movapd	%xmm5, %xmm3
	mulsd	%xmm5, %xmm3
	movapd	%xmm3, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	%xmm12, %xmm0
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	movapd	%xmm8, %xmm1
	mulsd	%xmm1, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm9, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm10, %xmm7
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	%xmm12, %xmm0
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm1, %xmm2
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	%xmm9, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	%xmm11, %xmm4
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	%xmm12, %xmm0
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm1, %xmm5
	mulsd	%xmm5, %xmm2
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	mulsd	%xmm9, %xmm3
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm10, %xmm7
	movapd	%xmm4, %xmm1
	mulsd	%xmm4, %xmm1
	movapd	%xmm1, %xmm4
	mulsd	%xmm11, %xmm4
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	%xmm12, %xmm0
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	mulsd	%xmm5, %xmm6
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	%xmm9, %xmm5
	movsd	%xmm5, -56(%rbp)
	movapd	%xmm7, %xmm1
	mulsd	%xmm7, %xmm1
	mulsd	%xmm10, %xmm1
	movsd	%xmm1, -64(%rbp)
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm11, %xmm3
	movsd	%xmm3, -72(%rbp)
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	mulsd	%xmm12, %xmm7
	movsd	%xmm7, -80(%rbp)
.L703:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L704
	cvttsd2sil	%xmm6, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE237:
	.size	double_mul_4, .-double_mul_4
	.globl	double_mul_5
	.type	double_mul_5, @function
double_mul_5:
.LFB238:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm6
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -136(%rbp)
	jmp	.L706
.L707:
	movapd	%xmm6, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm7, %xmm2
	movsd	-96(%rbp), %xmm9
	mulsd	%xmm9, %xmm2
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	mulsd	%xmm6, %xmm5
	movsd	-104(%rbp), %xmm10
	mulsd	%xmm10, %xmm5
	movsd	-64(%rbp), %xmm1
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	movsd	-112(%rbp), %xmm11
	mulsd	%xmm11, %xmm6
	movsd	-72(%rbp), %xmm4
	movapd	%xmm4, %xmm1
	mulsd	%xmm4, %xmm1
	movsd	-120(%rbp), %xmm12
	mulsd	%xmm12, %xmm1
	movsd	-80(%rbp), %xmm4
	movapd	%xmm4, %xmm0
	mulsd	%xmm4, %xmm0
	movsd	-128(%rbp), %xmm13
	mulsd	%xmm13, %xmm0
	movsd	-88(%rbp), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm7, %xmm8
	movsd	-136(%rbp), %xmm14
	mulsd	%xmm14, %xmm8
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm9, %xmm2
	movapd	%xmm5, %xmm7
	mulsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm11, %xmm7
	movapd	%xmm1, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm1
	mulsd	%xmm12, %xmm1
	movapd	%xmm0, %xmm3
	mulsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm0
	mulsd	%xmm13, %xmm0
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	%xmm14, %xmm8
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm9, %xmm2
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm11, %xmm7
	movapd	%xmm1, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm1
	mulsd	%xmm12, %xmm1
	movapd	%xmm0, %xmm5
	mulsd	%xmm0, %xmm5
	movapd	%xmm5, %xmm0
	mulsd	%xmm13, %xmm0
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	%xmm14, %xmm8
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm9, %xmm2
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm7, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm1, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm3, %xmm1
	mulsd	%xmm12, %xmm1
	movapd	%xmm0, %xmm3
	mulsd	%xmm0, %xmm3
	mulsd	%xmm13, %xmm3
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	%xmm14, %xmm8
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	mulsd	%xmm9, %xmm2
	movapd	%xmm4, %xmm7
	mulsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm5, %xmm6
	mulsd	%xmm5, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm1, %xmm0
	mulsd	%xmm1, %xmm0
	movapd	%xmm0, %xmm1
	mulsd	%xmm12, %xmm1
	movapd	%xmm3, %xmm0
	mulsd	%xmm3, %xmm0
	mulsd	%xmm13, %xmm0
	movapd	%xmm8, %xmm7
	mulsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm14, %xmm8
	movapd	%xmm2, %xmm7
	mulsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm9, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm11, %xmm7
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	%xmm12, %xmm1
	movapd	%xmm0, %xmm5
	mulsd	%xmm0, %xmm5
	mulsd	%xmm13, %xmm5
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	%xmm14, %xmm8
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm9, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm10, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm11, %xmm7
	movapd	%xmm1, %xmm4
	mulsd	%xmm1, %xmm4
	movapd	%xmm4, %xmm1
	mulsd	%xmm12, %xmm1
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm13, %xmm5
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	%xmm14, %xmm8
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm9, %xmm2
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	%xmm10, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm1, %xmm0
	mulsd	%xmm1, %xmm0
	movapd	%xmm0, %xmm1
	mulsd	%xmm12, %xmm1
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	%xmm13, %xmm4
	movapd	%xmm8, %xmm7
	mulsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm14, %xmm8
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	movapd	%xmm9, %xmm0
	mulsd	%xmm0, %xmm2
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	mulsd	%xmm10, %xmm3
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm11, %xmm7
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	%xmm12, %xmm1
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	%xmm13, %xmm4
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	%xmm14, %xmm8
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	movapd	%xmm9, %xmm0
	mulsd	%xmm0, %xmm6
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	%xmm10, %xmm5
	movsd	%xmm5, -56(%rbp)
	movapd	%xmm7, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	%xmm11, %xmm0
	movsd	%xmm0, -64(%rbp)
	movapd	%xmm1, %xmm3
	mulsd	%xmm1, %xmm3
	mulsd	%xmm12, %xmm3
	movsd	%xmm3, -72(%rbp)
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm13, %xmm3
	movsd	%xmm3, -80(%rbp)
	movapd	%xmm8, %xmm3
	mulsd	%xmm8, %xmm3
	mulsd	%xmm14, %xmm3
	movsd	%xmm3, -88(%rbp)
.L706:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L707
	cvttsd2sil	%xmm6, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE238:
	.size	double_mul_5, .-double_mul_5
	.globl	double_mul_6
	.type	double_mul_6, @function
double_mul_6:
.LFB239:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm2
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -56(%rbp)
	jmp	.L709
.L710:
	movapd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm7, %xmm2
	movsd	-112(%rbp), %xmm10
	mulsd	%xmm10, %xmm2
	movsd	-64(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	mulsd	%xmm6, %xmm5
	movsd	-120(%rbp), %xmm11
	mulsd	%xmm11, %xmm5
	movsd	-72(%rbp), %xmm7
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movsd	-128(%rbp), %xmm12
	mulsd	%xmm12, %xmm6
	movsd	-80(%rbp), %xmm3
	movapd	%xmm3, %xmm0
	mulsd	%xmm3, %xmm0
	movsd	-136(%rbp), %xmm13
	mulsd	%xmm13, %xmm0
	movsd	-88(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm3, %xmm4
	movsd	-144(%rbp), %xmm14
	mulsd	%xmm14, %xmm4
	movsd	-96(%rbp), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm7, %xmm8
	movsd	-152(%rbp), %xmm15
	mulsd	%xmm15, %xmm8
	movsd	-104(%rbp), %xmm7
	movapd	%xmm7, %xmm9
	mulsd	%xmm7, %xmm9
	mulsd	-56(%rbp), %xmm9
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm10, %xmm2
	movapd	%xmm5, %xmm7
	mulsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm12, %xmm7
	movapd	%xmm0, %xmm1
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	mulsd	%xmm13, %xmm0
	movapd	%xmm4, %xmm1
	mulsd	%xmm4, %xmm1
	mulsd	%xmm14, %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	%xmm15, %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-56(%rbp), %xmm9
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm10, %xmm2
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	%xmm11, %xmm4
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm12, %xmm7
	movapd	%xmm0, %xmm3
	mulsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm0
	mulsd	%xmm13, %xmm0
	movapd	%xmm1, %xmm3
	mulsd	%xmm1, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	%xmm15, %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-56(%rbp), %xmm9
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm10, %xmm2
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	%xmm11, %xmm4
	movapd	%xmm7, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	%xmm12, %xmm5
	movapd	%xmm0, %xmm1
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	mulsd	%xmm13, %xmm0
	movapd	%xmm3, %xmm1
	mulsd	%xmm3, %xmm1
	mulsd	%xmm14, %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	%xmm15, %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	movsd	-56(%rbp), %xmm3
	mulsd	%xmm3, %xmm9
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	mulsd	%xmm10, %xmm2
	movapd	%xmm4, %xmm7
	mulsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	mulsd	%xmm11, %xmm4
	movapd	%xmm5, %xmm6
	mulsd	%xmm5, %xmm6
	mulsd	%xmm12, %xmm6
	movapd	%xmm0, %xmm5
	mulsd	%xmm0, %xmm5
	movapd	%xmm5, %xmm0
	mulsd	%xmm13, %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	%xmm14, %xmm1
	movapd	%xmm8, %xmm5
	mulsd	%xmm8, %xmm5
	movapd	%xmm5, %xmm8
	mulsd	%xmm15, %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	movsd	%xmm3, -56(%rbp)
	mulsd	%xmm3, %xmm9
	movapd	%xmm2, %xmm7
	mulsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm10, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm11, %xmm4
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm12, %xmm7
	movapd	%xmm0, %xmm3
	mulsd	%xmm0, %xmm3
	movapd	%xmm3, %xmm0
	mulsd	%xmm13, %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	%xmm14, %xmm1
	movapd	%xmm8, %xmm5
	mulsd	%xmm8, %xmm5
	movapd	%xmm5, %xmm8
	mulsd	%xmm15, %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-56(%rbp), %xmm9
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm10, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm11, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm12, %xmm7
	movapd	%xmm0, %xmm4
	mulsd	%xmm0, %xmm4
	movapd	%xmm4, %xmm0
	mulsd	%xmm13, %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	%xmm14, %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	%xmm15, %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-56(%rbp), %xmm9
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm10, %xmm2
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	%xmm11, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	%xmm12, %xmm6
	movapd	%xmm0, %xmm5
	mulsd	%xmm0, %xmm5
	movapd	%xmm5, %xmm0
	mulsd	%xmm13, %xmm0
	movapd	%xmm1, %xmm4
	mulsd	%xmm1, %xmm4
	movapd	%xmm4, %xmm1
	mulsd	%xmm14, %xmm1
	movapd	%xmm8, %xmm5
	mulsd	%xmm8, %xmm5
	movapd	%xmm5, %xmm8
	mulsd	%xmm15, %xmm8
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	-56(%rbp), %xmm9
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm10, %xmm2
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	mulsd	%xmm11, %xmm3
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm12, %xmm7
	movapd	%xmm0, %xmm4
	mulsd	%xmm0, %xmm4
	movapd	%xmm4, %xmm0
	mulsd	%xmm13, %xmm0
	movapd	%xmm1, %xmm4
	mulsd	%xmm1, %xmm4
	mulsd	%xmm14, %xmm4
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	%xmm15, %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-56(%rbp), %xmm9
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm2
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	%xmm11, %xmm5
	movsd	%xmm5, -64(%rbp)
	movapd	%xmm7, %xmm3
	mulsd	%xmm7, %xmm3
	mulsd	%xmm12, %xmm3
	movsd	%xmm3, -72(%rbp)
	movapd	%xmm0, %xmm1
	mulsd	%xmm0, %xmm1
	mulsd	%xmm13, %xmm1
	movsd	%xmm1, -80(%rbp)
	movapd	%xmm4, %xmm5
	mulsd	%xmm4, %xmm5
	mulsd	%xmm14, %xmm5
	movsd	%xmm5, -88(%rbp)
	movapd	%xmm8, %xmm3
	mulsd	%xmm8, %xmm3
	mulsd	%xmm15, %xmm3
	movsd	%xmm3, -96(%rbp)
	movapd	%xmm9, %xmm3
	mulsd	%xmm9, %xmm3
	mulsd	-56(%rbp), %xmm3
	movsd	%xmm3, -104(%rbp)
.L709:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L710
	cvttsd2sil	%xmm2, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE239:
	.size	double_mul_6, .-double_mul_6
	.globl	double_mul_7
	.type	double_mul_7, @function
double_mul_7:
.LFB240:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm11
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -56(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -64(%rbp)
	jmp	.L712
.L713:
	movapd	%xmm11, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm7, %xmm2
	movsd	-136(%rbp), %xmm11
	mulsd	%xmm11, %xmm2
	movsd	-80(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	mulsd	%xmm6, %xmm5
	movsd	-144(%rbp), %xmm12
	mulsd	%xmm12, %xmm5
	movsd	-88(%rbp), %xmm7
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movsd	-152(%rbp), %xmm13
	mulsd	%xmm13, %xmm6
	movsd	-96(%rbp), %xmm0
	movapd	%xmm0, %xmm4
	mulsd	%xmm0, %xmm4
	movsd	-160(%rbp), %xmm14
	mulsd	%xmm14, %xmm4
	movsd	-104(%rbp), %xmm7
	movapd	%xmm7, %xmm1
	mulsd	%xmm7, %xmm1
	movsd	-168(%rbp), %xmm15
	mulsd	%xmm15, %xmm1
	movsd	-112(%rbp), %xmm3
	movapd	%xmm3, %xmm8
	mulsd	%xmm3, %xmm8
	movsd	-72(%rbp), %xmm0
	mulsd	%xmm0, %xmm8
	movsd	-120(%rbp), %xmm3
	movapd	%xmm3, %xmm9
	mulsd	%xmm3, %xmm9
	mulsd	-56(%rbp), %xmm9
	movsd	-128(%rbp), %xmm7
	movapd	%xmm7, %xmm10
	mulsd	%xmm7, %xmm10
	mulsd	-64(%rbp), %xmm10
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm11, %xmm2
	movapd	%xmm5, %xmm7
	mulsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	mulsd	%xmm12, %xmm5
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm13, %xmm7
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	mulsd	%xmm15, %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	%xmm0, %xmm8
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	-56(%rbp), %xmm9
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	mulsd	-64(%rbp), %xmm10
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm11, %xmm2
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	%xmm12, %xmm4
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm13, %xmm7
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	mulsd	%xmm15, %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	movsd	%xmm0, -72(%rbp)
	mulsd	%xmm0, %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-56(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-64(%rbp), %xmm10
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm11, %xmm2
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	%xmm12, %xmm4
	movapd	%xmm7, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	%xmm13, %xmm5
	movapd	%xmm3, %xmm0
	mulsd	%xmm3, %xmm0
	movapd	%xmm0, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm1, %xmm0
	mulsd	%xmm1, %xmm0
	mulsd	%xmm15, %xmm0
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	movsd	-72(%rbp), %xmm1
	mulsd	%xmm1, %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-56(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-64(%rbp), %xmm10
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	mulsd	%xmm11, %xmm2
	movapd	%xmm4, %xmm7
	mulsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	mulsd	%xmm12, %xmm4
	movapd	%xmm5, %xmm6
	mulsd	%xmm5, %xmm6
	mulsd	%xmm13, %xmm6
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	%xmm14, %xmm5
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	%xmm15, %xmm0
	movapd	%xmm8, %xmm3
	mulsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	movsd	%xmm1, -72(%rbp)
	mulsd	%xmm1, %xmm8
	movapd	%xmm9, %xmm3
	mulsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	mulsd	-56(%rbp), %xmm9
	movapd	%xmm10, %xmm3
	mulsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	mulsd	-64(%rbp), %xmm10
	movapd	%xmm2, %xmm7
	mulsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm11, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm12, %xmm4
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm13, %xmm7
	movapd	%xmm5, %xmm1
	mulsd	%xmm5, %xmm1
	movapd	%xmm1, %xmm5
	mulsd	%xmm14, %xmm5
	movapd	%xmm0, %xmm1
	mulsd	%xmm0, %xmm1
	mulsd	%xmm15, %xmm1
	movapd	%xmm8, %xmm3
	mulsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	movsd	-72(%rbp), %xmm0
	mulsd	%xmm0, %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-56(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-64(%rbp), %xmm10
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm11, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm12, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm13, %xmm7
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm14, %xmm5
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	mulsd	%xmm15, %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	movsd	%xmm0, -72(%rbp)
	mulsd	%xmm0, %xmm8
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	-56(%rbp), %xmm9
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	mulsd	-64(%rbp), %xmm10
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm11, %xmm2
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	%xmm12, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	%xmm13, %xmm6
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	%xmm14, %xmm4
	movapd	%xmm1, %xmm0
	mulsd	%xmm1, %xmm0
	movapd	%xmm0, %xmm1
	mulsd	%xmm15, %xmm1
	movapd	%xmm8, %xmm5
	mulsd	%xmm8, %xmm5
	movapd	%xmm5, %xmm8
	mulsd	-72(%rbp), %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-56(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-64(%rbp), %xmm10
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm11, %xmm2
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	mulsd	%xmm12, %xmm3
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm13, %xmm7
	movapd	%xmm4, %xmm0
	mulsd	%xmm4, %xmm0
	movapd	%xmm0, %xmm4
	mulsd	%xmm14, %xmm4
	movapd	%xmm1, %xmm0
	mulsd	%xmm1, %xmm0
	mulsd	%xmm15, %xmm0
	movapd	%xmm8, %xmm5
	mulsd	%xmm8, %xmm5
	movapd	%xmm5, %xmm8
	mulsd	-72(%rbp), %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-56(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-64(%rbp), %xmm10
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	%xmm12, %xmm5
	movsd	%xmm5, -80(%rbp)
	movapd	%xmm7, %xmm3
	mulsd	%xmm7, %xmm3
	mulsd	%xmm13, %xmm3
	movsd	%xmm3, -88(%rbp)
	movapd	%xmm4, %xmm1
	mulsd	%xmm4, %xmm1
	mulsd	%xmm14, %xmm1
	movsd	%xmm1, -96(%rbp)
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	mulsd	%xmm15, %xmm6
	movsd	%xmm6, -104(%rbp)
	movapd	%xmm8, %xmm7
	mulsd	%xmm8, %xmm7
	mulsd	-72(%rbp), %xmm7
	movsd	%xmm7, -112(%rbp)
	movapd	%xmm9, %xmm7
	mulsd	%xmm9, %xmm7
	mulsd	-56(%rbp), %xmm7
	movsd	%xmm7, -120(%rbp)
	movapd	%xmm10, %xmm2
	mulsd	%xmm10, %xmm2
	mulsd	-64(%rbp), %xmm2
	movsd	%xmm2, -128(%rbp)
.L712:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L713
	cvttsd2sil	%xmm11, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE240:
	.size	double_mul_7, .-double_mul_7
	.globl	double_mul_8
	.type	double_mul_8, @function
double_mul_8:
.LFB241:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$184, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm12
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -176(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -184(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -56(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -80(%rbp)
	jmp	.L715
.L716:
	movapd	%xmm12, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm7, %xmm2
	movsd	-160(%rbp), %xmm12
	mulsd	%xmm12, %xmm2
	movsd	-96(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	mulsd	%xmm6, %xmm5
	movsd	-168(%rbp), %xmm13
	mulsd	%xmm13, %xmm5
	movsd	-104(%rbp), %xmm7
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movsd	-176(%rbp), %xmm14
	mulsd	%xmm14, %xmm6
	movsd	-112(%rbp), %xmm0
	movapd	%xmm0, %xmm4
	mulsd	%xmm0, %xmm4
	movsd	-184(%rbp), %xmm15
	mulsd	%xmm15, %xmm4
	movsd	-120(%rbp), %xmm7
	movapd	%xmm7, %xmm1
	mulsd	%xmm7, %xmm1
	movsd	-88(%rbp), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	-128(%rbp), %xmm3
	movapd	%xmm3, %xmm8
	mulsd	%xmm3, %xmm8
	mulsd	-56(%rbp), %xmm8
	movsd	-136(%rbp), %xmm3
	movapd	%xmm3, %xmm9
	mulsd	%xmm3, %xmm9
	mulsd	-64(%rbp), %xmm9
	movsd	-144(%rbp), %xmm7
	movapd	%xmm7, %xmm10
	mulsd	%xmm7, %xmm10
	mulsd	-72(%rbp), %xmm10
	movsd	-152(%rbp), %xmm7
	movapd	%xmm7, %xmm11
	mulsd	%xmm7, %xmm11
	mulsd	-80(%rbp), %xmm11
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm12, %xmm2
	movapd	%xmm5, %xmm7
	mulsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	mulsd	%xmm13, %xmm5
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm14, %xmm7
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm15, %xmm3
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	mulsd	%xmm0, %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-56(%rbp), %xmm8
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	-64(%rbp), %xmm9
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	mulsd	-72(%rbp), %xmm10
	movapd	%xmm11, %xmm4
	mulsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	mulsd	-80(%rbp), %xmm11
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm12, %xmm2
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	%xmm13, %xmm4
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm14, %xmm7
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	mulsd	%xmm15, %xmm3
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movsd	%xmm0, -88(%rbp)
	mulsd	%xmm0, %xmm1
	movapd	%xmm8, %xmm5
	mulsd	%xmm8, %xmm5
	movapd	%xmm5, %xmm8
	mulsd	-56(%rbp), %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-64(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-72(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-80(%rbp), %xmm11
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm12, %xmm2
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	%xmm13, %xmm4
	movapd	%xmm7, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	%xmm14, %xmm5
	movapd	%xmm3, %xmm0
	mulsd	%xmm3, %xmm0
	movapd	%xmm0, %xmm3
	mulsd	%xmm15, %xmm3
	movapd	%xmm1, %xmm0
	mulsd	%xmm1, %xmm0
	movsd	-88(%rbp), %xmm1
	mulsd	%xmm1, %xmm0
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-56(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-64(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-72(%rbp), %xmm10
	movapd	%xmm11, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	mulsd	-80(%rbp), %xmm11
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	mulsd	%xmm12, %xmm2
	movapd	%xmm4, %xmm7
	mulsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	mulsd	%xmm13, %xmm4
	movapd	%xmm5, %xmm6
	mulsd	%xmm5, %xmm6
	mulsd	%xmm14, %xmm6
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	%xmm15, %xmm5
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	movsd	%xmm1, -88(%rbp)
	mulsd	%xmm1, %xmm0
	movapd	%xmm8, %xmm7
	mulsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	mulsd	-56(%rbp), %xmm8
	movapd	%xmm9, %xmm3
	mulsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	mulsd	-64(%rbp), %xmm9
	movapd	%xmm10, %xmm3
	mulsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	mulsd	-72(%rbp), %xmm10
	movapd	%xmm11, %xmm3
	mulsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	mulsd	-80(%rbp), %xmm11
	movapd	%xmm2, %xmm7
	mulsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm12, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm13, %xmm4
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm14, %xmm7
	movapd	%xmm5, %xmm1
	mulsd	%xmm5, %xmm1
	movapd	%xmm1, %xmm5
	mulsd	%xmm15, %xmm5
	movapd	%xmm0, %xmm1
	mulsd	%xmm0, %xmm1
	movsd	-88(%rbp), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-56(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-64(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-72(%rbp), %xmm10
	movapd	%xmm11, %xmm3
	mulsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	mulsd	-80(%rbp), %xmm11
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm12, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm13, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm14, %xmm7
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm15, %xmm5
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movsd	%xmm0, -88(%rbp)
	mulsd	%xmm0, %xmm1
	movapd	%xmm8, %xmm4
	mulsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	mulsd	-56(%rbp), %xmm8
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	-64(%rbp), %xmm9
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	mulsd	-72(%rbp), %xmm10
	movapd	%xmm11, %xmm4
	mulsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	mulsd	-80(%rbp), %xmm11
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm12, %xmm2
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	%xmm13, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	%xmm14, %xmm6
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	%xmm15, %xmm4
	movapd	%xmm1, %xmm0
	mulsd	%xmm1, %xmm0
	movapd	%xmm0, %xmm1
	mulsd	-88(%rbp), %xmm1
	movapd	%xmm8, %xmm0
	mulsd	%xmm8, %xmm0
	mulsd	-56(%rbp), %xmm0
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-64(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-72(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-80(%rbp), %xmm11
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm12, %xmm2
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	mulsd	%xmm13, %xmm3
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm14, %xmm7
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	%xmm15, %xmm4
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	movsd	-88(%rbp), %xmm8
	mulsd	%xmm8, %xmm1
	movapd	%xmm0, %xmm5
	mulsd	%xmm0, %xmm5
	movapd	%xmm5, %xmm0
	mulsd	-56(%rbp), %xmm0
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-64(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-72(%rbp), %xmm10
	movapd	%xmm11, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	mulsd	-80(%rbp), %xmm11
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	mulsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	%xmm13, %xmm5
	movsd	%xmm5, -96(%rbp)
	movapd	%xmm7, %xmm3
	mulsd	%xmm7, %xmm3
	mulsd	%xmm14, %xmm3
	movsd	%xmm3, -104(%rbp)
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm15, %xmm3
	movsd	%xmm3, -112(%rbp)
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	mulsd	%xmm8, %xmm6
	movsd	%xmm6, -120(%rbp)
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	mulsd	-56(%rbp), %xmm7
	movsd	%xmm7, -128(%rbp)
	movapd	%xmm9, %xmm7
	mulsd	%xmm9, %xmm7
	mulsd	-64(%rbp), %xmm7
	movsd	%xmm7, -136(%rbp)
	movapd	%xmm10, %xmm2
	mulsd	%xmm10, %xmm2
	mulsd	-72(%rbp), %xmm2
	movsd	%xmm2, -144(%rbp)
	movapd	%xmm11, %xmm2
	mulsd	%xmm11, %xmm2
	mulsd	-80(%rbp), %xmm2
	movsd	%xmm2, -152(%rbp)
.L715:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L716
	cvttsd2sil	%xmm12, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE241:
	.size	double_mul_8, .-double_mul_8
	.globl	double_mul_9
	.type	double_mul_9, @function
double_mul_9:
.LFB242:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$200, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm13
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -184(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -192(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -200(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -56(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -176(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -96(%rbp)
	jmp	.L718
.L719:
	movapd	%xmm13, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm7, %xmm2
	movsd	-184(%rbp), %xmm13
	mulsd	%xmm13, %xmm2
	movsd	-112(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	mulsd	%xmm6, %xmm5
	movsd	-192(%rbp), %xmm14
	mulsd	%xmm14, %xmm5
	movsd	-120(%rbp), %xmm7
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movsd	-200(%rbp), %xmm15
	mulsd	%xmm15, %xmm6
	movsd	-128(%rbp), %xmm0
	movapd	%xmm0, %xmm4
	mulsd	%xmm0, %xmm4
	mulsd	-104(%rbp), %xmm4
	movsd	-136(%rbp), %xmm0
	movapd	%xmm0, %xmm1
	mulsd	%xmm0, %xmm1
	mulsd	-56(%rbp), %xmm1
	movsd	-144(%rbp), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm7, %xmm8
	mulsd	-64(%rbp), %xmm8
	movsd	-152(%rbp), %xmm7
	movapd	%xmm7, %xmm9
	mulsd	%xmm7, %xmm9
	mulsd	-72(%rbp), %xmm9
	movsd	-160(%rbp), %xmm3
	movapd	%xmm3, %xmm10
	mulsd	%xmm3, %xmm10
	mulsd	-80(%rbp), %xmm10
	movsd	-168(%rbp), %xmm3
	movapd	%xmm3, %xmm11
	mulsd	%xmm3, %xmm11
	mulsd	-88(%rbp), %xmm11
	movsd	-176(%rbp), %xmm3
	movapd	%xmm3, %xmm12
	mulsd	%xmm3, %xmm12
	mulsd	-96(%rbp), %xmm12
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm13, %xmm2
	movapd	%xmm5, %xmm7
	mulsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	mulsd	%xmm14, %xmm5
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm15, %xmm7
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movsd	-104(%rbp), %xmm0
	mulsd	%xmm0, %xmm3
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	mulsd	-56(%rbp), %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-64(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-72(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-80(%rbp), %xmm10
	movapd	%xmm11, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	mulsd	-88(%rbp), %xmm11
	movapd	%xmm12, %xmm4
	mulsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	mulsd	-96(%rbp), %xmm12
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm13, %xmm2
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	%xmm14, %xmm4
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm15, %xmm7
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	movsd	%xmm0, -104(%rbp)
	mulsd	%xmm0, %xmm3
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	mulsd	-56(%rbp), %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-64(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-72(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-80(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-88(%rbp), %xmm11
	movapd	%xmm12, %xmm0
	mulsd	%xmm12, %xmm0
	movapd	%xmm0, %xmm12
	mulsd	-96(%rbp), %xmm12
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm13, %xmm2
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	%xmm14, %xmm4
	movapd	%xmm7, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	%xmm15, %xmm5
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	-104(%rbp), %xmm3
	movapd	%xmm1, %xmm0
	mulsd	%xmm1, %xmm0
	movapd	%xmm0, %xmm1
	mulsd	-56(%rbp), %xmm1
	movapd	%xmm8, %xmm7
	mulsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	mulsd	-64(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-72(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-80(%rbp), %xmm10
	movapd	%xmm11, %xmm7
	mulsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	mulsd	-88(%rbp), %xmm11
	movapd	%xmm12, %xmm7
	mulsd	%xmm12, %xmm7
	movapd	%xmm7, %xmm12
	mulsd	-96(%rbp), %xmm12
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	mulsd	%xmm13, %xmm2
	movapd	%xmm4, %xmm7
	mulsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	mulsd	%xmm14, %xmm4
	movapd	%xmm5, %xmm6
	mulsd	%xmm5, %xmm6
	mulsd	%xmm15, %xmm6
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	-104(%rbp), %xmm5
	movapd	%xmm1, %xmm0
	mulsd	%xmm1, %xmm0
	movapd	%xmm0, %xmm1
	mulsd	-56(%rbp), %xmm1
	movapd	%xmm8, %xmm7
	mulsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	mulsd	-64(%rbp), %xmm8
	movapd	%xmm9, %xmm3
	mulsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	mulsd	-72(%rbp), %xmm9
	movapd	%xmm10, %xmm3
	mulsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	mulsd	-80(%rbp), %xmm10
	movapd	%xmm11, %xmm7
	mulsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	mulsd	-88(%rbp), %xmm11
	movapd	%xmm12, %xmm7
	mulsd	%xmm12, %xmm7
	movapd	%xmm7, %xmm12
	mulsd	-96(%rbp), %xmm12
	movapd	%xmm2, %xmm7
	mulsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm13, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm14, %xmm4
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm15, %xmm7
	movapd	%xmm5, %xmm3
	mulsd	%xmm5, %xmm3
	movapd	%xmm3, %xmm5
	movsd	-104(%rbp), %xmm0
	mulsd	%xmm0, %xmm5
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	mulsd	-56(%rbp), %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-64(%rbp), %xmm8
	movapd	%xmm9, %xmm3
	mulsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	mulsd	-72(%rbp), %xmm9
	movapd	%xmm10, %xmm3
	mulsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	mulsd	-80(%rbp), %xmm10
	movapd	%xmm11, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	mulsd	-88(%rbp), %xmm11
	movapd	%xmm12, %xmm6
	mulsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	mulsd	-96(%rbp), %xmm12
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm13, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm15, %xmm7
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	mulsd	%xmm0, %xmm5
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	mulsd	-56(%rbp), %xmm1
	movapd	%xmm8, %xmm4
	mulsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	mulsd	-64(%rbp), %xmm8
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	-72(%rbp), %xmm9
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	mulsd	-80(%rbp), %xmm10
	movapd	%xmm11, %xmm4
	mulsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	mulsd	-88(%rbp), %xmm11
	movapd	%xmm12, %xmm4
	mulsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	mulsd	-96(%rbp), %xmm12
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm13, %xmm2
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	%xmm15, %xmm6
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	movsd	%xmm0, -104(%rbp)
	mulsd	%xmm0, %xmm4
	movapd	%xmm1, %xmm0
	mulsd	%xmm1, %xmm0
	movapd	%xmm0, %xmm1
	mulsd	-56(%rbp), %xmm1
	movapd	%xmm8, %xmm0
	mulsd	%xmm8, %xmm0
	mulsd	-64(%rbp), %xmm0
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-72(%rbp), %xmm9
	movapd	%xmm10, %xmm7
	mulsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	mulsd	-80(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-88(%rbp), %xmm11
	movapd	%xmm12, %xmm5
	mulsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	mulsd	-96(%rbp), %xmm12
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm13, %xmm2
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm15, %xmm7
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	movsd	-104(%rbp), %xmm8
	mulsd	%xmm8, %xmm4
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	mulsd	-56(%rbp), %xmm1
	movapd	%xmm0, %xmm5
	mulsd	%xmm0, %xmm5
	movapd	%xmm5, %xmm0
	mulsd	-64(%rbp), %xmm0
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-72(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-80(%rbp), %xmm10
	movapd	%xmm11, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	mulsd	-88(%rbp), %xmm11
	movapd	%xmm12, %xmm5
	mulsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	mulsd	-96(%rbp), %xmm12
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	mulsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	%xmm14, %xmm5
	movsd	%xmm5, -112(%rbp)
	movapd	%xmm7, %xmm3
	mulsd	%xmm7, %xmm3
	mulsd	%xmm15, %xmm3
	movsd	%xmm3, -120(%rbp)
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm8, %xmm3
	movsd	%xmm3, -128(%rbp)
	movapd	%xmm1, %xmm7
	mulsd	%xmm1, %xmm7
	mulsd	-56(%rbp), %xmm7
	movsd	%xmm7, -136(%rbp)
	movapd	%xmm0, %xmm5
	mulsd	%xmm0, %xmm5
	mulsd	-64(%rbp), %xmm5
	movsd	%xmm5, -144(%rbp)
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	mulsd	-72(%rbp), %xmm4
	movsd	%xmm4, -152(%rbp)
	movapd	%xmm10, %xmm2
	mulsd	%xmm10, %xmm2
	mulsd	-80(%rbp), %xmm2
	movsd	%xmm2, -160(%rbp)
	movapd	%xmm11, %xmm2
	mulsd	%xmm11, %xmm2
	mulsd	-88(%rbp), %xmm2
	movsd	%xmm2, -168(%rbp)
	movapd	%xmm12, %xmm2
	mulsd	%xmm12, %xmm2
	mulsd	-96(%rbp), %xmm2
	movsd	%xmm2, -176(%rbp)
.L718:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L719
	cvttsd2sil	%xmm13, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE242:
	.size	double_mul_9, .-double_mul_9
	.globl	double_mul_10
	.type	double_mul_10, @function
double_mul_10:
.LFB243:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$216, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm6
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -208(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -216(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -56(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -176(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -184(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -192(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -200(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -112(%rbp)
	jmp	.L721
.L722:
	movapd	%xmm6, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm7, %xmm2
	movsd	-208(%rbp), %xmm14
	mulsd	%xmm14, %xmm2
	movsd	-128(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	mulsd	%xmm6, %xmm5
	movsd	-216(%rbp), %xmm15
	mulsd	%xmm15, %xmm5
	movsd	-136(%rbp), %xmm7
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	-120(%rbp), %xmm6
	movsd	-144(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm3, %xmm4
	mulsd	-56(%rbp), %xmm4
	movsd	-152(%rbp), %xmm7
	movapd	%xmm7, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	-64(%rbp), %xmm0
	movsd	-160(%rbp), %xmm1
	movapd	%xmm1, %xmm8
	mulsd	%xmm1, %xmm8
	mulsd	-72(%rbp), %xmm8
	movsd	-168(%rbp), %xmm7
	movapd	%xmm7, %xmm9
	mulsd	%xmm7, %xmm9
	mulsd	-80(%rbp), %xmm9
	movsd	-176(%rbp), %xmm7
	movapd	%xmm7, %xmm10
	mulsd	%xmm7, %xmm10
	mulsd	-88(%rbp), %xmm10
	movsd	-184(%rbp), %xmm7
	movapd	%xmm7, %xmm11
	mulsd	%xmm7, %xmm11
	mulsd	-96(%rbp), %xmm11
	movsd	-192(%rbp), %xmm7
	movapd	%xmm7, %xmm12
	mulsd	%xmm7, %xmm12
	mulsd	-104(%rbp), %xmm12
	movsd	-200(%rbp), %xmm3
	movapd	%xmm3, %xmm13
	mulsd	%xmm3, %xmm13
	mulsd	-112(%rbp), %xmm13
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm14, %xmm2
	movapd	%xmm5, %xmm7
	mulsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	mulsd	%xmm15, %xmm5
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-120(%rbp), %xmm7
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	-56(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-64(%rbp), %xmm0
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-72(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-80(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-88(%rbp), %xmm10
	movapd	%xmm11, %xmm4
	mulsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	mulsd	-96(%rbp), %xmm11
	movapd	%xmm12, %xmm1
	mulsd	%xmm12, %xmm1
	movapd	%xmm1, %xmm12
	mulsd	-104(%rbp), %xmm12
	movapd	%xmm13, %xmm1
	mulsd	%xmm13, %xmm1
	movapd	%xmm1, %xmm13
	mulsd	-112(%rbp), %xmm13
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm14, %xmm2
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	%xmm15, %xmm4
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	-120(%rbp), %xmm7
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	mulsd	-56(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-64(%rbp), %xmm0
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-72(%rbp), %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-80(%rbp), %xmm9
	movapd	%xmm10, %xmm1
	mulsd	%xmm10, %xmm1
	movapd	%xmm1, %xmm10
	mulsd	-88(%rbp), %xmm10
	movapd	%xmm11, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	mulsd	-96(%rbp), %xmm11
	movapd	%xmm12, %xmm5
	mulsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	mulsd	-104(%rbp), %xmm12
	movapd	%xmm13, %xmm5
	mulsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	mulsd	-112(%rbp), %xmm13
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm14, %xmm2
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	%xmm15, %xmm4
	movapd	%xmm7, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	-120(%rbp), %xmm5
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	-56(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-64(%rbp), %xmm0
	movapd	%xmm8, %xmm7
	mulsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	mulsd	-72(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-80(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-88(%rbp), %xmm10
	movapd	%xmm11, %xmm1
	mulsd	%xmm11, %xmm1
	movapd	%xmm1, %xmm11
	mulsd	-96(%rbp), %xmm11
	movapd	%xmm12, %xmm6
	mulsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	mulsd	-104(%rbp), %xmm12
	movapd	%xmm13, %xmm6
	mulsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	mulsd	-112(%rbp), %xmm13
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	mulsd	%xmm14, %xmm2
	movapd	%xmm4, %xmm7
	mulsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	mulsd	%xmm15, %xmm4
	movapd	%xmm5, %xmm6
	mulsd	%xmm5, %xmm6
	mulsd	-120(%rbp), %xmm6
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	-56(%rbp), %xmm5
	movapd	%xmm0, %xmm1
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	mulsd	-64(%rbp), %xmm0
	movapd	%xmm8, %xmm1
	mulsd	%xmm8, %xmm1
	movapd	%xmm1, %xmm8
	mulsd	-72(%rbp), %xmm8
	movapd	%xmm9, %xmm1
	mulsd	%xmm9, %xmm1
	movapd	%xmm1, %xmm9
	mulsd	-80(%rbp), %xmm9
	movapd	%xmm10, %xmm3
	mulsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	mulsd	-88(%rbp), %xmm10
	movapd	%xmm11, %xmm3
	mulsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	mulsd	-96(%rbp), %xmm11
	movapd	%xmm12, %xmm3
	mulsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	mulsd	-104(%rbp), %xmm12
	movapd	%xmm13, %xmm3
	mulsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	mulsd	-112(%rbp), %xmm13
	movapd	%xmm2, %xmm7
	mulsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm14, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm15, %xmm4
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-120(%rbp), %xmm7
	movapd	%xmm5, %xmm3
	mulsd	%xmm5, %xmm3
	movapd	%xmm3, %xmm5
	mulsd	-56(%rbp), %xmm5
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-64(%rbp), %xmm0
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-72(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-80(%rbp), %xmm9
	movapd	%xmm10, %xmm3
	mulsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	mulsd	-88(%rbp), %xmm10
	movapd	%xmm11, %xmm3
	mulsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	mulsd	-96(%rbp), %xmm11
	movapd	%xmm12, %xmm1
	mulsd	%xmm12, %xmm1
	movapd	%xmm1, %xmm12
	mulsd	-104(%rbp), %xmm12
	movapd	%xmm13, %xmm3
	mulsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	mulsd	-112(%rbp), %xmm13
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm14, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	%xmm15, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	-120(%rbp), %xmm7
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	mulsd	-56(%rbp), %xmm5
	movapd	%xmm0, %xmm1
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm0
	mulsd	-64(%rbp), %xmm0
	movapd	%xmm8, %xmm4
	mulsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	mulsd	-72(%rbp), %xmm8
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	-80(%rbp), %xmm9
	movapd	%xmm10, %xmm1
	mulsd	%xmm10, %xmm1
	movapd	%xmm1, %xmm10
	mulsd	-88(%rbp), %xmm10
	movapd	%xmm11, %xmm4
	mulsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	mulsd	-96(%rbp), %xmm11
	movapd	%xmm12, %xmm4
	mulsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	mulsd	-104(%rbp), %xmm12
	movapd	%xmm13, %xmm1
	mulsd	%xmm13, %xmm1
	movapd	%xmm1, %xmm13
	mulsd	-112(%rbp), %xmm13
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	movapd	%xmm14, %xmm1
	mulsd	%xmm1, %xmm2
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	%xmm15, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movsd	-120(%rbp), %xmm14
	mulsd	%xmm14, %xmm6
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	-56(%rbp), %xmm4
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	-64(%rbp), %xmm0
	movapd	%xmm8, %xmm5
	mulsd	%xmm8, %xmm5
	movapd	%xmm5, %xmm8
	mulsd	-72(%rbp), %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-80(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-88(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-96(%rbp), %xmm11
	movapd	%xmm12, %xmm5
	mulsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	mulsd	-104(%rbp), %xmm12
	movapd	%xmm13, %xmm7
	mulsd	%xmm13, %xmm7
	movapd	%xmm7, %xmm13
	mulsd	-112(%rbp), %xmm13
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm1, %xmm2
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	mulsd	%xmm15, %xmm3
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	%xmm14, %xmm7
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	-56(%rbp), %xmm4
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-64(%rbp), %xmm0
	movapd	%xmm8, %xmm5
	mulsd	%xmm8, %xmm5
	movapd	%xmm5, %xmm8
	mulsd	-72(%rbp), %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-80(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-88(%rbp), %xmm10
	movapd	%xmm11, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	mulsd	-96(%rbp), %xmm11
	movapd	%xmm12, %xmm6
	mulsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	mulsd	-104(%rbp), %xmm12
	movapd	%xmm13, %xmm5
	mulsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	mulsd	-112(%rbp), %xmm13
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	%xmm15, %xmm5
	movsd	%xmm5, -128(%rbp)
	movapd	%xmm7, %xmm3
	mulsd	%xmm7, %xmm3
	mulsd	%xmm14, %xmm3
	movsd	%xmm3, -136(%rbp)
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	-56(%rbp), %xmm3
	movsd	%xmm3, -144(%rbp)
	movapd	%xmm0, %xmm1
	mulsd	%xmm0, %xmm1
	mulsd	-64(%rbp), %xmm1
	movsd	%xmm1, -152(%rbp)
	movapd	%xmm8, %xmm7
	mulsd	%xmm8, %xmm7
	mulsd	-72(%rbp), %xmm7
	movsd	%xmm7, -160(%rbp)
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	mulsd	-80(%rbp), %xmm4
	movsd	%xmm4, -168(%rbp)
	movapd	%xmm10, %xmm2
	mulsd	%xmm10, %xmm2
	mulsd	-88(%rbp), %xmm2
	movsd	%xmm2, -176(%rbp)
	movapd	%xmm11, %xmm2
	mulsd	%xmm11, %xmm2
	mulsd	-96(%rbp), %xmm2
	movsd	%xmm2, -184(%rbp)
	movapd	%xmm12, %xmm4
	mulsd	%xmm12, %xmm4
	mulsd	-104(%rbp), %xmm4
	movsd	%xmm4, -192(%rbp)
	movapd	%xmm13, %xmm2
	mulsd	%xmm13, %xmm2
	mulsd	-112(%rbp), %xmm2
	movsd	%xmm2, -200(%rbp)
.L721:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L722
	cvttsd2sil	%xmm6, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE243:
	.size	double_mul_10, .-double_mul_10
	.globl	double_mul_11
	.type	double_mul_11, @function
double_mul_11:
.LFB244:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$232, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm15
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -232(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -56(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -176(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -184(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -192(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -200(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -208(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -216(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -224(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -128(%rbp)
	jmp	.L724
.L725:
	movapd	%xmm15, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm7, %xmm2
	movsd	-232(%rbp), %xmm15
	mulsd	%xmm15, %xmm2
	movsd	-144(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	mulsd	%xmm6, %xmm5
	movsd	-136(%rbp), %xmm1
	mulsd	%xmm1, %xmm5
	movsd	-152(%rbp), %xmm7
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	-56(%rbp), %xmm6
	movsd	-160(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm3, %xmm4
	mulsd	-64(%rbp), %xmm4
	movsd	-168(%rbp), %xmm7
	movapd	%xmm7, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	-72(%rbp), %xmm0
	movsd	-176(%rbp), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm7, %xmm8
	mulsd	-80(%rbp), %xmm8
	movsd	-184(%rbp), %xmm3
	movapd	%xmm3, %xmm9
	mulsd	%xmm3, %xmm9
	mulsd	-88(%rbp), %xmm9
	movsd	-192(%rbp), %xmm7
	movapd	%xmm7, %xmm10
	mulsd	%xmm7, %xmm10
	mulsd	-96(%rbp), %xmm10
	movsd	-200(%rbp), %xmm7
	movapd	%xmm7, %xmm11
	mulsd	%xmm7, %xmm11
	mulsd	-104(%rbp), %xmm11
	movsd	-208(%rbp), %xmm3
	movapd	%xmm3, %xmm12
	mulsd	%xmm3, %xmm12
	mulsd	-112(%rbp), %xmm12
	movsd	-216(%rbp), %xmm7
	movapd	%xmm7, %xmm13
	mulsd	%xmm7, %xmm13
	mulsd	-120(%rbp), %xmm13
	movsd	-224(%rbp), %xmm7
	movapd	%xmm7, %xmm14
	mulsd	%xmm7, %xmm14
	mulsd	-128(%rbp), %xmm14
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm15, %xmm2
	movapd	%xmm5, %xmm7
	mulsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-56(%rbp), %xmm7
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	-64(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-72(%rbp), %xmm0
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-80(%rbp), %xmm8
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	-88(%rbp), %xmm9
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	mulsd	-96(%rbp), %xmm10
	movapd	%xmm11, %xmm4
	mulsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	mulsd	-104(%rbp), %xmm11
	movapd	%xmm12, %xmm4
	mulsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	mulsd	-112(%rbp), %xmm12
	movapd	%xmm13, %xmm6
	mulsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	mulsd	-120(%rbp), %xmm13
	movapd	%xmm14, %xmm4
	mulsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	mulsd	-128(%rbp), %xmm14
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm15, %xmm2
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	movsd	%xmm1, -136(%rbp)
	mulsd	%xmm1, %xmm4
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	-56(%rbp), %xmm7
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	mulsd	-64(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-72(%rbp), %xmm0
	movapd	%xmm8, %xmm5
	mulsd	%xmm8, %xmm5
	movapd	%xmm5, %xmm8
	mulsd	-80(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-88(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-96(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-104(%rbp), %xmm11
	movapd	%xmm12, %xmm5
	mulsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	mulsd	-112(%rbp), %xmm12
	movapd	%xmm13, %xmm1
	mulsd	%xmm13, %xmm1
	movapd	%xmm1, %xmm13
	mulsd	-120(%rbp), %xmm13
	movapd	%xmm14, %xmm6
	mulsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	mulsd	-128(%rbp), %xmm14
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm15, %xmm2
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	-136(%rbp), %xmm4
	movapd	%xmm7, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	-56(%rbp), %xmm5
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	-64(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-72(%rbp), %xmm0
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-80(%rbp), %xmm8
	movapd	%xmm9, %xmm1
	mulsd	%xmm9, %xmm1
	movapd	%xmm1, %xmm9
	mulsd	-88(%rbp), %xmm9
	movapd	%xmm10, %xmm1
	mulsd	%xmm10, %xmm1
	movapd	%xmm1, %xmm10
	mulsd	-96(%rbp), %xmm10
	movapd	%xmm11, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	mulsd	-104(%rbp), %xmm11
	movapd	%xmm12, %xmm6
	mulsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	mulsd	-112(%rbp), %xmm12
	movapd	%xmm13, %xmm7
	mulsd	%xmm13, %xmm7
	movapd	%xmm7, %xmm13
	mulsd	-120(%rbp), %xmm13
	movapd	%xmm14, %xmm1
	mulsd	%xmm14, %xmm1
	movapd	%xmm1, %xmm14
	mulsd	-128(%rbp), %xmm14
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	mulsd	%xmm15, %xmm2
	movapd	%xmm4, %xmm7
	mulsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	mulsd	-136(%rbp), %xmm4
	movapd	%xmm5, %xmm6
	mulsd	%xmm5, %xmm6
	mulsd	-56(%rbp), %xmm6
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	-64(%rbp), %xmm5
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	-72(%rbp), %xmm0
	movapd	%xmm8, %xmm7
	mulsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	mulsd	-80(%rbp), %xmm8
	movapd	%xmm9, %xmm1
	mulsd	%xmm9, %xmm1
	movapd	%xmm1, %xmm9
	mulsd	-88(%rbp), %xmm9
	movapd	%xmm10, %xmm7
	mulsd	%xmm10, %xmm7
	movapd	%xmm7, %xmm10
	mulsd	-96(%rbp), %xmm10
	movapd	%xmm11, %xmm1
	mulsd	%xmm11, %xmm1
	movapd	%xmm1, %xmm11
	mulsd	-104(%rbp), %xmm11
	movapd	%xmm12, %xmm3
	mulsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	mulsd	-112(%rbp), %xmm12
	movapd	%xmm13, %xmm1
	mulsd	%xmm13, %xmm1
	movapd	%xmm1, %xmm13
	mulsd	-120(%rbp), %xmm13
	movapd	%xmm14, %xmm1
	mulsd	%xmm14, %xmm1
	movapd	%xmm1, %xmm14
	mulsd	-128(%rbp), %xmm14
	movapd	%xmm2, %xmm7
	mulsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm15, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	-136(%rbp), %xmm4
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-56(%rbp), %xmm7
	movapd	%xmm5, %xmm3
	mulsd	%xmm5, %xmm3
	movapd	%xmm3, %xmm5
	mulsd	-64(%rbp), %xmm5
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-72(%rbp), %xmm0
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-80(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-88(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-96(%rbp), %xmm10
	movapd	%xmm11, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	mulsd	-104(%rbp), %xmm11
	movapd	%xmm12, %xmm1
	mulsd	%xmm12, %xmm1
	movapd	%xmm1, %xmm12
	mulsd	-112(%rbp), %xmm12
	movapd	%xmm13, %xmm6
	mulsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	mulsd	-120(%rbp), %xmm13
	movapd	%xmm14, %xmm6
	mulsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	mulsd	-128(%rbp), %xmm14
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm15, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movsd	-136(%rbp), %xmm1
	mulsd	%xmm1, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	-56(%rbp), %xmm7
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	mulsd	-64(%rbp), %xmm5
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-72(%rbp), %xmm0
	movapd	%xmm8, %xmm4
	mulsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	mulsd	-80(%rbp), %xmm8
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	-88(%rbp), %xmm9
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	mulsd	-96(%rbp), %xmm10
	movapd	%xmm11, %xmm4
	mulsd	%xmm11, %xmm4
	movapd	%xmm4, %xmm11
	mulsd	-104(%rbp), %xmm11
	movapd	%xmm12, %xmm4
	mulsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	mulsd	-112(%rbp), %xmm12
	movapd	%xmm13, %xmm4
	mulsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	mulsd	-120(%rbp), %xmm13
	movapd	%xmm14, %xmm4
	mulsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	mulsd	-128(%rbp), %xmm14
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm15, %xmm2
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	%xmm1, %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	-56(%rbp), %xmm6
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	-64(%rbp), %xmm4
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	-72(%rbp), %xmm0
	movapd	%xmm8, %xmm5
	mulsd	%xmm8, %xmm5
	movapd	%xmm5, %xmm8
	mulsd	-80(%rbp), %xmm8
	movapd	%xmm9, %xmm7
	mulsd	%xmm9, %xmm7
	movapd	%xmm7, %xmm9
	mulsd	-88(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-96(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-104(%rbp), %xmm11
	movapd	%xmm12, %xmm7
	mulsd	%xmm12, %xmm7
	movapd	%xmm7, %xmm12
	mulsd	-112(%rbp), %xmm12
	movapd	%xmm13, %xmm5
	mulsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	mulsd	-120(%rbp), %xmm13
	movapd	%xmm14, %xmm5
	mulsd	%xmm14, %xmm5
	movapd	%xmm5, %xmm14
	mulsd	-128(%rbp), %xmm14
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm15, %xmm2
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	movsd	%xmm1, -136(%rbp)
	mulsd	%xmm1, %xmm3
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-56(%rbp), %xmm7
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	-64(%rbp), %xmm4
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-72(%rbp), %xmm0
	movapd	%xmm8, %xmm1
	mulsd	%xmm8, %xmm1
	movapd	%xmm1, %xmm8
	mulsd	-80(%rbp), %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-88(%rbp), %xmm9
	movapd	%xmm10, %xmm1
	mulsd	%xmm10, %xmm1
	movapd	%xmm1, %xmm10
	mulsd	-96(%rbp), %xmm10
	movapd	%xmm11, %xmm1
	mulsd	%xmm11, %xmm1
	movapd	%xmm1, %xmm11
	mulsd	-104(%rbp), %xmm11
	movapd	%xmm12, %xmm5
	mulsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	mulsd	-112(%rbp), %xmm12
	movapd	%xmm13, %xmm5
	mulsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	mulsd	-120(%rbp), %xmm13
	movapd	%xmm14, %xmm5
	mulsd	%xmm14, %xmm5
	movapd	%xmm5, %xmm14
	mulsd	-128(%rbp), %xmm14
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	mulsd	%xmm15, %xmm6
	movapd	%xmm6, %xmm15
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	-136(%rbp), %xmm5
	movsd	%xmm5, -144(%rbp)
	movapd	%xmm7, %xmm3
	mulsd	%xmm7, %xmm3
	mulsd	-56(%rbp), %xmm3
	movsd	%xmm3, -152(%rbp)
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	-64(%rbp), %xmm3
	movsd	%xmm3, -160(%rbp)
	movapd	%xmm0, %xmm1
	mulsd	%xmm0, %xmm1
	mulsd	-72(%rbp), %xmm1
	movsd	%xmm1, -168(%rbp)
	movapd	%xmm8, %xmm5
	mulsd	%xmm8, %xmm5
	mulsd	-80(%rbp), %xmm5
	movsd	%xmm5, -176(%rbp)
	movapd	%xmm9, %xmm2
	mulsd	%xmm9, %xmm2
	mulsd	-88(%rbp), %xmm2
	movsd	%xmm2, -184(%rbp)
	movapd	%xmm10, %xmm3
	mulsd	%xmm10, %xmm3
	mulsd	-96(%rbp), %xmm3
	movsd	%xmm3, -192(%rbp)
	movapd	%xmm11, %xmm2
	mulsd	%xmm11, %xmm2
	mulsd	-104(%rbp), %xmm2
	movsd	%xmm2, -200(%rbp)
	movapd	%xmm12, %xmm2
	mulsd	%xmm12, %xmm2
	mulsd	-112(%rbp), %xmm2
	movsd	%xmm2, -208(%rbp)
	movapd	%xmm13, %xmm3
	mulsd	%xmm13, %xmm3
	mulsd	-120(%rbp), %xmm3
	movsd	%xmm3, -216(%rbp)
	movapd	%xmm14, %xmm2
	mulsd	%xmm14, %xmm2
	mulsd	-128(%rbp), %xmm2
	movsd	%xmm2, -224(%rbp)
.L724:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L725
	cvttsd2sil	%xmm15, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE244:
	.size	double_mul_11, .-double_mul_11
	.globl	double_mul_12
	.type	double_mul_12, @function
double_mul_12:
.LFB245:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$248, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm15
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -248(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -56(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -176(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -184(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -192(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -200(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -208(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -216(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -224(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -232(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -240(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -144(%rbp)
	jmp	.L727
.L728:
	movapd	%xmm15, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm7, %xmm2
	movsd	-248(%rbp), %xmm15
	mulsd	%xmm15, %xmm2
	movsd	-152(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	mulsd	%xmm6, %xmm5
	mulsd	-56(%rbp), %xmm5
	movsd	-160(%rbp), %xmm7
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	-64(%rbp), %xmm6
	movsd	-168(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm3, %xmm4
	mulsd	-72(%rbp), %xmm4
	movsd	-176(%rbp), %xmm7
	movapd	%xmm7, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	-80(%rbp), %xmm0
	movsd	-184(%rbp), %xmm7
	movapd	%xmm7, %xmm1
	mulsd	%xmm7, %xmm1
	mulsd	-88(%rbp), %xmm1
	movsd	-192(%rbp), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm7, %xmm8
	mulsd	-96(%rbp), %xmm8
	movsd	-200(%rbp), %xmm7
	movapd	%xmm7, %xmm9
	mulsd	%xmm7, %xmm9
	mulsd	-104(%rbp), %xmm9
	movsd	-208(%rbp), %xmm7
	movapd	%xmm7, %xmm10
	mulsd	%xmm7, %xmm10
	mulsd	-112(%rbp), %xmm10
	movsd	-216(%rbp), %xmm7
	movapd	%xmm7, %xmm11
	mulsd	%xmm7, %xmm11
	mulsd	-120(%rbp), %xmm11
	movsd	-224(%rbp), %xmm7
	movapd	%xmm7, %xmm12
	mulsd	%xmm7, %xmm12
	mulsd	-128(%rbp), %xmm12
	movsd	-232(%rbp), %xmm7
	movapd	%xmm7, %xmm13
	mulsd	%xmm7, %xmm13
	mulsd	-136(%rbp), %xmm13
	movsd	-240(%rbp), %xmm7
	movapd	%xmm7, %xmm14
	mulsd	%xmm7, %xmm14
	mulsd	-144(%rbp), %xmm14
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm15, %xmm2
	movapd	%xmm5, %xmm7
	mulsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	mulsd	-56(%rbp), %xmm5
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-64(%rbp), %xmm7
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	-72(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-80(%rbp), %xmm0
	movapd	%xmm1, %xmm4
	mulsd	%xmm1, %xmm4
	movapd	%xmm4, %xmm1
	mulsd	-88(%rbp), %xmm1
	movapd	%xmm8, %xmm4
	mulsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	mulsd	-96(%rbp), %xmm8
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	-104(%rbp), %xmm9
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	mulsd	-112(%rbp), %xmm10
	movapd	%xmm11, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	mulsd	-120(%rbp), %xmm11
	movapd	%xmm12, %xmm4
	mulsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	mulsd	-128(%rbp), %xmm12
	movapd	%xmm13, %xmm4
	mulsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	mulsd	-136(%rbp), %xmm13
	movapd	%xmm14, %xmm4
	mulsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	mulsd	-144(%rbp), %xmm14
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm15, %xmm2
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	-56(%rbp), %xmm4
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	-64(%rbp), %xmm7
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	mulsd	-72(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-80(%rbp), %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	-88(%rbp), %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-96(%rbp), %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-104(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-112(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-120(%rbp), %xmm11
	movapd	%xmm12, %xmm6
	mulsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	mulsd	-128(%rbp), %xmm12
	movapd	%xmm13, %xmm6
	mulsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	mulsd	-136(%rbp), %xmm13
	movapd	%xmm14, %xmm6
	mulsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	mulsd	-144(%rbp), %xmm14
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm15, %xmm2
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	-56(%rbp), %xmm4
	movapd	%xmm7, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	-64(%rbp), %xmm5
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	-72(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-80(%rbp), %xmm0
	movapd	%xmm1, %xmm7
	mulsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	mulsd	-88(%rbp), %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-96(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-104(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-112(%rbp), %xmm10
	movapd	%xmm11, %xmm7
	mulsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	mulsd	-120(%rbp), %xmm11
	movapd	%xmm12, %xmm6
	mulsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	mulsd	-128(%rbp), %xmm12
	movapd	%xmm13, %xmm6
	mulsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	mulsd	-136(%rbp), %xmm13
	movapd	%xmm14, %xmm7
	mulsd	%xmm14, %xmm7
	movapd	%xmm7, %xmm14
	mulsd	-144(%rbp), %xmm14
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	mulsd	%xmm15, %xmm2
	movapd	%xmm4, %xmm7
	mulsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	mulsd	-56(%rbp), %xmm4
	movapd	%xmm5, %xmm6
	mulsd	%xmm5, %xmm6
	mulsd	-64(%rbp), %xmm6
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	-72(%rbp), %xmm5
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	-80(%rbp), %xmm0
	movapd	%xmm1, %xmm7
	mulsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	mulsd	-88(%rbp), %xmm1
	movapd	%xmm8, %xmm3
	mulsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	mulsd	-96(%rbp), %xmm8
	movapd	%xmm9, %xmm3
	mulsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	mulsd	-104(%rbp), %xmm9
	movapd	%xmm10, %xmm3
	mulsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	mulsd	-112(%rbp), %xmm10
	movapd	%xmm11, %xmm3
	mulsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	mulsd	-120(%rbp), %xmm11
	movapd	%xmm12, %xmm3
	mulsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	mulsd	-128(%rbp), %xmm12
	movapd	%xmm13, %xmm3
	mulsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	mulsd	-136(%rbp), %xmm13
	movapd	%xmm14, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm14
	mulsd	-144(%rbp), %xmm14
	movapd	%xmm2, %xmm7
	mulsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm15, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	-56(%rbp), %xmm4
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-64(%rbp), %xmm7
	movapd	%xmm5, %xmm3
	mulsd	%xmm5, %xmm3
	movapd	%xmm3, %xmm5
	mulsd	-72(%rbp), %xmm5
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-80(%rbp), %xmm0
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	mulsd	-88(%rbp), %xmm1
	movapd	%xmm8, %xmm3
	mulsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	mulsd	-96(%rbp), %xmm8
	movapd	%xmm9, %xmm3
	mulsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	mulsd	-104(%rbp), %xmm9
	movapd	%xmm10, %xmm3
	mulsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	mulsd	-112(%rbp), %xmm10
	movapd	%xmm11, %xmm3
	mulsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	mulsd	-120(%rbp), %xmm11
	movapd	%xmm12, %xmm3
	mulsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	mulsd	-128(%rbp), %xmm12
	movapd	%xmm13, %xmm3
	mulsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	mulsd	-136(%rbp), %xmm13
	movapd	%xmm14, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm14
	mulsd	-144(%rbp), %xmm14
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	%xmm15, %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	-56(%rbp), %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	-64(%rbp), %xmm7
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	mulsd	-72(%rbp), %xmm5
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-80(%rbp), %xmm0
	movapd	%xmm1, %xmm4
	mulsd	%xmm1, %xmm4
	movapd	%xmm4, %xmm1
	mulsd	-88(%rbp), %xmm1
	movapd	%xmm8, %xmm4
	mulsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	mulsd	-96(%rbp), %xmm8
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	-104(%rbp), %xmm9
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	mulsd	-112(%rbp), %xmm10
	movapd	%xmm11, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	mulsd	-120(%rbp), %xmm11
	movapd	%xmm12, %xmm4
	mulsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	mulsd	-128(%rbp), %xmm12
	movapd	%xmm13, %xmm4
	mulsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	mulsd	-136(%rbp), %xmm13
	movapd	%xmm14, %xmm4
	mulsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	mulsd	-144(%rbp), %xmm14
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	%xmm15, %xmm2
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	-56(%rbp), %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	-64(%rbp), %xmm6
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	-72(%rbp), %xmm4
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	-80(%rbp), %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	-88(%rbp), %xmm1
	movapd	%xmm8, %xmm7
	mulsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	mulsd	-96(%rbp), %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-104(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-112(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-120(%rbp), %xmm11
	movapd	%xmm12, %xmm5
	mulsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	mulsd	-128(%rbp), %xmm12
	movapd	%xmm13, %xmm5
	mulsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	mulsd	-136(%rbp), %xmm13
	movapd	%xmm14, %xmm5
	mulsd	%xmm14, %xmm5
	movapd	%xmm5, %xmm14
	mulsd	-144(%rbp), %xmm14
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm15, %xmm2
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	mulsd	-56(%rbp), %xmm3
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-64(%rbp), %xmm7
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	-72(%rbp), %xmm4
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-80(%rbp), %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	-88(%rbp), %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-96(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-104(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-112(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-120(%rbp), %xmm11
	movapd	%xmm12, %xmm5
	mulsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	mulsd	-128(%rbp), %xmm12
	movapd	%xmm13, %xmm5
	mulsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	mulsd	-136(%rbp), %xmm13
	movapd	%xmm14, %xmm6
	mulsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	mulsd	-144(%rbp), %xmm14
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	mulsd	%xmm15, %xmm6
	movapd	%xmm6, %xmm15
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	-56(%rbp), %xmm5
	movsd	%xmm5, -152(%rbp)
	movapd	%xmm7, %xmm3
	mulsd	%xmm7, %xmm3
	mulsd	-64(%rbp), %xmm3
	movsd	%xmm3, -160(%rbp)
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	-72(%rbp), %xmm3
	movsd	%xmm3, -168(%rbp)
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	mulsd	-80(%rbp), %xmm6
	movsd	%xmm6, -176(%rbp)
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	mulsd	-88(%rbp), %xmm5
	movsd	%xmm5, -184(%rbp)
	movapd	%xmm8, %xmm2
	mulsd	%xmm8, %xmm2
	mulsd	-96(%rbp), %xmm2
	movsd	%xmm2, -192(%rbp)
	movapd	%xmm9, %xmm2
	mulsd	%xmm9, %xmm2
	mulsd	-104(%rbp), %xmm2
	movsd	%xmm2, -200(%rbp)
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	mulsd	-112(%rbp), %xmm4
	movsd	%xmm4, -208(%rbp)
	movapd	%xmm11, %xmm2
	mulsd	%xmm11, %xmm2
	mulsd	-120(%rbp), %xmm2
	movsd	%xmm2, -216(%rbp)
	movapd	%xmm12, %xmm2
	mulsd	%xmm12, %xmm2
	mulsd	-128(%rbp), %xmm2
	movsd	%xmm2, -224(%rbp)
	movapd	%xmm13, %xmm2
	mulsd	%xmm13, %xmm2
	mulsd	-136(%rbp), %xmm2
	movsd	%xmm2, -232(%rbp)
	movapd	%xmm14, %xmm2
	mulsd	%xmm14, %xmm2
	mulsd	-144(%rbp), %xmm2
	movsd	%xmm2, -240(%rbp)
.L727:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L728
	cvttsd2sil	%xmm15, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE245:
	.size	double_mul_12, .-double_mul_12
	.globl	double_mul_13
	.type	double_mul_13, @function
double_mul_13:
.LFB246:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$264, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -56(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -176(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -184(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -192(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -200(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -208(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -216(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -224(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -232(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -240(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -248(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -256(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -264(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -272(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -160(%rbp)
	jmp	.L730
.L731:
	movsd	-168(%rbp), %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm7, %xmm2
	mulsd	-56(%rbp), %xmm2
	movsd	-176(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	mulsd	%xmm6, %xmm5
	mulsd	-64(%rbp), %xmm5
	movsd	-184(%rbp), %xmm7
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	-72(%rbp), %xmm6
	movsd	-192(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm3, %xmm4
	mulsd	-80(%rbp), %xmm4
	movsd	-200(%rbp), %xmm7
	movapd	%xmm7, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	-88(%rbp), %xmm0
	movsd	-208(%rbp), %xmm7
	movapd	%xmm7, %xmm1
	mulsd	%xmm7, %xmm1
	mulsd	-96(%rbp), %xmm1
	movsd	-216(%rbp), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm7, %xmm8
	mulsd	-104(%rbp), %xmm8
	movsd	-224(%rbp), %xmm7
	movapd	%xmm7, %xmm9
	mulsd	%xmm7, %xmm9
	mulsd	-112(%rbp), %xmm9
	movsd	-232(%rbp), %xmm7
	movapd	%xmm7, %xmm10
	mulsd	%xmm7, %xmm10
	mulsd	-120(%rbp), %xmm10
	movsd	-240(%rbp), %xmm7
	movapd	%xmm7, %xmm11
	mulsd	%xmm7, %xmm11
	mulsd	-128(%rbp), %xmm11
	movsd	-248(%rbp), %xmm7
	movapd	%xmm7, %xmm12
	mulsd	%xmm7, %xmm12
	mulsd	-136(%rbp), %xmm12
	movsd	-256(%rbp), %xmm7
	movapd	%xmm7, %xmm13
	mulsd	%xmm7, %xmm13
	mulsd	-144(%rbp), %xmm13
	movsd	-264(%rbp), %xmm7
	movapd	%xmm7, %xmm14
	mulsd	%xmm7, %xmm14
	mulsd	-152(%rbp), %xmm14
	movsd	-272(%rbp), %xmm7
	movapd	%xmm7, %xmm15
	mulsd	%xmm7, %xmm15
	mulsd	-160(%rbp), %xmm15
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	-56(%rbp), %xmm2
	movapd	%xmm5, %xmm7
	mulsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	mulsd	-64(%rbp), %xmm5
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-72(%rbp), %xmm7
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	-80(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-88(%rbp), %xmm0
	movapd	%xmm1, %xmm4
	mulsd	%xmm1, %xmm4
	movapd	%xmm4, %xmm1
	mulsd	-96(%rbp), %xmm1
	movapd	%xmm8, %xmm4
	mulsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	mulsd	-104(%rbp), %xmm8
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	-112(%rbp), %xmm9
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	mulsd	-120(%rbp), %xmm10
	movapd	%xmm11, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	mulsd	-128(%rbp), %xmm11
	movapd	%xmm12, %xmm4
	mulsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	mulsd	-136(%rbp), %xmm12
	movapd	%xmm13, %xmm4
	mulsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	mulsd	-144(%rbp), %xmm13
	movapd	%xmm14, %xmm4
	mulsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	mulsd	-152(%rbp), %xmm14
	movapd	%xmm15, %xmm4
	mulsd	%xmm15, %xmm4
	movapd	%xmm4, %xmm15
	mulsd	-160(%rbp), %xmm15
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	-56(%rbp), %xmm2
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	-64(%rbp), %xmm4
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	-72(%rbp), %xmm7
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	mulsd	-80(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-88(%rbp), %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	-96(%rbp), %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-104(%rbp), %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-112(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-120(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-128(%rbp), %xmm11
	movapd	%xmm12, %xmm6
	mulsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	mulsd	-136(%rbp), %xmm12
	movapd	%xmm13, %xmm6
	mulsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	mulsd	-144(%rbp), %xmm13
	movapd	%xmm14, %xmm6
	mulsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	mulsd	-152(%rbp), %xmm14
	movapd	%xmm15, %xmm5
	mulsd	%xmm15, %xmm5
	movapd	%xmm5, %xmm15
	mulsd	-160(%rbp), %xmm15
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	-56(%rbp), %xmm2
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	-64(%rbp), %xmm4
	movapd	%xmm7, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	-72(%rbp), %xmm5
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	-80(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-88(%rbp), %xmm0
	movapd	%xmm1, %xmm7
	mulsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	mulsd	-96(%rbp), %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-104(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-112(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-120(%rbp), %xmm10
	movapd	%xmm11, %xmm7
	mulsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	mulsd	-128(%rbp), %xmm11
	movapd	%xmm12, %xmm6
	mulsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	mulsd	-136(%rbp), %xmm12
	movapd	%xmm13, %xmm6
	mulsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	mulsd	-144(%rbp), %xmm13
	movapd	%xmm14, %xmm7
	mulsd	%xmm14, %xmm7
	movapd	%xmm7, %xmm14
	mulsd	-152(%rbp), %xmm14
	movapd	%xmm15, %xmm6
	mulsd	%xmm15, %xmm6
	movapd	%xmm6, %xmm15
	mulsd	-160(%rbp), %xmm15
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	mulsd	-56(%rbp), %xmm2
	movapd	%xmm4, %xmm7
	mulsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	mulsd	-64(%rbp), %xmm4
	movapd	%xmm5, %xmm6
	mulsd	%xmm5, %xmm6
	mulsd	-72(%rbp), %xmm6
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	-80(%rbp), %xmm5
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	-88(%rbp), %xmm0
	movapd	%xmm1, %xmm7
	mulsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	mulsd	-96(%rbp), %xmm1
	movapd	%xmm8, %xmm3
	mulsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	mulsd	-104(%rbp), %xmm8
	movapd	%xmm9, %xmm3
	mulsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	mulsd	-112(%rbp), %xmm9
	movapd	%xmm10, %xmm3
	mulsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	mulsd	-120(%rbp), %xmm10
	movapd	%xmm11, %xmm3
	mulsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	mulsd	-128(%rbp), %xmm11
	movapd	%xmm12, %xmm3
	mulsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	mulsd	-136(%rbp), %xmm12
	movapd	%xmm13, %xmm3
	mulsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	mulsd	-144(%rbp), %xmm13
	movapd	%xmm14, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm14
	mulsd	-152(%rbp), %xmm14
	movapd	%xmm15, %xmm3
	mulsd	%xmm15, %xmm3
	movapd	%xmm3, %xmm15
	mulsd	-160(%rbp), %xmm15
	movapd	%xmm2, %xmm7
	mulsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	-56(%rbp), %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	-64(%rbp), %xmm4
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-72(%rbp), %xmm7
	movapd	%xmm5, %xmm3
	mulsd	%xmm5, %xmm3
	movapd	%xmm3, %xmm5
	mulsd	-80(%rbp), %xmm5
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-88(%rbp), %xmm0
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	mulsd	-96(%rbp), %xmm1
	movapd	%xmm8, %xmm3
	mulsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	mulsd	-104(%rbp), %xmm8
	movapd	%xmm9, %xmm3
	mulsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	mulsd	-112(%rbp), %xmm9
	movapd	%xmm10, %xmm3
	mulsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	mulsd	-120(%rbp), %xmm10
	movapd	%xmm11, %xmm3
	mulsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	mulsd	-128(%rbp), %xmm11
	movapd	%xmm12, %xmm3
	mulsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	mulsd	-136(%rbp), %xmm12
	movapd	%xmm13, %xmm3
	mulsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	mulsd	-144(%rbp), %xmm13
	movapd	%xmm14, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm14
	mulsd	-152(%rbp), %xmm14
	movapd	%xmm15, %xmm3
	mulsd	%xmm15, %xmm3
	movapd	%xmm3, %xmm15
	mulsd	-160(%rbp), %xmm15
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	-56(%rbp), %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	-64(%rbp), %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	-72(%rbp), %xmm7
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	mulsd	-80(%rbp), %xmm5
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-88(%rbp), %xmm0
	movapd	%xmm1, %xmm4
	mulsd	%xmm1, %xmm4
	movapd	%xmm4, %xmm1
	mulsd	-96(%rbp), %xmm1
	movapd	%xmm8, %xmm4
	mulsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	mulsd	-104(%rbp), %xmm8
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	-112(%rbp), %xmm9
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	mulsd	-120(%rbp), %xmm10
	movapd	%xmm11, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	mulsd	-128(%rbp), %xmm11
	movapd	%xmm12, %xmm4
	mulsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	mulsd	-136(%rbp), %xmm12
	movapd	%xmm13, %xmm4
	mulsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	mulsd	-144(%rbp), %xmm13
	movapd	%xmm14, %xmm4
	mulsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	mulsd	-152(%rbp), %xmm14
	movapd	%xmm15, %xmm4
	mulsd	%xmm15, %xmm4
	movapd	%xmm4, %xmm15
	mulsd	-160(%rbp), %xmm15
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	-56(%rbp), %xmm2
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	-64(%rbp), %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	-72(%rbp), %xmm6
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	-80(%rbp), %xmm4
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	-88(%rbp), %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	-96(%rbp), %xmm1
	movapd	%xmm8, %xmm7
	mulsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	mulsd	-104(%rbp), %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-112(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-120(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-128(%rbp), %xmm11
	movapd	%xmm12, %xmm5
	mulsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	mulsd	-136(%rbp), %xmm12
	movapd	%xmm13, %xmm5
	mulsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	mulsd	-144(%rbp), %xmm13
	movapd	%xmm14, %xmm5
	mulsd	%xmm14, %xmm5
	movapd	%xmm5, %xmm14
	mulsd	-152(%rbp), %xmm14
	movapd	%xmm15, %xmm5
	mulsd	%xmm15, %xmm5
	movapd	%xmm5, %xmm15
	mulsd	-160(%rbp), %xmm15
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	-56(%rbp), %xmm2
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	mulsd	-64(%rbp), %xmm3
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-72(%rbp), %xmm7
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	-80(%rbp), %xmm4
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-88(%rbp), %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	-96(%rbp), %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-104(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-112(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-120(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-128(%rbp), %xmm11
	movapd	%xmm12, %xmm5
	mulsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	mulsd	-136(%rbp), %xmm12
	movapd	%xmm13, %xmm5
	mulsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	mulsd	-144(%rbp), %xmm13
	movapd	%xmm14, %xmm6
	mulsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	mulsd	-152(%rbp), %xmm14
	movapd	%xmm15, %xmm6
	mulsd	%xmm15, %xmm6
	movapd	%xmm6, %xmm15
	mulsd	-160(%rbp), %xmm15
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	mulsd	-56(%rbp), %xmm6
	movsd	%xmm6, -168(%rbp)
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	-64(%rbp), %xmm5
	movsd	%xmm5, -176(%rbp)
	movapd	%xmm7, %xmm3
	mulsd	%xmm7, %xmm3
	mulsd	-72(%rbp), %xmm3
	movsd	%xmm3, -184(%rbp)
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	-80(%rbp), %xmm3
	movsd	%xmm3, -192(%rbp)
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	mulsd	-88(%rbp), %xmm6
	movsd	%xmm6, -200(%rbp)
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	mulsd	-96(%rbp), %xmm5
	movsd	%xmm5, -208(%rbp)
	movapd	%xmm8, %xmm2
	mulsd	%xmm8, %xmm2
	mulsd	-104(%rbp), %xmm2
	movsd	%xmm2, -216(%rbp)
	movapd	%xmm9, %xmm2
	mulsd	%xmm9, %xmm2
	mulsd	-112(%rbp), %xmm2
	movsd	%xmm2, -224(%rbp)
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	mulsd	-120(%rbp), %xmm4
	movsd	%xmm4, -232(%rbp)
	movapd	%xmm11, %xmm2
	mulsd	%xmm11, %xmm2
	mulsd	-128(%rbp), %xmm2
	movsd	%xmm2, -240(%rbp)
	movapd	%xmm12, %xmm2
	mulsd	%xmm12, %xmm2
	mulsd	-136(%rbp), %xmm2
	movsd	%xmm2, -248(%rbp)
	movapd	%xmm13, %xmm2
	mulsd	%xmm13, %xmm2
	mulsd	-144(%rbp), %xmm2
	movsd	%xmm2, -256(%rbp)
	movapd	%xmm14, %xmm2
	mulsd	%xmm14, %xmm2
	mulsd	-152(%rbp), %xmm2
	movsd	%xmm2, -264(%rbp)
	movapd	%xmm15, %xmm5
	mulsd	%xmm15, %xmm5
	mulsd	-160(%rbp), %xmm5
	movsd	%xmm5, -272(%rbp)
.L730:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L731
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-264(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-272(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE246:
	.size	double_mul_13, .-double_mul_13
	.globl	double_mul_14
	.type	double_mul_14, @function
double_mul_14:
.LFB247:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$280, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -184(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -192(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -200(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -208(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -216(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -224(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -232(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -240(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -248(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -256(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -264(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -272(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -280(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -288(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	192(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	192(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -176(%rbp)
	jmp	.L733
.L734:
	movsd	-184(%rbp), %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm7, %xmm2
	mulsd	-64(%rbp), %xmm2
	movsd	-192(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	mulsd	%xmm6, %xmm5
	mulsd	-72(%rbp), %xmm5
	movsd	-200(%rbp), %xmm7
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	-80(%rbp), %xmm6
	movsd	-208(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm3, %xmm4
	mulsd	-88(%rbp), %xmm4
	movsd	-216(%rbp), %xmm7
	movapd	%xmm7, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	-96(%rbp), %xmm0
	movsd	-224(%rbp), %xmm7
	movapd	%xmm7, %xmm1
	mulsd	%xmm7, %xmm1
	mulsd	-104(%rbp), %xmm1
	movsd	-232(%rbp), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm7, %xmm8
	mulsd	-112(%rbp), %xmm8
	movsd	-240(%rbp), %xmm7
	movapd	%xmm7, %xmm9
	mulsd	%xmm7, %xmm9
	mulsd	-120(%rbp), %xmm9
	movsd	-248(%rbp), %xmm7
	movapd	%xmm7, %xmm10
	mulsd	%xmm7, %xmm10
	mulsd	-128(%rbp), %xmm10
	movsd	-256(%rbp), %xmm7
	movapd	%xmm7, %xmm11
	mulsd	%xmm7, %xmm11
	mulsd	-136(%rbp), %xmm11
	movsd	-264(%rbp), %xmm7
	movapd	%xmm7, %xmm12
	mulsd	%xmm7, %xmm12
	mulsd	-144(%rbp), %xmm12
	movsd	-272(%rbp), %xmm7
	movapd	%xmm7, %xmm13
	mulsd	%xmm7, %xmm13
	mulsd	-152(%rbp), %xmm13
	movsd	-280(%rbp), %xmm7
	movapd	%xmm7, %xmm14
	mulsd	%xmm7, %xmm14
	mulsd	-160(%rbp), %xmm14
	movsd	-288(%rbp), %xmm7
	movapd	%xmm7, %xmm15
	mulsd	%xmm7, %xmm15
	mulsd	-168(%rbp), %xmm15
	movsd	-56(%rbp), %xmm7
	movapd	%xmm7, %xmm3
	mulsd	%xmm7, %xmm3
	mulsd	-176(%rbp), %xmm3
	movsd	%xmm3, -56(%rbp)
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	-64(%rbp), %xmm2
	movapd	%xmm5, %xmm7
	mulsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	mulsd	-72(%rbp), %xmm5
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-80(%rbp), %xmm7
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	-88(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-96(%rbp), %xmm0
	movapd	%xmm1, %xmm4
	mulsd	%xmm1, %xmm4
	movapd	%xmm4, %xmm1
	mulsd	-104(%rbp), %xmm1
	movapd	%xmm8, %xmm4
	mulsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	mulsd	-112(%rbp), %xmm8
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	-120(%rbp), %xmm9
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	mulsd	-128(%rbp), %xmm10
	movapd	%xmm11, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	mulsd	-136(%rbp), %xmm11
	movapd	%xmm12, %xmm4
	mulsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	mulsd	-144(%rbp), %xmm12
	movapd	%xmm13, %xmm4
	mulsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	mulsd	-152(%rbp), %xmm13
	movapd	%xmm14, %xmm4
	mulsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	mulsd	-160(%rbp), %xmm14
	movapd	%xmm15, %xmm4
	mulsd	%xmm15, %xmm4
	movapd	%xmm4, %xmm15
	mulsd	-168(%rbp), %xmm15
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm4
	mulsd	%xmm6, %xmm4
	mulsd	-176(%rbp), %xmm4
	movsd	%xmm4, -56(%rbp)
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	-64(%rbp), %xmm2
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	-72(%rbp), %xmm4
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	-80(%rbp), %xmm7
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	mulsd	-88(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-96(%rbp), %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	-104(%rbp), %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-112(%rbp), %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-120(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-128(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-136(%rbp), %xmm11
	movapd	%xmm12, %xmm6
	mulsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	mulsd	-144(%rbp), %xmm12
	movapd	%xmm13, %xmm6
	mulsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	mulsd	-152(%rbp), %xmm13
	movapd	%xmm14, %xmm6
	mulsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	mulsd	-160(%rbp), %xmm14
	movapd	%xmm15, %xmm5
	mulsd	%xmm15, %xmm5
	movapd	%xmm5, %xmm15
	mulsd	-168(%rbp), %xmm15
	movsd	-56(%rbp), %xmm5
	movapd	%xmm5, %xmm6
	mulsd	%xmm5, %xmm6
	mulsd	-176(%rbp), %xmm6
	movsd	%xmm6, -56(%rbp)
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	-64(%rbp), %xmm2
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	-72(%rbp), %xmm4
	movapd	%xmm7, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	-80(%rbp), %xmm5
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	-88(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-96(%rbp), %xmm0
	movapd	%xmm1, %xmm7
	mulsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	mulsd	-104(%rbp), %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-112(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-120(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-128(%rbp), %xmm10
	movapd	%xmm11, %xmm7
	mulsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	mulsd	-136(%rbp), %xmm11
	movapd	%xmm12, %xmm6
	mulsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	mulsd	-144(%rbp), %xmm12
	movapd	%xmm13, %xmm6
	mulsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	mulsd	-152(%rbp), %xmm13
	movapd	%xmm14, %xmm7
	mulsd	%xmm14, %xmm7
	movapd	%xmm7, %xmm14
	mulsd	-160(%rbp), %xmm14
	movapd	%xmm15, %xmm6
	mulsd	%xmm15, %xmm6
	movapd	%xmm6, %xmm15
	mulsd	-168(%rbp), %xmm15
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-176(%rbp), %xmm7
	movsd	%xmm7, -56(%rbp)
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	mulsd	-64(%rbp), %xmm2
	movapd	%xmm4, %xmm7
	mulsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	mulsd	-72(%rbp), %xmm4
	movapd	%xmm5, %xmm6
	mulsd	%xmm5, %xmm6
	mulsd	-80(%rbp), %xmm6
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	-88(%rbp), %xmm5
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	-96(%rbp), %xmm0
	movapd	%xmm1, %xmm7
	mulsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	mulsd	-104(%rbp), %xmm1
	movapd	%xmm8, %xmm3
	mulsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	mulsd	-112(%rbp), %xmm8
	movapd	%xmm9, %xmm3
	mulsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	mulsd	-120(%rbp), %xmm9
	movapd	%xmm10, %xmm3
	mulsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	mulsd	-128(%rbp), %xmm10
	movapd	%xmm11, %xmm3
	mulsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	mulsd	-136(%rbp), %xmm11
	movapd	%xmm12, %xmm3
	mulsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	mulsd	-144(%rbp), %xmm12
	movapd	%xmm13, %xmm3
	mulsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	mulsd	-152(%rbp), %xmm13
	movapd	%xmm14, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm14
	mulsd	-160(%rbp), %xmm14
	movapd	%xmm15, %xmm3
	mulsd	%xmm15, %xmm3
	movapd	%xmm3, %xmm15
	mulsd	-168(%rbp), %xmm15
	movsd	-56(%rbp), %xmm7
	movapd	%xmm7, %xmm3
	mulsd	%xmm7, %xmm3
	mulsd	-176(%rbp), %xmm3
	movsd	%xmm3, -56(%rbp)
	movapd	%xmm2, %xmm7
	mulsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	-64(%rbp), %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	-72(%rbp), %xmm4
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-80(%rbp), %xmm7
	movapd	%xmm5, %xmm3
	mulsd	%xmm5, %xmm3
	movapd	%xmm3, %xmm5
	mulsd	-88(%rbp), %xmm5
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-96(%rbp), %xmm0
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	mulsd	-104(%rbp), %xmm1
	movapd	%xmm8, %xmm3
	mulsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	mulsd	-112(%rbp), %xmm8
	movapd	%xmm9, %xmm3
	mulsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	mulsd	-120(%rbp), %xmm9
	movapd	%xmm10, %xmm3
	mulsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	mulsd	-128(%rbp), %xmm10
	movapd	%xmm11, %xmm3
	mulsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	mulsd	-136(%rbp), %xmm11
	movapd	%xmm12, %xmm3
	mulsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	mulsd	-144(%rbp), %xmm12
	movapd	%xmm13, %xmm3
	mulsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	mulsd	-152(%rbp), %xmm13
	movapd	%xmm14, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm14
	mulsd	-160(%rbp), %xmm14
	movapd	%xmm15, %xmm3
	mulsd	%xmm15, %xmm3
	movapd	%xmm3, %xmm15
	mulsd	-168(%rbp), %xmm15
	movsd	-56(%rbp), %xmm3
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	mulsd	-176(%rbp), %xmm6
	movsd	%xmm6, -56(%rbp)
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	-64(%rbp), %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	-72(%rbp), %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	-80(%rbp), %xmm7
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	mulsd	-88(%rbp), %xmm5
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-96(%rbp), %xmm0
	movapd	%xmm1, %xmm4
	mulsd	%xmm1, %xmm4
	movapd	%xmm4, %xmm1
	mulsd	-104(%rbp), %xmm1
	movapd	%xmm8, %xmm4
	mulsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	mulsd	-112(%rbp), %xmm8
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	-120(%rbp), %xmm9
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	mulsd	-128(%rbp), %xmm10
	movapd	%xmm11, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	mulsd	-136(%rbp), %xmm11
	movapd	%xmm12, %xmm4
	mulsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	mulsd	-144(%rbp), %xmm12
	movapd	%xmm13, %xmm4
	mulsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	mulsd	-152(%rbp), %xmm13
	movapd	%xmm14, %xmm4
	mulsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	mulsd	-160(%rbp), %xmm14
	movapd	%xmm15, %xmm4
	mulsd	%xmm15, %xmm4
	movapd	%xmm4, %xmm15
	mulsd	-168(%rbp), %xmm15
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm4
	mulsd	%xmm6, %xmm4
	mulsd	-176(%rbp), %xmm4
	movsd	%xmm4, -56(%rbp)
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	-64(%rbp), %xmm2
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	-72(%rbp), %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	-80(%rbp), %xmm6
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	-88(%rbp), %xmm4
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	-96(%rbp), %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	-104(%rbp), %xmm1
	movapd	%xmm8, %xmm7
	mulsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	mulsd	-112(%rbp), %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-120(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-128(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-136(%rbp), %xmm11
	movapd	%xmm12, %xmm5
	mulsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	mulsd	-144(%rbp), %xmm12
	movapd	%xmm13, %xmm5
	mulsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	mulsd	-152(%rbp), %xmm13
	movapd	%xmm14, %xmm5
	mulsd	%xmm14, %xmm5
	movapd	%xmm5, %xmm14
	mulsd	-160(%rbp), %xmm14
	movapd	%xmm15, %xmm5
	mulsd	%xmm15, %xmm5
	movapd	%xmm5, %xmm15
	mulsd	-168(%rbp), %xmm15
	movsd	-56(%rbp), %xmm7
	movapd	%xmm7, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	-176(%rbp), %xmm5
	movsd	%xmm5, -56(%rbp)
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	-64(%rbp), %xmm2
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	mulsd	-72(%rbp), %xmm3
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-80(%rbp), %xmm7
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	-88(%rbp), %xmm4
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-96(%rbp), %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	-104(%rbp), %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-112(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-120(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-128(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-136(%rbp), %xmm11
	movapd	%xmm12, %xmm5
	mulsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	mulsd	-144(%rbp), %xmm12
	movapd	%xmm13, %xmm5
	mulsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	mulsd	-152(%rbp), %xmm13
	movapd	%xmm14, %xmm6
	mulsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	mulsd	-160(%rbp), %xmm14
	movapd	%xmm15, %xmm6
	mulsd	%xmm15, %xmm6
	movapd	%xmm6, %xmm15
	mulsd	-168(%rbp), %xmm15
	movsd	-56(%rbp), %xmm5
	movapd	%xmm5, %xmm6
	mulsd	%xmm5, %xmm6
	mulsd	-176(%rbp), %xmm6
	movsd	%xmm6, -56(%rbp)
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	mulsd	-64(%rbp), %xmm6
	movsd	%xmm6, -184(%rbp)
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	-72(%rbp), %xmm5
	movsd	%xmm5, -192(%rbp)
	movapd	%xmm7, %xmm3
	mulsd	%xmm7, %xmm3
	mulsd	-80(%rbp), %xmm3
	movsd	%xmm3, -200(%rbp)
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	-88(%rbp), %xmm3
	movsd	%xmm3, -208(%rbp)
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	mulsd	-96(%rbp), %xmm6
	movsd	%xmm6, -216(%rbp)
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	mulsd	-104(%rbp), %xmm5
	movsd	%xmm5, -224(%rbp)
	movapd	%xmm8, %xmm2
	mulsd	%xmm8, %xmm2
	mulsd	-112(%rbp), %xmm2
	movsd	%xmm2, -232(%rbp)
	movapd	%xmm9, %xmm2
	mulsd	%xmm9, %xmm2
	mulsd	-120(%rbp), %xmm2
	movsd	%xmm2, -240(%rbp)
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	mulsd	-128(%rbp), %xmm4
	movsd	%xmm4, -248(%rbp)
	movapd	%xmm11, %xmm2
	mulsd	%xmm11, %xmm2
	mulsd	-136(%rbp), %xmm2
	movsd	%xmm2, -256(%rbp)
	movapd	%xmm12, %xmm2
	mulsd	%xmm12, %xmm2
	mulsd	-144(%rbp), %xmm2
	movsd	%xmm2, -264(%rbp)
	movapd	%xmm13, %xmm2
	mulsd	%xmm13, %xmm2
	mulsd	-152(%rbp), %xmm2
	movsd	%xmm2, -272(%rbp)
	movapd	%xmm14, %xmm2
	mulsd	%xmm14, %xmm2
	mulsd	-160(%rbp), %xmm2
	movsd	%xmm2, -280(%rbp)
	movapd	%xmm15, %xmm5
	mulsd	%xmm15, %xmm5
	mulsd	-168(%rbp), %xmm5
	movsd	%xmm5, -288(%rbp)
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm2
	mulsd	%xmm6, %xmm2
	mulsd	-176(%rbp), %xmm2
	movsd	%xmm2, -56(%rbp)
.L733:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L734
	cvttsd2sil	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-264(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-272(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-280(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-288(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE247:
	.size	double_mul_14, .-double_mul_14
	.globl	double_mul_15
	.type	double_mul_15, @function
double_mul_15:
.LFB248:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$296, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -200(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -208(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -216(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -224(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -232(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -240(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -248(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -256(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -264(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -272(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -280(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -288(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -296(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -304(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -176(%rbp)
	movq	-24(%rbp), %rax
	movsd	192(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	192(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -184(%rbp)
	movq	-24(%rbp), %rax
	movsd	200(%rax), %xmm1
	movsd	.LC10(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	4(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC5(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movq	-24(%rbp), %rax
	movsd	200(%rax), %xmm0
	mulsd	%xmm1, %xmm0
	movsd	.LC6(%rip), %xmm1
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -192(%rbp)
	jmp	.L736
.L737:
	movsd	-200(%rbp), %xmm7
	movapd	%xmm7, %xmm2
	mulsd	%xmm7, %xmm2
	mulsd	-72(%rbp), %xmm2
	movsd	-208(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	mulsd	%xmm6, %xmm5
	mulsd	-80(%rbp), %xmm5
	movsd	-216(%rbp), %xmm7
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	-88(%rbp), %xmm6
	movsd	-224(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	mulsd	%xmm3, %xmm4
	mulsd	-96(%rbp), %xmm4
	movsd	-232(%rbp), %xmm7
	movapd	%xmm7, %xmm0
	mulsd	%xmm7, %xmm0
	mulsd	-104(%rbp), %xmm0
	movsd	-240(%rbp), %xmm7
	movapd	%xmm7, %xmm1
	mulsd	%xmm7, %xmm1
	mulsd	-112(%rbp), %xmm1
	movsd	-248(%rbp), %xmm7
	movapd	%xmm7, %xmm8
	mulsd	%xmm7, %xmm8
	mulsd	-120(%rbp), %xmm8
	movsd	-256(%rbp), %xmm7
	movapd	%xmm7, %xmm9
	mulsd	%xmm7, %xmm9
	mulsd	-128(%rbp), %xmm9
	movsd	-264(%rbp), %xmm7
	movapd	%xmm7, %xmm10
	mulsd	%xmm7, %xmm10
	mulsd	-136(%rbp), %xmm10
	movsd	-272(%rbp), %xmm7
	movapd	%xmm7, %xmm11
	mulsd	%xmm7, %xmm11
	mulsd	-144(%rbp), %xmm11
	movsd	-280(%rbp), %xmm7
	movapd	%xmm7, %xmm12
	mulsd	%xmm7, %xmm12
	mulsd	-152(%rbp), %xmm12
	movsd	-288(%rbp), %xmm7
	movapd	%xmm7, %xmm13
	mulsd	%xmm7, %xmm13
	mulsd	-160(%rbp), %xmm13
	movsd	-296(%rbp), %xmm7
	movapd	%xmm7, %xmm14
	mulsd	%xmm7, %xmm14
	mulsd	-168(%rbp), %xmm14
	movsd	-304(%rbp), %xmm7
	movapd	%xmm7, %xmm15
	mulsd	%xmm7, %xmm15
	mulsd	-176(%rbp), %xmm15
	movsd	-56(%rbp), %xmm7
	movapd	%xmm7, %xmm3
	mulsd	%xmm7, %xmm3
	mulsd	-184(%rbp), %xmm3
	movsd	%xmm3, -56(%rbp)
	movsd	-64(%rbp), %xmm3
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	mulsd	-192(%rbp), %xmm7
	movsd	%xmm7, -64(%rbp)
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	-72(%rbp), %xmm2
	movapd	%xmm5, %xmm7
	mulsd	%xmm5, %xmm7
	movapd	%xmm7, %xmm5
	mulsd	-80(%rbp), %xmm5
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-88(%rbp), %xmm7
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	-96(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-104(%rbp), %xmm0
	movapd	%xmm1, %xmm4
	mulsd	%xmm1, %xmm4
	movapd	%xmm4, %xmm1
	mulsd	-112(%rbp), %xmm1
	movapd	%xmm8, %xmm4
	mulsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	mulsd	-120(%rbp), %xmm8
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	-128(%rbp), %xmm9
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	mulsd	-136(%rbp), %xmm10
	movapd	%xmm11, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	mulsd	-144(%rbp), %xmm11
	movapd	%xmm12, %xmm4
	mulsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	mulsd	-152(%rbp), %xmm12
	movapd	%xmm13, %xmm4
	mulsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	mulsd	-160(%rbp), %xmm13
	movapd	%xmm14, %xmm4
	mulsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	mulsd	-168(%rbp), %xmm14
	movapd	%xmm15, %xmm4
	mulsd	%xmm15, %xmm4
	movapd	%xmm4, %xmm15
	mulsd	-176(%rbp), %xmm15
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm4
	mulsd	%xmm6, %xmm4
	mulsd	-184(%rbp), %xmm4
	movsd	%xmm4, -56(%rbp)
	movsd	-64(%rbp), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	mulsd	-192(%rbp), %xmm6
	movsd	%xmm6, -64(%rbp)
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	-72(%rbp), %xmm2
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	-80(%rbp), %xmm4
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	-88(%rbp), %xmm7
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	movapd	%xmm5, %xmm3
	mulsd	-96(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-104(%rbp), %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	-112(%rbp), %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-120(%rbp), %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-128(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-136(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-144(%rbp), %xmm11
	movapd	%xmm12, %xmm6
	mulsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	mulsd	-152(%rbp), %xmm12
	movapd	%xmm13, %xmm6
	mulsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	mulsd	-160(%rbp), %xmm13
	movapd	%xmm14, %xmm6
	mulsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	mulsd	-168(%rbp), %xmm14
	movapd	%xmm15, %xmm5
	mulsd	%xmm15, %xmm5
	movapd	%xmm5, %xmm15
	mulsd	-176(%rbp), %xmm15
	movsd	-56(%rbp), %xmm5
	movapd	%xmm5, %xmm6
	mulsd	%xmm5, %xmm6
	mulsd	-184(%rbp), %xmm6
	movsd	%xmm6, -56(%rbp)
	movsd	-64(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	mulsd	%xmm6, %xmm5
	mulsd	-192(%rbp), %xmm5
	movsd	%xmm5, -64(%rbp)
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	-72(%rbp), %xmm2
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	-80(%rbp), %xmm4
	movapd	%xmm7, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	-88(%rbp), %xmm5
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	-96(%rbp), %xmm3
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-104(%rbp), %xmm0
	movapd	%xmm1, %xmm7
	mulsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	mulsd	-112(%rbp), %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-120(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-128(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-136(%rbp), %xmm10
	movapd	%xmm11, %xmm7
	mulsd	%xmm11, %xmm7
	movapd	%xmm7, %xmm11
	mulsd	-144(%rbp), %xmm11
	movapd	%xmm12, %xmm6
	mulsd	%xmm12, %xmm6
	movapd	%xmm6, %xmm12
	mulsd	-152(%rbp), %xmm12
	movapd	%xmm13, %xmm6
	mulsd	%xmm13, %xmm6
	movapd	%xmm6, %xmm13
	mulsd	-160(%rbp), %xmm13
	movapd	%xmm14, %xmm7
	mulsd	%xmm14, %xmm7
	movapd	%xmm7, %xmm14
	mulsd	-168(%rbp), %xmm14
	movapd	%xmm15, %xmm6
	mulsd	%xmm15, %xmm6
	movapd	%xmm6, %xmm15
	mulsd	-176(%rbp), %xmm15
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-184(%rbp), %xmm7
	movsd	%xmm7, -56(%rbp)
	movsd	-64(%rbp), %xmm7
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	-192(%rbp), %xmm6
	movsd	%xmm6, -64(%rbp)
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	movapd	%xmm6, %xmm2
	mulsd	-72(%rbp), %xmm2
	movapd	%xmm4, %xmm7
	mulsd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	mulsd	-80(%rbp), %xmm4
	movapd	%xmm5, %xmm6
	mulsd	%xmm5, %xmm6
	mulsd	-88(%rbp), %xmm6
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	-96(%rbp), %xmm5
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	-104(%rbp), %xmm0
	movapd	%xmm1, %xmm7
	mulsd	%xmm1, %xmm7
	movapd	%xmm7, %xmm1
	mulsd	-112(%rbp), %xmm1
	movapd	%xmm8, %xmm3
	mulsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	mulsd	-120(%rbp), %xmm8
	movapd	%xmm9, %xmm3
	mulsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	mulsd	-128(%rbp), %xmm9
	movapd	%xmm10, %xmm3
	mulsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	mulsd	-136(%rbp), %xmm10
	movapd	%xmm11, %xmm3
	mulsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	mulsd	-144(%rbp), %xmm11
	movapd	%xmm12, %xmm3
	mulsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	mulsd	-152(%rbp), %xmm12
	movapd	%xmm13, %xmm3
	mulsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	mulsd	-160(%rbp), %xmm13
	movapd	%xmm14, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm14
	mulsd	-168(%rbp), %xmm14
	movapd	%xmm15, %xmm3
	mulsd	%xmm15, %xmm3
	movapd	%xmm3, %xmm15
	mulsd	-176(%rbp), %xmm15
	movsd	-56(%rbp), %xmm7
	movapd	%xmm7, %xmm3
	mulsd	%xmm7, %xmm3
	mulsd	-184(%rbp), %xmm3
	movsd	%xmm3, -56(%rbp)
	movsd	-64(%rbp), %xmm3
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	mulsd	-192(%rbp), %xmm7
	movsd	%xmm7, -64(%rbp)
	movapd	%xmm2, %xmm7
	mulsd	%xmm2, %xmm7
	movapd	%xmm7, %xmm2
	mulsd	-72(%rbp), %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	movapd	%xmm3, %xmm4
	mulsd	-80(%rbp), %xmm4
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-88(%rbp), %xmm7
	movapd	%xmm5, %xmm3
	mulsd	%xmm5, %xmm3
	movapd	%xmm3, %xmm5
	mulsd	-96(%rbp), %xmm5
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-104(%rbp), %xmm0
	movapd	%xmm1, %xmm6
	mulsd	%xmm1, %xmm6
	movapd	%xmm6, %xmm1
	mulsd	-112(%rbp), %xmm1
	movapd	%xmm8, %xmm3
	mulsd	%xmm8, %xmm3
	movapd	%xmm3, %xmm8
	mulsd	-120(%rbp), %xmm8
	movapd	%xmm9, %xmm3
	mulsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	mulsd	-128(%rbp), %xmm9
	movapd	%xmm10, %xmm3
	mulsd	%xmm10, %xmm3
	movapd	%xmm3, %xmm10
	mulsd	-136(%rbp), %xmm10
	movapd	%xmm11, %xmm3
	mulsd	%xmm11, %xmm3
	movapd	%xmm3, %xmm11
	mulsd	-144(%rbp), %xmm11
	movapd	%xmm12, %xmm3
	mulsd	%xmm12, %xmm3
	movapd	%xmm3, %xmm12
	mulsd	-152(%rbp), %xmm12
	movapd	%xmm13, %xmm3
	mulsd	%xmm13, %xmm3
	movapd	%xmm3, %xmm13
	mulsd	-160(%rbp), %xmm13
	movapd	%xmm14, %xmm3
	mulsd	%xmm14, %xmm3
	movapd	%xmm3, %xmm14
	mulsd	-168(%rbp), %xmm14
	movapd	%xmm15, %xmm3
	mulsd	%xmm15, %xmm3
	movapd	%xmm3, %xmm15
	mulsd	-176(%rbp), %xmm15
	movsd	-56(%rbp), %xmm3
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	mulsd	-184(%rbp), %xmm6
	movsd	%xmm6, -56(%rbp)
	movsd	-64(%rbp), %xmm6
	movapd	%xmm6, %xmm3
	mulsd	%xmm6, %xmm3
	mulsd	-192(%rbp), %xmm3
	movsd	%xmm3, -64(%rbp)
	movapd	%xmm2, %xmm3
	mulsd	%xmm2, %xmm3
	movapd	%xmm3, %xmm2
	mulsd	-72(%rbp), %xmm2
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	-80(%rbp), %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	movapd	%xmm6, %xmm7
	mulsd	-88(%rbp), %xmm7
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	movapd	%xmm4, %xmm5
	mulsd	-96(%rbp), %xmm5
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-104(%rbp), %xmm0
	movapd	%xmm1, %xmm4
	mulsd	%xmm1, %xmm4
	movapd	%xmm4, %xmm1
	mulsd	-112(%rbp), %xmm1
	movapd	%xmm8, %xmm4
	mulsd	%xmm8, %xmm4
	movapd	%xmm4, %xmm8
	mulsd	-120(%rbp), %xmm8
	movapd	%xmm9, %xmm4
	mulsd	%xmm9, %xmm4
	movapd	%xmm4, %xmm9
	mulsd	-128(%rbp), %xmm9
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	movapd	%xmm4, %xmm10
	mulsd	-136(%rbp), %xmm10
	movapd	%xmm11, %xmm6
	mulsd	%xmm11, %xmm6
	movapd	%xmm6, %xmm11
	mulsd	-144(%rbp), %xmm11
	movapd	%xmm12, %xmm4
	mulsd	%xmm12, %xmm4
	movapd	%xmm4, %xmm12
	mulsd	-152(%rbp), %xmm12
	movapd	%xmm13, %xmm4
	mulsd	%xmm13, %xmm4
	movapd	%xmm4, %xmm13
	mulsd	-160(%rbp), %xmm13
	movapd	%xmm14, %xmm4
	mulsd	%xmm14, %xmm4
	movapd	%xmm4, %xmm14
	mulsd	-168(%rbp), %xmm14
	movapd	%xmm15, %xmm4
	mulsd	%xmm15, %xmm4
	movapd	%xmm4, %xmm15
	mulsd	-176(%rbp), %xmm15
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm4
	mulsd	%xmm6, %xmm4
	mulsd	-184(%rbp), %xmm4
	movsd	%xmm4, -56(%rbp)
	movsd	-64(%rbp), %xmm4
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	mulsd	-192(%rbp), %xmm6
	movsd	%xmm6, -64(%rbp)
	movapd	%xmm2, %xmm4
	mulsd	%xmm2, %xmm4
	movapd	%xmm4, %xmm2
	mulsd	-72(%rbp), %xmm2
	movapd	%xmm3, %xmm6
	mulsd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	mulsd	-80(%rbp), %xmm3
	movapd	%xmm7, %xmm6
	mulsd	%xmm7, %xmm6
	mulsd	-88(%rbp), %xmm6
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	mulsd	-96(%rbp), %xmm4
	movapd	%xmm0, %xmm7
	mulsd	%xmm0, %xmm7
	movapd	%xmm7, %xmm0
	mulsd	-104(%rbp), %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	-112(%rbp), %xmm1
	movapd	%xmm8, %xmm7
	mulsd	%xmm8, %xmm7
	movapd	%xmm7, %xmm8
	mulsd	-120(%rbp), %xmm8
	movapd	%xmm9, %xmm5
	mulsd	%xmm9, %xmm5
	movapd	%xmm5, %xmm9
	mulsd	-128(%rbp), %xmm9
	movapd	%xmm10, %xmm5
	mulsd	%xmm10, %xmm5
	movapd	%xmm5, %xmm10
	mulsd	-136(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-144(%rbp), %xmm11
	movapd	%xmm12, %xmm5
	mulsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	mulsd	-152(%rbp), %xmm12
	movapd	%xmm13, %xmm5
	mulsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	mulsd	-160(%rbp), %xmm13
	movapd	%xmm14, %xmm5
	mulsd	%xmm14, %xmm5
	movapd	%xmm5, %xmm14
	mulsd	-168(%rbp), %xmm14
	movapd	%xmm15, %xmm5
	mulsd	%xmm15, %xmm5
	movapd	%xmm5, %xmm15
	mulsd	-176(%rbp), %xmm15
	movsd	-56(%rbp), %xmm7
	movapd	%xmm7, %xmm5
	mulsd	%xmm7, %xmm5
	mulsd	-184(%rbp), %xmm5
	movsd	%xmm5, -56(%rbp)
	movsd	-64(%rbp), %xmm5
	movapd	%xmm5, %xmm7
	mulsd	%xmm5, %xmm7
	mulsd	-192(%rbp), %xmm7
	movsd	%xmm7, -64(%rbp)
	movapd	%xmm2, %xmm5
	mulsd	%xmm2, %xmm5
	movapd	%xmm5, %xmm2
	mulsd	-72(%rbp), %xmm2
	movapd	%xmm3, %xmm7
	mulsd	%xmm3, %xmm7
	movapd	%xmm7, %xmm3
	mulsd	-80(%rbp), %xmm3
	movapd	%xmm6, %xmm7
	mulsd	%xmm6, %xmm7
	mulsd	-88(%rbp), %xmm7
	movapd	%xmm4, %xmm6
	mulsd	%xmm4, %xmm6
	movapd	%xmm6, %xmm4
	mulsd	-96(%rbp), %xmm4
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	mulsd	-104(%rbp), %xmm0
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	movapd	%xmm5, %xmm1
	mulsd	-112(%rbp), %xmm1
	movapd	%xmm8, %xmm6
	mulsd	%xmm8, %xmm6
	movapd	%xmm6, %xmm8
	mulsd	-120(%rbp), %xmm8
	movapd	%xmm9, %xmm6
	mulsd	%xmm9, %xmm6
	movapd	%xmm6, %xmm9
	mulsd	-128(%rbp), %xmm9
	movapd	%xmm10, %xmm6
	mulsd	%xmm10, %xmm6
	movapd	%xmm6, %xmm10
	mulsd	-136(%rbp), %xmm10
	movapd	%xmm11, %xmm5
	mulsd	%xmm11, %xmm5
	movapd	%xmm5, %xmm11
	mulsd	-144(%rbp), %xmm11
	movapd	%xmm12, %xmm5
	mulsd	%xmm12, %xmm5
	movapd	%xmm5, %xmm12
	mulsd	-152(%rbp), %xmm12
	movapd	%xmm13, %xmm5
	mulsd	%xmm13, %xmm5
	movapd	%xmm5, %xmm13
	mulsd	-160(%rbp), %xmm13
	movapd	%xmm14, %xmm6
	mulsd	%xmm14, %xmm6
	movapd	%xmm6, %xmm14
	mulsd	-168(%rbp), %xmm14
	movapd	%xmm15, %xmm6
	mulsd	%xmm15, %xmm6
	movapd	%xmm6, %xmm15
	mulsd	-176(%rbp), %xmm15
	movsd	-56(%rbp), %xmm5
	movapd	%xmm5, %xmm6
	mulsd	%xmm5, %xmm6
	mulsd	-184(%rbp), %xmm6
	movsd	%xmm6, -56(%rbp)
	movsd	-64(%rbp), %xmm6
	movapd	%xmm6, %xmm5
	mulsd	%xmm6, %xmm5
	mulsd	-192(%rbp), %xmm5
	movsd	%xmm5, -64(%rbp)
	movapd	%xmm2, %xmm6
	mulsd	%xmm2, %xmm6
	mulsd	-72(%rbp), %xmm6
	movsd	%xmm6, -200(%rbp)
	movapd	%xmm3, %xmm5
	mulsd	%xmm3, %xmm5
	mulsd	-80(%rbp), %xmm5
	movsd	%xmm5, -208(%rbp)
	movapd	%xmm7, %xmm3
	mulsd	%xmm7, %xmm3
	mulsd	-88(%rbp), %xmm3
	movsd	%xmm3, -216(%rbp)
	movapd	%xmm4, %xmm3
	mulsd	%xmm4, %xmm3
	mulsd	-96(%rbp), %xmm3
	movsd	%xmm3, -224(%rbp)
	movapd	%xmm0, %xmm6
	mulsd	%xmm0, %xmm6
	mulsd	-104(%rbp), %xmm6
	movsd	%xmm6, -232(%rbp)
	movapd	%xmm1, %xmm5
	mulsd	%xmm1, %xmm5
	mulsd	-112(%rbp), %xmm5
	movsd	%xmm5, -240(%rbp)
	movapd	%xmm8, %xmm2
	mulsd	%xmm8, %xmm2
	mulsd	-120(%rbp), %xmm2
	movsd	%xmm2, -248(%rbp)
	movapd	%xmm9, %xmm2
	mulsd	%xmm9, %xmm2
	mulsd	-128(%rbp), %xmm2
	movsd	%xmm2, -256(%rbp)
	movapd	%xmm10, %xmm4
	mulsd	%xmm10, %xmm4
	mulsd	-136(%rbp), %xmm4
	movsd	%xmm4, -264(%rbp)
	movapd	%xmm11, %xmm2
	mulsd	%xmm11, %xmm2
	mulsd	-144(%rbp), %xmm2
	movsd	%xmm2, -272(%rbp)
	movapd	%xmm12, %xmm2
	mulsd	%xmm12, %xmm2
	mulsd	-152(%rbp), %xmm2
	movsd	%xmm2, -280(%rbp)
	movapd	%xmm13, %xmm2
	mulsd	%xmm13, %xmm2
	mulsd	-160(%rbp), %xmm2
	movsd	%xmm2, -288(%rbp)
	movapd	%xmm14, %xmm2
	mulsd	%xmm14, %xmm2
	mulsd	-168(%rbp), %xmm2
	movsd	%xmm2, -296(%rbp)
	movapd	%xmm15, %xmm5
	mulsd	%xmm15, %xmm5
	mulsd	-176(%rbp), %xmm5
	movsd	%xmm5, -304(%rbp)
	movsd	-56(%rbp), %xmm6
	movapd	%xmm6, %xmm2
	mulsd	%xmm6, %xmm2
	mulsd	-184(%rbp), %xmm2
	movsd	%xmm2, -56(%rbp)
	movsd	-64(%rbp), %xmm5
	movapd	%xmm5, %xmm2
	mulsd	%xmm5, %xmm2
	mulsd	-192(%rbp), %xmm2
	movsd	%xmm2, -64(%rbp)
.L736:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L737
	cvttsd2sil	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-264(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-272(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-280(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-288(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-296(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-304(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE248:
	.size	double_mul_15, .-double_mul_15
	.globl	double_mul_benchmarks
	.section	.data.rel.local
	.align 32
	.type	double_mul_benchmarks, @object
	.size	double_mul_benchmarks, 128
double_mul_benchmarks:
	.quad	double_mul_0
	.quad	double_mul_1
	.quad	double_mul_2
	.quad	double_mul_3
	.quad	double_mul_4
	.quad	double_mul_5
	.quad	double_mul_6
	.quad	double_mul_7
	.quad	double_mul_8
	.quad	double_mul_9
	.quad	double_mul_10
	.quad	double_mul_11
	.quad	double_mul_12
	.quad	double_mul_13
	.quad	double_mul_14
	.quad	double_mul_15
	.text
	.globl	double_div_0
	.type	double_div_0, @function
double_div_0:
.LFB249:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$56, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm7
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	jmp	.L739
.L740:
	movsd	-56(%rbp), %xmm2
	movapd	%xmm2, %xmm3
	movapd	%xmm7, %xmm0
	divsd	%xmm0, %xmm3
	movapd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm2, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm2, %xmm6
	divsd	%xmm5, %xmm6
	movapd	%xmm2, %xmm7
	divsd	%xmm6, %xmm7
	movapd	%xmm2, %xmm3
	divsd	%xmm7, %xmm3
	movapd	%xmm2, %xmm4
	divsd	%xmm3, %xmm4
	movapd	%xmm2, %xmm5
	divsd	%xmm4, %xmm5
	movapd	%xmm2, %xmm6
	divsd	%xmm5, %xmm6
	divsd	%xmm6, %xmm2
	movapd	%xmm2, %xmm7
.L739:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L740
	cvttsd2sil	%xmm7, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE249:
	.size	double_div_0, .-double_div_0
	.globl	double_div_1
	.type	double_div_1, @function
double_div_1:
.LFB250:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$72, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm5
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	jmp	.L742
.L743:
	movsd	-64(%rbp), %xmm2
	movapd	%xmm2, %xmm6
	divsd	%xmm5, %xmm6
	movsd	-72(%rbp), %xmm7
	movapd	%xmm7, %xmm1
	divsd	-56(%rbp), %xmm1
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm7, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm0
	movapd	%xmm0, %xmm4
	divsd	%xmm1, %xmm4
	movapd	%xmm2, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm0, %xmm1
	divsd	%xmm4, %xmm1
	movapd	%xmm2, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm0, %xmm3
	divsd	%xmm1, %xmm3
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm0, %xmm7
	movapd	%xmm7, %xmm1
	divsd	%xmm3, %xmm1
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm1
	divsd	%xmm0, %xmm1
	divsd	%xmm3, %xmm2
	movapd	%xmm2, %xmm5
	divsd	%xmm1, %xmm7
	movsd	%xmm7, -56(%rbp)
.L742:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L743
	cvttsd2sil	%xmm5, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE250:
	.size	double_div_1, .-double_div_1
	.globl	double_div_2
	.type	double_div_2, @function
double_div_2:
.LFB251:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$88, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm3
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	jmp	.L745
.L746:
	movsd	-72(%rbp), %xmm2
	movapd	%xmm2, %xmm6
	movapd	%xmm3, %xmm5
	divsd	%xmm5, %xmm6
	movsd	-80(%rbp), %xmm7
	movapd	%xmm7, %xmm3
	divsd	-56(%rbp), %xmm3
	movsd	-88(%rbp), %xmm4
	movapd	%xmm4, %xmm1
	divsd	-64(%rbp), %xmm1
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm7, %xmm0
	divsd	%xmm3, %xmm0
	movapd	%xmm4, %xmm6
	movapd	%xmm6, %xmm3
	divsd	%xmm1, %xmm3
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm6, %xmm5
	movapd	%xmm5, %xmm11
	divsd	%xmm3, %xmm11
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm0
	movapd	%xmm0, %xmm6
	divsd	%xmm1, %xmm6
	movapd	%xmm5, %xmm10
	divsd	%xmm11, %xmm10
	movapd	%xmm2, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm0, %xmm4
	divsd	%xmm6, %xmm4
	movapd	%xmm5, %xmm1
	divsd	%xmm10, %xmm1
	movapd	%xmm2, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm0, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm5, %xmm4
	movapd	%xmm4, %xmm9
	divsd	%xmm1, %xmm9
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm0, %xmm7
	movapd	%xmm7, %xmm1
	divsd	%xmm3, %xmm1
	movapd	%xmm4, %xmm6
	movapd	%xmm6, %xmm0
	divsd	%xmm9, %xmm0
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm5
	divsd	%xmm1, %xmm5
	movapd	%xmm6, %xmm8
	divsd	%xmm0, %xmm8
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm1
	divsd	%xmm5, %xmm1
	movapd	%xmm6, %xmm0
	divsd	%xmm8, %xmm0
	divsd	%xmm3, %xmm2
	movapd	%xmm2, %xmm3
	divsd	%xmm1, %xmm7
	movsd	%xmm7, -56(%rbp)
	movapd	%xmm6, %xmm4
	divsd	%xmm0, %xmm4
	movsd	%xmm4, -64(%rbp)
.L745:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L746
	cvttsd2sil	%xmm3, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE251:
	.size	double_div_2, .-double_div_2
	.globl	double_div_3
	.type	double_div_3, @function
double_div_3:
.LFB252:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$104, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm3
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	jmp	.L748
.L749:
	movsd	-80(%rbp), %xmm2
	movapd	%xmm2, %xmm6
	movapd	%xmm3, %xmm7
	divsd	%xmm7, %xmm6
	movsd	-88(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	divsd	-56(%rbp), %xmm4
	movsd	-96(%rbp), %xmm7
	movapd	%xmm7, %xmm0
	divsd	-64(%rbp), %xmm0
	movsd	-104(%rbp), %xmm11
	movapd	%xmm11, %xmm15
	divsd	-72(%rbp), %xmm15
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm3, %xmm6
	movapd	%xmm6, %xmm1
	divsd	%xmm4, %xmm1
	movapd	%xmm7, %xmm3
	divsd	%xmm0, %xmm3
	movapd	%xmm11, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm7, %xmm10
	divsd	%xmm3, %xmm10
	movapd	%xmm11, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm6, %xmm4
	movapd	%xmm4, %xmm6
	divsd	%xmm0, %xmm6
	movapd	%xmm7, %xmm0
	movapd	%xmm0, %xmm1
	divsd	%xmm10, %xmm1
	movapd	%xmm11, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm2, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm4, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm0, %xmm9
	divsd	%xmm1, %xmm9
	movapd	%xmm11, %xmm15
	divsd	%xmm12, %xmm15
	movapd	%xmm2, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm1
	divsd	%xmm9, %xmm1
	movapd	%xmm11, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm7, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm11, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm1
	divsd	%xmm3, %xmm1
	movapd	%xmm6, %xmm8
	divsd	%xmm0, %xmm8
	movapd	%xmm11, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm6, %xmm5
	movapd	%xmm5, %xmm6
	divsd	%xmm8, %xmm6
	movapd	%xmm11, %xmm1
	divsd	%xmm12, %xmm1
	divsd	%xmm3, %xmm2
	movapd	%xmm2, %xmm3
	divsd	%xmm0, %xmm7
	movsd	%xmm7, -56(%rbp)
	divsd	%xmm6, %xmm5
	movsd	%xmm5, -64(%rbp)
	movapd	%xmm11, %xmm4
	divsd	%xmm1, %xmm4
	movsd	%xmm4, -72(%rbp)
.L748:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L749
	cvttsd2sil	%xmm3, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE252:
	.size	double_div_3, .-double_div_3
	.globl	double_div_4
	.type	double_div_4, @function
double_div_4:
.LFB253:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$120, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm3
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	jmp	.L751
.L752:
	movsd	-88(%rbp), %xmm2
	movapd	%xmm2, %xmm6
	divsd	%xmm3, %xmm6
	movsd	-96(%rbp), %xmm0
	movapd	%xmm0, %xmm1
	divsd	-56(%rbp), %xmm1
	movsd	-104(%rbp), %xmm7
	movapd	%xmm7, %xmm4
	divsd	-64(%rbp), %xmm4
	movsd	-112(%rbp), %xmm10
	movapd	%xmm10, %xmm13
	divsd	-72(%rbp), %xmm13
	movsd	-120(%rbp), %xmm8
	movapd	%xmm8, %xmm9
	divsd	-80(%rbp), %xmm9
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm0, %xmm6
	movapd	%xmm6, %xmm3
	divsd	%xmm1, %xmm3
	movapd	%xmm7, %xmm0
	movapd	%xmm0, %xmm1
	divsd	%xmm4, %xmm1
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm15
	divsd	%xmm9, %xmm15
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm0, %xmm9
	divsd	%xmm1, %xmm9
	movapd	%xmm10, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm6, %xmm4
	movapd	%xmm4, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm0, %xmm1
	divsd	%xmm9, %xmm1
	movapd	%xmm10, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm8, %xmm9
	movapd	%xmm9, %xmm12
	divsd	%xmm14, %xmm12
	movapd	%xmm2, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm4, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm0, %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm10, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm2, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm1
	divsd	%xmm8, %xmm1
	movapd	%xmm10, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm9, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm7, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm6
	divsd	%xmm1, %xmm6
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm9, %xmm8
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm5
	divsd	%xmm3, %xmm5
	movapd	%xmm0, %xmm1
	divsd	%xmm6, %xmm1
	movapd	%xmm10, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm6
	divsd	%xmm1, %xmm6
	movapd	%xmm10, %xmm1
	divsd	%xmm11, %xmm1
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	divsd	%xmm3, %xmm2
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm7
	movsd	%xmm7, -56(%rbp)
	divsd	%xmm6, %xmm0
	movsd	%xmm0, -64(%rbp)
	movapd	%xmm10, %xmm5
	divsd	%xmm1, %xmm5
	movsd	%xmm5, -72(%rbp)
	divsd	%xmm14, %xmm8
	movsd	%xmm8, -80(%rbp)
.L751:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L752
	cvttsd2sil	%xmm3, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE253:
	.size	double_div_4, .-double_div_4
	.globl	double_div_5
	.type	double_div_5, @function
double_div_5:
.LFB254:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$136, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm3
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	jmp	.L754
.L755:
	movsd	-96(%rbp), %xmm2
	movapd	%xmm2, %xmm6
	divsd	%xmm3, %xmm6
	movsd	-104(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	divsd	-64(%rbp), %xmm4
	movsd	-112(%rbp), %xmm0
	movapd	%xmm0, %xmm7
	divsd	-72(%rbp), %xmm7
	movsd	-120(%rbp), %xmm11
	movapd	%xmm11, %xmm1
	divsd	-80(%rbp), %xmm1
	movsd	-128(%rbp), %xmm8
	movapd	%xmm8, %xmm15
	divsd	-88(%rbp), %xmm15
	movsd	-136(%rbp), %xmm12
	movapd	%xmm12, %xmm13
	divsd	-56(%rbp), %xmm13
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm10
	divsd	%xmm7, %xmm10
	movapd	%xmm11, %xmm14
	divsd	%xmm1, %xmm14
	movapd	%xmm8, %xmm9
	divsd	%xmm15, %xmm9
	movapd	%xmm12, %xmm7
	divsd	%xmm13, %xmm7
	movsd	%xmm7, -56(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm0, %xmm1
	divsd	%xmm10, %xmm1
	movapd	%xmm11, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm8, %xmm15
	divsd	%xmm9, %xmm15
	movapd	%xmm12, %xmm14
	movapd	%xmm14, %xmm10
	divsd	-56(%rbp), %xmm10
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm6, %xmm4
	movapd	%xmm4, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm0, %xmm9
	divsd	%xmm1, %xmm9
	movapd	%xmm11, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm13
	divsd	%xmm15, %xmm13
	movapd	%xmm14, %xmm5
	divsd	%xmm10, %xmm5
	movsd	%xmm5, -56(%rbp)
	movapd	%xmm2, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm4, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm0, %xmm1
	divsd	%xmm9, %xmm1
	movapd	%xmm11, %xmm15
	divsd	%xmm12, %xmm15
	movapd	%xmm8, %xmm9
	movapd	%xmm9, %xmm10
	divsd	%xmm13, %xmm10
	movapd	%xmm14, %xmm12
	movapd	%xmm12, %xmm13
	divsd	-56(%rbp), %xmm13
	movapd	%xmm2, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm4, %xmm7
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm11, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm15
	divsd	%xmm10, %xmm15
	movapd	%xmm12, %xmm3
	divsd	%xmm13, %xmm3
	movsd	%xmm3, -56(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm7, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm1
	divsd	%xmm8, %xmm1
	movapd	%xmm11, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm9, %xmm8
	movapd	%xmm8, %xmm10
	divsd	%xmm15, %xmm10
	movapd	%xmm12, %xmm14
	movapd	%xmm14, %xmm9
	divsd	-56(%rbp), %xmm9
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm5
	divsd	%xmm3, %xmm5
	movapd	%xmm0, %xmm6
	divsd	%xmm1, %xmm6
	movapd	%xmm11, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm15
	divsd	%xmm10, %xmm15
	movapd	%xmm14, %xmm13
	divsd	%xmm9, %xmm13
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm1
	divsd	%xmm6, %xmm1
	movapd	%xmm11, %xmm6
	divsd	%xmm12, %xmm6
	movapd	%xmm8, %xmm10
	divsd	%xmm15, %xmm10
	movapd	%xmm14, %xmm9
	divsd	%xmm13, %xmm9
	divsd	%xmm3, %xmm2
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm7
	movsd	%xmm7, -64(%rbp)
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -72(%rbp)
	movapd	%xmm11, %xmm5
	divsd	%xmm6, %xmm5
	movsd	%xmm5, -80(%rbp)
	divsd	%xmm10, %xmm8
	movsd	%xmm8, -88(%rbp)
	movapd	%xmm14, %xmm12
	divsd	%xmm9, %xmm12
	movsd	%xmm12, -56(%rbp)
.L754:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L755
	cvttsd2sil	%xmm3, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE254:
	.size	double_div_5, .-double_div_5
	.globl	double_div_6
	.type	double_div_6, @function
double_div_6:
.LFB255:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$152, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm3
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	jmp	.L757
.L758:
	movsd	-112(%rbp), %xmm2
	movapd	%xmm2, %xmm6
	divsd	%xmm3, %xmm6
	movsd	-120(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	divsd	-80(%rbp), %xmm4
	movsd	-128(%rbp), %xmm0
	movapd	%xmm0, %xmm7
	divsd	-88(%rbp), %xmm7
	movsd	-136(%rbp), %xmm11
	movapd	%xmm11, %xmm1
	divsd	-96(%rbp), %xmm1
	movsd	-144(%rbp), %xmm8
	movapd	%xmm8, %xmm15
	divsd	-104(%rbp), %xmm15
	movsd	-152(%rbp), %xmm12
	movapd	%xmm12, %xmm13
	divsd	-72(%rbp), %xmm13
	movsd	-64(%rbp), %xmm5
	divsd	-56(%rbp), %xmm5
	movsd	%xmm5, -56(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm10
	divsd	%xmm7, %xmm10
	movapd	%xmm11, %xmm14
	divsd	%xmm1, %xmm14
	movapd	%xmm8, %xmm9
	divsd	%xmm15, %xmm9
	movapd	%xmm12, %xmm7
	divsd	%xmm13, %xmm7
	movsd	%xmm7, -72(%rbp)
	movsd	-64(%rbp), %xmm4
	divsd	-56(%rbp), %xmm4
	movsd	%xmm4, -56(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	movapd	%xmm5, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm0, %xmm1
	divsd	%xmm10, %xmm1
	movapd	%xmm11, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm8, %xmm15
	divsd	%xmm9, %xmm15
	movapd	%xmm12, %xmm14
	movapd	%xmm14, %xmm10
	divsd	-72(%rbp), %xmm10
	movsd	-64(%rbp), %xmm6
	divsd	-56(%rbp), %xmm6
	movsd	%xmm6, -56(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm5, %xmm4
	movapd	%xmm4, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm0, %xmm9
	divsd	%xmm1, %xmm9
	movapd	%xmm11, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm13
	divsd	%xmm15, %xmm13
	movapd	%xmm14, %xmm5
	divsd	%xmm10, %xmm5
	movsd	%xmm5, -72(%rbp)
	movsd	-64(%rbp), %xmm7
	divsd	-56(%rbp), %xmm7
	movsd	%xmm7, -56(%rbp)
	movapd	%xmm2, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm4, %xmm3
	movapd	%xmm3, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm0, %xmm1
	divsd	%xmm9, %xmm1
	movapd	%xmm11, %xmm15
	divsd	%xmm12, %xmm15
	movapd	%xmm8, %xmm9
	movapd	%xmm9, %xmm10
	divsd	%xmm13, %xmm10
	movapd	%xmm14, %xmm12
	movapd	%xmm12, %xmm13
	divsd	-72(%rbp), %xmm13
	movsd	-64(%rbp), %xmm4
	divsd	-56(%rbp), %xmm4
	movsd	%xmm4, -56(%rbp)
	movapd	%xmm2, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm3, %xmm7
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm11, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm15
	divsd	%xmm10, %xmm15
	movapd	%xmm12, %xmm3
	divsd	%xmm13, %xmm3
	movsd	%xmm3, -72(%rbp)
	movsd	-64(%rbp), %xmm5
	divsd	-56(%rbp), %xmm5
	movsd	%xmm5, -56(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm7, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm6
	movapd	%xmm6, %xmm1
	divsd	%xmm8, %xmm1
	movapd	%xmm11, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm9, %xmm8
	movapd	%xmm8, %xmm10
	divsd	%xmm15, %xmm10
	movapd	%xmm12, %xmm14
	movapd	%xmm14, %xmm9
	divsd	-72(%rbp), %xmm9
	movsd	-64(%rbp), %xmm0
	divsd	-56(%rbp), %xmm0
	movsd	%xmm0, -56(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm5
	divsd	%xmm3, %xmm5
	movapd	%xmm6, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm11, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm15
	divsd	%xmm10, %xmm15
	movapd	%xmm14, %xmm13
	divsd	%xmm9, %xmm13
	movsd	-64(%rbp), %xmm3
	divsd	-56(%rbp), %xmm3
	movsd	%xmm3, -56(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm11, %xmm5
	movapd	%xmm5, %xmm0
	divsd	%xmm12, %xmm0
	movapd	%xmm8, %xmm10
	divsd	%xmm15, %xmm10
	movapd	%xmm14, %xmm12
	movapd	%xmm12, %xmm9
	divsd	%xmm13, %xmm9
	movsd	-64(%rbp), %xmm11
	movapd	%xmm11, %xmm14
	divsd	-56(%rbp), %xmm14
	divsd	%xmm3, %xmm2
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm7
	movsd	%xmm7, -80(%rbp)
	divsd	%xmm1, %xmm6
	movsd	%xmm6, -88(%rbp)
	divsd	%xmm0, %xmm5
	movsd	%xmm5, -96(%rbp)
	divsd	%xmm10, %xmm8
	movsd	%xmm8, -104(%rbp)
	divsd	%xmm9, %xmm12
	movsd	%xmm12, -72(%rbp)
	divsd	%xmm14, %xmm11
	movsd	%xmm11, -56(%rbp)
.L757:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L758
	cvttsd2sil	%xmm3, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE255:
	.size	double_div_6, .-double_div_6
	.globl	double_div_7
	.type	double_div_7, @function
double_div_7:
.LFB256:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm3
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	jmp	.L760
.L761:
	movsd	-128(%rbp), %xmm2
	movapd	%xmm2, %xmm6
	movapd	%xmm3, %xmm7
	divsd	%xmm7, %xmm6
	movsd	-136(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	divsd	-96(%rbp), %xmm4
	movsd	-144(%rbp), %xmm0
	movapd	%xmm0, %xmm7
	divsd	-104(%rbp), %xmm7
	movsd	-152(%rbp), %xmm10
	movapd	%xmm10, %xmm13
	divsd	-112(%rbp), %xmm13
	movsd	-160(%rbp), %xmm8
	movapd	%xmm8, %xmm9
	divsd	-120(%rbp), %xmm9
	movsd	-168(%rbp), %xmm11
	movapd	%xmm11, %xmm14
	divsd	-72(%rbp), %xmm14
	movsd	-88(%rbp), %xmm5
	divsd	-56(%rbp), %xmm5
	movsd	%xmm5, -56(%rbp)
	movsd	-80(%rbp), %xmm1
	divsd	-64(%rbp), %xmm1
	movsd	%xmm1, -64(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm1
	divsd	%xmm7, %xmm1
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm15
	divsd	%xmm9, %xmm15
	movapd	%xmm11, %xmm13
	movapd	%xmm13, %xmm7
	divsd	%xmm14, %xmm7
	movsd	%xmm7, -72(%rbp)
	movsd	-88(%rbp), %xmm4
	divsd	-56(%rbp), %xmm4
	movsd	%xmm4, -56(%rbp)
	movsd	-80(%rbp), %xmm14
	divsd	-64(%rbp), %xmm14
	movsd	%xmm14, -64(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	movapd	%xmm5, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm0, %xmm9
	divsd	%xmm1, %xmm9
	movapd	%xmm10, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm12
	divsd	-72(%rbp), %xmm12
	movsd	%xmm12, -72(%rbp)
	movsd	-88(%rbp), %xmm3
	divsd	-56(%rbp), %xmm3
	movsd	%xmm3, -56(%rbp)
	movsd	-80(%rbp), %xmm6
	divsd	-64(%rbp), %xmm6
	movsd	%xmm6, -64(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm5, %xmm4
	movapd	%xmm4, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm0, %xmm1
	divsd	%xmm9, %xmm1
	movapd	%xmm10, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm8, %xmm9
	movapd	%xmm9, %xmm12
	divsd	%xmm14, %xmm12
	movapd	%xmm13, %xmm5
	divsd	-72(%rbp), %xmm5
	movsd	%xmm5, -72(%rbp)
	movsd	-88(%rbp), %xmm7
	divsd	-56(%rbp), %xmm7
	movsd	%xmm7, -56(%rbp)
	movsd	-80(%rbp), %xmm8
	divsd	-64(%rbp), %xmm8
	movsd	%xmm8, -64(%rbp)
	movapd	%xmm2, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm4, %xmm3
	movapd	%xmm3, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm0, %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm10, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm13, %xmm12
	movapd	%xmm12, %xmm6
	divsd	-72(%rbp), %xmm6
	movsd	%xmm6, -72(%rbp)
	movsd	-88(%rbp), %xmm4
	divsd	-56(%rbp), %xmm4
	movsd	%xmm4, -56(%rbp)
	movsd	-80(%rbp), %xmm1
	divsd	-64(%rbp), %xmm1
	movsd	%xmm1, -64(%rbp)
	movapd	%xmm2, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm3, %xmm7
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm1
	divsd	%xmm8, %xmm1
	movapd	%xmm10, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm9, %xmm8
	movapd	%xmm8, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm12, %xmm9
	movapd	%xmm9, %xmm11
	divsd	-72(%rbp), %xmm11
	movsd	-88(%rbp), %xmm3
	divsd	-56(%rbp), %xmm3
	movsd	%xmm3, -56(%rbp)
	movsd	-80(%rbp), %xmm5
	divsd	-64(%rbp), %xmm5
	movsd	%xmm5, -64(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm7, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm13
	movapd	%xmm13, %xmm9
	divsd	%xmm11, %xmm9
	movsd	-88(%rbp), %xmm4
	divsd	-56(%rbp), %xmm4
	movsd	%xmm4, -56(%rbp)
	movsd	-80(%rbp), %xmm15
	divsd	-64(%rbp), %xmm15
	movsd	%xmm15, -64(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm5
	divsd	%xmm3, %xmm5
	movapd	%xmm6, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm10, %xmm0
	movapd	%xmm0, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm13, %xmm3
	divsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movsd	-88(%rbp), %xmm12
	movapd	%xmm12, %xmm3
	divsd	-56(%rbp), %xmm3
	movsd	%xmm3, -56(%rbp)
	movsd	-80(%rbp), %xmm10
	divsd	-64(%rbp), %xmm10
	movsd	%xmm10, -64(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	divsd	%xmm1, %xmm5
	movapd	%xmm0, %xmm1
	divsd	%xmm11, %xmm1
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm10
	divsd	%xmm9, %xmm10
	movapd	%xmm12, %xmm15
	movapd	%xmm15, %xmm11
	divsd	-56(%rbp), %xmm11
	movsd	-80(%rbp), %xmm12
	movapd	%xmm12, %xmm9
	divsd	-64(%rbp), %xmm9
	divsd	%xmm3, %xmm2
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm7
	movsd	%xmm7, -96(%rbp)
	divsd	%xmm5, %xmm6
	movsd	%xmm6, -104(%rbp)
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -112(%rbp)
	divsd	%xmm14, %xmm8
	movsd	%xmm8, -120(%rbp)
	divsd	%xmm10, %xmm13
	movsd	%xmm13, -72(%rbp)
	divsd	%xmm11, %xmm15
	movsd	%xmm15, -56(%rbp)
	divsd	%xmm9, %xmm12
	movsd	%xmm12, -64(%rbp)
.L760:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L761
	cvttsd2sil	%xmm3, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE256:
	.size	double_div_7, .-double_div_7
	.globl	double_div_8
	.type	double_div_8, @function
double_div_8:
.LFB257:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$184, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movapd	%xmm1, %xmm3
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -176(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -184(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	jmp	.L763
.L764:
	movsd	-144(%rbp), %xmm2
	movapd	%xmm2, %xmm6
	movapd	%xmm3, %xmm7
	divsd	%xmm7, %xmm6
	movsd	-152(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	divsd	-112(%rbp), %xmm4
	movsd	-160(%rbp), %xmm0
	movapd	%xmm0, %xmm7
	divsd	-120(%rbp), %xmm7
	movsd	-168(%rbp), %xmm10
	movapd	%xmm10, %xmm13
	divsd	-128(%rbp), %xmm13
	movsd	-176(%rbp), %xmm8
	movapd	%xmm8, %xmm9
	divsd	-136(%rbp), %xmm9
	movsd	-184(%rbp), %xmm11
	movapd	%xmm11, %xmm14
	divsd	-80(%rbp), %xmm14
	movsd	-104(%rbp), %xmm5
	divsd	-56(%rbp), %xmm5
	movsd	%xmm5, -56(%rbp)
	movsd	-96(%rbp), %xmm1
	divsd	-64(%rbp), %xmm1
	movsd	%xmm1, -64(%rbp)
	movsd	-88(%rbp), %xmm15
	divsd	-72(%rbp), %xmm15
	movsd	%xmm15, -72(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm1
	divsd	%xmm7, %xmm1
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm15
	divsd	%xmm9, %xmm15
	movapd	%xmm11, %xmm13
	movapd	%xmm13, %xmm7
	divsd	%xmm14, %xmm7
	movsd	%xmm7, -80(%rbp)
	movsd	-104(%rbp), %xmm4
	divsd	-56(%rbp), %xmm4
	movsd	%xmm4, -56(%rbp)
	movsd	-96(%rbp), %xmm14
	divsd	-64(%rbp), %xmm14
	movsd	%xmm14, -64(%rbp)
	movsd	-88(%rbp), %xmm11
	divsd	-72(%rbp), %xmm11
	movsd	%xmm11, -72(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	movapd	%xmm5, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm0, %xmm9
	divsd	%xmm1, %xmm9
	movapd	%xmm10, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm12
	divsd	-80(%rbp), %xmm12
	movsd	%xmm12, -80(%rbp)
	movsd	-104(%rbp), %xmm3
	divsd	-56(%rbp), %xmm3
	movsd	%xmm3, -56(%rbp)
	movsd	-96(%rbp), %xmm6
	divsd	-64(%rbp), %xmm6
	movsd	%xmm6, -64(%rbp)
	movsd	-88(%rbp), %xmm1
	divsd	-72(%rbp), %xmm1
	movsd	%xmm1, -72(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm5, %xmm4
	movapd	%xmm4, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm0, %xmm1
	divsd	%xmm9, %xmm1
	movapd	%xmm10, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm8, %xmm9
	movapd	%xmm9, %xmm12
	divsd	%xmm14, %xmm12
	movapd	%xmm13, %xmm5
	divsd	-80(%rbp), %xmm5
	movsd	%xmm5, -80(%rbp)
	movsd	-104(%rbp), %xmm7
	divsd	-56(%rbp), %xmm7
	movsd	%xmm7, -56(%rbp)
	movsd	-96(%rbp), %xmm8
	divsd	-64(%rbp), %xmm8
	movsd	%xmm8, -64(%rbp)
	movsd	-88(%rbp), %xmm14
	divsd	-72(%rbp), %xmm14
	movsd	%xmm14, -72(%rbp)
	movapd	%xmm2, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm4, %xmm3
	movapd	%xmm3, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm0, %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm10, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm13, %xmm12
	movapd	%xmm12, %xmm6
	divsd	-80(%rbp), %xmm6
	movsd	%xmm6, -80(%rbp)
	movsd	-104(%rbp), %xmm4
	divsd	-56(%rbp), %xmm4
	movsd	%xmm4, -56(%rbp)
	movsd	-96(%rbp), %xmm1
	divsd	-64(%rbp), %xmm1
	movsd	%xmm1, -64(%rbp)
	movsd	-88(%rbp), %xmm13
	divsd	-72(%rbp), %xmm13
	movsd	%xmm13, -72(%rbp)
	movapd	%xmm2, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm3, %xmm7
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm1
	divsd	%xmm8, %xmm1
	movapd	%xmm10, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm9, %xmm8
	movapd	%xmm8, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm12, %xmm9
	movapd	%xmm9, %xmm11
	divsd	-80(%rbp), %xmm11
	movsd	-104(%rbp), %xmm3
	divsd	-56(%rbp), %xmm3
	movsd	%xmm3, -56(%rbp)
	movsd	-96(%rbp), %xmm5
	divsd	-64(%rbp), %xmm5
	movsd	%xmm5, -64(%rbp)
	movsd	-88(%rbp), %xmm14
	divsd	-72(%rbp), %xmm14
	movsd	%xmm14, -72(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm7, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm13
	movapd	%xmm13, %xmm9
	divsd	%xmm11, %xmm9
	movsd	-104(%rbp), %xmm4
	divsd	-56(%rbp), %xmm4
	movsd	%xmm4, -56(%rbp)
	movsd	-96(%rbp), %xmm15
	divsd	-64(%rbp), %xmm15
	movsd	%xmm15, -64(%rbp)
	movsd	-88(%rbp), %xmm1
	divsd	-72(%rbp), %xmm1
	movsd	%xmm1, -72(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm5
	divsd	%xmm3, %xmm5
	movapd	%xmm6, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm10, %xmm0
	movapd	%xmm0, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm13, %xmm3
	divsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movsd	-104(%rbp), %xmm12
	movapd	%xmm12, %xmm3
	divsd	-56(%rbp), %xmm3
	movsd	%xmm3, -56(%rbp)
	movsd	-96(%rbp), %xmm10
	divsd	-64(%rbp), %xmm10
	movsd	%xmm10, -64(%rbp)
	movsd	-88(%rbp), %xmm14
	divsd	-72(%rbp), %xmm14
	movsd	%xmm14, -72(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	divsd	%xmm1, %xmm5
	movapd	%xmm0, %xmm1
	divsd	%xmm11, %xmm1
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm10
	divsd	%xmm9, %xmm10
	movsd	%xmm10, -80(%rbp)
	movapd	%xmm12, %xmm15
	movapd	%xmm15, %xmm11
	divsd	-56(%rbp), %xmm11
	movsd	-96(%rbp), %xmm12
	movapd	%xmm12, %xmm9
	divsd	-64(%rbp), %xmm9
	movsd	-88(%rbp), %xmm10
	divsd	-72(%rbp), %xmm10
	divsd	%xmm3, %xmm2
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm7
	movsd	%xmm7, -112(%rbp)
	divsd	%xmm5, %xmm6
	movsd	%xmm6, -120(%rbp)
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -128(%rbp)
	divsd	%xmm14, %xmm8
	movsd	%xmm8, -136(%rbp)
	divsd	-80(%rbp), %xmm13
	movsd	%xmm13, -80(%rbp)
	divsd	%xmm11, %xmm15
	movsd	%xmm15, -56(%rbp)
	divsd	%xmm9, %xmm12
	movsd	%xmm12, -64(%rbp)
	movsd	-88(%rbp), %xmm14
	divsd	%xmm10, %xmm14
	movsd	%xmm14, -72(%rbp)
.L763:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L764
	cvttsd2sil	%xmm3, %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE257:
	.size	double_div_8, .-double_div_8
	.globl	double_div_9
	.type	double_div_9, @function
double_div_9:
.LFB258:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$200, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -176(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -184(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -192(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -200(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -208(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	jmp	.L766
.L767:
	movsd	-168(%rbp), %xmm2
	movapd	%xmm2, %xmm6
	divsd	-128(%rbp), %xmm6
	movsd	-176(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	divsd	-136(%rbp), %xmm4
	movsd	-184(%rbp), %xmm0
	movapd	%xmm0, %xmm7
	divsd	-144(%rbp), %xmm7
	movsd	-192(%rbp), %xmm10
	movapd	%xmm10, %xmm13
	divsd	-152(%rbp), %xmm13
	movsd	-200(%rbp), %xmm8
	movapd	%xmm8, %xmm9
	divsd	-160(%rbp), %xmm9
	movsd	-208(%rbp), %xmm11
	movapd	%xmm11, %xmm14
	divsd	-88(%rbp), %xmm14
	movsd	-120(%rbp), %xmm5
	divsd	-64(%rbp), %xmm5
	movsd	%xmm5, -64(%rbp)
	movsd	-112(%rbp), %xmm1
	divsd	-72(%rbp), %xmm1
	movsd	%xmm1, -72(%rbp)
	movsd	-96(%rbp), %xmm15
	divsd	-56(%rbp), %xmm15
	movsd	%xmm15, -56(%rbp)
	movsd	-104(%rbp), %xmm12
	divsd	-80(%rbp), %xmm12
	movsd	%xmm12, -80(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm1
	divsd	%xmm7, %xmm1
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm15
	divsd	%xmm9, %xmm15
	movapd	%xmm11, %xmm13
	movapd	%xmm13, %xmm7
	divsd	%xmm14, %xmm7
	movsd	%xmm7, -88(%rbp)
	movsd	-120(%rbp), %xmm4
	divsd	-64(%rbp), %xmm4
	movsd	%xmm4, -64(%rbp)
	movsd	-112(%rbp), %xmm14
	divsd	-72(%rbp), %xmm14
	movsd	%xmm14, -72(%rbp)
	movsd	-96(%rbp), %xmm11
	divsd	-56(%rbp), %xmm11
	movsd	%xmm11, -56(%rbp)
	movsd	-104(%rbp), %xmm9
	divsd	-80(%rbp), %xmm9
	movsd	%xmm9, -80(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	movapd	%xmm5, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm0, %xmm9
	divsd	%xmm1, %xmm9
	movapd	%xmm10, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm12
	divsd	-88(%rbp), %xmm12
	movsd	%xmm12, -88(%rbp)
	movsd	-120(%rbp), %xmm3
	divsd	-64(%rbp), %xmm3
	movsd	%xmm3, -64(%rbp)
	movsd	-112(%rbp), %xmm6
	divsd	-72(%rbp), %xmm6
	movsd	%xmm6, -72(%rbp)
	movsd	-96(%rbp), %xmm1
	divsd	-56(%rbp), %xmm1
	movsd	%xmm1, -56(%rbp)
	movsd	-104(%rbp), %xmm15
	divsd	-80(%rbp), %xmm15
	movsd	%xmm15, -80(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm5, %xmm4
	movapd	%xmm4, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm0, %xmm1
	divsd	%xmm9, %xmm1
	movapd	%xmm10, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm8, %xmm9
	movapd	%xmm9, %xmm12
	divsd	%xmm14, %xmm12
	movapd	%xmm13, %xmm5
	divsd	-88(%rbp), %xmm5
	movsd	%xmm5, -88(%rbp)
	movsd	-120(%rbp), %xmm7
	divsd	-64(%rbp), %xmm7
	movsd	%xmm7, -64(%rbp)
	movsd	-112(%rbp), %xmm8
	divsd	-72(%rbp), %xmm8
	movsd	%xmm8, -72(%rbp)
	movsd	-96(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-104(%rbp), %xmm11
	divsd	-80(%rbp), %xmm11
	movsd	%xmm11, -80(%rbp)
	movapd	%xmm2, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm4, %xmm3
	movapd	%xmm3, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm0, %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm10, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm13, %xmm12
	movapd	%xmm12, %xmm6
	divsd	-88(%rbp), %xmm6
	movsd	%xmm6, -88(%rbp)
	movsd	-120(%rbp), %xmm4
	divsd	-64(%rbp), %xmm4
	movsd	%xmm4, -64(%rbp)
	movsd	-112(%rbp), %xmm1
	divsd	-72(%rbp), %xmm1
	movsd	%xmm1, -72(%rbp)
	movsd	-96(%rbp), %xmm13
	divsd	-56(%rbp), %xmm13
	movsd	%xmm13, -56(%rbp)
	movsd	-104(%rbp), %xmm15
	divsd	-80(%rbp), %xmm15
	movsd	%xmm15, -80(%rbp)
	movapd	%xmm2, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm3, %xmm7
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm1
	divsd	%xmm8, %xmm1
	movapd	%xmm10, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm9, %xmm8
	movapd	%xmm8, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm12, %xmm9
	movapd	%xmm9, %xmm11
	divsd	-88(%rbp), %xmm11
	movsd	-120(%rbp), %xmm3
	divsd	-64(%rbp), %xmm3
	movsd	%xmm3, -64(%rbp)
	movsd	-112(%rbp), %xmm5
	divsd	-72(%rbp), %xmm5
	movsd	%xmm5, -72(%rbp)
	movsd	-96(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-104(%rbp), %xmm12
	divsd	-80(%rbp), %xmm12
	movsd	%xmm12, -80(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm7, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm13
	movapd	%xmm13, %xmm9
	divsd	%xmm11, %xmm9
	movsd	-120(%rbp), %xmm4
	divsd	-64(%rbp), %xmm4
	movsd	%xmm4, -64(%rbp)
	movsd	-112(%rbp), %xmm15
	divsd	-72(%rbp), %xmm15
	movsd	%xmm15, -72(%rbp)
	movsd	-96(%rbp), %xmm1
	divsd	-56(%rbp), %xmm1
	movsd	%xmm1, -56(%rbp)
	movsd	-104(%rbp), %xmm11
	divsd	-80(%rbp), %xmm11
	movsd	%xmm11, -80(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm5
	divsd	%xmm3, %xmm5
	movapd	%xmm6, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm10, %xmm0
	movapd	%xmm0, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm13, %xmm3
	divsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movsd	-120(%rbp), %xmm12
	movapd	%xmm12, %xmm3
	divsd	-64(%rbp), %xmm3
	movsd	%xmm3, -64(%rbp)
	movsd	-112(%rbp), %xmm10
	divsd	-72(%rbp), %xmm10
	movsd	%xmm10, -72(%rbp)
	movsd	-96(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-104(%rbp), %xmm3
	divsd	-80(%rbp), %xmm3
	movsd	%xmm3, -80(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	divsd	%xmm1, %xmm5
	movapd	%xmm0, %xmm1
	divsd	%xmm11, %xmm1
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm10
	divsd	%xmm9, %xmm10
	movsd	%xmm10, -88(%rbp)
	movapd	%xmm12, %xmm15
	movapd	%xmm15, %xmm11
	divsd	-64(%rbp), %xmm11
	movsd	-112(%rbp), %xmm12
	movapd	%xmm12, %xmm9
	divsd	-72(%rbp), %xmm9
	movsd	-96(%rbp), %xmm10
	divsd	-56(%rbp), %xmm10
	movsd	%xmm10, -56(%rbp)
	movsd	-104(%rbp), %xmm10
	divsd	-80(%rbp), %xmm10
	divsd	%xmm3, %xmm2
	movsd	%xmm2, -128(%rbp)
	divsd	%xmm4, %xmm7
	movsd	%xmm7, -136(%rbp)
	divsd	%xmm5, %xmm6
	movsd	%xmm6, -144(%rbp)
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -152(%rbp)
	divsd	%xmm14, %xmm8
	movsd	%xmm8, -160(%rbp)
	divsd	-88(%rbp), %xmm13
	movsd	%xmm13, -88(%rbp)
	divsd	%xmm11, %xmm15
	movsd	%xmm15, -64(%rbp)
	divsd	%xmm9, %xmm12
	movsd	%xmm12, -72(%rbp)
	movsd	-96(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-104(%rbp), %xmm3
	divsd	%xmm10, %xmm3
	movsd	%xmm3, -80(%rbp)
.L766:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L767
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE258:
	.size	double_div_9, .-double_div_9
	.globl	double_div_10
	.type	double_div_10, @function
double_div_10:
.LFB259:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$216, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -184(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -192(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -200(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -208(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -176(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -216(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -224(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	jmp	.L769
.L770:
	movsd	-184(%rbp), %xmm2
	movapd	%xmm2, %xmm6
	divsd	-144(%rbp), %xmm6
	movsd	-192(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	divsd	-152(%rbp), %xmm4
	movsd	-200(%rbp), %xmm0
	movapd	%xmm0, %xmm7
	divsd	-160(%rbp), %xmm7
	movsd	-208(%rbp), %xmm10
	movapd	%xmm10, %xmm13
	divsd	-168(%rbp), %xmm13
	movsd	-216(%rbp), %xmm8
	movapd	%xmm8, %xmm9
	divsd	-176(%rbp), %xmm9
	movsd	-224(%rbp), %xmm11
	movapd	%xmm11, %xmm14
	divsd	-96(%rbp), %xmm14
	movsd	-136(%rbp), %xmm5
	divsd	-72(%rbp), %xmm5
	movsd	%xmm5, -72(%rbp)
	movsd	-128(%rbp), %xmm1
	divsd	-80(%rbp), %xmm1
	movsd	%xmm1, -80(%rbp)
	movsd	-104(%rbp), %xmm15
	divsd	-56(%rbp), %xmm15
	movsd	%xmm15, -56(%rbp)
	movsd	-112(%rbp), %xmm12
	divsd	-64(%rbp), %xmm12
	movsd	%xmm12, -64(%rbp)
	movsd	-120(%rbp), %xmm5
	divsd	-88(%rbp), %xmm5
	movsd	%xmm5, -88(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm1
	divsd	%xmm7, %xmm1
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm15
	divsd	%xmm9, %xmm15
	movapd	%xmm11, %xmm13
	movapd	%xmm13, %xmm7
	divsd	%xmm14, %xmm7
	movsd	%xmm7, -96(%rbp)
	movsd	-136(%rbp), %xmm4
	divsd	-72(%rbp), %xmm4
	movsd	%xmm4, -72(%rbp)
	movsd	-128(%rbp), %xmm14
	divsd	-80(%rbp), %xmm14
	movsd	%xmm14, -80(%rbp)
	movsd	-104(%rbp), %xmm11
	divsd	-56(%rbp), %xmm11
	movsd	%xmm11, -56(%rbp)
	movsd	-112(%rbp), %xmm9
	divsd	-64(%rbp), %xmm9
	movsd	%xmm9, -64(%rbp)
	movsd	-120(%rbp), %xmm7
	divsd	-88(%rbp), %xmm7
	movsd	%xmm7, -88(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	movapd	%xmm5, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm0, %xmm9
	divsd	%xmm1, %xmm9
	movapd	%xmm10, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm12
	divsd	-96(%rbp), %xmm12
	movsd	%xmm12, -96(%rbp)
	movsd	-136(%rbp), %xmm3
	divsd	-72(%rbp), %xmm3
	movsd	%xmm3, -72(%rbp)
	movsd	-128(%rbp), %xmm6
	divsd	-80(%rbp), %xmm6
	movsd	%xmm6, -80(%rbp)
	movsd	-104(%rbp), %xmm1
	divsd	-56(%rbp), %xmm1
	movsd	%xmm1, -56(%rbp)
	movsd	-112(%rbp), %xmm15
	divsd	-64(%rbp), %xmm15
	movsd	%xmm15, -64(%rbp)
	movsd	-120(%rbp), %xmm12
	divsd	-88(%rbp), %xmm12
	movsd	%xmm12, -88(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm5, %xmm4
	movapd	%xmm4, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm0, %xmm1
	divsd	%xmm9, %xmm1
	movapd	%xmm10, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm8, %xmm9
	movapd	%xmm9, %xmm12
	divsd	%xmm14, %xmm12
	movapd	%xmm13, %xmm5
	divsd	-96(%rbp), %xmm5
	movsd	%xmm5, -96(%rbp)
	movsd	-136(%rbp), %xmm7
	divsd	-72(%rbp), %xmm7
	movsd	%xmm7, -72(%rbp)
	movsd	-128(%rbp), %xmm8
	divsd	-80(%rbp), %xmm8
	movsd	%xmm8, -80(%rbp)
	movsd	-104(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-112(%rbp), %xmm11
	divsd	-64(%rbp), %xmm11
	movsd	%xmm11, -64(%rbp)
	movsd	-120(%rbp), %xmm5
	divsd	-88(%rbp), %xmm5
	movsd	%xmm5, -88(%rbp)
	movapd	%xmm2, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm4, %xmm3
	movapd	%xmm3, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm0, %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm10, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm13, %xmm12
	movapd	%xmm12, %xmm6
	divsd	-96(%rbp), %xmm6
	movsd	%xmm6, -96(%rbp)
	movsd	-136(%rbp), %xmm4
	divsd	-72(%rbp), %xmm4
	movsd	%xmm4, -72(%rbp)
	movsd	-128(%rbp), %xmm1
	divsd	-80(%rbp), %xmm1
	movsd	%xmm1, -80(%rbp)
	movsd	-104(%rbp), %xmm13
	divsd	-56(%rbp), %xmm13
	movsd	%xmm13, -56(%rbp)
	movsd	-112(%rbp), %xmm15
	divsd	-64(%rbp), %xmm15
	movsd	%xmm15, -64(%rbp)
	movsd	-120(%rbp), %xmm6
	divsd	-88(%rbp), %xmm6
	movsd	%xmm6, -88(%rbp)
	movapd	%xmm2, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm3, %xmm7
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm1
	divsd	%xmm8, %xmm1
	movapd	%xmm10, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm9, %xmm8
	movapd	%xmm8, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm12, %xmm9
	movapd	%xmm9, %xmm11
	divsd	-96(%rbp), %xmm11
	movsd	-136(%rbp), %xmm3
	divsd	-72(%rbp), %xmm3
	movsd	%xmm3, -72(%rbp)
	movsd	-128(%rbp), %xmm5
	divsd	-80(%rbp), %xmm5
	movsd	%xmm5, -80(%rbp)
	movsd	-104(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-112(%rbp), %xmm12
	divsd	-64(%rbp), %xmm12
	movsd	%xmm12, -64(%rbp)
	movsd	-120(%rbp), %xmm3
	divsd	-88(%rbp), %xmm3
	movsd	%xmm3, -88(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm7, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm13
	movapd	%xmm13, %xmm9
	divsd	%xmm11, %xmm9
	movsd	-136(%rbp), %xmm4
	divsd	-72(%rbp), %xmm4
	movsd	%xmm4, -72(%rbp)
	movsd	-128(%rbp), %xmm15
	divsd	-80(%rbp), %xmm15
	movsd	%xmm15, -80(%rbp)
	movsd	-104(%rbp), %xmm1
	divsd	-56(%rbp), %xmm1
	movsd	%xmm1, -56(%rbp)
	movsd	-112(%rbp), %xmm11
	divsd	-64(%rbp), %xmm11
	movsd	%xmm11, -64(%rbp)
	movsd	-120(%rbp), %xmm4
	divsd	-88(%rbp), %xmm4
	movsd	%xmm4, -88(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm5
	divsd	%xmm3, %xmm5
	movapd	%xmm6, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm10, %xmm0
	movapd	%xmm0, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm13, %xmm3
	divsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movsd	-136(%rbp), %xmm12
	movapd	%xmm12, %xmm3
	divsd	-72(%rbp), %xmm3
	movsd	%xmm3, -72(%rbp)
	movsd	-128(%rbp), %xmm10
	divsd	-80(%rbp), %xmm10
	movsd	%xmm10, -80(%rbp)
	movsd	-104(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-112(%rbp), %xmm3
	divsd	-64(%rbp), %xmm3
	movsd	%xmm3, -64(%rbp)
	movsd	-120(%rbp), %xmm10
	divsd	-88(%rbp), %xmm10
	movsd	%xmm10, -88(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	divsd	%xmm1, %xmm5
	movapd	%xmm0, %xmm1
	divsd	%xmm11, %xmm1
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm10
	divsd	%xmm9, %xmm10
	movsd	%xmm10, -96(%rbp)
	movapd	%xmm12, %xmm15
	movapd	%xmm15, %xmm11
	divsd	-72(%rbp), %xmm11
	movsd	-128(%rbp), %xmm12
	movapd	%xmm12, %xmm9
	divsd	-80(%rbp), %xmm9
	movsd	-104(%rbp), %xmm10
	divsd	-56(%rbp), %xmm10
	movsd	%xmm10, -56(%rbp)
	movsd	-112(%rbp), %xmm10
	divsd	-64(%rbp), %xmm10
	movsd	%xmm10, -64(%rbp)
	movsd	-120(%rbp), %xmm10
	divsd	-88(%rbp), %xmm10
	divsd	%xmm3, %xmm2
	movsd	%xmm2, -144(%rbp)
	divsd	%xmm4, %xmm7
	movsd	%xmm7, -152(%rbp)
	divsd	%xmm5, %xmm6
	movsd	%xmm6, -160(%rbp)
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -168(%rbp)
	divsd	%xmm14, %xmm8
	movsd	%xmm8, -176(%rbp)
	divsd	-96(%rbp), %xmm13
	movsd	%xmm13, -96(%rbp)
	divsd	%xmm11, %xmm15
	movsd	%xmm15, -72(%rbp)
	divsd	%xmm9, %xmm12
	movsd	%xmm12, -80(%rbp)
	movsd	-104(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-112(%rbp), %xmm3
	divsd	-64(%rbp), %xmm3
	movsd	%xmm3, -64(%rbp)
	movsd	-120(%rbp), %xmm4
	divsd	%xmm10, %xmm4
	movsd	%xmm4, -88(%rbp)
.L769:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L770
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE259:
	.size	double_div_10, .-double_div_10
	.globl	double_div_11
	.type	double_div_11, @function
double_div_11:
.LFB260:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$232, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -200(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -208(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -176(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -216(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -184(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -224(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -192(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -232(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -240(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	56(%rax), %eax
	subl	$10, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	jmp	.L772
.L773:
	movsd	-200(%rbp), %xmm2
	movapd	%xmm2, %xmm6
	divsd	-160(%rbp), %xmm6
	movsd	-208(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	divsd	-168(%rbp), %xmm4
	movsd	-216(%rbp), %xmm0
	movapd	%xmm0, %xmm7
	divsd	-176(%rbp), %xmm7
	movsd	-224(%rbp), %xmm10
	movapd	%xmm10, %xmm13
	divsd	-184(%rbp), %xmm13
	movsd	-232(%rbp), %xmm8
	movapd	%xmm8, %xmm9
	divsd	-192(%rbp), %xmm9
	movsd	-240(%rbp), %xmm11
	movapd	%xmm11, %xmm14
	divsd	-104(%rbp), %xmm14
	movsd	-152(%rbp), %xmm5
	divsd	-80(%rbp), %xmm5
	movsd	%xmm5, -80(%rbp)
	movsd	-144(%rbp), %xmm1
	divsd	-88(%rbp), %xmm1
	movsd	%xmm1, -88(%rbp)
	movsd	-112(%rbp), %xmm15
	divsd	-56(%rbp), %xmm15
	movsd	%xmm15, -56(%rbp)
	movsd	-120(%rbp), %xmm12
	divsd	-64(%rbp), %xmm12
	movsd	%xmm12, -64(%rbp)
	movsd	-128(%rbp), %xmm5
	divsd	-72(%rbp), %xmm5
	movsd	%xmm5, -72(%rbp)
	movsd	-136(%rbp), %xmm15
	divsd	-96(%rbp), %xmm15
	movsd	%xmm15, -96(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm1
	divsd	%xmm7, %xmm1
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm15
	divsd	%xmm9, %xmm15
	movapd	%xmm11, %xmm13
	movapd	%xmm13, %xmm7
	divsd	%xmm14, %xmm7
	movsd	%xmm7, -104(%rbp)
	movsd	-152(%rbp), %xmm4
	divsd	-80(%rbp), %xmm4
	movsd	%xmm4, -80(%rbp)
	movsd	-144(%rbp), %xmm14
	divsd	-88(%rbp), %xmm14
	movsd	%xmm14, -88(%rbp)
	movsd	-112(%rbp), %xmm11
	divsd	-56(%rbp), %xmm11
	movsd	%xmm11, -56(%rbp)
	movsd	-120(%rbp), %xmm9
	divsd	-64(%rbp), %xmm9
	movsd	%xmm9, -64(%rbp)
	movsd	-128(%rbp), %xmm7
	divsd	-72(%rbp), %xmm7
	movsd	%xmm7, -72(%rbp)
	movsd	-136(%rbp), %xmm9
	divsd	-96(%rbp), %xmm9
	movsd	%xmm9, -96(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	movapd	%xmm5, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm0, %xmm9
	divsd	%xmm1, %xmm9
	movapd	%xmm10, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm12
	divsd	-104(%rbp), %xmm12
	movsd	%xmm12, -104(%rbp)
	movsd	-152(%rbp), %xmm3
	divsd	-80(%rbp), %xmm3
	movsd	%xmm3, -80(%rbp)
	movsd	-144(%rbp), %xmm6
	divsd	-88(%rbp), %xmm6
	movsd	%xmm6, -88(%rbp)
	movsd	-112(%rbp), %xmm1
	divsd	-56(%rbp), %xmm1
	movsd	%xmm1, -56(%rbp)
	movsd	-120(%rbp), %xmm15
	divsd	-64(%rbp), %xmm15
	movsd	%xmm15, -64(%rbp)
	movsd	-128(%rbp), %xmm12
	divsd	-72(%rbp), %xmm12
	movsd	%xmm12, -72(%rbp)
	movsd	-136(%rbp), %xmm6
	divsd	-96(%rbp), %xmm6
	movsd	%xmm6, -96(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm5, %xmm4
	movapd	%xmm4, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm0, %xmm1
	divsd	%xmm9, %xmm1
	movapd	%xmm10, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm8, %xmm9
	movapd	%xmm9, %xmm12
	divsd	%xmm14, %xmm12
	movapd	%xmm13, %xmm5
	divsd	-104(%rbp), %xmm5
	movsd	%xmm5, -104(%rbp)
	movsd	-152(%rbp), %xmm7
	divsd	-80(%rbp), %xmm7
	movsd	%xmm7, -80(%rbp)
	movsd	-144(%rbp), %xmm8
	divsd	-88(%rbp), %xmm8
	movsd	%xmm8, -88(%rbp)
	movsd	-112(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-120(%rbp), %xmm11
	divsd	-64(%rbp), %xmm11
	movsd	%xmm11, -64(%rbp)
	movsd	-128(%rbp), %xmm5
	divsd	-72(%rbp), %xmm5
	movsd	%xmm5, -72(%rbp)
	movsd	-136(%rbp), %xmm7
	divsd	-96(%rbp), %xmm7
	movsd	%xmm7, -96(%rbp)
	movapd	%xmm2, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm4, %xmm3
	movapd	%xmm3, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm0, %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm10, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm13, %xmm12
	movapd	%xmm12, %xmm6
	divsd	-104(%rbp), %xmm6
	movsd	%xmm6, -104(%rbp)
	movsd	-152(%rbp), %xmm4
	divsd	-80(%rbp), %xmm4
	movsd	%xmm4, -80(%rbp)
	movsd	-144(%rbp), %xmm1
	divsd	-88(%rbp), %xmm1
	movsd	%xmm1, -88(%rbp)
	movsd	-112(%rbp), %xmm13
	divsd	-56(%rbp), %xmm13
	movsd	%xmm13, -56(%rbp)
	movsd	-120(%rbp), %xmm15
	divsd	-64(%rbp), %xmm15
	movsd	%xmm15, -64(%rbp)
	movsd	-128(%rbp), %xmm6
	divsd	-72(%rbp), %xmm6
	movsd	%xmm6, -72(%rbp)
	movsd	-136(%rbp), %xmm1
	divsd	-96(%rbp), %xmm1
	movsd	%xmm1, -96(%rbp)
	movapd	%xmm2, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm3, %xmm7
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm1
	divsd	%xmm8, %xmm1
	movapd	%xmm10, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm9, %xmm8
	movapd	%xmm8, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm12, %xmm9
	movapd	%xmm9, %xmm11
	divsd	-104(%rbp), %xmm11
	movsd	-152(%rbp), %xmm3
	divsd	-80(%rbp), %xmm3
	movsd	%xmm3, -80(%rbp)
	movsd	-144(%rbp), %xmm5
	divsd	-88(%rbp), %xmm5
	movsd	%xmm5, -88(%rbp)
	movsd	-112(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-120(%rbp), %xmm12
	divsd	-64(%rbp), %xmm12
	movsd	%xmm12, -64(%rbp)
	movsd	-128(%rbp), %xmm3
	divsd	-72(%rbp), %xmm3
	movsd	%xmm3, -72(%rbp)
	movsd	-136(%rbp), %xmm5
	divsd	-96(%rbp), %xmm5
	movsd	%xmm5, -96(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm7, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm13
	movapd	%xmm13, %xmm9
	divsd	%xmm11, %xmm9
	movsd	-152(%rbp), %xmm4
	divsd	-80(%rbp), %xmm4
	movsd	%xmm4, -80(%rbp)
	movsd	-144(%rbp), %xmm15
	divsd	-88(%rbp), %xmm15
	movsd	%xmm15, -88(%rbp)
	movsd	-112(%rbp), %xmm1
	divsd	-56(%rbp), %xmm1
	movsd	%xmm1, -56(%rbp)
	movsd	-120(%rbp), %xmm11
	divsd	-64(%rbp), %xmm11
	movsd	%xmm11, -64(%rbp)
	movsd	-128(%rbp), %xmm4
	divsd	-72(%rbp), %xmm4
	movsd	%xmm4, -72(%rbp)
	movsd	-136(%rbp), %xmm11
	divsd	-96(%rbp), %xmm11
	movsd	%xmm11, -96(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm5
	divsd	%xmm3, %xmm5
	movapd	%xmm6, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm10, %xmm0
	movapd	%xmm0, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm13, %xmm3
	divsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movsd	-152(%rbp), %xmm12
	movapd	%xmm12, %xmm3
	divsd	-80(%rbp), %xmm3
	movsd	%xmm3, -80(%rbp)
	movsd	-144(%rbp), %xmm10
	divsd	-88(%rbp), %xmm10
	movsd	%xmm10, -88(%rbp)
	movsd	-112(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-120(%rbp), %xmm3
	divsd	-64(%rbp), %xmm3
	movsd	%xmm3, -64(%rbp)
	movsd	-128(%rbp), %xmm10
	divsd	-72(%rbp), %xmm10
	movsd	%xmm10, -72(%rbp)
	movsd	-136(%rbp), %xmm14
	divsd	-96(%rbp), %xmm14
	movsd	%xmm14, -96(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	divsd	%xmm1, %xmm5
	movapd	%xmm0, %xmm1
	divsd	%xmm11, %xmm1
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm10
	divsd	%xmm9, %xmm10
	movsd	%xmm10, -104(%rbp)
	movapd	%xmm12, %xmm15
	movapd	%xmm15, %xmm11
	divsd	-80(%rbp), %xmm11
	movsd	-144(%rbp), %xmm12
	movapd	%xmm12, %xmm9
	divsd	-88(%rbp), %xmm9
	movsd	-112(%rbp), %xmm10
	divsd	-56(%rbp), %xmm10
	movsd	%xmm10, -56(%rbp)
	movsd	-120(%rbp), %xmm10
	divsd	-64(%rbp), %xmm10
	movsd	%xmm10, -64(%rbp)
	movsd	-128(%rbp), %xmm10
	divsd	-72(%rbp), %xmm10
	movsd	%xmm10, -72(%rbp)
	movsd	-136(%rbp), %xmm10
	divsd	-96(%rbp), %xmm10
	divsd	%xmm3, %xmm2
	movsd	%xmm2, -160(%rbp)
	divsd	%xmm4, %xmm7
	movsd	%xmm7, -168(%rbp)
	divsd	%xmm5, %xmm6
	movsd	%xmm6, -176(%rbp)
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -184(%rbp)
	divsd	%xmm14, %xmm8
	movsd	%xmm8, -192(%rbp)
	divsd	-104(%rbp), %xmm13
	movsd	%xmm13, -104(%rbp)
	divsd	%xmm11, %xmm15
	movsd	%xmm15, -80(%rbp)
	divsd	%xmm9, %xmm12
	movsd	%xmm12, -88(%rbp)
	movsd	-112(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-120(%rbp), %xmm3
	divsd	-64(%rbp), %xmm3
	movsd	%xmm3, -64(%rbp)
	movsd	-128(%rbp), %xmm4
	divsd	-72(%rbp), %xmm4
	movsd	%xmm4, -72(%rbp)
	movsd	-136(%rbp), %xmm14
	divsd	%xmm10, %xmm14
	movsd	%xmm14, -96(%rbp)
.L772:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L773
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE260:
	.size	double_div_11, .-double_div_11
	.globl	double_div_12
	.type	double_div_12, @function
double_div_12:
.LFB261:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$248, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -176(%rbp)
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -216(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -184(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -224(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -192(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -232(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -200(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -240(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -208(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -248(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -256(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	56(%rax), %eax
	subl	$10, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	60(%rax), %eax
	subl	$11, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	jmp	.L775
.L776:
	movsd	-216(%rbp), %xmm2
	movapd	%xmm2, %xmm6
	divsd	-176(%rbp), %xmm6
	movsd	-224(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	divsd	-184(%rbp), %xmm4
	movsd	-232(%rbp), %xmm0
	movapd	%xmm0, %xmm7
	divsd	-192(%rbp), %xmm7
	movsd	-240(%rbp), %xmm10
	movapd	%xmm10, %xmm13
	divsd	-200(%rbp), %xmm13
	movsd	-248(%rbp), %xmm8
	movapd	%xmm8, %xmm9
	divsd	-208(%rbp), %xmm9
	movsd	-256(%rbp), %xmm11
	movapd	%xmm11, %xmm14
	divsd	-112(%rbp), %xmm14
	movsd	-168(%rbp), %xmm5
	divsd	-88(%rbp), %xmm5
	movsd	%xmm5, -88(%rbp)
	movsd	-160(%rbp), %xmm1
	divsd	-96(%rbp), %xmm1
	movsd	%xmm1, -96(%rbp)
	movsd	-120(%rbp), %xmm15
	divsd	-56(%rbp), %xmm15
	movsd	%xmm15, -56(%rbp)
	movsd	-128(%rbp), %xmm12
	divsd	-64(%rbp), %xmm12
	movsd	%xmm12, -64(%rbp)
	movsd	-136(%rbp), %xmm5
	divsd	-72(%rbp), %xmm5
	movsd	%xmm5, -72(%rbp)
	movsd	-144(%rbp), %xmm15
	divsd	-80(%rbp), %xmm15
	movsd	%xmm15, -80(%rbp)
	movsd	-152(%rbp), %xmm12
	divsd	-104(%rbp), %xmm12
	movsd	%xmm12, -104(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm1
	divsd	%xmm7, %xmm1
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm15
	divsd	%xmm9, %xmm15
	movapd	%xmm11, %xmm13
	movapd	%xmm13, %xmm7
	divsd	%xmm14, %xmm7
	movsd	%xmm7, -112(%rbp)
	movsd	-168(%rbp), %xmm4
	divsd	-88(%rbp), %xmm4
	movsd	%xmm4, -88(%rbp)
	movsd	-160(%rbp), %xmm14
	divsd	-96(%rbp), %xmm14
	movsd	%xmm14, -96(%rbp)
	movsd	-120(%rbp), %xmm11
	divsd	-56(%rbp), %xmm11
	movsd	%xmm11, -56(%rbp)
	movsd	-128(%rbp), %xmm9
	divsd	-64(%rbp), %xmm9
	movsd	%xmm9, -64(%rbp)
	movsd	-136(%rbp), %xmm7
	divsd	-72(%rbp), %xmm7
	movsd	%xmm7, -72(%rbp)
	movsd	-144(%rbp), %xmm9
	divsd	-80(%rbp), %xmm9
	movsd	%xmm9, -80(%rbp)
	movsd	-152(%rbp), %xmm11
	divsd	-104(%rbp), %xmm11
	movsd	%xmm11, -104(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	movapd	%xmm5, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm0, %xmm9
	divsd	%xmm1, %xmm9
	movapd	%xmm10, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm12
	divsd	-112(%rbp), %xmm12
	movsd	%xmm12, -112(%rbp)
	movsd	-168(%rbp), %xmm3
	divsd	-88(%rbp), %xmm3
	movsd	%xmm3, -88(%rbp)
	movsd	-160(%rbp), %xmm6
	divsd	-96(%rbp), %xmm6
	movsd	%xmm6, -96(%rbp)
	movsd	-120(%rbp), %xmm1
	divsd	-56(%rbp), %xmm1
	movsd	%xmm1, -56(%rbp)
	movsd	-128(%rbp), %xmm15
	divsd	-64(%rbp), %xmm15
	movsd	%xmm15, -64(%rbp)
	movsd	-136(%rbp), %xmm12
	divsd	-72(%rbp), %xmm12
	movsd	%xmm12, -72(%rbp)
	movsd	-144(%rbp), %xmm6
	divsd	-80(%rbp), %xmm6
	movsd	%xmm6, -80(%rbp)
	movsd	-152(%rbp), %xmm1
	divsd	-104(%rbp), %xmm1
	movsd	%xmm1, -104(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm5, %xmm4
	movapd	%xmm4, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm0, %xmm1
	divsd	%xmm9, %xmm1
	movapd	%xmm10, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm8, %xmm9
	movapd	%xmm9, %xmm12
	divsd	%xmm14, %xmm12
	movapd	%xmm13, %xmm5
	divsd	-112(%rbp), %xmm5
	movsd	%xmm5, -112(%rbp)
	movsd	-168(%rbp), %xmm7
	divsd	-88(%rbp), %xmm7
	movsd	%xmm7, -88(%rbp)
	movsd	-160(%rbp), %xmm8
	divsd	-96(%rbp), %xmm8
	movsd	%xmm8, -96(%rbp)
	movsd	-120(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-128(%rbp), %xmm11
	divsd	-64(%rbp), %xmm11
	movsd	%xmm11, -64(%rbp)
	movsd	-136(%rbp), %xmm5
	divsd	-72(%rbp), %xmm5
	movsd	%xmm5, -72(%rbp)
	movsd	-144(%rbp), %xmm7
	divsd	-80(%rbp), %xmm7
	movsd	%xmm7, -80(%rbp)
	movsd	-152(%rbp), %xmm8
	divsd	-104(%rbp), %xmm8
	movsd	%xmm8, -104(%rbp)
	movapd	%xmm2, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm4, %xmm3
	movapd	%xmm3, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm0, %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm10, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm13, %xmm12
	movapd	%xmm12, %xmm6
	divsd	-112(%rbp), %xmm6
	movsd	%xmm6, -112(%rbp)
	movsd	-168(%rbp), %xmm4
	divsd	-88(%rbp), %xmm4
	movsd	%xmm4, -88(%rbp)
	movsd	-160(%rbp), %xmm1
	divsd	-96(%rbp), %xmm1
	movsd	%xmm1, -96(%rbp)
	movsd	-120(%rbp), %xmm13
	divsd	-56(%rbp), %xmm13
	movsd	%xmm13, -56(%rbp)
	movsd	-128(%rbp), %xmm15
	divsd	-64(%rbp), %xmm15
	movsd	%xmm15, -64(%rbp)
	movsd	-136(%rbp), %xmm6
	divsd	-72(%rbp), %xmm6
	movsd	%xmm6, -72(%rbp)
	movsd	-144(%rbp), %xmm1
	divsd	-80(%rbp), %xmm1
	movsd	%xmm1, -80(%rbp)
	movsd	-152(%rbp), %xmm13
	divsd	-104(%rbp), %xmm13
	movsd	%xmm13, -104(%rbp)
	movapd	%xmm2, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm3, %xmm7
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm1
	divsd	%xmm8, %xmm1
	movapd	%xmm10, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm9, %xmm8
	movapd	%xmm8, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm12, %xmm9
	movapd	%xmm9, %xmm11
	divsd	-112(%rbp), %xmm11
	movsd	-168(%rbp), %xmm3
	divsd	-88(%rbp), %xmm3
	movsd	%xmm3, -88(%rbp)
	movsd	-160(%rbp), %xmm5
	divsd	-96(%rbp), %xmm5
	movsd	%xmm5, -96(%rbp)
	movsd	-120(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-128(%rbp), %xmm12
	divsd	-64(%rbp), %xmm12
	movsd	%xmm12, -64(%rbp)
	movsd	-136(%rbp), %xmm3
	divsd	-72(%rbp), %xmm3
	movsd	%xmm3, -72(%rbp)
	movsd	-144(%rbp), %xmm5
	divsd	-80(%rbp), %xmm5
	movsd	%xmm5, -80(%rbp)
	movsd	-152(%rbp), %xmm12
	divsd	-104(%rbp), %xmm12
	movsd	%xmm12, -104(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm7, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm13
	movapd	%xmm13, %xmm9
	divsd	%xmm11, %xmm9
	movsd	-168(%rbp), %xmm4
	divsd	-88(%rbp), %xmm4
	movsd	%xmm4, -88(%rbp)
	movsd	-160(%rbp), %xmm15
	divsd	-96(%rbp), %xmm15
	movsd	%xmm15, -96(%rbp)
	movsd	-120(%rbp), %xmm1
	divsd	-56(%rbp), %xmm1
	movsd	%xmm1, -56(%rbp)
	movsd	-128(%rbp), %xmm11
	divsd	-64(%rbp), %xmm11
	movsd	%xmm11, -64(%rbp)
	movsd	-136(%rbp), %xmm4
	divsd	-72(%rbp), %xmm4
	movsd	%xmm4, -72(%rbp)
	movsd	-144(%rbp), %xmm11
	divsd	-80(%rbp), %xmm11
	movsd	%xmm11, -80(%rbp)
	movsd	-152(%rbp), %xmm1
	divsd	-104(%rbp), %xmm1
	movsd	%xmm1, -104(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm5
	divsd	%xmm3, %xmm5
	movapd	%xmm6, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm10, %xmm0
	movapd	%xmm0, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm13, %xmm3
	divsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movsd	-168(%rbp), %xmm12
	movapd	%xmm12, %xmm3
	divsd	-88(%rbp), %xmm3
	movsd	%xmm3, -88(%rbp)
	movsd	-160(%rbp), %xmm10
	divsd	-96(%rbp), %xmm10
	movsd	%xmm10, -96(%rbp)
	movsd	-120(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-128(%rbp), %xmm3
	divsd	-64(%rbp), %xmm3
	movsd	%xmm3, -64(%rbp)
	movsd	-136(%rbp), %xmm10
	divsd	-72(%rbp), %xmm10
	movsd	%xmm10, -72(%rbp)
	movsd	-144(%rbp), %xmm14
	divsd	-80(%rbp), %xmm14
	movsd	%xmm14, -80(%rbp)
	movsd	-152(%rbp), %xmm3
	divsd	-104(%rbp), %xmm3
	movsd	%xmm3, -104(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	divsd	%xmm1, %xmm5
	movapd	%xmm0, %xmm1
	divsd	%xmm11, %xmm1
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm10
	divsd	%xmm9, %xmm10
	movsd	%xmm10, -112(%rbp)
	movapd	%xmm12, %xmm15
	movapd	%xmm15, %xmm11
	divsd	-88(%rbp), %xmm11
	movsd	-160(%rbp), %xmm12
	movapd	%xmm12, %xmm9
	divsd	-96(%rbp), %xmm9
	movsd	-120(%rbp), %xmm10
	divsd	-56(%rbp), %xmm10
	movsd	%xmm10, -56(%rbp)
	movsd	-128(%rbp), %xmm10
	divsd	-64(%rbp), %xmm10
	movsd	%xmm10, -64(%rbp)
	movsd	-136(%rbp), %xmm10
	divsd	-72(%rbp), %xmm10
	movsd	%xmm10, -72(%rbp)
	movsd	-144(%rbp), %xmm10
	divsd	-80(%rbp), %xmm10
	movsd	%xmm10, -80(%rbp)
	movsd	-152(%rbp), %xmm10
	divsd	-104(%rbp), %xmm10
	divsd	%xmm3, %xmm2
	movsd	%xmm2, -176(%rbp)
	divsd	%xmm4, %xmm7
	movsd	%xmm7, -184(%rbp)
	divsd	%xmm5, %xmm6
	movsd	%xmm6, -192(%rbp)
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -200(%rbp)
	divsd	%xmm14, %xmm8
	movsd	%xmm8, -208(%rbp)
	divsd	-112(%rbp), %xmm13
	movsd	%xmm13, -112(%rbp)
	divsd	%xmm11, %xmm15
	movsd	%xmm15, -88(%rbp)
	divsd	%xmm9, %xmm12
	movsd	%xmm12, -96(%rbp)
	movsd	-120(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-128(%rbp), %xmm3
	divsd	-64(%rbp), %xmm3
	movsd	%xmm3, -64(%rbp)
	movsd	-136(%rbp), %xmm4
	divsd	-72(%rbp), %xmm4
	movsd	%xmm4, -72(%rbp)
	movsd	-144(%rbp), %xmm14
	divsd	-80(%rbp), %xmm14
	movsd	%xmm14, -80(%rbp)
	movsd	-152(%rbp), %xmm3
	divsd	%xmm10, %xmm3
	movsd	%xmm3, -104(%rbp)
.L775:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L776
	cvttsd2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE261:
	.size	double_div_12, .-double_div_12
	.globl	double_div_13
	.type	double_div_13, @function
double_div_13:
.LFB262:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$264, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -192(%rbp)
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -232(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -200(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -240(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -208(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -248(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -216(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -256(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -224(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -264(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -272(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -184(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -176(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	56(%rax), %eax
	subl	$10, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	60(%rax), %eax
	subl	$11, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	64(%rax), %eax
	subl	$12, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -168(%rbp)
	jmp	.L778
.L779:
	movsd	-232(%rbp), %xmm2
	movapd	%xmm2, %xmm6
	divsd	-192(%rbp), %xmm6
	movsd	-240(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	divsd	-200(%rbp), %xmm4
	movsd	-248(%rbp), %xmm0
	movapd	%xmm0, %xmm7
	divsd	-208(%rbp), %xmm7
	movsd	-256(%rbp), %xmm10
	movapd	%xmm10, %xmm13
	divsd	-216(%rbp), %xmm13
	movsd	-264(%rbp), %xmm8
	movapd	%xmm8, %xmm9
	divsd	-224(%rbp), %xmm9
	movsd	-272(%rbp), %xmm11
	movapd	%xmm11, %xmm14
	divsd	-120(%rbp), %xmm14
	movsd	-184(%rbp), %xmm5
	divsd	-96(%rbp), %xmm5
	movsd	%xmm5, -96(%rbp)
	movsd	-176(%rbp), %xmm1
	divsd	-104(%rbp), %xmm1
	movsd	%xmm1, -104(%rbp)
	movsd	-128(%rbp), %xmm15
	divsd	-56(%rbp), %xmm15
	movsd	%xmm15, -56(%rbp)
	movsd	-136(%rbp), %xmm12
	divsd	-64(%rbp), %xmm12
	movsd	%xmm12, -64(%rbp)
	movsd	-144(%rbp), %xmm5
	divsd	-72(%rbp), %xmm5
	movsd	%xmm5, -72(%rbp)
	movsd	-152(%rbp), %xmm15
	divsd	-80(%rbp), %xmm15
	movsd	%xmm15, -80(%rbp)
	movsd	-160(%rbp), %xmm12
	divsd	-88(%rbp), %xmm12
	movsd	%xmm12, -88(%rbp)
	movsd	-168(%rbp), %xmm1
	divsd	-112(%rbp), %xmm1
	movsd	%xmm1, -112(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm1
	divsd	%xmm7, %xmm1
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm15
	divsd	%xmm9, %xmm15
	movapd	%xmm11, %xmm13
	movapd	%xmm13, %xmm7
	divsd	%xmm14, %xmm7
	movsd	%xmm7, -120(%rbp)
	movsd	-184(%rbp), %xmm4
	divsd	-96(%rbp), %xmm4
	movsd	%xmm4, -96(%rbp)
	movsd	-176(%rbp), %xmm14
	divsd	-104(%rbp), %xmm14
	movsd	%xmm14, -104(%rbp)
	movsd	-128(%rbp), %xmm11
	divsd	-56(%rbp), %xmm11
	movsd	%xmm11, -56(%rbp)
	movsd	-136(%rbp), %xmm9
	divsd	-64(%rbp), %xmm9
	movsd	%xmm9, -64(%rbp)
	movsd	-144(%rbp), %xmm7
	divsd	-72(%rbp), %xmm7
	movsd	%xmm7, -72(%rbp)
	movsd	-152(%rbp), %xmm9
	divsd	-80(%rbp), %xmm9
	movsd	%xmm9, -80(%rbp)
	movsd	-160(%rbp), %xmm11
	divsd	-88(%rbp), %xmm11
	movsd	%xmm11, -88(%rbp)
	movsd	-168(%rbp), %xmm14
	divsd	-112(%rbp), %xmm14
	movsd	%xmm14, -112(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	movapd	%xmm5, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm0, %xmm9
	divsd	%xmm1, %xmm9
	movapd	%xmm10, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm12
	divsd	-120(%rbp), %xmm12
	movsd	%xmm12, -120(%rbp)
	movsd	-184(%rbp), %xmm3
	divsd	-96(%rbp), %xmm3
	movsd	%xmm3, -96(%rbp)
	movsd	-176(%rbp), %xmm6
	divsd	-104(%rbp), %xmm6
	movsd	%xmm6, -104(%rbp)
	movsd	-128(%rbp), %xmm1
	divsd	-56(%rbp), %xmm1
	movsd	%xmm1, -56(%rbp)
	movsd	-136(%rbp), %xmm15
	divsd	-64(%rbp), %xmm15
	movsd	%xmm15, -64(%rbp)
	movsd	-144(%rbp), %xmm12
	divsd	-72(%rbp), %xmm12
	movsd	%xmm12, -72(%rbp)
	movsd	-152(%rbp), %xmm6
	divsd	-80(%rbp), %xmm6
	movsd	%xmm6, -80(%rbp)
	movsd	-160(%rbp), %xmm1
	divsd	-88(%rbp), %xmm1
	movsd	%xmm1, -88(%rbp)
	movsd	-168(%rbp), %xmm3
	divsd	-112(%rbp), %xmm3
	movsd	%xmm3, -112(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm5, %xmm4
	movapd	%xmm4, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm0, %xmm1
	divsd	%xmm9, %xmm1
	movapd	%xmm10, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm8, %xmm9
	movapd	%xmm9, %xmm12
	divsd	%xmm14, %xmm12
	movapd	%xmm13, %xmm5
	divsd	-120(%rbp), %xmm5
	movsd	%xmm5, -120(%rbp)
	movsd	-184(%rbp), %xmm7
	divsd	-96(%rbp), %xmm7
	movsd	%xmm7, -96(%rbp)
	movsd	-176(%rbp), %xmm8
	divsd	-104(%rbp), %xmm8
	movsd	%xmm8, -104(%rbp)
	movsd	-128(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-136(%rbp), %xmm11
	divsd	-64(%rbp), %xmm11
	movsd	%xmm11, -64(%rbp)
	movsd	-144(%rbp), %xmm5
	divsd	-72(%rbp), %xmm5
	movsd	%xmm5, -72(%rbp)
	movsd	-152(%rbp), %xmm7
	divsd	-80(%rbp), %xmm7
	movsd	%xmm7, -80(%rbp)
	movsd	-160(%rbp), %xmm8
	divsd	-88(%rbp), %xmm8
	movsd	%xmm8, -88(%rbp)
	movsd	-168(%rbp), %xmm11
	divsd	-112(%rbp), %xmm11
	movsd	%xmm11, -112(%rbp)
	movapd	%xmm2, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm4, %xmm3
	movapd	%xmm3, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm0, %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm10, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm13, %xmm12
	movapd	%xmm12, %xmm6
	divsd	-120(%rbp), %xmm6
	movsd	%xmm6, -120(%rbp)
	movsd	-184(%rbp), %xmm4
	divsd	-96(%rbp), %xmm4
	movsd	%xmm4, -96(%rbp)
	movsd	-176(%rbp), %xmm1
	divsd	-104(%rbp), %xmm1
	movsd	%xmm1, -104(%rbp)
	movsd	-128(%rbp), %xmm13
	divsd	-56(%rbp), %xmm13
	movsd	%xmm13, -56(%rbp)
	movsd	-136(%rbp), %xmm15
	divsd	-64(%rbp), %xmm15
	movsd	%xmm15, -64(%rbp)
	movsd	-144(%rbp), %xmm6
	divsd	-72(%rbp), %xmm6
	movsd	%xmm6, -72(%rbp)
	movsd	-152(%rbp), %xmm1
	divsd	-80(%rbp), %xmm1
	movsd	%xmm1, -80(%rbp)
	movsd	-160(%rbp), %xmm13
	divsd	-88(%rbp), %xmm13
	movsd	%xmm13, -88(%rbp)
	movsd	-168(%rbp), %xmm4
	divsd	-112(%rbp), %xmm4
	movsd	%xmm4, -112(%rbp)
	movapd	%xmm2, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm3, %xmm7
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm1
	divsd	%xmm8, %xmm1
	movapd	%xmm10, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm9, %xmm8
	movapd	%xmm8, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm12, %xmm9
	movapd	%xmm9, %xmm11
	divsd	-120(%rbp), %xmm11
	movsd	-184(%rbp), %xmm3
	divsd	-96(%rbp), %xmm3
	movsd	%xmm3, -96(%rbp)
	movsd	-176(%rbp), %xmm5
	divsd	-104(%rbp), %xmm5
	movsd	%xmm5, -104(%rbp)
	movsd	-128(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-136(%rbp), %xmm12
	divsd	-64(%rbp), %xmm12
	movsd	%xmm12, -64(%rbp)
	movsd	-144(%rbp), %xmm3
	divsd	-72(%rbp), %xmm3
	movsd	%xmm3, -72(%rbp)
	movsd	-152(%rbp), %xmm5
	divsd	-80(%rbp), %xmm5
	movsd	%xmm5, -80(%rbp)
	movsd	-160(%rbp), %xmm12
	divsd	-88(%rbp), %xmm12
	movsd	%xmm12, -88(%rbp)
	movsd	-168(%rbp), %xmm14
	divsd	-112(%rbp), %xmm14
	movsd	%xmm14, -112(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm7, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm13
	movapd	%xmm13, %xmm9
	divsd	%xmm11, %xmm9
	movsd	-184(%rbp), %xmm4
	divsd	-96(%rbp), %xmm4
	movsd	%xmm4, -96(%rbp)
	movsd	-176(%rbp), %xmm15
	divsd	-104(%rbp), %xmm15
	movsd	%xmm15, -104(%rbp)
	movsd	-128(%rbp), %xmm1
	divsd	-56(%rbp), %xmm1
	movsd	%xmm1, -56(%rbp)
	movsd	-136(%rbp), %xmm11
	divsd	-64(%rbp), %xmm11
	movsd	%xmm11, -64(%rbp)
	movsd	-144(%rbp), %xmm4
	divsd	-72(%rbp), %xmm4
	movsd	%xmm4, -72(%rbp)
	movsd	-152(%rbp), %xmm11
	divsd	-80(%rbp), %xmm11
	movsd	%xmm11, -80(%rbp)
	movsd	-160(%rbp), %xmm1
	divsd	-88(%rbp), %xmm1
	movsd	%xmm1, -88(%rbp)
	movsd	-168(%rbp), %xmm15
	divsd	-112(%rbp), %xmm15
	movsd	%xmm15, -112(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm5
	divsd	%xmm3, %xmm5
	movapd	%xmm6, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm10, %xmm0
	movapd	%xmm0, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm13, %xmm3
	divsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movsd	-184(%rbp), %xmm12
	movapd	%xmm12, %xmm3
	divsd	-96(%rbp), %xmm3
	movsd	%xmm3, -96(%rbp)
	movsd	-176(%rbp), %xmm10
	divsd	-104(%rbp), %xmm10
	movsd	%xmm10, -104(%rbp)
	movsd	-128(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-136(%rbp), %xmm3
	divsd	-64(%rbp), %xmm3
	movsd	%xmm3, -64(%rbp)
	movsd	-144(%rbp), %xmm10
	divsd	-72(%rbp), %xmm10
	movsd	%xmm10, -72(%rbp)
	movsd	-152(%rbp), %xmm14
	divsd	-80(%rbp), %xmm14
	movsd	%xmm14, -80(%rbp)
	movsd	-160(%rbp), %xmm3
	divsd	-88(%rbp), %xmm3
	movsd	%xmm3, -88(%rbp)
	movsd	-168(%rbp), %xmm10
	divsd	-112(%rbp), %xmm10
	movsd	%xmm10, -112(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	divsd	%xmm1, %xmm5
	movapd	%xmm0, %xmm1
	divsd	%xmm11, %xmm1
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm10
	divsd	%xmm9, %xmm10
	movsd	%xmm10, -120(%rbp)
	movapd	%xmm12, %xmm15
	movapd	%xmm15, %xmm11
	divsd	-96(%rbp), %xmm11
	movsd	-176(%rbp), %xmm12
	movapd	%xmm12, %xmm9
	divsd	-104(%rbp), %xmm9
	movsd	-128(%rbp), %xmm10
	divsd	-56(%rbp), %xmm10
	movsd	%xmm10, -56(%rbp)
	movsd	-136(%rbp), %xmm10
	divsd	-64(%rbp), %xmm10
	movsd	%xmm10, -64(%rbp)
	movsd	-144(%rbp), %xmm10
	divsd	-72(%rbp), %xmm10
	movsd	%xmm10, -72(%rbp)
	movsd	-152(%rbp), %xmm10
	divsd	-80(%rbp), %xmm10
	movsd	%xmm10, -80(%rbp)
	movsd	-160(%rbp), %xmm10
	divsd	-88(%rbp), %xmm10
	movsd	%xmm10, -88(%rbp)
	movsd	-168(%rbp), %xmm10
	divsd	-112(%rbp), %xmm10
	divsd	%xmm3, %xmm2
	movsd	%xmm2, -192(%rbp)
	divsd	%xmm4, %xmm7
	movsd	%xmm7, -200(%rbp)
	divsd	%xmm5, %xmm6
	movsd	%xmm6, -208(%rbp)
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -216(%rbp)
	divsd	%xmm14, %xmm8
	movsd	%xmm8, -224(%rbp)
	divsd	-120(%rbp), %xmm13
	movsd	%xmm13, -120(%rbp)
	divsd	%xmm11, %xmm15
	movsd	%xmm15, -96(%rbp)
	divsd	%xmm9, %xmm12
	movsd	%xmm12, -104(%rbp)
	movsd	-128(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-136(%rbp), %xmm3
	divsd	-64(%rbp), %xmm3
	movsd	%xmm3, -64(%rbp)
	movsd	-144(%rbp), %xmm4
	divsd	-72(%rbp), %xmm4
	movsd	%xmm4, -72(%rbp)
	movsd	-152(%rbp), %xmm14
	divsd	-80(%rbp), %xmm14
	movsd	%xmm14, -80(%rbp)
	movsd	-160(%rbp), %xmm3
	divsd	-88(%rbp), %xmm3
	movsd	%xmm3, -88(%rbp)
	movsd	-168(%rbp), %xmm15
	divsd	%xmm10, %xmm15
	movsd	%xmm15, -112(%rbp)
.L778:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L779
	cvttsd2sil	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-264(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-272(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE262:
	.size	double_div_13, .-double_div_13
	.globl	double_div_14
	.type	double_div_14, @function
double_div_14:
.LFB263:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$280, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -208(%rbp)
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -248(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -216(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -256(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -224(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -264(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -232(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -272(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -240(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -280(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -288(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -200(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -192(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	56(%rax), %eax
	subl	$10, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	60(%rax), %eax
	subl	$11, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	64(%rax), %eax
	subl	$12, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -176(%rbp)
	movq	-24(%rbp), %rax
	movsd	192(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	68(%rax), %eax
	subl	$13, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -184(%rbp)
	jmp	.L781
.L782:
	movsd	-248(%rbp), %xmm2
	movapd	%xmm2, %xmm6
	divsd	-208(%rbp), %xmm6
	movsd	-256(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	divsd	-216(%rbp), %xmm4
	movsd	-264(%rbp), %xmm0
	movapd	%xmm0, %xmm7
	divsd	-224(%rbp), %xmm7
	movsd	-272(%rbp), %xmm10
	movapd	%xmm10, %xmm13
	divsd	-232(%rbp), %xmm13
	movsd	-280(%rbp), %xmm8
	movapd	%xmm8, %xmm9
	divsd	-240(%rbp), %xmm9
	movsd	-288(%rbp), %xmm11
	movapd	%xmm11, %xmm14
	divsd	-128(%rbp), %xmm14
	movsd	-200(%rbp), %xmm5
	divsd	-104(%rbp), %xmm5
	movsd	%xmm5, -104(%rbp)
	movsd	-192(%rbp), %xmm1
	divsd	-112(%rbp), %xmm1
	movsd	%xmm1, -112(%rbp)
	movsd	-136(%rbp), %xmm15
	divsd	-56(%rbp), %xmm15
	movsd	%xmm15, -56(%rbp)
	movsd	-144(%rbp), %xmm12
	divsd	-64(%rbp), %xmm12
	movsd	%xmm12, -64(%rbp)
	movsd	-152(%rbp), %xmm5
	divsd	-72(%rbp), %xmm5
	movsd	%xmm5, -72(%rbp)
	movsd	-160(%rbp), %xmm15
	divsd	-80(%rbp), %xmm15
	movsd	%xmm15, -80(%rbp)
	movsd	-168(%rbp), %xmm12
	divsd	-88(%rbp), %xmm12
	movsd	%xmm12, -88(%rbp)
	movsd	-176(%rbp), %xmm1
	divsd	-96(%rbp), %xmm1
	movsd	%xmm1, -96(%rbp)
	movsd	-184(%rbp), %xmm15
	divsd	-120(%rbp), %xmm15
	movsd	%xmm15, -120(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm1
	divsd	%xmm7, %xmm1
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm15
	divsd	%xmm9, %xmm15
	movapd	%xmm11, %xmm13
	movapd	%xmm13, %xmm7
	divsd	%xmm14, %xmm7
	movsd	%xmm7, -128(%rbp)
	movsd	-200(%rbp), %xmm4
	divsd	-104(%rbp), %xmm4
	movsd	%xmm4, -104(%rbp)
	movsd	-192(%rbp), %xmm14
	divsd	-112(%rbp), %xmm14
	movsd	%xmm14, -112(%rbp)
	movsd	-136(%rbp), %xmm11
	divsd	-56(%rbp), %xmm11
	movsd	%xmm11, -56(%rbp)
	movsd	-144(%rbp), %xmm9
	divsd	-64(%rbp), %xmm9
	movsd	%xmm9, -64(%rbp)
	movsd	-152(%rbp), %xmm7
	divsd	-72(%rbp), %xmm7
	movsd	%xmm7, -72(%rbp)
	movsd	-160(%rbp), %xmm9
	divsd	-80(%rbp), %xmm9
	movsd	%xmm9, -80(%rbp)
	movsd	-168(%rbp), %xmm11
	divsd	-88(%rbp), %xmm11
	movsd	%xmm11, -88(%rbp)
	movsd	-176(%rbp), %xmm14
	divsd	-96(%rbp), %xmm14
	movsd	%xmm14, -96(%rbp)
	movsd	-184(%rbp), %xmm4
	divsd	-120(%rbp), %xmm4
	movsd	%xmm4, -120(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	movapd	%xmm5, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm0, %xmm9
	divsd	%xmm1, %xmm9
	movapd	%xmm10, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm12
	divsd	-128(%rbp), %xmm12
	movsd	%xmm12, -128(%rbp)
	movsd	-200(%rbp), %xmm3
	divsd	-104(%rbp), %xmm3
	movsd	%xmm3, -104(%rbp)
	movsd	-192(%rbp), %xmm6
	divsd	-112(%rbp), %xmm6
	movsd	%xmm6, -112(%rbp)
	movsd	-136(%rbp), %xmm1
	divsd	-56(%rbp), %xmm1
	movsd	%xmm1, -56(%rbp)
	movsd	-144(%rbp), %xmm15
	divsd	-64(%rbp), %xmm15
	movsd	%xmm15, -64(%rbp)
	movsd	-152(%rbp), %xmm12
	divsd	-72(%rbp), %xmm12
	movsd	%xmm12, -72(%rbp)
	movsd	-160(%rbp), %xmm6
	divsd	-80(%rbp), %xmm6
	movsd	%xmm6, -80(%rbp)
	movsd	-168(%rbp), %xmm1
	divsd	-88(%rbp), %xmm1
	movsd	%xmm1, -88(%rbp)
	movsd	-176(%rbp), %xmm3
	divsd	-96(%rbp), %xmm3
	movsd	%xmm3, -96(%rbp)
	movsd	-184(%rbp), %xmm15
	divsd	-120(%rbp), %xmm15
	movsd	%xmm15, -120(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm5, %xmm4
	movapd	%xmm4, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm0, %xmm1
	divsd	%xmm9, %xmm1
	movapd	%xmm10, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm8, %xmm9
	movapd	%xmm9, %xmm12
	divsd	%xmm14, %xmm12
	movapd	%xmm13, %xmm5
	divsd	-128(%rbp), %xmm5
	movsd	%xmm5, -128(%rbp)
	movsd	-200(%rbp), %xmm7
	divsd	-104(%rbp), %xmm7
	movsd	%xmm7, -104(%rbp)
	movsd	-192(%rbp), %xmm8
	divsd	-112(%rbp), %xmm8
	movsd	%xmm8, -112(%rbp)
	movsd	-136(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-144(%rbp), %xmm11
	divsd	-64(%rbp), %xmm11
	movsd	%xmm11, -64(%rbp)
	movsd	-152(%rbp), %xmm5
	divsd	-72(%rbp), %xmm5
	movsd	%xmm5, -72(%rbp)
	movsd	-160(%rbp), %xmm7
	divsd	-80(%rbp), %xmm7
	movsd	%xmm7, -80(%rbp)
	movsd	-168(%rbp), %xmm8
	divsd	-88(%rbp), %xmm8
	movsd	%xmm8, -88(%rbp)
	movsd	-176(%rbp), %xmm11
	divsd	-96(%rbp), %xmm11
	movsd	%xmm11, -96(%rbp)
	movsd	-184(%rbp), %xmm14
	divsd	-120(%rbp), %xmm14
	movsd	%xmm14, -120(%rbp)
	movapd	%xmm2, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm4, %xmm3
	movapd	%xmm3, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm0, %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm10, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm13, %xmm12
	movapd	%xmm12, %xmm6
	divsd	-128(%rbp), %xmm6
	movsd	%xmm6, -128(%rbp)
	movsd	-200(%rbp), %xmm4
	divsd	-104(%rbp), %xmm4
	movsd	%xmm4, -104(%rbp)
	movsd	-192(%rbp), %xmm1
	divsd	-112(%rbp), %xmm1
	movsd	%xmm1, -112(%rbp)
	movsd	-136(%rbp), %xmm13
	divsd	-56(%rbp), %xmm13
	movsd	%xmm13, -56(%rbp)
	movsd	-144(%rbp), %xmm15
	divsd	-64(%rbp), %xmm15
	movsd	%xmm15, -64(%rbp)
	movsd	-152(%rbp), %xmm6
	divsd	-72(%rbp), %xmm6
	movsd	%xmm6, -72(%rbp)
	movsd	-160(%rbp), %xmm1
	divsd	-80(%rbp), %xmm1
	movsd	%xmm1, -80(%rbp)
	movsd	-168(%rbp), %xmm13
	divsd	-88(%rbp), %xmm13
	movsd	%xmm13, -88(%rbp)
	movsd	-176(%rbp), %xmm4
	divsd	-96(%rbp), %xmm4
	movsd	%xmm4, -96(%rbp)
	movsd	-184(%rbp), %xmm15
	divsd	-120(%rbp), %xmm15
	movsd	%xmm15, -120(%rbp)
	movapd	%xmm2, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm3, %xmm7
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm1
	divsd	%xmm8, %xmm1
	movapd	%xmm10, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm9, %xmm8
	movapd	%xmm8, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm12, %xmm9
	movapd	%xmm9, %xmm11
	divsd	-128(%rbp), %xmm11
	movsd	-200(%rbp), %xmm3
	divsd	-104(%rbp), %xmm3
	movsd	%xmm3, -104(%rbp)
	movsd	-192(%rbp), %xmm5
	divsd	-112(%rbp), %xmm5
	movsd	%xmm5, -112(%rbp)
	movsd	-136(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-144(%rbp), %xmm12
	divsd	-64(%rbp), %xmm12
	movsd	%xmm12, -64(%rbp)
	movsd	-152(%rbp), %xmm3
	divsd	-72(%rbp), %xmm3
	movsd	%xmm3, -72(%rbp)
	movsd	-160(%rbp), %xmm5
	divsd	-80(%rbp), %xmm5
	movsd	%xmm5, -80(%rbp)
	movsd	-168(%rbp), %xmm12
	divsd	-88(%rbp), %xmm12
	movsd	%xmm12, -88(%rbp)
	movsd	-176(%rbp), %xmm14
	divsd	-96(%rbp), %xmm14
	movsd	%xmm14, -96(%rbp)
	movsd	-184(%rbp), %xmm5
	divsd	-120(%rbp), %xmm5
	movsd	%xmm5, -120(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm7, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm13
	movapd	%xmm13, %xmm9
	divsd	%xmm11, %xmm9
	movsd	-200(%rbp), %xmm4
	divsd	-104(%rbp), %xmm4
	movsd	%xmm4, -104(%rbp)
	movsd	-192(%rbp), %xmm15
	divsd	-112(%rbp), %xmm15
	movsd	%xmm15, -112(%rbp)
	movsd	-136(%rbp), %xmm1
	divsd	-56(%rbp), %xmm1
	movsd	%xmm1, -56(%rbp)
	movsd	-144(%rbp), %xmm11
	divsd	-64(%rbp), %xmm11
	movsd	%xmm11, -64(%rbp)
	movsd	-152(%rbp), %xmm4
	divsd	-72(%rbp), %xmm4
	movsd	%xmm4, -72(%rbp)
	movsd	-160(%rbp), %xmm11
	divsd	-80(%rbp), %xmm11
	movsd	%xmm11, -80(%rbp)
	movsd	-168(%rbp), %xmm1
	divsd	-88(%rbp), %xmm1
	movsd	%xmm1, -88(%rbp)
	movsd	-176(%rbp), %xmm15
	divsd	-96(%rbp), %xmm15
	movsd	%xmm15, -96(%rbp)
	movsd	-184(%rbp), %xmm1
	divsd	-120(%rbp), %xmm1
	movsd	%xmm1, -120(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm5
	divsd	%xmm3, %xmm5
	movapd	%xmm6, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm10, %xmm0
	movapd	%xmm0, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm13, %xmm3
	divsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movsd	-200(%rbp), %xmm12
	movapd	%xmm12, %xmm3
	divsd	-104(%rbp), %xmm3
	movsd	%xmm3, -104(%rbp)
	movsd	-192(%rbp), %xmm10
	divsd	-112(%rbp), %xmm10
	movsd	%xmm10, -112(%rbp)
	movsd	-136(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-144(%rbp), %xmm3
	divsd	-64(%rbp), %xmm3
	movsd	%xmm3, -64(%rbp)
	movsd	-152(%rbp), %xmm10
	divsd	-72(%rbp), %xmm10
	movsd	%xmm10, -72(%rbp)
	movsd	-160(%rbp), %xmm14
	divsd	-80(%rbp), %xmm14
	movsd	%xmm14, -80(%rbp)
	movsd	-168(%rbp), %xmm3
	divsd	-88(%rbp), %xmm3
	movsd	%xmm3, -88(%rbp)
	movsd	-176(%rbp), %xmm10
	divsd	-96(%rbp), %xmm10
	movsd	%xmm10, -96(%rbp)
	movsd	-184(%rbp), %xmm14
	divsd	-120(%rbp), %xmm14
	movsd	%xmm14, -120(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	divsd	%xmm1, %xmm5
	movapd	%xmm0, %xmm1
	divsd	%xmm11, %xmm1
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm10
	divsd	%xmm9, %xmm10
	movsd	%xmm10, -128(%rbp)
	movapd	%xmm12, %xmm15
	movapd	%xmm15, %xmm11
	divsd	-104(%rbp), %xmm11
	movsd	-192(%rbp), %xmm12
	movapd	%xmm12, %xmm9
	divsd	-112(%rbp), %xmm9
	movsd	-136(%rbp), %xmm10
	divsd	-56(%rbp), %xmm10
	movsd	%xmm10, -56(%rbp)
	movsd	-144(%rbp), %xmm10
	divsd	-64(%rbp), %xmm10
	movsd	%xmm10, -64(%rbp)
	movsd	-152(%rbp), %xmm10
	divsd	-72(%rbp), %xmm10
	movsd	%xmm10, -72(%rbp)
	movsd	-160(%rbp), %xmm10
	divsd	-80(%rbp), %xmm10
	movsd	%xmm10, -80(%rbp)
	movsd	-168(%rbp), %xmm10
	divsd	-88(%rbp), %xmm10
	movsd	%xmm10, -88(%rbp)
	movsd	-176(%rbp), %xmm10
	divsd	-96(%rbp), %xmm10
	movsd	%xmm10, -96(%rbp)
	movsd	-184(%rbp), %xmm10
	divsd	-120(%rbp), %xmm10
	divsd	%xmm3, %xmm2
	movsd	%xmm2, -208(%rbp)
	divsd	%xmm4, %xmm7
	movsd	%xmm7, -216(%rbp)
	divsd	%xmm5, %xmm6
	movsd	%xmm6, -224(%rbp)
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -232(%rbp)
	divsd	%xmm14, %xmm8
	movsd	%xmm8, -240(%rbp)
	divsd	-128(%rbp), %xmm13
	movsd	%xmm13, -128(%rbp)
	divsd	%xmm11, %xmm15
	movsd	%xmm15, -104(%rbp)
	divsd	%xmm9, %xmm12
	movsd	%xmm12, -112(%rbp)
	movsd	-136(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-144(%rbp), %xmm3
	divsd	-64(%rbp), %xmm3
	movsd	%xmm3, -64(%rbp)
	movsd	-152(%rbp), %xmm4
	divsd	-72(%rbp), %xmm4
	movsd	%xmm4, -72(%rbp)
	movsd	-160(%rbp), %xmm14
	divsd	-80(%rbp), %xmm14
	movsd	%xmm14, -80(%rbp)
	movsd	-168(%rbp), %xmm3
	divsd	-88(%rbp), %xmm3
	movsd	%xmm3, -88(%rbp)
	movsd	-176(%rbp), %xmm15
	divsd	-96(%rbp), %xmm15
	movsd	%xmm15, -96(%rbp)
	movsd	-184(%rbp), %xmm14
	divsd	%xmm10, %xmm14
	movsd	%xmm14, -120(%rbp)
.L781:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L782
	cvttsd2sil	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-264(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-272(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-280(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-288(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE263:
	.size	double_div_14, .-double_div_14
	.globl	double_div_15
	.type	double_div_15, @function
double_div_15:
.LFB264:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$296, %rsp
	.cfi_offset 3, -24
	movq	%rdi, -40(%rbp)
	movq	%rsi, -48(%rbp)
	movq	-40(%rbp), %rbx
	movq	-48(%rbp), %rax
	movq	%rax, -24(%rbp)
	movq	-24(%rbp), %rax
	movsd	80(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -224(%rbp)
	movq	-24(%rbp), %rax
	movl	12(%rax), %eax
	addl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -264(%rbp)
	movq	-24(%rbp), %rax
	movsd	88(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -232(%rbp)
	movq	-24(%rbp), %rax
	movl	16(%rax), %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -272(%rbp)
	movq	-24(%rbp), %rax
	movsd	96(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -240(%rbp)
	movq	-24(%rbp), %rax
	movl	20(%rax), %eax
	subl	$1, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -280(%rbp)
	movq	-24(%rbp), %rax
	movsd	104(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -248(%rbp)
	movq	-24(%rbp), %rax
	movl	24(%rax), %eax
	subl	$2, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -288(%rbp)
	movq	-24(%rbp), %rax
	movsd	112(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -256(%rbp)
	movq	-24(%rbp), %rax
	movl	28(%rax), %eax
	subl	$3, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -296(%rbp)
	movq	-24(%rbp), %rax
	movsd	120(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -136(%rbp)
	movq	-24(%rbp), %rax
	movl	32(%rax), %eax
	subl	$4, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -304(%rbp)
	movq	-24(%rbp), %rax
	movsd	128(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -112(%rbp)
	movq	-24(%rbp), %rax
	movl	36(%rax), %eax
	subl	$5, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -216(%rbp)
	movq	-24(%rbp), %rax
	movsd	136(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -120(%rbp)
	movq	-24(%rbp), %rax
	movl	40(%rax), %eax
	subl	$6, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -208(%rbp)
	movq	-24(%rbp), %rax
	movsd	144(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -56(%rbp)
	movq	-24(%rbp), %rax
	movl	44(%rax), %eax
	subl	$7, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -144(%rbp)
	movq	-24(%rbp), %rax
	movsd	152(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -64(%rbp)
	movq	-24(%rbp), %rax
	movl	48(%rax), %eax
	subl	$8, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -152(%rbp)
	movq	-24(%rbp), %rax
	movsd	160(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -72(%rbp)
	movq	-24(%rbp), %rax
	movl	52(%rax), %eax
	subl	$9, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -160(%rbp)
	movq	-24(%rbp), %rax
	movsd	168(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -80(%rbp)
	movq	-24(%rbp), %rax
	movl	56(%rax), %eax
	subl	$10, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -168(%rbp)
	movq	-24(%rbp), %rax
	movsd	176(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -88(%rbp)
	movq	-24(%rbp), %rax
	movl	60(%rax), %eax
	subl	$11, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -176(%rbp)
	movq	-24(%rbp), %rax
	movsd	184(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -96(%rbp)
	movq	-24(%rbp), %rax
	movl	64(%rax), %eax
	subl	$12, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -184(%rbp)
	movq	-24(%rbp), %rax
	movsd	192(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -104(%rbp)
	movq	-24(%rbp), %rax
	movl	68(%rax), %eax
	subl	$13, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -192(%rbp)
	movq	-24(%rbp), %rax
	movsd	200(%rax), %xmm1
	movsd	.LC11(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -128(%rbp)
	movq	-24(%rbp), %rax
	movl	72(%rax), %eax
	subl	$14, %eax
	pxor	%xmm1, %xmm1
	cvtsi2sdl	%eax, %xmm1
	movsd	.LC12(%rip), %xmm0
	mulsd	%xmm0, %xmm1
	movsd	%xmm1, -200(%rbp)
	jmp	.L784
.L785:
	movsd	-264(%rbp), %xmm2
	movapd	%xmm2, %xmm6
	divsd	-224(%rbp), %xmm6
	movsd	-272(%rbp), %xmm3
	movapd	%xmm3, %xmm4
	divsd	-232(%rbp), %xmm4
	movsd	-280(%rbp), %xmm0
	movapd	%xmm0, %xmm7
	divsd	-240(%rbp), %xmm7
	movsd	-288(%rbp), %xmm10
	movapd	%xmm10, %xmm13
	divsd	-248(%rbp), %xmm13
	movsd	-296(%rbp), %xmm8
	movapd	%xmm8, %xmm9
	divsd	-256(%rbp), %xmm9
	movsd	-304(%rbp), %xmm11
	movapd	%xmm11, %xmm14
	divsd	-136(%rbp), %xmm14
	movsd	-216(%rbp), %xmm5
	divsd	-112(%rbp), %xmm5
	movsd	%xmm5, -112(%rbp)
	movsd	-208(%rbp), %xmm1
	divsd	-120(%rbp), %xmm1
	movsd	%xmm1, -120(%rbp)
	movsd	-144(%rbp), %xmm15
	divsd	-56(%rbp), %xmm15
	movsd	%xmm15, -56(%rbp)
	movsd	-152(%rbp), %xmm12
	divsd	-64(%rbp), %xmm12
	movsd	%xmm12, -64(%rbp)
	movsd	-160(%rbp), %xmm5
	divsd	-72(%rbp), %xmm5
	movsd	%xmm5, -72(%rbp)
	movsd	-168(%rbp), %xmm15
	divsd	-80(%rbp), %xmm15
	movsd	%xmm15, -80(%rbp)
	movsd	-176(%rbp), %xmm12
	divsd	-88(%rbp), %xmm12
	movsd	%xmm12, -88(%rbp)
	movsd	-184(%rbp), %xmm1
	divsd	-96(%rbp), %xmm1
	movsd	%xmm1, -96(%rbp)
	movsd	-192(%rbp), %xmm15
	divsd	-104(%rbp), %xmm15
	movsd	%xmm15, -104(%rbp)
	movsd	-200(%rbp), %xmm1
	divsd	-128(%rbp), %xmm1
	movsd	%xmm1, -128(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm3, %xmm6
	movapd	%xmm6, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm1
	divsd	%xmm7, %xmm1
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm15
	divsd	%xmm9, %xmm15
	movapd	%xmm11, %xmm13
	movapd	%xmm13, %xmm7
	divsd	%xmm14, %xmm7
	movsd	%xmm7, -136(%rbp)
	movsd	-216(%rbp), %xmm4
	divsd	-112(%rbp), %xmm4
	movsd	%xmm4, -112(%rbp)
	movsd	-208(%rbp), %xmm14
	divsd	-120(%rbp), %xmm14
	movsd	%xmm14, -120(%rbp)
	movsd	-144(%rbp), %xmm11
	divsd	-56(%rbp), %xmm11
	movsd	%xmm11, -56(%rbp)
	movsd	-152(%rbp), %xmm9
	divsd	-64(%rbp), %xmm9
	movsd	%xmm9, -64(%rbp)
	movsd	-160(%rbp), %xmm7
	divsd	-72(%rbp), %xmm7
	movsd	%xmm7, -72(%rbp)
	movsd	-168(%rbp), %xmm9
	divsd	-80(%rbp), %xmm9
	movsd	%xmm9, -80(%rbp)
	movsd	-176(%rbp), %xmm11
	divsd	-88(%rbp), %xmm11
	movsd	%xmm11, -88(%rbp)
	movsd	-184(%rbp), %xmm14
	divsd	-96(%rbp), %xmm14
	movsd	%xmm14, -96(%rbp)
	movsd	-192(%rbp), %xmm4
	divsd	-104(%rbp), %xmm4
	movsd	%xmm4, -104(%rbp)
	movsd	-200(%rbp), %xmm7
	divsd	-128(%rbp), %xmm7
	movsd	%xmm7, -128(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	movapd	%xmm5, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm0, %xmm9
	divsd	%xmm1, %xmm9
	movapd	%xmm10, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm12
	divsd	-136(%rbp), %xmm12
	movsd	%xmm12, -136(%rbp)
	movsd	-216(%rbp), %xmm3
	divsd	-112(%rbp), %xmm3
	movsd	%xmm3, -112(%rbp)
	movsd	-208(%rbp), %xmm6
	divsd	-120(%rbp), %xmm6
	movsd	%xmm6, -120(%rbp)
	movsd	-144(%rbp), %xmm1
	divsd	-56(%rbp), %xmm1
	movsd	%xmm1, -56(%rbp)
	movsd	-152(%rbp), %xmm15
	divsd	-64(%rbp), %xmm15
	movsd	%xmm15, -64(%rbp)
	movsd	-160(%rbp), %xmm12
	divsd	-72(%rbp), %xmm12
	movsd	%xmm12, -72(%rbp)
	movsd	-168(%rbp), %xmm6
	divsd	-80(%rbp), %xmm6
	movsd	%xmm6, -80(%rbp)
	movsd	-176(%rbp), %xmm1
	divsd	-88(%rbp), %xmm1
	movsd	%xmm1, -88(%rbp)
	movsd	-184(%rbp), %xmm3
	divsd	-96(%rbp), %xmm3
	movsd	%xmm3, -96(%rbp)
	movsd	-192(%rbp), %xmm15
	divsd	-104(%rbp), %xmm15
	movsd	%xmm15, -104(%rbp)
	movsd	-200(%rbp), %xmm12
	divsd	-128(%rbp), %xmm12
	movsd	%xmm12, -128(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm5, %xmm4
	movapd	%xmm4, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm0, %xmm1
	divsd	%xmm9, %xmm1
	movapd	%xmm10, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm8, %xmm9
	movapd	%xmm9, %xmm12
	divsd	%xmm14, %xmm12
	movapd	%xmm13, %xmm5
	divsd	-136(%rbp), %xmm5
	movsd	%xmm5, -136(%rbp)
	movsd	-216(%rbp), %xmm7
	divsd	-112(%rbp), %xmm7
	movsd	%xmm7, -112(%rbp)
	movsd	-208(%rbp), %xmm8
	divsd	-120(%rbp), %xmm8
	movsd	%xmm8, -120(%rbp)
	movsd	-144(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-152(%rbp), %xmm11
	divsd	-64(%rbp), %xmm11
	movsd	%xmm11, -64(%rbp)
	movsd	-160(%rbp), %xmm5
	divsd	-72(%rbp), %xmm5
	movsd	%xmm5, -72(%rbp)
	movsd	-168(%rbp), %xmm7
	divsd	-80(%rbp), %xmm7
	movsd	%xmm7, -80(%rbp)
	movsd	-176(%rbp), %xmm8
	divsd	-88(%rbp), %xmm8
	movsd	%xmm8, -88(%rbp)
	movsd	-184(%rbp), %xmm11
	divsd	-96(%rbp), %xmm11
	movsd	%xmm11, -96(%rbp)
	movsd	-192(%rbp), %xmm14
	divsd	-104(%rbp), %xmm14
	movsd	%xmm14, -104(%rbp)
	movsd	-200(%rbp), %xmm5
	divsd	-128(%rbp), %xmm5
	movsd	%xmm5, -128(%rbp)
	movapd	%xmm2, %xmm7
	divsd	%xmm3, %xmm7
	movapd	%xmm4, %xmm3
	movapd	%xmm3, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm0, %xmm8
	divsd	%xmm1, %xmm8
	movapd	%xmm10, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm13, %xmm12
	movapd	%xmm12, %xmm6
	divsd	-136(%rbp), %xmm6
	movsd	%xmm6, -136(%rbp)
	movsd	-216(%rbp), %xmm4
	divsd	-112(%rbp), %xmm4
	movsd	%xmm4, -112(%rbp)
	movsd	-208(%rbp), %xmm1
	divsd	-120(%rbp), %xmm1
	movsd	%xmm1, -120(%rbp)
	movsd	-144(%rbp), %xmm13
	divsd	-56(%rbp), %xmm13
	movsd	%xmm13, -56(%rbp)
	movsd	-152(%rbp), %xmm15
	divsd	-64(%rbp), %xmm15
	movsd	%xmm15, -64(%rbp)
	movsd	-160(%rbp), %xmm6
	divsd	-72(%rbp), %xmm6
	movsd	%xmm6, -72(%rbp)
	movsd	-168(%rbp), %xmm1
	divsd	-80(%rbp), %xmm1
	movsd	%xmm1, -80(%rbp)
	movsd	-176(%rbp), %xmm13
	divsd	-88(%rbp), %xmm13
	movsd	%xmm13, -88(%rbp)
	movsd	-184(%rbp), %xmm4
	divsd	-96(%rbp), %xmm4
	movsd	%xmm4, -96(%rbp)
	movsd	-192(%rbp), %xmm15
	divsd	-104(%rbp), %xmm15
	movsd	%xmm15, -104(%rbp)
	movsd	-200(%rbp), %xmm6
	divsd	-128(%rbp), %xmm6
	movsd	%xmm6, -128(%rbp)
	movapd	%xmm2, %xmm6
	divsd	%xmm7, %xmm6
	movapd	%xmm3, %xmm7
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm0, %xmm1
	divsd	%xmm8, %xmm1
	movapd	%xmm10, %xmm13
	divsd	%xmm14, %xmm13
	movapd	%xmm9, %xmm8
	movapd	%xmm8, %xmm15
	divsd	%xmm11, %xmm15
	movapd	%xmm12, %xmm9
	movapd	%xmm9, %xmm11
	divsd	-136(%rbp), %xmm11
	movsd	-216(%rbp), %xmm3
	divsd	-112(%rbp), %xmm3
	movsd	%xmm3, -112(%rbp)
	movsd	-208(%rbp), %xmm5
	divsd	-120(%rbp), %xmm5
	movsd	%xmm5, -120(%rbp)
	movsd	-144(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-152(%rbp), %xmm12
	divsd	-64(%rbp), %xmm12
	movsd	%xmm12, -64(%rbp)
	movsd	-160(%rbp), %xmm3
	divsd	-72(%rbp), %xmm3
	movsd	%xmm3, -72(%rbp)
	movsd	-168(%rbp), %xmm5
	divsd	-80(%rbp), %xmm5
	movsd	%xmm5, -80(%rbp)
	movsd	-176(%rbp), %xmm12
	divsd	-88(%rbp), %xmm12
	movsd	%xmm12, -88(%rbp)
	movsd	-184(%rbp), %xmm14
	divsd	-96(%rbp), %xmm14
	movsd	%xmm14, -96(%rbp)
	movsd	-192(%rbp), %xmm5
	divsd	-104(%rbp), %xmm5
	movsd	%xmm5, -104(%rbp)
	movsd	-200(%rbp), %xmm3
	divsd	-128(%rbp), %xmm3
	movsd	%xmm3, -128(%rbp)
	movapd	%xmm2, %xmm5
	divsd	%xmm6, %xmm5
	movapd	%xmm7, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm0, %xmm6
	movapd	%xmm6, %xmm0
	divsd	%xmm1, %xmm0
	movapd	%xmm10, %xmm12
	divsd	%xmm13, %xmm12
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm9, %xmm13
	movapd	%xmm13, %xmm9
	divsd	%xmm11, %xmm9
	movsd	-216(%rbp), %xmm4
	divsd	-112(%rbp), %xmm4
	movsd	%xmm4, -112(%rbp)
	movsd	-208(%rbp), %xmm15
	divsd	-120(%rbp), %xmm15
	movsd	%xmm15, -120(%rbp)
	movsd	-144(%rbp), %xmm1
	divsd	-56(%rbp), %xmm1
	movsd	%xmm1, -56(%rbp)
	movsd	-152(%rbp), %xmm11
	divsd	-64(%rbp), %xmm11
	movsd	%xmm11, -64(%rbp)
	movsd	-160(%rbp), %xmm4
	divsd	-72(%rbp), %xmm4
	movsd	%xmm4, -72(%rbp)
	movsd	-168(%rbp), %xmm11
	divsd	-80(%rbp), %xmm11
	movsd	%xmm11, -80(%rbp)
	movsd	-176(%rbp), %xmm1
	divsd	-88(%rbp), %xmm1
	movsd	%xmm1, -88(%rbp)
	movsd	-184(%rbp), %xmm15
	divsd	-96(%rbp), %xmm15
	movsd	%xmm15, -96(%rbp)
	movsd	-192(%rbp), %xmm1
	divsd	-104(%rbp), %xmm1
	movsd	%xmm1, -104(%rbp)
	movsd	-200(%rbp), %xmm11
	divsd	-128(%rbp), %xmm11
	movsd	%xmm11, -128(%rbp)
	movapd	%xmm2, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm7, %xmm5
	divsd	%xmm3, %xmm5
	movapd	%xmm6, %xmm1
	divsd	%xmm0, %xmm1
	movapd	%xmm10, %xmm0
	movapd	%xmm0, %xmm11
	divsd	%xmm12, %xmm11
	movapd	%xmm8, %xmm15
	divsd	%xmm14, %xmm15
	movapd	%xmm13, %xmm3
	divsd	%xmm9, %xmm3
	movapd	%xmm3, %xmm9
	movsd	-216(%rbp), %xmm12
	movapd	%xmm12, %xmm3
	divsd	-112(%rbp), %xmm3
	movsd	%xmm3, -112(%rbp)
	movsd	-208(%rbp), %xmm10
	divsd	-120(%rbp), %xmm10
	movsd	%xmm10, -120(%rbp)
	movsd	-144(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-152(%rbp), %xmm3
	divsd	-64(%rbp), %xmm3
	movsd	%xmm3, -64(%rbp)
	movsd	-160(%rbp), %xmm10
	divsd	-72(%rbp), %xmm10
	movsd	%xmm10, -72(%rbp)
	movsd	-168(%rbp), %xmm14
	divsd	-80(%rbp), %xmm14
	movsd	%xmm14, -80(%rbp)
	movsd	-176(%rbp), %xmm3
	divsd	-88(%rbp), %xmm3
	movsd	%xmm3, -88(%rbp)
	movsd	-184(%rbp), %xmm10
	divsd	-96(%rbp), %xmm10
	movsd	%xmm10, -96(%rbp)
	movsd	-192(%rbp), %xmm14
	divsd	-104(%rbp), %xmm14
	movsd	%xmm14, -104(%rbp)
	movsd	-200(%rbp), %xmm3
	divsd	-128(%rbp), %xmm3
	movsd	%xmm3, -128(%rbp)
	movapd	%xmm2, %xmm3
	divsd	%xmm4, %xmm3
	movapd	%xmm7, %xmm4
	divsd	%xmm5, %xmm4
	movapd	%xmm6, %xmm5
	divsd	%xmm1, %xmm5
	movapd	%xmm0, %xmm1
	divsd	%xmm11, %xmm1
	movapd	%xmm8, %xmm14
	divsd	%xmm15, %xmm14
	movapd	%xmm13, %xmm10
	divsd	%xmm9, %xmm10
	movsd	%xmm10, -136(%rbp)
	movapd	%xmm12, %xmm15
	movapd	%xmm15, %xmm11
	divsd	-112(%rbp), %xmm11
	movsd	-208(%rbp), %xmm12
	movapd	%xmm12, %xmm9
	divsd	-120(%rbp), %xmm9
	movsd	-144(%rbp), %xmm10
	divsd	-56(%rbp), %xmm10
	movsd	%xmm10, -56(%rbp)
	movsd	-152(%rbp), %xmm10
	divsd	-64(%rbp), %xmm10
	movsd	%xmm10, -64(%rbp)
	movsd	-160(%rbp), %xmm10
	divsd	-72(%rbp), %xmm10
	movsd	%xmm10, -72(%rbp)
	movsd	-168(%rbp), %xmm10
	divsd	-80(%rbp), %xmm10
	movsd	%xmm10, -80(%rbp)
	movsd	-176(%rbp), %xmm10
	divsd	-88(%rbp), %xmm10
	movsd	%xmm10, -88(%rbp)
	movsd	-184(%rbp), %xmm10
	divsd	-96(%rbp), %xmm10
	movsd	%xmm10, -96(%rbp)
	movsd	-192(%rbp), %xmm10
	divsd	-104(%rbp), %xmm10
	movsd	%xmm10, -104(%rbp)
	movsd	-200(%rbp), %xmm10
	divsd	-128(%rbp), %xmm10
	divsd	%xmm3, %xmm2
	movsd	%xmm2, -224(%rbp)
	divsd	%xmm4, %xmm7
	movsd	%xmm7, -232(%rbp)
	divsd	%xmm5, %xmm6
	movsd	%xmm6, -240(%rbp)
	divsd	%xmm1, %xmm0
	movsd	%xmm0, -248(%rbp)
	divsd	%xmm14, %xmm8
	movsd	%xmm8, -256(%rbp)
	divsd	-136(%rbp), %xmm13
	movsd	%xmm13, -136(%rbp)
	divsd	%xmm11, %xmm15
	movsd	%xmm15, -112(%rbp)
	divsd	%xmm9, %xmm12
	movsd	%xmm12, -120(%rbp)
	movsd	-144(%rbp), %xmm14
	divsd	-56(%rbp), %xmm14
	movsd	%xmm14, -56(%rbp)
	movsd	-152(%rbp), %xmm3
	divsd	-64(%rbp), %xmm3
	movsd	%xmm3, -64(%rbp)
	movsd	-160(%rbp), %xmm4
	divsd	-72(%rbp), %xmm4
	movsd	%xmm4, -72(%rbp)
	movsd	-168(%rbp), %xmm14
	divsd	-80(%rbp), %xmm14
	movsd	%xmm14, -80(%rbp)
	movsd	-176(%rbp), %xmm3
	divsd	-88(%rbp), %xmm3
	movsd	%xmm3, -88(%rbp)
	movsd	-184(%rbp), %xmm15
	divsd	-96(%rbp), %xmm15
	movsd	%xmm15, -96(%rbp)
	movsd	-192(%rbp), %xmm14
	divsd	-104(%rbp), %xmm14
	movsd	%xmm14, -104(%rbp)
	movsd	-200(%rbp), %xmm3
	divsd	%xmm10, %xmm3
	movsd	%xmm3, -128(%rbp)
.L784:
	movq	%rbx, %rax
	leaq	-1(%rax), %rbx
	testq	%rax, %rax
	jne	.L785
	cvttsd2sil	-224(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-264(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-232(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-272(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-240(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-280(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-248(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-288(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-256(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-296(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-136(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-304(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-112(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-216(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-120(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-208(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-56(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-144(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-64(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-152(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-72(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-160(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-80(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-168(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-88(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-176(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-96(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-184(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-104(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-192(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-128(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	cvttsd2sil	-200(%rbp), %eax
	movl	%eax, %edi
	call	use_int@PLT
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE264:
	.size	double_div_15, .-double_div_15
	.globl	double_div_benchmarks
	.section	.data.rel.local
	.align 32
	.type	double_div_benchmarks, @object
	.size	double_div_benchmarks, 128
double_div_benchmarks:
	.quad	double_div_0
	.quad	double_div_1
	.quad	double_div_2
	.quad	double_div_3
	.quad	double_div_4
	.quad	double_div_5
	.quad	double_div_6
	.quad	double_div_7
	.quad	double_div_8
	.quad	double_div_9
	.quad	double_div_10
	.quad	double_div_11
	.quad	double_div_12
	.quad	double_div_13
	.quad	double_div_14
	.quad	double_div_15
	.text
	.globl	initialize
	.type	initialize, @function
initialize:
.LFB265:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	.cfi_offset 3, -24
	movq	%rdi, -32(%rbp)
	movq	%rsi, -40(%rbp)
	movq	-40(%rbp), %rax
	movq	%rax, -16(%rbp)
	cmpq	$0, -32(%rbp)
	jne	.L791
	movl	$0, %ebx
	jmp	.L789
.L790:
	leal	1(%rbx), %ecx
	movq	-16(%rbp), %rax
	movslq	%ebx, %rdx
	movl	%ecx, 12(%rax,%rdx,4)
	movq	-16(%rbp), %rax
	movslq	%ebx, %rdx
	addq	$10, %rdx
	movsd	.LC1(%rip), %xmm0
	movsd	%xmm0, (%rax,%rdx,8)
	addl	$1, %ebx
.L789:
	cmpl	$15, %ebx
	jle	.L790
	jmp	.L786
.L791:
	nop
.L786:
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE265:
	.size	initialize, .-initialize
	.section	.rodata
	.align 8
.LC13:
	.string	"[-W <warmup>] [-N <repetitions>]\n"
.LC14:
	.string	"W:N:"
	.align 8
.LC16:
	.string	"integer bit parallelism: %.2f\n"
	.align 8
.LC17:
	.string	"integer add parallelism: %.2f\n"
	.align 8
.LC18:
	.string	"integer mul parallelism: %.2f\n"
	.align 8
.LC19:
	.string	"integer div parallelism: %.2f\n"
	.align 8
.LC20:
	.string	"integer mod parallelism: %.2f\n"
.LC21:
	.string	"int64 bit parallelism: %.2f\n"
.LC22:
	.string	"int64 add parallelism: %.2f\n"
.LC23:
	.string	"int64 mul parallelism: %.2f\n"
.LC24:
	.string	"int64 div parallelism: %.2f\n"
.LC25:
	.string	"int64 mod parallelism: %.2f\n"
.LC26:
	.string	"float add parallelism: %.2f\n"
.LC27:
	.string	"float mul parallelism: %.2f\n"
.LC28:
	.string	"float div parallelism: %.2f\n"
.LC29:
	.string	"double add parallelism: %.2f\n"
.LC30:
	.string	"double mul parallelism: %.2f\n"
.LC31:
	.string	"double div parallelism: %.2f\n"
	.text
	.globl	main
	.type	main, @function
main:
.LFB266:
	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	subq	$272, %rsp
	movl	%edi, -260(%rbp)
	movq	%rsi, -272(%rbp)
	movq	%fs:40, %rax
	movq	%rax, -8(%rbp)
	xorl	%eax, %eax
	movl	$0, -252(%rbp)
	movl	$0, %edi
	call	get_enough@PLT
	cmpl	$999999, %eax
	jle	.L793
	movl	$1, %eax
	jmp	.L794
.L793:
	movl	$11, %eax
.L794:
	movl	%eax, -248(%rbp)
	leaq	.LC13(%rip), %rax
	movq	%rax, -240(%rbp)
	movl	$1, -224(%rbp)
	movl	$1000, -220(%rbp)
	movl	$-1023, -216(%rbp)
	jmp	.L795
.L798:
	cmpl	$78, -244(%rbp)
	je	.L796
	cmpl	$87, -244(%rbp)
	jne	.L797
	movq	myoptarg(%rip), %rax
	movq	%rax, %rdi
	call	atoi@PLT
	movl	%eax, -252(%rbp)
	jmp	.L795
.L796:
	movq	myoptarg(%rip), %rax
	movq	%rax, %rdi
	call	atoi@PLT
	movl	%eax, -248(%rbp)
	jmp	.L795
.L797:
	movq	-240(%rbp), %rdx
	movq	-272(%rbp), %rcx
	movl	-260(%rbp), %eax
	movq	%rcx, %rsi
	movl	%eax, %edi
	call	lmbench_usage@PLT
	nop
.L795:
	movq	-272(%rbp), %rcx
	movl	-260(%rbp), %eax
	leaq	.LC14(%rip), %rdx
	movq	%rcx, %rsi
	movl	%eax, %edi
	call	mygetopt@PLT
	movl	%eax, -244(%rbp)
	cmpl	$-1, -244(%rbp)
	jne	.L798
	leaq	-224(%rbp), %rcx
	movl	-248(%rbp), %edx
	movl	-252(%rbp), %eax
	movl	%eax, %esi
	leaq	integer_bit_benchmarks(%rip), %rax
	movq	%rax, %rdi
	call	max_parallelism
	movq	%xmm0, %rax
	movq	%rax, -232(%rbp)
	movsd	-232(%rbp), %xmm0
	pxor	%xmm1, %xmm1
	comisd	%xmm1, %xmm0
	jbe	.L799
	movq	stderr(%rip), %rax
	movq	-232(%rbp), %rdx
	movq	%rdx, %xmm0
	leaq	.LC16(%rip), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$1, %eax
	call	fprintf@PLT
.L799:
	leaq	-224(%rbp), %rcx
	movl	-248(%rbp), %edx
	movl	-252(%rbp), %eax
	movl	%eax, %esi
	leaq	integer_add_benchmarks(%rip), %rax
	movq	%rax, %rdi
	call	max_parallelism
	movq	%xmm0, %rax
	movq	%rax, -232(%rbp)
	movsd	-232(%rbp), %xmm0
	pxor	%xmm1, %xmm1
	comisd	%xmm1, %xmm0
	jbe	.L801
	movq	stderr(%rip), %rax
	movq	-232(%rbp), %rdx
	movq	%rdx, %xmm0
	leaq	.LC17(%rip), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$1, %eax
	call	fprintf@PLT
.L801:
	leaq	-224(%rbp), %rcx
	movl	-248(%rbp), %edx
	movl	-252(%rbp), %eax
	movl	%eax, %esi
	leaq	integer_mul_benchmarks(%rip), %rax
	movq	%rax, %rdi
	call	max_parallelism
	movq	%xmm0, %rax
	movq	%rax, -232(%rbp)
	movsd	-232(%rbp), %xmm0
	pxor	%xmm1, %xmm1
	comisd	%xmm1, %xmm0
	jbe	.L803
	movq	stderr(%rip), %rax
	movq	-232(%rbp), %rdx
	movq	%rdx, %xmm0
	leaq	.LC18(%rip), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$1, %eax
	call	fprintf@PLT
.L803:
	leaq	-224(%rbp), %rcx
	movl	-248(%rbp), %edx
	movl	-252(%rbp), %eax
	movl	%eax, %esi
	leaq	integer_div_benchmarks(%rip), %rax
	movq	%rax, %rdi
	call	max_parallelism
	movq	%xmm0, %rax
	movq	%rax, -232(%rbp)
	movsd	-232(%rbp), %xmm0
	pxor	%xmm1, %xmm1
	comisd	%xmm1, %xmm0
	jbe	.L805
	movq	stderr(%rip), %rax
	movq	-232(%rbp), %rdx
	movq	%rdx, %xmm0
	leaq	.LC19(%rip), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$1, %eax
	call	fprintf@PLT
.L805:
	leaq	-224(%rbp), %rcx
	movl	-248(%rbp), %edx
	movl	-252(%rbp), %eax
	movl	%eax, %esi
	leaq	integer_mod_benchmarks(%rip), %rax
	movq	%rax, %rdi
	call	max_parallelism
	movq	%xmm0, %rax
	movq	%rax, -232(%rbp)
	movsd	-232(%rbp), %xmm0
	pxor	%xmm1, %xmm1
	comisd	%xmm1, %xmm0
	jbe	.L807
	movq	stderr(%rip), %rax
	movq	-232(%rbp), %rdx
	movq	%rdx, %xmm0
	leaq	.LC20(%rip), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$1, %eax
	call	fprintf@PLT
.L807:
	leaq	-224(%rbp), %rcx
	movl	-248(%rbp), %edx
	movl	-252(%rbp), %eax
	movl	%eax, %esi
	leaq	int64_bit_benchmarks(%rip), %rax
	movq	%rax, %rdi
	call	max_parallelism
	movq	%xmm0, %rax
	movq	%rax, -232(%rbp)
	movsd	-232(%rbp), %xmm0
	pxor	%xmm1, %xmm1
	comisd	%xmm1, %xmm0
	jbe	.L809
	movq	stderr(%rip), %rax
	movq	-232(%rbp), %rdx
	movq	%rdx, %xmm0
	leaq	.LC21(%rip), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$1, %eax
	call	fprintf@PLT
.L809:
	leaq	-224(%rbp), %rcx
	movl	-248(%rbp), %edx
	movl	-252(%rbp), %eax
	movl	%eax, %esi
	leaq	int64_add_benchmarks(%rip), %rax
	movq	%rax, %rdi
	call	max_parallelism
	movq	%xmm0, %rax
	movq	%rax, -232(%rbp)
	movsd	-232(%rbp), %xmm0
	pxor	%xmm1, %xmm1
	comisd	%xmm1, %xmm0
	jbe	.L811
	movq	stderr(%rip), %rax
	movq	-232(%rbp), %rdx
	movq	%rdx, %xmm0
	leaq	.LC22(%rip), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$1, %eax
	call	fprintf@PLT
.L811:
	leaq	-224(%rbp), %rcx
	movl	-248(%rbp), %edx
	movl	-252(%rbp), %eax
	movl	%eax, %esi
	leaq	int64_mul_benchmarks(%rip), %rax
	movq	%rax, %rdi
	call	max_parallelism
	movq	%xmm0, %rax
	movq	%rax, -232(%rbp)
	movsd	-232(%rbp), %xmm0
	pxor	%xmm1, %xmm1
	comisd	%xmm1, %xmm0
	jbe	.L813
	movq	stderr(%rip), %rax
	movq	-232(%rbp), %rdx
	movq	%rdx, %xmm0
	leaq	.LC23(%rip), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$1, %eax
	call	fprintf@PLT
.L813:
	leaq	-224(%rbp), %rcx
	movl	-248(%rbp), %edx
	movl	-252(%rbp), %eax
	movl	%eax, %esi
	leaq	int64_div_benchmarks(%rip), %rax
	movq	%rax, %rdi
	call	max_parallelism
	movq	%xmm0, %rax
	movq	%rax, -232(%rbp)
	movsd	-232(%rbp), %xmm0
	pxor	%xmm1, %xmm1
	comisd	%xmm1, %xmm0
	jbe	.L815
	movq	stderr(%rip), %rax
	movq	-232(%rbp), %rdx
	movq	%rdx, %xmm0
	leaq	.LC24(%rip), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$1, %eax
	call	fprintf@PLT
.L815:
	leaq	-224(%rbp), %rcx
	movl	-248(%rbp), %edx
	movl	-252(%rbp), %eax
	movl	%eax, %esi
	leaq	int64_mod_benchmarks(%rip), %rax
	movq	%rax, %rdi
	call	max_parallelism
	movq	%xmm0, %rax
	movq	%rax, -232(%rbp)
	movsd	-232(%rbp), %xmm0
	pxor	%xmm1, %xmm1
	comisd	%xmm1, %xmm0
	jbe	.L817
	movq	stderr(%rip), %rax
	movq	-232(%rbp), %rdx
	movq	%rdx, %xmm0
	leaq	.LC25(%rip), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$1, %eax
	call	fprintf@PLT
.L817:
	leaq	-224(%rbp), %rcx
	movl	-248(%rbp), %edx
	movl	-252(%rbp), %eax
	movl	%eax, %esi
	leaq	float_add_benchmarks(%rip), %rax
	movq	%rax, %rdi
	call	max_parallelism
	movq	%xmm0, %rax
	movq	%rax, -232(%rbp)
	movsd	-232(%rbp), %xmm0
	pxor	%xmm1, %xmm1
	comisd	%xmm1, %xmm0
	jbe	.L819
	movq	stderr(%rip), %rax
	movq	-232(%rbp), %rdx
	movq	%rdx, %xmm0
	leaq	.LC26(%rip), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$1, %eax
	call	fprintf@PLT
.L819:
	leaq	-224(%rbp), %rcx
	movl	-248(%rbp), %edx
	movl	-252(%rbp), %eax
	movl	%eax, %esi
	leaq	float_mul_benchmarks(%rip), %rax
	movq	%rax, %rdi
	call	max_parallelism
	movq	%xmm0, %rax
	movq	%rax, -232(%rbp)
	movsd	-232(%rbp), %xmm0
	pxor	%xmm1, %xmm1
	comisd	%xmm1, %xmm0
	jbe	.L821
	movq	stderr(%rip), %rax
	movq	-232(%rbp), %rdx
	movq	%rdx, %xmm0
	leaq	.LC27(%rip), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$1, %eax
	call	fprintf@PLT
.L821:
	leaq	-224(%rbp), %rcx
	movl	-248(%rbp), %edx
	movl	-252(%rbp), %eax
	movl	%eax, %esi
	leaq	float_div_benchmarks(%rip), %rax
	movq	%rax, %rdi
	call	max_parallelism
	movq	%xmm0, %rax
	movq	%rax, -232(%rbp)
	movsd	-232(%rbp), %xmm0
	pxor	%xmm1, %xmm1
	comisd	%xmm1, %xmm0
	jbe	.L823
	movq	stderr(%rip), %rax
	movq	-232(%rbp), %rdx
	movq	%rdx, %xmm0
	leaq	.LC28(%rip), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$1, %eax
	call	fprintf@PLT
.L823:
	leaq	-224(%rbp), %rcx
	movl	-248(%rbp), %edx
	movl	-252(%rbp), %eax
	movl	%eax, %esi
	leaq	double_add_benchmarks(%rip), %rax
	movq	%rax, %rdi
	call	max_parallelism
	movq	%xmm0, %rax
	movq	%rax, -232(%rbp)
	movsd	-232(%rbp), %xmm0
	pxor	%xmm1, %xmm1
	comisd	%xmm1, %xmm0
	jbe	.L825
	movq	stderr(%rip), %rax
	movq	-232(%rbp), %rdx
	movq	%rdx, %xmm0
	leaq	.LC29(%rip), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$1, %eax
	call	fprintf@PLT
.L825:
	leaq	-224(%rbp), %rcx
	movl	-248(%rbp), %edx
	movl	-252(%rbp), %eax
	movl	%eax, %esi
	leaq	double_mul_benchmarks(%rip), %rax
	movq	%rax, %rdi
	call	max_parallelism
	movq	%xmm0, %rax
	movq	%rax, -232(%rbp)
	movsd	-232(%rbp), %xmm0
	pxor	%xmm1, %xmm1
	comisd	%xmm1, %xmm0
	jbe	.L827
	movq	stderr(%rip), %rax
	movq	-232(%rbp), %rdx
	movq	%rdx, %xmm0
	leaq	.LC30(%rip), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$1, %eax
	call	fprintf@PLT
.L827:
	leaq	-224(%rbp), %rcx
	movl	-248(%rbp), %edx
	movl	-252(%rbp), %eax
	movl	%eax, %esi
	leaq	double_div_benchmarks(%rip), %rax
	movq	%rax, %rdi
	call	max_parallelism
	movq	%xmm0, %rax
	movq	%rax, -232(%rbp)
	movsd	-232(%rbp), %xmm0
	pxor	%xmm1, %xmm1
	comisd	%xmm1, %xmm0
	jbe	.L829
	movq	stderr(%rip), %rax
	movq	-232(%rbp), %rdx
	movq	%rdx, %xmm0
	leaq	.LC31(%rip), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$1, %eax
	call	fprintf@PLT
.L829:
	movl	$0, %eax
	movq	-8(%rbp), %rdx
	subq	%fs:40, %rdx
	je	.L832
	call	__stack_chk_fail@PLT
.L832:
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE266:
	.size	main, .-main
	.section	.rodata
	.align 8
.LC1:
	.long	0
	.long	1072693248
	.align 8
.LC2:
	.long	0
	.long	-1074790400
	.align 4
.LC3:
	.long	1149222912
	.align 4
.LC4:
	.long	1090519040
	.align 8
.LC5:
	.long	0
	.long	1069547520
	.align 8
.LC6:
	.long	0
	.long	1083129856
	.align 4
.LC7:
	.long	1068827891
	.align 4
.LC8:
	.long	1078530011
	.align 8
.LC9:
	.long	0
	.long	1083176960
	.align 8
.LC10:
	.long	0
	.long	1075838976
	.align 8
.LC11:
	.long	1708926943
	.long	1073127582
	.align 8
.LC12:
	.long	1405670641
	.long	1074340347
	.ident	"GCC: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0"
	.section	.note.GNU-stack,"",@progbits
	.section	.note.gnu.property,"a"
	.align 8
	.long	1f - 0f
	.long	4f - 1f
	.long	5
0:
	.string	"GNU"
1:
	.align 8
	.long	0xc0000002
	.long	3f - 2f
2:
	.long	0x3
3:
	.align 8
4:
